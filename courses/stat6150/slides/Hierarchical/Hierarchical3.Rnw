\documentclass[handout, aspectratio=169]{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Bayesian variable selection}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}
\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE>>=
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
library(arm)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle}

\section{Bayesian regression}
\begin{frame}
\frametitle{Bayesian regression}

Consider the model 
\[ y = X\beta + \epsilon \]
with 
\[ \epsilon \sim N(0,\sigma^2 \I) \]
where 
\begin{itemize}
\item $y$ is a vector of length $n$
\item $\beta$ is an unknown vector of length $p$
\item $X$ is a known $n\times p$ design matrix 
\item $\sigma^2$ is an unknown scalar
\end{itemize}

\vspace{0.1in} \pause

For a given design matrix $X$, we are interested in the posterior 
\[ p(\beta,\sigma^2|y), \]
\pause
but we may also be interested in which columns of $X$ should be included, i.e. what explanatory variables should we keep in the model.

\end{frame}




\subsection{Default Bayesian inference}
\frame{\frametitle{Default Bayesian regression}
	Assume the standard noninformative prior
	\[ p(\beta,\sigma^2) \propto 1/\sigma^2 \]
	\pause then the posterior is 
	\[ \begin{array}{rl}
	p(\beta,\sigma^2|y) &= p(\beta|\sigma^2,y) p(\sigma^2|y) \pause \\
	\beta|\sigma^2,y &\sim N(\hat{\beta}_{MLE}, \sigma^2 V_\beta) \pause \\
	\sigma^2|y &\sim IG\left(\frac{n-p}{2}, \frac{[n-p]s^2}{2}\right) \pause \\
	\beta|y &\sim t_{n-p}(\hat{\beta}_{MLE}, s^2V_{\beta}) \pause \\
	\\
  V_\beta &= (X^\top X)^{-1} \pause \\
	\hat{\beta}_{MLE} &= V_\beta X^\top y \pause \\
	s^2 &= \frac{1}{n-p}(y-X\hat{\beta}_{MLE})^\top(y-X\hat{\beta}_{MLE})
	\end{array} \]
	\pause The posterior is proper if $n>p$ and rank$(X)=p$. 
}




\subsection{Cricket chirps}
\begin{frame}
\frametitle{Information about chirps per 15 seconds}

Let
\begin{itemize}
\item $Y_i$ is the average number of chirps per 15 seconds and 
\item $X_i$ is the temperature in Fahrenheit.
\end{itemize}

\vspace{0.1in} \pause

And we assume 
\[ Y_i \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_i, \sigma^2) \]
then 
\begin{itemize}
\item $\beta_0$ is the expected number of chirps at 0 degrees Fahrenheit
\item $\beta_1$ is the expected increase in number of chirps (per 15 seconds) for each degree increase in Fahrenheit.
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Cricket chirps}
	As an example, consider the relationship between the number of cricket chirps (in 15 seconds) and temperature (in Fahrenheit). From example in {\tt LearnBayes::blinreg}.
<<chirps_data, fig.width=10>>=
chirps=c(20,16.0,19.8,18.4,17.1,15.5,14.7,17.1,15.4,16.2,15,17.2,16,17,14.1)
temp=c(88.6,71.6,93.3,84.3,80.6,75.2,69.7,82,69.4,83.3,78.6,82.6,80.6,83.5,76.3)
qplot(temp,chirps)  + 
  theme_bw()
@
\end{frame}




\begin{frame}[fragile]
\frametitle{Default Bayesian regression}

<<lm, echo=TRUE>>=
summary(m <- lm(chirps~temp))
confint(m) # Credible intervals
@
\end{frame}


\subsection{Subjective Bayesian inference}
\frame{\frametitle{Fully conjugate subjective Bayesian inference}
	If we assume the following normal-inverse-gamma prior,
	\[ \beta|\sigma^2 \sim N(b_0, \sigma^2 B_0) \qquad \sigma^2 \sim IG(a,b) \]
	\pause then the posterior is 
	\[ \beta|\sigma^2,y \sim N(b_n, \sigma^2 B_n) \qquad \sigma^2|y \sim IG(a',b') \]
	\pause with
	\[ \begin{array}{rl}
	B_n^{-1} &= B_0^{-1} + \frac{1}{\sigma^2}X^\top X \\
	b_n &= B_n^{-1}\left[ B_0^{-1}b_0 + \frac{1}{\sigma^2}X^\top y \right] \\
	a' &= a + \frac{n}{2} \\
	b' &= b + \frac{1}{2}(y-X b_0)^\top (XB_0X^\top+\I)^{-1}(y-X b_0) 
	\end{array} \]
}


\begin{frame}
\frametitle{Information about chirps per 15 seconds}

Let
\begin{itemize}
\item $Y_i$ is the average number of chirps per 15 seconds and 
\item $X_i$ is the temperature in Fahrenheit.
\end{itemize}

\vspace{0.1in} \pause

And we assume 
\[ Y_i \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_i, \sigma^2) \]
then 
\begin{itemize}
\item $\beta_0$ is the expected number of chirps at 0 degrees Fahrenheit
\item $\beta_1$ is the expected increase in number of chirps (per 15 seconds) for each degree increase in Fahrenheit.
\end{itemize}
\pause
Perhaps a reasonable prior is $p(\beta_0,\beta_1,\sigma^2) \propto N(\beta_0;0,10^2) N(\beta_1;0,1^2) \frac{1}{\sigma^2}$.

\end{frame}



\begin{frame}[fragile]
\frametitle{Subjective Bayesian regression}
<<subjective, echo=TRUE, warning=FALSE>>=
m = arm::bayesglm(chirps~temp, 
                  prior.mean.for.intercept  = 0,   # E[\beta_0]
                  prior.scale.for.intercept = 10,  # SD[\beta_0]
                  prior.df.for.intercept    = Inf, # normal prior for \beta_0
                  prior.mean  = 0,                 # E[\beta_1]
                  prior.scale = 1,                 # SD[\beta_1]
                  prior.df    = Inf,               # normal prior
                  scaled = FALSE)                  # scale prior?
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Subjective Bayesian regression}
<<subjective2, echo=TRUE, warning=FALSE>>=
summary(m)
@
\end{frame}






\begin{frame}[fragile]
\frametitle{Subjective vs Default}
<<t-distribution, message=FALSE, echo=TRUE>>=
tmp = lm(chirps~temp) # default analysis
tmp$coefficients
confint(tmp)

m$coefficients # subjective analysis
confint(m)
@
\end{frame}




\begin{frame}[fragile]
\frametitle{Subjective vs Default}
<<t-distribution_fit, message=FALSE, fig.width=10>>=
qplot(temp, chirps) +
  geom_smooth(method='lm',formula=y~x, se=FALSE, color='red', linetype=2, size=2) + 
  geom_abline(intercept=m$coefficients[1], slope=m$coefficients[2], color='blue', linetype=3, size=2) + 
  theme_bw()
@
\end{frame}





\begin{frame}[fragile]
\frametitle{Shrinkage (as $V[\beta_1]$ gets smaller)}
<<shrinkage, fig.width=10>>=
d = ddply(data.frame(V=10^seq(-2,2,by=0.2)), .(V), function(x) {
  m = bayesglm(chirps~temp, prior.mean=0, prior.scale=x$V, prior.df=Inf)
  data.frame(beta0=m$coefficients[1], beta1=m$coefficients[2])
})
tmp = melt(d, id="V", value.name="estimate")
ggplot(tmp, aes(V, estimate)) +
  geom_line() +
  scale_x_log10() + 
  facet_wrap(~variable, scales='free') + 
  theme_bw()
@
\end{frame}




\begin{frame}[fragile]
\frametitle{Shrinkage (as $V[\beta_1]$ gets smaller)}
<<shrinkage2, fig.width=10>>=
ggplot(data.frame(chirps=chirps, temperature=temp), aes(temp, chirps)) + 
  geom_point() + 
  geom_abline(data=d, aes(intercept=beta0, slope=beta1, color=V)) + 
  scale_colour_gradient(trans = "log10") + 
  theme_bw()
@
\end{frame}




\section{Zellner's g-prior}
\begin{frame}
\frametitle{Zellner's g-prior}

Let 
\[ y = X\beta + \epsilon, \quad \epsilon \sim N(\sigma^2 \I). \]
If we choose the conjugate prior $\beta \sim N(b_0, \sigma^2 B_0)$, we still need to choose $b_0$ and $B_0$. \pause It seems natural to set $b_0=0$ which will shrink the estimates for $\beta$ toward zero, i.e. toward no effect. \pause But how should we choose $B_0$?

\vspace{0.1in} \pause 

One option is \alert{Zellner's $g$-prior} where $B_0 = g[X^\top X]^{-1}$ \pause where $g$ is either set or learned.

\end{frame}



\begin{frame}
\frametitle{Zellner's g-prior posterior}

Suppose 
\[ y \sim N(X\beta,\sigma^2\I) \]
where $X$ is $n\times p$ \pause 
and you use Zellner's g-prior
\[ \beta \sim N(b_0, g\sigma^2 (X'X)^{-1}) \]
and assume $p(\sigma^2) \propto 1/\sigma^2$. 

\vspace{0.1in} \pause

The posterior is then 
\[ 
\beta|\sigma^2,y \sim N\left(\frac{1}{1+g}b_0 + \frac{g}{1+g}\hat{\beta}_{MLE}, \frac{\sigma^2 g}{g+1}(X'X)^{-1} \right) 
 \]

\end{frame}



\begin{frame}
\frametitle{Setting $g$}

In Zellner's g-prior, 
\[ \beta \sim N(b_0, g\sigma^2 (X'X)^{-1}), p(\sigma^2)\propto 1/\sigma^2 \]
we need to determine how to set g.

\vspace{0.1in} \pause

Here are some thoughts:
\begin{itemize}
\item $g\to 0$ makes posterior equal to the prior, \pause
\item $g=1$ puts equal weight to prior and likelihood, \pause
\item $g=n$ means prior has the equivalent weight of 1 observation, \pause
\item $g\to \infty$ recovers a uniform prior,
\item empirical Bayes estimate of $g$, $\hat{g}_{EB} =\mbox{argmax}_g p(y|g)$, or 
\item put a prior on $g$ and perform a fully Bayesian analysis.
\end{itemize}
\end{frame}





\subsection{Marginal likelihood}
\begin{frame}
\frametitle{Marginal likelihood}

The marginal likelihood under Zellner's $g$-prior is 
\[ \begin{array}{rl}
p(y|g) = \frac{\mG\left(\frac{n-1}{2}\right)}{\pi^{\frac{n-1}{2}}n^{1/2}} ||y-\overline{y}||^{-(n-1)} \frac{(1+g)^{\frac{n-p-1}{2}}}{(1+g[1-R^2])^{\frac{n-1}{2}}}
\end{array} \]
where $R^2$ is the coefficient of determination.

\vspace{0.1in} \pause

We use the marginal likelihood as evidence in favor of the model, i.e. when comparing models those with higher marginal likelihoods should be preferred over the rest.
\end{frame}





\begin{frame}
\frametitle{Why the marginal likelihood?}

By Bayes' rule, we have 
\[ p(\theta|y,M) = p(y|\theta,M)p(\theta|M)/p(y|M) \]
\pause
Rearranging yields
\[ 
p(y|M) = p(y|\theta,M)p(\theta|M)/p(\theta|y,M)
\]
\pause
Taking logarithms yields
\[ 
\log p(y|M) = \log(y|\theta,M) + \log p(\theta|M) - \log p(\theta|y,M) 
\]
\pause
To compare with other model selection criterion, 
multiply by -2 and plug in $\theta = \hat\theta_{MLE}$:
\[
-2 \log p(y|M) = -2 \log(y|\hat\theta_{MLE},M) + 
2 \left[ \log p(\hat\theta_{MLE}|y,M) - \log p(\hat\theta_{MLE}|M) \right]
\]
where the penalty is the logarithm of the ratio of the posterior to 
the prior evaluated at the MLE.

\end{frame}







\subsection{Model selection}
\begin{frame}
\frametitle{Model selection}

If $\beta$ is a vector of length $p$, let $\gamma$ be a vector with binary elements that indicate whether that component of $\beta$ is non-zero, i.e. that explanatory variable is included. \pause For example, 
\[ \gamma = (1,0,1,1,0,0,0,1) \]
indicates that $\beta$ is of length 8 and that the first, third, fourth, and eighth elements are non-zero. \pause
Then we have $X_\gamma$ which indicates the design matrix that only has columns corresponding to those columns in $\gamma$ that are non-zero \pause and $\beta_\gamma$ is the subset of $\beta$ including elements of $\beta$ where $\gamma$ is 1. \pause

\vspace{0.1in} \pause

Now, we have $2^p$ models $M_\gamma$ of the form
\[ y = X_\gamma \beta_\gamma + \epsilon \]
where $\epsilon \sim N(0,\sigma^2\I)$. \pause Two special cases are 
\[ \begin{array}{rl}
\gamma_{null} &= (0,\ldots,0) \\
\gamma_{full} &= (1,\ldots,1)
\end{array} \]
\end{frame}



\begin{frame}
\frametitle{Model selection (cont.)}

If we want to compare $M_\gamma$ to $M_{null}$ using a common $g$, we can use the Bayes Factor 
\[ 
BF(M_\gamma:M_{null}) = \frac{p(y|M_\gamma,g)}{p(y|M_{null},g)} = \frac{(1+g)^{\frac{n-p_{\gamma}-1}{2}}}{(1+g[1-R_\gamma^2])^{\frac{n-1}{2}}}
\] 

Then, for any two models with a common $g$, we can compare these models using 
{\small
\[
BF(M_\gamma:M_{\gamma'}) 
= \frac{p(y|M_\gamma,g)}{p(y|M_{\gamma'},g)} \pause
= \frac{p(y|M_\gamma,g)/p(y|M_{null},g)}{p(y|M_{\gamma'},g)/p(y|M_{null},g)} \pause
= \frac{BF(M_\gamma:M_{null}) }{BF(M_{\gamma'}:M_{null})}  
\]
}
\pause
If the base model is the null model, then the common parameters amongst the models are $\sigma^2$ and possibly an intercept $\alpha$. \pause We can place an improper prior on these parameters, typically $p(\alpha,\sigma^2)\propto 1/\sigma^2$, and not affect the Bayes Factors.
\end{frame}



\begin{frame}[fragile]
\frametitle{Zellner's g-prior in R}
<<echo=TRUE, warning=FALSE, message=FALSE>>=
library(BMS)
m0 = zlm(chirps~1   , g='UIP') # g=n
m1 = zlm(chirps~scale(temp), g='UIP') # g=n
(bf = exp(m1$marg.lik-m0$marg.lik))
summary(m1)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Zellner's g-prior in R}
<<echo=TRUE, warning=FALSE, message=FALSE>>=
library(bayess)
m = BayesReg(chirps, temp, g=length(chirps)) # explanatory variables are scaled
@
\end{frame}



\begin{frame}
\frametitle{Limiting Bayes Factors}

If the base model is the null model, then 
\[ 
BF(M_\gamma:M_{null}) = \frac{(1+g)^{(n-p_\gamma-1)/2}}{(1+g[1-R_\gamma^2])^{(n-1)/2}}
\]
where $p_\gamma$ is the number of non-zero elements in $\gamma$, i.e. the number of explanatory variables included in the model.

\vspace{0.1in} \pause 

\begin{itemize}
\item As $g\to \infty$, $BF(M_\gamma:M_{null}) \to 0$. \pause (Lindley's Paradox) \pause
\item As $n\to\infty$, $BF(M_\gamma:M_{null})\to \infty$. \pause
\item As $R_\gamma^2 \to 1$, $BF(M_\gamma:M_{null}) \to (1+g)^{(n-p_\gamma-1)/2}$. \pause (information paradox)
\end{itemize}

\pause

If $M^*$ is the true model, we would like 
\[ 
BF(M^*:M_\gamma) \stackrel{a.s.}{\longrightarrow} \infty, 
\quad \mbox{as } n\to\infty
\]
for any other model $M_\gamma$. \pause This is called \alert{model selection consistency}.

\end{frame}



\subsection{Empirical Bayes}
\begin{frame}
\frametitle{Empirical Bayes}

The empirical Bayes approach chooses $g$ such that it maximizes $p(y|M_\gamma,g)$. \pause It turns out that $g_\gamma^{EB} = \max(F_\gamma-1,0)$, where 
\[ 
F_\gamma = \frac{R_\gamma^2/p_\gamma}{(1-R_\gamma^2)/(n-p_\gamma-1)}.
\]
\pause
Plugging this back into the expression for the Bayes Factor, we find that 
\[ 
BF^{EB}(M_\gamma:M_{null}) \to \infty
\]
\pause
as $R_\gamma \to 1$ and thus the empirical Bayes approach does not suffer from either paradox. \pause This empirical Bayes approach is model selection consistent if the true model is not the null model, but is inconsistent if it is.
\end{frame}



\subsection{Fully Bayesian}
\begin{frame}
\frametitle{Fully Bayesian}

Alternatively, we can perform a fully Bayes analysis by putting a prior on $g$. \pause The \alert{Zellner-Siow} prior is 
\[ g \sim IG\left( \frac{1}{2}, \frac{n}{2} \right) \]

\vspace{0.1in} \pause

For this prior, we have $BF^{EB}(M_\gamma:M_{null}) \to \infty$ as $R_\gamma^2 \to 1$ and thus do not suffer from any paradoxes \pause and we have model selection consistency, i.e. $BF(M^*:M_\gamma) \stackrel{a.s.}{\longrightarrow} \infty$ for true model $M^*$ compared to any other model $M_\gamma$. 

\vspace{0.1in} \pause

There are other priors for $g$ that have these properties.

\end{frame}



\end{document}
