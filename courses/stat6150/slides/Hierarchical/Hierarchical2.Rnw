\documentclass[handout, aspectratio=169]{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Hierarchical models}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}
\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE>>=
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle}

%\section{Normal hierarchical model}
\begin{frame}
\frametitle{Normal hierarchical model}

Let 
\[ Y_{ig} \ind N(\theta_g, \sigma^2) \]
for $i=1,\ldots,n_g$, $g=1,\ldots,G$, and $\sum_{g=1}^G n_g = n$. \pause Now consider the following model assumptions:
\begin{itemize}[<+->]
\item $\theta_g\ind N(\mu,\tau^2)$
\item $\theta_g\ind La(\mu,\tau)$
\item $\theta_g\ind t_v(\mu,\tau^2)$
\item $\theta_g\ind \pi \delta_0 + (1- \pi) N(\mu,\tau^2)$
\item $\theta_g\ind \pi \delta_0 + (1- \pi) t_v(\mu,\tau^2)$
\end{itemize}

\vspace{0.2in} \pause

To perform a Bayesian analysis, we need a prior on $\mu$, $\tau^2$, and 
(in the case of the discrete mixture) $\pi$. 

\end{frame}


\section{Gibbs sampling}
\begin{frame}
\frametitle{Normal hierarchical model}

Consider the model 
\[ \begin{array}{rl}
Y_{ig} &\ind N(\theta_g, \sigma^2) \pause \\
\theta_g &\ind N(\mu,\tau^2) 
\end{array} \]
\pause 
where $i=1,\ldots,n_g$, $g=1,\ldots,G$, and $n=\sum_{g=1}^G n_g$
\pause
with prior distribution
\[ \begin{array}{rl}
p(\mu,\sigma^2,\tau^2)=p(\mu)p(\sigma^2)p(\tau^2) \propto \frac{1}{\sigma^2} Ca^+(\tau;0,C).
\end{array} \]

\vspace{0.1in} \pause

For background on why we are using these priors for the variances, 
see Gelman (2006) \url{https://projecteuclid.org/euclid.ba/1340371048}: 
``Prior distributions for variance parameters in hierarchical models (comment 
on article by Browne and Draper)''.

\end{frame}


\subsection{Multi-step}
\begin{frame}
\frametitle{Gibbs sampler for normal hierarchical model}

Here is a possible Gibbs sampler for this model:
\pause

\begin{itemize}
\item For $g=1,\ldots,G$, sample $\theta_g \sim p(\theta_g|\cdots)$. \pause 
\item Sample $\sigma^2 \sim p(\sigma^2|\cdots)$. \pause 
\item Sample $\mu \sim p(\mu|\cdots)$. \pause 
\item Sample $\tau^2 \sim p(\tau^2|\cdots)$. 
\end{itemize}

\vspace{0.1in} \pause 

How many steps exist in this Gibbs sampler? \pause G+3? 4?
\end{frame}

\subsection{2-Step}
\begin{frame}
\frametitle{2-Step Gibbs sampler for normal hierarchical model}

Here is a 2-step Gibbs sampler: 
\pause 
 
\begin{enumerate}
\item Sample $\theta = (\theta_1,\ldots,\theta_G) \sim p(\theta|\cdots)$. \pause 
\item Sample $\mu,\sigma^2,\tau^2 \sim p(\mu,\sigma^2,\tau^2|\cdots)$. 
\end{enumerate}

\pause 
There is stronger theoretical support for 2-step Gibbs sampler, 
thus, if we can, it is prudent to construct a 2-step Gibbs sampler.

\end{frame}


\subsection{Sampling $\theta$}
\begin{frame}
\frametitle{Sampling $\theta$}

The full conditional for $\theta$ is 
\[ \begin{array}{rl}
p(\theta|\cdots) 
&\propto p(\theta,\mu,\sigma^2,\tau^2|y) \pause \\
&\propto p(y|\theta,\sigma^2)p(\theta|\mu,\tau^2)p(\mu,\sigma^2,\tau^2) \pause \\
&\propto p(y|\theta,\sigma^2)p(\theta|\mu,\tau^2) \pause \\
&=\prod_{g=1}^G p(y_g|\theta_g,\sigma^2) p(\theta_g|\mu,\tau^2) 
\end{array} \]
where $y_g = (y_{1,g},\ldots,y_{n_g,g})$. 
\pause
We now know that the $\theta_g$ are conditionally independent of each other.
\end{frame}


\subsection{Sampling $\theta_g$}
\begin{frame}
\frametitle{Sampling $\theta_g$}
The full conditional for $\theta_g$ is \pause 
\[ \begin{array}{rl}
p(\theta_g|\cdots) 
&\propto p(y_g|\theta_g,\sigma^2) p(\theta_g|\mu,\tau^2) \pause \\
&= \prod_{i=1}^{n_g} N(y_{ig};\theta_g,\sigma^2) N(\theta_g;\mu,\tau^2) 
\end{array} \]
\pause
Notice that this does not include $\theta_{g'}$ for any $g'\ne g$. \pause
This is an alternative way to conclude that the $\theta_g$ are conditionally
independent of each other.

\vspace{0.1in} \pause

Thus
\[
\theta_g|\cdots \ind N(\mu_g, \tau_g^2)
\]
where
\[ \begin{array}{rl}
\tau_g^2 &= [\tau^{-2} + n_g \sigma^{-2}]^{-1} \\
\mu_g &= \tau_g^2[\mu\tau^{-2} + \overline{y}_g n_g \sigma^{-2}] \\
\overline{y}_g &= \frac{1}{n_g} \sum_{i=1}^{n_g} y_{ig}.
\end{array} \]

\end{frame}


\subsection{Sampling $\mu,\sigma^2,\tau^2$}
\begin{frame}
\frametitle{Sampling $\mu,\sigma^2,\tau^2$}

The full conditional for $\mu,\sigma^2,\tau^2$ is 
\[ \begin{array}{rl}
p(\mu,\sigma^2,\tau^2|\cdots) 
&\propto p(y|\theta,\sigma^2)p(\theta|\mu,\tau^2)p(\mu)p(\sigma^2)p(\tau^2) \pause \\
&= p(y|\theta,\sigma^2)p(\sigma^2) p(\theta|\mu,\tau^2)p(\mu)p(\tau^2)
\end{array} \]
\pause 
So we know that, conditional on $\theta$ and $y$,
$\sigma^2$ is independent of $\mu$ and $\tau^2$.
\end{frame}



\subsection{Sampling $\sigma^2$}
\begin{frame}
\frametitle{Sampling $\sigma^2$}

Recall that 
\[ 
y_{ig} \ind N(\theta_g,\sigma^2) 
\mbox{ and } 
p(\sigma^2) \propto 1/\sigma^2. 
\] 
\pause 
Thus, we are in the scenario of normal data with a known mean and unknown 
variance and the unknown variance has our default prior.
\pause
Thus, we should know the full conditional is 
$\sigma^2|\cdots \sim IG\left(\frac{n}{2}, \frac{1}{2} \sum_{g=1}^G \sum_{i=1}^{n_g} (y_{ig}-\theta_g)^2\right)$.

\vspace{0.1in} \pause 

To derive the full conditional, use 
\[ \begin{array}{rl}
p(\sigma^2|\cdots)
&\propto \prod_{g=1}^G \prod_{i=1}^{n_g} (\sigma^2)^{-1/2} \exp\left( -\frac{1}{2\sigma^2} (y_{ig}-\theta_g)^2 \right) \frac{1}{\sigma^2} \\
&= (\sigma^2)^{-n/2-1} \exp\left( \left. -\frac{1}{2} \sum_{g=1}^G \sum_{i=1}^{n_g} (y_{ig}-\theta_g)^2 \right/ \sigma^2 \right)
\end{array} \]
\pause
which is the kernel of a $IG\left(\frac{n}{2}, \frac{1}{2} \sum_{g=1}^G \sum_{i=1}^{n_g} (y_{ig}-\theta_g)^2\right)$.

\end{frame}


\section{Sampling $\mu,\tau^2$}
\begin{frame}
\frametitle{Sampling $\mu,\tau^2$}
\small

Recall that 
\[
\theta_g \ind N(\mu,\tau^2) 
\mbox{ and } 
p(\mu,\tau^2) \propto Ca^+(\tau;0,C).
\]

\pause
This is a non-standard distribution, but is extremely close a normal model
with unknown mean and variance with the standard non-informative prior 
$p(\mu,\tau^2) \propto 1/\tau^2$ or the conjugate normal-inverse-gamma
prior.

\vspace{0.1in} \pause

Here are some options for sampling from this distribution:
\begin{itemize}
\item random-walk Metropolis (in 2 dimensions),
\item independent Metropolis-Hastings using posterior from standard non-informative prior as the proposal, or
\item rejection sampling using posterior from standard non-informative prior as the proposal
\end{itemize}
The posterior under the standard non-informative prior is 
\[ 
\tau^2|\cdots \sim \mbox{Inv-}\chi^2(G-1,s_\theta^2) 
\mbox{ and } 
\mu|\tau^2,\ldots \sim N\left(\overline{\theta}, \tau^2/G\right)
\]
where $\overline{\theta} = \frac{1}{G} \sum_{g=1}^G \theta_g$ and 
$s_\theta^2 = \frac{1}{G-1}(\theta_g-\overline{\theta})^2$. 
\pause
What is the MH ratio?


\end{frame}


\subsection{Summary}
\begin{frame}
\frametitle{Markov chain Monte Carlo for normal hierarchical model}

\begin{enumerate}
\item Sample $\theta\sim p(\theta|\cdots)$:
  \begin{enumerate}
  \item For $g=1,\ldots,G$, sample $\theta_g \sim N(\mu_g,\tau_g^2)$.
  \end{enumerate}
\item Sample $\mu,\sigma^2,\tau^2$:
  \begin{enumerate}
  \item Sample $\sigma^2 \sim IG(n/2,SSE/2)$.
  \item Sample $\mu,\tau^2$ using independent Metropolis-Hastings using posterior from standard non-informative prior as the proposal.
  \end{enumerate}
\end{enumerate}

\vspace{0.1in} \pause

What happens if $\theta_g \ind La(\mu,\tau)$ or $\theta_g \ind t_v(\mu,\tau^2)$?

\end{frame}


\section{Scale mixtures of normals}
\begin{frame}
\frametitle{Scale mixtures of normals}

Recall that if 
\[ \theta|\phi \sim N(\phi,V) \mbox{ and } \phi \sim N(m,C) \]
\pause
then 
\[ \theta \pause\sim N(m,V+C).  \]
\pause
This is called a location mixture.

\vspace{0.2in} 

Now, if 
\[ \theta|\phi \sim N(m,C\phi) \]
\pause
and we assume a mixing distribution for $\phi$, \pause we have a scale mixture. 
\pause
Since the top level distributional assumption is normal, we refer to this as a \alert{scale mixture of normals}. 

\end{frame}



\subsection{$t$ distribution}
\begin{frame}
\frametitle{$t$ distribution}

Let 
\[ \theta|\phi \sim N(m,\phi C) \mbox{ and } \phi \sim IG(a,b) \]
\pause 
then 
\[ \begin{array}{rl}
p(\theta) &= \int p(\theta|\phi)p(\phi) d\phi \pause \\
&= (2\pi\sqrt{C})^{-1/2} \frac{b^a}{\mG(a)} \int \phi^{-1/2} e^{-(\theta-m)^2/2\phi C} \phi^{-(a+1)} e^{-b/\phi} d\phi \pause\\
&= (2\pi C)^{-1/2} \frac{b^a}{\mG(a)} \int \phi^{-(a+1/2+1)} e^{-[b+(\theta-m)^2/2C]/\phi} d\phi \pause\\
&= (2\pi C)^{-1/2} \frac{b^a}{\mG(a)} \frac{\mG(a+1/2)}{[b+(\theta-m)^2/2C]^{a+1/2}} \pause\\
&= \frac{\mG([2a+1]/2)}{\mG(2a/2)\sqrt{2a \pi bC/a}} \left[ 1+\frac{1}{2a}\frac{(\theta-m)^2}{bC/a} \right]^{-[2a+1]/2} 
\end{array} \]
\pause
Thus 
\[ \theta \sim t_{2a}(m,bC/a) \]
\pause
i.e. $\theta$ has a $t$ distribution with $2a$ degrees of freedom, location $m$,
scale $bC/a$, and variance $\frac{bC}{a-1}$. 
\pause 

\end{frame}



\begin{frame}
\frametitle{Hierarchical $t$ distribution}

Let $m=\mu$, $C=1$, $a=\nu/2$, and $b=\nu\tau^2/2$, \pause i.e. 

\[
\theta|\phi \sim N(\mu,\phi) \mbox{ and } \phi \sim IG(\nu/2,\nu\tau^2/2).
\]
\pause
 
Then, we have 

\[ 
\theta \sim t_\nu(\mu,\tau^2),
\]
i.e. a $t$ distribution with $\nu$ degrees of freedom, location $\mu$, and scale 
$\tau^2$. 

\vspace{0.2in} \pause

Notice that the parameterization has a redundancy between $C$ and $a/b$, 
i.e. we could have chosen $C=\tau^2$, $a=\nu/2$, and $b=\nu/2$ and we would have 
obtained the same marginal distribution for $\theta$.

\end{frame}


\begin{frame}
\frametitle{Laplace distribution}

Let 
\[ 
\theta|\phi \sim N(m,\phi C^2) \mbox{ and } \phi \sim Exp(1/2b^2) 
\]
where $E[\phi] = 2b^2$ and $Var[\phi] = 4b^4$.
\pause 
Then, 
by an extension of equation (4) in Park and Casella (2008), 
we have 
\[
p(\theta) = \frac{1}{2Cb}e^{-\frac{|\theta-m|}{Cb}}.
\]
\pause 
This is the pdf for a Laplace (double exponential) distribution with 
location $m$ and scale $Cb$ 
\pause
which we write
\[ 
\theta \sim La(m,Cb).
\]
\pause
\pause and say $\theta$ has a 
Laplace distribution with location $m$ and scale $Cb$ 
\pause
and $E[\theta] = m$ and $Var[\theta] = 2[Cb]^2 = 2C^2b^2$.

\end{frame}




\begin{frame}
\frametitle{Hierarchical Laplace distribution}

Let $m=\mu$, $C=1$, and $b=\tau$ \pause i.e. 

\[
\theta|\phi \sim N(\mu,\phi) \mbox{ and } \phi \sim Exp(1/2\tau^2).
\]
\pause
 
Then, we have 

\[ 
\theta \sim La(\mu,\tau),
\]
i.e. a Laplace distribution with location $\mu$ and scale $\tau$. 

\vspace{0.2in} \pause

Notice that the parameterization has a redundancy between $C$ and $b$, 
i.e. we could have chosen $C=\tau$ and $b=1$ and we would have 
obtained the same marginal distribution for $\theta$.
\end{frame}


\section{Normal hierarchical model}
\begin{frame}
\frametitle{Normal hierarchical model}

Recall our hierarchical model
\[ Y_{ig} \ind N(\theta_g, \sigma^2) \]
for $g=1,\ldots,G$ and $i=1,\ldots,n_g$. \pause Now consider the following model assumptions: \pause
\[ \begin{array}{l@{,\quad}l@{\implies}l}
\theta_g|\phi_g\ind N(\mu,\phi_g) & \phi_g=\tau^2 \pause & \theta_g \ind N(\mu,\tau^2) \pause \\
\theta_g|\phi_g\ind N(\mu,\phi_g) & \phi_g\ind Exp(1/2\tau^2) \pause & \theta_g \ind La(\mu,\tau) \pause \\
\theta_g|\phi_g\ind N(\mu,\phi_g) &\phi_g\ind IG(v/2,v\tau^2/2) \pause & \theta_g \ind t_v(\mu,\tau^2) \pause
\end{array} \]


\vspace{0.2in} \pause

For simplicity, let's assume $\sigma^2 \sim IG(a,b)$, $\mu \sim N(m,C)$, and $\tau \sim Ca^+(0,c)$ and that $\sigma^2$, $\mu$, and $\tau$ are \emph{a priori} independent.

\end{frame}


\section{MCMC}
\begin{frame}
\frametitle{Gibbs sampling}

The following Gibbs sampler will converge to the posterior $p(\theta,\sigma,\mu,\phi,\tau|y)$:

\begin{enumerate}
\item Independently, sample $\theta_g \sim p(\theta_g|\cdots)$.
\item Sample $\mu,\sigma,\tau \sim p(\mu,\sigma,\tau|\cdots)$:
  \begin{enumerate}
  \item Sample $\mu \sim p(\mu|\cdots)$.
  \item Sample $\sigma \sim p(\sigma|\cdots)$.
  \item Sample $\tau \sim p(\tau|\cdots)$.
  \end{enumerate}
\item Independently, sample $\phi_g \sim p(\phi_g|\cdots)$.
\end{enumerate}

\vspace{0.1in} \pause

Steps 1, 2a, and 2b will be the same for all models, but 
steps 2c and 3 will be different.
\end{frame}


\subsection{$\theta$}
\begin{frame}
\frametitle{Sample $\theta$}

\[ Y_{ig} \ind N(\theta_g,\sigma^2) \mbox{ and } \theta_g \sim N(\mu,\phi_g) \]

\vspace{0.1in} \pause

\[ \begin{array}{rl}
p(\theta|\cdots) 
&\propto \left[\prod_{g=1}^G \prod_{i=1}^{n_g} e^{-(y_{ig}-\theta_g)^2/2\sigma^2} \right] \left[ \prod_{g=1}^G e^{-(\theta_g-\mu)^2/2\phi_g} \right] \\
&\propto \prod_{g=1}^G \left[\prod_{i=1}^{n_g} e^{-(y_{ig}-\theta_g)^2/2\sigma^2} e^{-(\theta_g-\mu)^2/2\phi_g} \right] 
\end{array} \]
\pause
Thus $\theta_g$ are conditionally independent given everything else. \pause It should be obvious that 
\[ \theta_g|\cdots \sim \pause N\left(\mu_g', \phi_g' \right) \]
with 
\[
\phi_g' = \left[\frac{1}{\phi_g} + \frac{n_g}{\sigma^2} \right]^{-1} 
\quad \mbox{and} \quad
\mu_g' = \phi_g' \left[ \frac{1}{\phi_g}\mu + \frac{n_g}{\sigma^2}\overline{y}_g  \right]
\]
where $\overline{y}_g = \sum_{i=1}^{n_g} y_{ig} / n_g$.
\end{frame}



\subsection{$\mu$}
\begin{frame}
\frametitle{Sample $\mu$}

\[ \theta_g \ind N(\mu,\phi_g) \mbox{ and } \mu \sim N(m,C) \]

\vspace{0.2in} \pause

Immediately, we should know that 

\[ \mu|\cdots \sim \pause N(m',C') \]
with 
\[ \begin{array}{rl}
C' &= \left(\frac{1}{C} + \sum_{g=1}^G \frac{1}{\phi_g} \right)^{-1} \\
m' &= C'\left(\frac{1}{C}m + \sum_{g=1}^G \frac{1}{\phi_g}\theta_g \right)
\end{array} \]

\end{frame}




\subsection{$\sigma$}
\begin{frame}
\frametitle{Sample $\sigma^2$}

\[ Y_{ig} \ind N(\theta_g,\sigma^2) \mbox{ and } \sigma^2 \sim IG(a,b) \]

\vspace{0.2in} \pause

This is just a normal data model with an unknown variance that has the conjugate prior. \pause The only difficulty is that we have several groups here. \pause But very quickly you should be able to determine that 
\[ \sigma^2|\cdots \sim IG(a',b') \]
where 
\[ \begin{array}{rl}
a' &= a + \frac{1}{2}\sum_{g=1}^G n_g = a + \frac{n}{2}\\
b' &= b + \frac{1}{2}\sum_{g=1}^G \sum_{i=1}^{n_g} (y_{ig}-\theta_g)^2. 
\end{array} \]
\end{frame}

\subsection{Distributional assumption for $\theta_g$}
\begin{frame}
\frametitle{Distributional assumption for $\theta_g$}

\[ Y_{ig} \ind N(\theta_g, \sigma^2) \mbox{ and } \theta_g \ind N(\mu,\phi_g) \]

\[ \begin{array}{rl}
\phi_g &= \tau \\
\phi_g &\ind Exp(1/2\tau^2) \\
\phi_g &\ind IG(v/2,v\tau^2/2)
\end{array} \]

The steps that are left are 1) sample $\phi$ and 2) sample $\tau^2$,

\end{frame}


\subsection{$\phi$}
\begin{frame}
\frametitle{Sample $\phi$ for normal model}
For normal model, $\phi_g = \tau$, so we will address this when we sample $\tau$. 
\end{frame}



\begin{frame}
\frametitle{Sample $\phi$ for Laplace model}
\small

For Laplace model, 
\[ \theta_g \ind N(\mu,\phi_g) \mbox{ and } \phi_g \ind Exp(1/2\tau^2), \]
\pause 
so the full conditional is 
\[ 
p(\phi|\cdots) \propto \left[ \prod_{g=1}^G N(\theta_g; \mu, \phi_g) Exp(\phi_g;1/2\tau^2) \right].
\]
\pause
So the individual $\phi_g$ are conditionally independent \pause with
\[ 
p(\phi_g|\cdots) \propto N(\theta_g; \mu, \phi_g) Exp(\phi_g;1/2\tau^2) \propto \phi_g^{-1/2} e^{-(\theta_g-\mu)^2/2\phi_g} e^{-\phi_g/2\tau^2}
 \]
\pause
If we perform the transformation $\eta_g = 1/\phi_g$, \pause we have 
\[
p(\eta_g|\cdots) \propto \eta_g^{-3/2} e^{-\frac{(\theta_g-\mu)^2}{2}\eta_g - \frac{1}{2\tau^2\eta_g}}
\] 
\pause
which is the kernel of an inverse Gaussian distribution with mean $\sqrt{1/\tau^2(\theta_g-\mu)^2}$ and scale $1/\tau^2$ where the parameterization is such that the variance is $\mu^3/\lambda$ (different from the {\tt mgcv::rig} parameterization).

\end{frame}

 
 



\begin{frame}
\frametitle{Sample $\phi$ for $t$ model}
For the $t$ model, 
\[ \theta_g \ind N(\mu,\phi_g) \mbox{ and } \phi_g \ind IG(v/2,v\tau^2/2), \]
\pause
so we have 
\[ \phi_g|\cdots \ind IG\left(\frac{v+1}{2}, \frac{v\tau^2+(\theta_g-\mu)^2}{2}\right). \]
Since this is just $G$ independent normal data models with a known mean and independent conjugate inverse gamma priors on the variance.
\end{frame}




\subsection{$\tau$}
\begin{frame}
\frametitle{Sample $\tau$ for normal model}
\small
 
Let 
\[ \theta_g \ind N(\mu,\tau^2) \mbox{ and } \tau \sim Ca^+(0,c) \]
\pause
and $\eta = \tau^2$. 
\pause
The full conditional is 
\[ p(\eta|\cdots) \propto \eta^{-G/2} e^{-\sum_{g=1}^G (\theta_g-\mu)^2/2\eta} \left( 1+\eta/c^2\right)^{-1} \eta^{-1/2}. \]

\vspace{0.1in} \pause

Let's use Metropolis-Hastings with proposal distribution 
\[  \eta^* \sim IG\left(\frac{G-1}{2}, \sum_{g=1}^G \frac{(\theta_g-\mu)^2}{2}\right) \]
and acceptance probability $\min\{1,\rho\}$ where
\[ \rho = \frac{\left( 1+\eta^*/c^2\right)^{-1}}{\left( 1+\eta^{(i)}/c^2\right)^{-1}} = \frac{ 1+\eta^{(i)}/c^2}{1+\eta^*/c^2} \]
where $\eta^{(i)}$ and $\eta^*$ are the current and proposed value respective. 

\vspace{0.1in} \pause

Then we calculate $\tau = \sqrt{\eta}$.

\end{frame}



\begin{frame}
\frametitle{Sample $\tau$ for Laplace model}
Let 
\[ \phi_g \sim Exp(1/2\tau^2) \mbox{ and } \tau \sim Ca^+(0,c) \]
so the full conditional is 
\[ p(\eta|\cdots) \propto \eta^{-G} e^{-\sum_{g=1}^G \phi_g/2\eta} \left( 1+\eta/c^2\right)^{-1} \eta^{-1/2}. \]

\pause

Let's use Metropolis-Hastings with proposal distribution 
\[ \eta^* \sim IG\left(G-\frac{1}{2}, \sum_{g=1}^G \frac{\phi_g}{2}\right) \]
and acceptance probability $\min\{1,\rho\}$ where again
\[ \rho = \frac{ 1+\eta^{(i)}/c^2}{1+\eta^*/c^2}.\]

\end{frame}




\begin{frame}
\frametitle{Sample $\tau$ for $t$ model}
Let 
\[ \phi_g \sim IG(v/2, v\tau^2/2) \mbox{ and } \tau \sim Ca^+(0,c) \]
so the full conditional is 
\[ p(\eta|\cdots) \propto \eta^{G v/2} e^{-\frac{\eta}{2} \sum_{g=1}^G \frac{1}{\phi_g}} \left( 1+\eta/c^2\right)^{-1} \eta^{-1/2}. \]

\pause

Let's use Metropolis-Hastings with proposal distribution 
\[ \eta^* \sim Ga\left(\frac{G v + 1}{2}, \frac{1}{2}\sum_{g=1}^G \frac{1}{\phi_g}\right) \]
and acceptance probability $\min\{1,\rho\}$ where again
\[ \rho = \frac{ 1+\eta^{(i)}/c^2}{1+\eta^*/c^2} .\]

\pause

Then we calculate $\tau = \sqrt{\eta}$.

\end{frame}



\subsection{Point-mass distributions}
\begin{frame}
\frametitle{Dealing with point-mass distributions}
We would also like to consider models with 
\[ \theta_g \ind \pi \delta_0 + (1-\pi) N(\mu,\phi_g) \]
where $\phi_g = \tau^2$ corresponds to a normal and 
\[ \phi_g \ind IG(v/2,v\tau^2/2) \]
corresponds to a $t$ distribution for the non-zero $\theta_g$. 

\vspace{0.2in} \pause

Similar to the previous, the $\theta_g$ are conditionally independent. \pause
To sample $\theta_g$, we calculate 
\[ \begin{array}{rl}
\pi' &= \frac{\pi \prod_{i=1}^{n_g} N(y_{ig}; 0, \sigma^2)}{\pi \prod_{i=1}^{n_g} N(y_{ig}; 0, \sigma^2) + (1-\pi) \prod_{i=1}^{n_g} N(y_{ig}; \mu, \phi_g+\sigma^2) }. \\
\phi_g' &= \left( \frac{1}{\phi_g} + \frac{n_g}{\sigma^2} \right)^{-1} \\
\mu_g' &= \phi_g' \left( \frac{1}{\phi_g}\mu + \frac{n_g}{\sigma^2}\overline{y}_g \right)
\end{array} \]
\end{frame}


\begin{frame}
\frametitle{Dealing with point-mass distributions (cont.)}
Let 
\[ \theta_g \ind \pi \delta_0 + (1-\pi) f(\theta_g) \]
and $\pi \sim Beta(s,f)$.

\vspace{0.2in} \pause

The full conditional for $\pi$ is
\[ \pi|\cdots \sim Beta\left(s+\sum_{g=1}^G \I(\theta_g=0), f+\sum_{g=1}^G \I(\theta_g\ne 0)\right)\]
and $\mu$ and $\phi_g$ get updated using only those $\theta_g$ that are non-zero. 

\end{frame}



\begin{frame}
\frametitle{Dealing with point-mass distributions (cont.)}

Updating $\mu$, $\phi_g$, and $\tau$ will be very similar.
\pause
Specifically,
\begin{itemize}
\item \[ \mu|\cdots \sim \pause N(m',C') \]
with 
\[ \begin{array}{rl}
C' &= \left(\frac{1}{C} + \sum_{g:\theta_g\ne 0} \frac{1}{\phi_g} \right)^{-1} \\
m' &= C'\left(\frac{1}{C}m + \sum_{g:\theta_g\ne 0} \frac{1}{\phi_g}\theta_g \right),
\end{array} \]
\pause
\item $\phi_g$:
  \begin{itemize}
  \item if $\theta_g = 0$, then $\phi_g$ from the prior
  \item if $\theta_g \ne 0$, then $\phi_g$ from its conditional distribution
  \end{itemize}
\item $\tau$ will be the same as before

\end{itemize}


\end{frame}




\end{document}
