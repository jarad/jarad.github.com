\documentclass[handout]{beamer}

\usepackage{verbatim}

\graphicspath{{figures/}}

\usefonttheme[onlymath]{serif}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{State-space models}
\subtitle{Hidden Markov models}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}
\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, message=FALSE, warning=FALSE>>=
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle}


\section{General state space models}

\subsection{Structure, notation, and terminology}
\frame{\frametitle{Structure}
 \[ \begin{array}{rl@{\qquad}rl@{\qquad}l}
\multicolumn{2}{l}{\mbox{Observation equation:}} \\
 \uncover<2->{Y_t &= f_t(\theta_t,v_t)} & \uncover<4->{Y_t &\sim p_t(y_t|\theta_t,\dots)} \\ 
 \\
\multicolumn{2}{l}{\mbox{State transition (evolution) equation:}} \\
 \uncover<3->{\theta_t &= g_t(\theta_{t-1},w_t)} & \uncover<5->{\theta_t &\sim p_t(\theta_t| \theta_{t-1},\ldots)}
 \end{array} \]
 
 \vspace{0.1in}
 
 \uncover<6->{
 \begin{center}
 \includegraphics{stateSpaceModel}
 \end{center}
 }
}

\frame{\frametitle{Notation and terminology}
 \begin{tabular}{ll}
 Observation equation: & $Y_t=f_t(\theta_t,v_t)$ \\ \pause
 Observations: & $Y_t$ \\ \pause
 Observation (measurement) error: & $v_t$ \\ \pause
 \\
 State transition (evolution) equation: & $\theta_t = g_t(\theta_{t-1},w_t)$ \\ \pause
 Latent (unobserved) state: & $\theta_t$ \\ \pause
 Evolution noise & $w_t$ 
 \end{tabular}
}


\subsection{Examples}
\frame{\frametitle{Stochastic volatility}
\setkeys{Gin}{width=0.5\textwidth}

\begin{eqnarray*}
y_t &\sim& N(0,\sigma_t^2) \\
\log \sigma_t &\sim& N(\mu + \phi [\log \sigma_{t-1} - \mu],W) \pause
\end{eqnarray*} 

\begin{center}
\includegraphics{volatility1}
\end{center}
}

\frame{\frametitle{Stochastic volatility}
\setkeys{Gin}{width=0.5\textwidth}

\begin{eqnarray*}
y_t &\sim& N(0,\sigma_t^2) \\
\log \sigma_t &\sim& N(\mu + \phi (\log \sigma_{t-1} - \mu),W)
\end{eqnarray*}

\begin{center}
\includegraphics{volatility2}
\end{center}
}

\frame{\frametitle{Markov switching model}
\setkeys{Gin}{width=0.5\textwidth}

 \begin{eqnarray*}
 y_t &\sim& N(\theta_t,\sigma^2) \\
 \theta_t &\sim & p\delta_{\theta_{t-1}} +(1-p) \delta_{1-\theta_{t-1}} \\
 \theta_0 &=& 0 \pause
 \end{eqnarray*}

\begin{center}
\includegraphics{switching1}
\end{center}
}

\frame{\frametitle{Markov switching model}
\setkeys{Gin}{width=0.5\textwidth}

 \begin{eqnarray*}
 y_t &\sim& N(\theta_t,\sigma^2) \\
 \theta_t &\sim & p\delta_{\theta_{t-1}} +(1-p) \delta_{1-\theta_{t-1}} \\
 \theta_0 &=& 0 
 \end{eqnarray*}

\begin{center}
\includegraphics{switching2}
\end{center}
}

\subsection{Inferential goals}
\frame{\frametitle{Goals:}
\begin{itemize}
\item Filtering
\item Smoothing
\item Forecasting
\end{itemize}
}

\begin{frame}
\frametitle{What do we know?}
\pause
\begin{itemize} 
\item $p(y_t|\theta_t)$ for all $t$ \pause
\item $p(\theta_t|\theta_{t-1})$ for all $t$ \pause
\item $p(\theta_0)$
\end{itemize}

\vspace{0.1in} \pause

In principle, we could have subscripts for the distributions/densities, \pause
e.g.
\begin{itemize} 
\item $p_t(y_t|\theta_t)$ for all $t$ \pause
\item $p_t(\theta_t|\theta_{t-1})$ for all $t$ 
\end{itemize}
to indicate that the form of the distribution/density has changed. 
\pause
But, most in most models the form stays the same and only the state changes with
time.

\vspace{0.1in} \pause

For simplicity, we will assume a time-homogeneous process and therefore 
remove the subscript.
\end{frame}

\frame{\frametitle{Filtering}
{\tiny
 Goal: $ p(\theta_t|y_{1:t})$ where $y_{1:t} = (y_1,y_2,\ldots,y_t)$ \pause (filtered distribution) \pause
 
 \vspace{0.1in}

 Recursive procedure: \pause
 \begin{itemize}
 \item Assume $p(\theta_{t-1}|y_{1:t-1})$ \pause
 \item Prior for $\theta_t$ \pause
 \begin{eqnarray*} p(\theta_t|y_{1:t-1}) \pause &=& \int p(\theta_t,\theta_{t-1}|y_{1:t-1}) d\theta_{t-1} \\
 \pause &=& \int p(\theta_t|\theta_{t-1},y_{1:t-1})p(\theta_{t-1}|y_{1:t-1}) d\theta_{t-1} \\
 \pause &=& \int p(\theta_t|\theta_{t-1})p(\theta_{t-1}|y_{1:t-1}) d\theta_{t-1} \\
 \end{eqnarray*} \pause
 \item One-step ahead predictive distribution for $y_t$ \pause
 \begin{eqnarray*} p(y_t|y_{1:t-1}) \pause &=& \pause \int p(y_t,\theta_t|y_{1:t-1}) d\theta_t \\
 \pause &=& \int p(y_t|\theta_t,y_{1:t-1})p(\theta_t|y_{1:t-1})d\theta_t \\
 \pause &=& \int p(y_t|\theta_t)p(\theta_t|y_{1:t-1})d\theta_t \\
 \end{eqnarray*} \pause
 \item Filtered distribution for $\theta_t$ \pause
 \[ p(\theta_t|y_{1:t}) \pause = \frac{p(y_t|\theta_t,y_{1:t-1})p(\theta_t|y_{1:t-1})}{p(y_t|y_{1:t-1})} \pause = \frac{p(y_t|\theta_t)p(\theta_t|y_{1:t-1})}{p(y_t|y_{1:t-1})}\]
 \end{itemize}
 }
}

\frame{\frametitle{What do we know now?}
\begin{itemize}
\item $p(y_t|\theta_t)$ for all $t$ 
\item $p(\theta_t|\theta_{t-1})$ for all $t$ 
\item $p(\theta_0)$ \pause
\item $p(\theta_{t}|y_{1:t-1})$ for all $t$ \pause
\item $p(y_t|y_{1:t-1})$ for all $t$
\end{itemize}
}

\frame{\frametitle{Smoothing}
 Goal: $p(\theta_{t}|y_{1:T})$ for $t<T$
 
 \vspace{0.1in} \pause
{\tiny
 \begin{itemize}
 \item Backward transition probability $p(\theta_t|\theta_{t+1},y_{1:t})$ \pause
 \begin{eqnarray*} 
 p(\theta_t|\theta_{t+1},y_{1:T}) \pause &=& p(\theta_t|\theta_{t+1},y_{1:t}) \\ \pause 
 &=& \frac{p(\theta_{t+1}|\theta_t,y_{1:t})p(\theta_t|y_{1:t})}{p(\theta_{t+1}|y_{1:t})} \\ \pause 
 &=& \frac{p(\theta_{t+1}|\theta_t)p(\theta_t|y_{1:t})}{p(\theta_{t+1}|y_{1:t})} \pause
 \end{eqnarray*}
 
 \item Recursive smoothing distributions $p(\theta_t|y_{1:T})$ starting from $p(\theta_T|y_{1:T})$
 \begin{eqnarray*} 
 p(\theta_t|y_{1:T}) \pause &=& \int p(\theta_t,\theta_{t+1}|y_{1:T}) d\theta_{t+1} \\ \pause
 &=& \int p(\theta_{t+1}|y_{1:T})p(\theta_t|\theta_{t+1},y_{1:T})d\theta_{t+1} \\ \pause
   &=& \int p(\theta_{t+1}|y_{1:T}) \frac{p(\theta_{t+1}|\theta_t)p(\theta_t|y_{1:t})}{p(\theta_{t+1}|y_{1:t})} d\theta_{t+1} \\ \pause
   &=& p(\theta_t|y_{1:t}) \int \frac{p(\theta_{t+1}|\theta_t)}{p(\theta_{t+1}|y_{1:t})} p(\theta_{t+1}|y_{1:T}) d\theta_{t+1} 
   \end{eqnarray*}
 \end{itemize}
 }
}

\frame{\frametitle{Forecasting}
 Goal: $p(y_{t+k},\theta_{t+k}|y_{1:t})$

 {\footnotesize
 \vspace{0.1in} \pause

 \[ p(y_{t+k},\theta_{t+k}|y_{1:t}) \pause = p(y_{t+k}|\theta_{t+k}) p(\theta_{t+k}|y_{1:t}) \]

 \vspace{0.1in} \pause

Recursively, given $p(\theta_{t+(k-1)}|y_{1:t})$ \pause
 \begin{eqnarray*}
 p(\theta_{t+k}|y_{1:t}) \pause &=& \int p(\theta_{t+k},\theta_{t+(k-1)}|y_{1:t})\, d\theta_{t+(k-1)} \\ \pause
 &=& \int p(\theta_{t+k}|\theta_{t+(k-1)},y_{1:t})p(\theta_{t+(k-1)}|y_{1:t}) d\theta_{t+(k-1)} \\ \pause
 &=& \int p(\theta_{t+k}|\theta_{t+(k-1)})p(\theta_{t+(k-1)}|y_{1:t}) d\theta_{t+(k-1)}
 \end{eqnarray*}
 }
}

\frame{\frametitle{Filtering in a Markov switching model}
{\tiny
\vspace{-0.1in}
 \begin{eqnarray*}
 y_t &\sim& N(\theta_t,\sigma^2) \\
 \theta_t &\sim & p\delta_{\theta_{t-1}} +(1-p) \delta_{1-\theta_{t-1}} \\
 \theta_0 &=& 0 \pause
 \end{eqnarray*}
 
 \vspace{0.1in} 

 \begin{itemize}
 \item Note: $p(\theta_t=1) = 1-p(\theta_t=0)$ for all $t$ \pause
 \item Suppose $q=p(\theta_{t-1}=1|y_{1:t-1})$. \pause What is $p(\theta_{t}=1|y_{1:t-1})$? \pause
 \[ p(\theta_{t}=1|y_{1:t-1}) \pause = \sum_{k=0}^1 p(\theta_{t}=1|\theta_{t-1}=k)p(\theta_{t-1}=k|y_{1:t-1}) \pause = (1-p)(1-q)+pq \pause = p_1 \]
 \item What is $p(\theta_{t}=1|y_{1:t-1})$? \pause
 \[ p(\theta_{t}=0|y_{1:t-1}) \pause = \sum_{k=0}^1 p(\theta_{t}=0|\theta_{t-1}=k)p(\theta_{t-1}=k|y_{1:t-1}) \pause = p(1-q)+(1-p)q\pause = p_0 \]
 \item What is $p(y_t|y_{1:t-1})$? \pause
 \begin{eqnarray*} 
 p(y_t|y_{1:t-1}) \pause &=& \sum_{k=0}^1 p(y_t|\theta_t=k) p(\theta_t=k|y_{1:t-1})  \pause 
 = p_0 N(y_t;0,\sigma^2)+p_1 N(y_t;1,\sigma^2) \pause
 \end{eqnarray*}
 \item What is $p(\theta_t=1|y_{1:t})$? \pause
 \[ p(\theta_t=1|y_{1:t}) \pause = \frac{p(y_t|\theta_t=1)p(\theta_t=1|y_{1:t-1})}{p(y_t|y_{1:t-1})}= \frac{p_1 N(y_t;1,\sigma^2)}{p_0 N(y_t;0,\sigma^2)+p_1 N(y_t;1,\sigma^2)} \]
 \end{itemize}
}
}



\begin{frame}
\frametitle{Hidden Markov model}

\begin{definition}
A hidden Markov model (HMM) is a state-space model whose state is finite.
\end{definition}
{\tiny (Note: this is not a universal definition.)}

\vspace{0.1in} \pause

So let 
\begin{itemize}
\item $\pi_t^{t'}$ be the probability distribution for the state at time $t$
given information up to time $t'$,
\pause
e.g. $\pi_{t,i}^{t'} = P(\theta_t=i|y_{1:t'})$. \pause
\item $P$ be the transition probability matrix, 
\pause
e.g. $P_{ij}$ is the probability of moving from state $i$ to state $j$ in 
1 time step. \pause
\item $p(y_t|\theta_t)$ be the observation density or mass function.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Inference in a hidden Markov model}

\small

Assume $\pi_0^0$ is given.
\pause

\begin{itemize}
\item What is forecast distribution at time $t$ given only $\pi_0^0$,
\pause
i.e. $\pi_t^0$?
\pause
Recursively, we have 
\[ 
\pi_t^0 = \pi_{t-1}^0 P.
\]
\pause
Alternatively, we have 
\[
\pi^0_t = \pi_0 P^t \qquad P^t = P^{t-1}P \quad \mbox{and} \quad P^1 = P
\]
\pause
\item What is the filtered distribution at time $t$,
i.e. $\pi_{t,i}^{t}$?
\pause
Find this recursively via
\[
\pi_{t,i}^{t} \propto p(y_t|\theta_{t} = i) \pi_{t-1}^{t-1} \cdot P_{\cdot,i}
\]
% \pause
% \item What is the smoothed distribution at time $t$,
% \pause
% i.e. $\pi_{t,i}^{T}$ with $t<T$?
% \pause
% Find this recursively through
% \[
% \pi_{t,i}^T
% \]
\end{itemize}
\pause
Although smoothing can be useful, 
it is often of more use in Bayesian analyses to perform backward sampling.

\end{frame}


\begin{frame}
\frametitle{Joint posterior}

The joint distribution for $\theta = (\theta_0,\theta_1,\ldots,\theta_T)$ can be decomposed as
\[ \begin{array}{rl}
p(\theta|y) 
&= p(\theta_0,\theta_1,\ldots,\theta_T|y_{1:T}) \pause \\
&= p(\theta_T|y_{1:T}) \prod_{t=T}^1  p(\theta_{t-1}|\theta_t,y_{1:T}) \pause \\
&= p(\theta_T|y_{1:T}) \prod_{t=T}^1   p(\theta_{t-1}|\theta_t,y_{1:t-1}) \\
\end{array} \]
\pause
where 
\[ \begin{array}{rl} 
p(\theta_{t-1}|\theta_t,y_{1:t-1}) 
&= \frac{p(\theta_t|\theta_{t-1},y_{1:t-1})p(\theta_{t-1}|y_{1:t-1})}{p(\theta_t|y_{1:t-1})} \pause \\
&= \frac{p(\theta_t|\theta_{t-1})p(\theta_{t-1}|y_{1:t-1})}{p(\theta_t|y_{1:t-1})} \pause \\
&\propto p(\theta_t|\theta_{t-1})p(\theta_{t-1}|y_{1:t-1})
\end{array} \]
\end{frame}


\begin{frame}
\frametitle{Backward sampling}

The joint distribution for $\theta$ can be decomposed as
\[ 
p(\theta|y) 
= p(\theta_T|y_{1:T})\prod_{t=1}^T  p(\theta_{t-1}|\theta_t,y_{1:t-1})
\]
and 
\[
p(\theta_{t-1}|\theta_t,y_{1:t-1}) \propto p(\theta_t|\theta_{t-1})p(\theta_{t-1}|y_{1:t-1}).
\]
Suppose we have all the filtered distributions,
\pause 
i.e. $\pi_t^t$ for $t=0,\ldots,T$. 

\vspace{0.1in} \pause

An algorithm to obtain a joint sample for $\theta$ is 
\begin{enumerate}
\item Sample $\theta_T \sim p(\theta_T|y_{1:T})$ 
\pause 
which is a discrete distribution with 
$P(\theta_T = i|y_{1:T}) = \pi_{T,i}^T$. 
\item For $t=T,\ldots,1$, sample $\theta_{t-1}$ from a discrete distribution 
with 
\[
P(\theta_{t-1} = i|\theta_t,y_{1:t-1}) 
\propto P_{i,\theta_t} \pi_{T-1,i}^{T-1}
= \frac{P_{i,\theta_t} \pi_{T-1,i}^{T-1}}{\sum_{i'=1}^S P_{i',\theta_t} \pi_{T-1,i'}^{T-1}}.
\]
\end{enumerate}
\end{frame}



\subsection{Markov model}
\begin{frame}
\frametitle{Markov model}

Consider a Markov model where the states are observed directly,
\pause
but the transition probability matrix $\Psi$ is unknown.
If the sequence of states are $y_{1:t} = (y_1,\ldots,y_t)$, 
\pause
we are interested in the posterior 
\[ 
p(\Psi|y_{1:t}). 
\]
\pause
Since this is a row stochastic matrix $\Psi$, 
\pause
we have 
\[ 
\sum_{j=1}^S \Psi_{ij} = 1 \quad \forall\, i.
\]
So what priors are reasonable for $\Psi$?

\end{frame}



\begin{frame}
\frametitle{Priors for row stochastic matrices}

\pause

One option is a set of independent Dirichlet distributions for each row,
\pause
i.e. let $\Psi_{i\cdot}$ be the $i$th row of $\Psi$, then
\[ 
\Psi_{i\cdot} \sim Dir(A_i)
\]
where $A_i$ is a vector of length $S$ and $A$ is the matrix with rows $A_i$.

\vspace{0.1in} \pause

Do we want more structure here?
\begin{itemize}
\item sparsity (many zero elements)
\item similarity between rows
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Dirichlet distribution}

The Dirichlet distribution (named after Peter Gustav Lejeune Dirichlet), i.e. $P\sim Dir(a)$, is a probability distribution for a probability vector of length $H$. \pause The probability density function for the Dirichlet distribution is 
\[
p(P;a) = \frac{\mG(a_1+\cdots+a_H)}{\mG(a_1)\cdots \mG(a_H)} \prod_{h=1}^H p_h^{a_h-1}
\]
where $p_h\ge 0$, $\sum_{h=1}^H p_h=1$, and $a_h>0$. 

\vspace{0.2in} \pause

Letting $a_0 = \sum_{h=1}^H a_h$, then some moments are
\begin{itemize}[<+->]
\item $E[p_h] = \frac{a_h}{a_0}$,
\item $V[p_h] = \frac{a_h(a_0-a_h)}{a_0^2(a_0+1)}$,
\item $Cov(p_h,p_k) = -\frac{a_ha_k}{a_0^2(a_0+1)}$, and 
\item $\mbox{mode}(p_h) = \frac{a_h-1}{a_0-H}$ for $a_h>1$.
\end{itemize}

A special case is $H=2$ which is the beta distribution.

\end{frame}


\subsection{Conjugate prior for multinomial distribution}
\begin{frame}
\frametitle{Conjugate prior for multinomial distribution}

The Dirichlet distribution is the natural conjugate prior for the multinomial distribution. \pause If 
\[
Y \sim Mult(n,\pi) \qquad \mbox{and} \qquad \pi \sim Dir(a)
\]
then 
\[ 
\pi|y \sim Dir(a+y).
\]

\vspace{0.2in} \pause

Some possible default priors are 
\begin{itemize}
\item $a=1$ which is the uniform density over $\pi$,
\item $a=1/2$ is Jeffreys prior for the multinomial, 
\item $a=1/S$ and 
\item $a=0$, an improper prior that is uniform on $\log(\pi_h)$. 
The resulting posterior is proper if $y_h>0$ for all $h$. 
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{Dirichlet priors for Markov models}

Let $A$ be the hyperparameter with rows $A_i$ such that 
\[ 
\Psi_i \stackrel{ind}{\sim} Dir(A_i)
\]
\pause
and
$C$ be the count matrix of observed transitions, 
\pause
i.e. $C_i$ is the count vector of transitions from $i$ to all states and 
$C_{ij}$ is the count of transitions from $i$ to $j$. 

\vspace{0.1in} \pause

The posterior distribution $p(\Psi|y_t)$ is fully conjugate with $A'=A+C$ such 
that 
\[ 
\Psi_i|y \stackrel{ind}{\sim} Dir(A_i') \stackrel{d}{=} Dir(A_i+C_i)
\]
\pause
where $A'_i$ is the $i$th row of $A'$.


\end{frame}


\begin{frame}
\frametitle{Inference for HMM with unknown transition matrix $\Psi$}

Suppose we have a HMM with unknown transition matrix $\Psi$. 
\pause
How can we perform posterior inference?

\vspace{0.1in} \pause


If we assume $\Psi_i \ind Dir(A)$, 
\pause
then a Gibbs sampling approach is
\begin{enumerate}
\item Sample $\theta_{1:t}|\Psi,y \sim \prod_{t=1}^T p(\theta_{t-1}|\theta_t,y_{1:t},\Psi)$.
\item For $i=1,\ldots,S$, sample $\Psi_i|\theta,y \ind Dir(A_i+C_i)$ where 
$C_i$ is the count vector of transitions from $i$ to all states.
\end{enumerate}

\end{frame}

\end{document}

