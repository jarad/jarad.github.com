\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Finite mixture models}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE,
               cache=TRUE)
options(width=100)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("tidyverse")

library("ggplot2")
library("grid")
library("gridExtra")
library("hexbin")

library("edgeR") # bioconductor
library("rjags")
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\begin{document}

\frame{\maketitle}




\section{Multinomial distribution}
\begin{frame}
\frametitle{Categorical distribution}

Let $Z \sim Cat(H,p)$ represent a categorical distribution with 
\begin{itemize}
\item $P(Z=h) = p_h$ for $h=1,\ldots,H$ and 
\item $\sum_{h=1}^H p_h = 1$.
\end{itemize}

\vspace{0.2in} \pause

Example: discrete choice model

\vspace{0.1in} \pause

Suppose we have a set of $H$ categories and we label these $1,\ldots,H$. \pause Independently, consumers choose one of the $H$ categories with the same probability. \pause Then a reasonable model is $Z_i\stackrel{ind}{\sim} Cat(H,p)$.

\end{frame}


\begin{frame}
\frametitle{Multinomial distribution}

If we count the number of times the consumer chose each category, i.e. 
\[ 
Y_h = \sum_{i=1}^n \I(Z_i=h),
\]
\pause
then the result is the multinomial distribution, i.e. $Y\sim Mult(n, p)$. \pause The multinomial distribution has probability mass function 
\[ 
p(y;n,p) = \frac{n!}{y_1!\cdots y_H!} \prod_{h=1}^H p_h^{y_h} 
\]
which has 
\begin{itemize}
\item $E[Y_i] = np_i$,
\item $V[Y_i] = np_i(1-p_i)$, and 
\item $Cov[Y_i,Y_j] = -np_ip_j$ for $(i\ne j)$.
\end{itemize}

\vspace{0.2in} \pause

A special case is $H=2$ which is the binomial distribution. 

\end{frame}


\section{Dirichlet distribution}
\begin{frame}
\frametitle{Dirichlet distribution}

The Dirichlet distribution (named after Peter Gustav Lejeune Dirichlet), i.e. $P\sim Dir(a)$, is a probability distribution for a probability vector of length $H$. \pause The probability density function for the Dirichlet distribution is 
\[
p(P;a) = \frac{\mG(a_1+\cdots+a_H)}{\mG(a_1)\cdots \mG(a_H)} \prod_{h=1}^H p_h^{a_h-1}
\]
where $p_h\ge 0$, $\sum_{h=1}^H p_h=1$, 
and concentration parameters $a_h>0$. 

\vspace{0.2in} \pause

Letting $a_0 = \sum_{h=1}^H a_h$, \pause then some moments are
\begin{itemize}
\item $E[p_h] = \frac{a_h}{a_0}$, \pause
\item $V[p_h] = \frac{a_h(a_0-a_h)}{a_0^2(a_0+1)}$, \pause
\item $Cov(p_h,p_k) = -\frac{a_ha_k}{a_0^2(a_0+1)}$, \pause and 
\item $\mbox{mode}(p_h) = \frac{a_h-1}{a_0-H}$ for $a_h>1$.
\end{itemize}

\pause

A special case is $H=2$ which is the beta distribution.

\end{frame}


\subsection{Conjugate prior for multinomial distribution}
\begin{frame}
\frametitle{Conjugate prior for multinomial distribution}

The Dirichlet distribution is the natural conjugate prior for the multinomial distribution. \pause If 
\[
Y \sim Mult(n,\pi) \qquad \mbox{and} \qquad \pi \sim Dir(a)
\]
then 
\[ 
\pi|y \sim Dir(a+y).
\]

\vspace{0.2in} \pause

Some possible default priors are 
\begin{itemize}
\item $a=1$ which is the uniform density over $\pi$, \pause
\item $a=1/2$ is Jeffreys prior for the multinomial, \pause
\item $a = 1/H$, \pause and 
\item $a=0$, an improper prior that is uniform on $\log(\pi_h)$. The resulting posterior is proper if $y_h>0$ for all $h$. 
\end{itemize}

\end{frame}



\section{Finite mixtures}


\begin{frame}
\frametitle{Complicated distributions}
<<rnaseq, message=FALSE>>=
library("edgeR")
d = read.table("rna_seq.txt", header=TRUE)

# Keep only total columns
d = d[,c(1, grep("total", names(d)))]


#regmatches(names(d), regexpr('_[B,M]{1,2}\\.', names(d)))


# Rename columns
names(d)[-1] = paste(c("B73","Mo17","B73xMo17","Mo17xB73"), 
                     rep(1:4, each=4), 
                     sep="_")

# Order columns
B_cols = c(2,6,10,14)
d = d[,c(1, B_cols, B_cols+1, B_cols+2, B_cols+3)]

variety = factor(gsub("_[0-9]{1,2}", "", names(d)[-1]), 
                 levels = c("B73","Mo17","B73xMo17","Mo17xB73"))

# phi, alpha, delta parameterization
design = cbind(1,
               ifelse(variety=='Mo17', -1, 1),
               ifelse(variety=='B73',  -1, 1),
               ifelse(variety=='B73xMo17', 1, ifelse(variety=='Mo17xB73',-1,0)))

# GLM fit using edgeR
fit = d[,-1] %>% 
  DGEList() %>%
  calcNormFactors %>%
  estimateCommonDisp %>%
  estimateGLMTagwiseDisp(design) %>%
  glmFit(design)

# Calculate gene-specific estimates for phi, alpha, delta, and psi
hat = data.frame(gene = 1:length(fit$dispersion),
                 phi   = fit$coefficients[,1] + mean(fit$offset[1,]),
                 alpha = fit$coefficients[,2],
                 delta = fit$coefficients[,3],
                 gamma = fit$coefficients[,4],
                 psi   = log(fit$dispersion))

hat$gene = d$GeneID
@

<<rnaseq_plot, dependson='rnaseq', message=FALSE, fig.keep='last', fig.height=10, fig.width=12>>=
parms = names(hat)[-1]
plots = list()

n = length(parms)
for (i in 1:n) {
  for (j in 1:n) {
    p = (i-1)*n+j # plot number
    if (i>j) 
      plots[[p]] = grid.rect(gp = gpar(col = "white"))
    if (i==j) {
      plots[[p]] = ggplot(hat, aes_string(parms[i])) + 
        geom_histogram(fill = 'gray') +
        theme_bw() + 
        labs(x = parse(text = parms[i]))
    }
    if (i<j) {   
      plots[[p]] = ggplot(hat, aes_string(parms[j], parms[i])) + 
        stat_binhex(bins =  100) +
        theme_bw() + 
        scale_fill_gradientn(trans = 'log',
                             breaks = c(1,10,100,1000), 
                             guide = "none", 
                             colours = c("gray","black")) + 
        labs(x = parse(text=parms[j]), 
             y = parse(text=parms[i]))
    }
  }
}

do.call("grid.arrange", c(plots, ncol=5))
@
\end{frame}


\begin{frame}
\frametitle{Finite mixtures}

Let's focus on modeling the univariate distribution for $\hat\phi$

<<phi, dependson='rnaseq', message=FALSE>>=
ggplot(hat, aes(phi)) + 
  geom_histogram(aes(y=..density..)) +
  theme_bw()
@
\end{frame}







\subsection{Finite mixture}
\begin{frame}
\frametitle{Finite mixture}

A model for the marginal distribution for $Y_i=\hat\phi_i$ is

\[ 
Y_i \stackrel{ind}{\sim} \sum_{h=1}^H \pi_h N(\mu_h, \sigma_h^2)
\]
where $\sum_{h=1}^H \pi_h = 1$. 

\vspace{0.2in} \pause

Alternatively, we can introduce a latent variable $\zeta_i=h$ if observation $i$ came from group $h$. \pause
Then 
\[ \begin{array}{rl}
Y_i|\zeta_i=z &\stackrel{ind}{\sim} N(\mu_{z},\sigma_{z}^2) \\ 
\zeta_i &\stackrel{ind}{\sim} Cat(H,\pi)
\end{array} \]
where $\zeta\sim Cat(H,\pi)$ is a categorical random variable with $P(\zeta=h) = \pi_h$ for $h=1,\ldots,H$ and $\pi=(\pi_1,\ldots,\pi_H)$. 

\end{frame}



\begin{frame}
\frametitle{A possible prior}

Let's assume 
\[ \begin{array}{rl}
\pi &\sim Dir(a) \\
\mu_h|\sigma_h^2 & \stackrel{ind}{\sim} N(m_h, v_h^2\sigma_h^2) \\
\sigma_h^2 &\stackrel{ind}{\sim} IG(c_h,d_h)
\end{array} \]
and $\pi$ is independent of $\mu=(\mu_1,\ldots,\mu_H)$ and $\sigma^2 = (\sigma_1^2,\ldots,\sigma_H^2)$.

\vspace{0.2in} \pause

Commonly, we have $m_h=m$, $v_h=v$, $c_h=c$, and $d_h=d$. \pause If the data have been standardized (scaled and centered), a reasonable default prior is 
\begin{itemize}
\item $m=0$, 
\item $v=1$, 
\item $c=2$, 
\item $d=4$, (BDA3 pg 535) and
\item $a$ is $1/H$ (BDA3 pg 536). 
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{MCMC}
\tiny

\vspace{-0.05in}

The steps of a Gibbs sampler with stationary distribution 
\[ 
p(\pi,\mu,\sigma^2,\zeta|y) \propto p(y|\zeta,\mu,\sigma^2) p(\zeta|\pi) p(\mu|\sigma^2) p(\sigma^2) p(\pi)
\]
has steps 

\begin{enumerate}
\item For $i=1,\ldots,n$, sample $\zeta_i$ from its full conditional (as the $\zeta$ are conditionally independent across $i$):
\[ 
P(\zeta_i=h|\ldots) \propto \pi_h N(y_i; \mu_h, \sigma_h^2) 
\]
\item Jointly sample $\pi$ and $\mu,\sigma^2$ as they are conditionally independent.
  \begin{enumerate}\tiny
  \item Sample $\pi \sim Dir(a+Z)$ where $n=(Z_1,\ldots,Z_H)$ and $Z_h = \sum_{i=1}^n \I(\zeta_i=h)$. 
  \item For $h=1,\ldots,H$, sample $\mu_h,\sigma_h^2$ from their full conditional (as these are conditionally independent across $h$):
\[ 
\sigma_h^2 \stackrel{ind}{\sim} IG(c'_h, d'_h) \quad \mu_h|\sigma_h^2 \stackrel{ind}{\sim} N(m_h', v_h'^2\sigma_h^2) 
\]
where 
\[ \begin{array}{rl}
v_h'^2 &= (1/v_h^2 + Z_h)^{-1} \\
m_h' &= v_h'^2(m_h/v_h^2 + Z_h\overline{y}_h) \\
c_h' &= c_d + Z_h/2 \\
d_h' &= d_h + \frac{1}{2}\left(\sum_{i:\zeta_i=h} (y_i-\overline{y}_h)^2 + \left(\frac{Z_h}{1+Z_h/v_h^2}\right)(\overline{y}_h - m_h)^2 \right) \\
\overline{y}_h &= \frac{1}{Z_h} \sum_{i:\zeta_i=h} y_i
\end{array} \]
\end{enumerate}
\end{enumerate}

\end{frame}




% \begin{frame}
% \frametitle{MCMC - Option 1}
% The steps of a Gibbs sampler are 
% \begin{enumerate}
% \item Sample $\zeta_i \stackrel{ind}{\sim} p(\zeta_i|\ldots)$ where 
% \[ 
% P(\zeta_i=1|\ldots) = \frac{\pi_1 N(y_i; \mu_1, \sigma_1^2)}{\pi_0 N(y_i; \mu_0, \sigma_0^2)+\pi_1 N(y_i; \mu_1, \sigma_1^2)}.
% \]
% \item For $h=0,1$, sample $\mu_h\sim p(\mu_h|\ldots)$ where
% \[
% p(\mu_h|\ldots) \propto \left[ \prod_{i:\zeta_i=h} N(y_i;\mu_h,\sigma_h^2) \right] N(\mu_h;m_h,v_h^2\phantom{\sigma_h^2}) 
% \]
% \end{enumerate}
% \end{frame}


\subsection{JAGS}
\begin{frame}[fragile]
<<jags, echo=TRUE>>=
library("rjags")
jags_model = "
model {
  for (i in 1:n) {
    y[i] ~ dnorm(mu[zeta[i]], tau[zeta[i]])
    zeta[i] ~ dcat(pi[])
  }

  for (i in 1:H) {
    mu[i] ~ dnorm(0,1e-5)
    tau[i] ~ dgamma(1,1)
    sigma[i] <- 1/sqrt(tau[i])
  }

  pi ~ ddirich(a)
}"
@

<<jags_data, dependson='rnaseq', echo=TRUE>>=
tmp = hat[sample(nrow(hat), 1000),]
dat = list(n=nrow(tmp), H=3, y=tmp$phi, a=rep(1,3))
@

<<jags_run, dependson=c('jags','jags_data'), echo=TRUE, results='hide'>>=
jm = jags.model(textConnection(jags_model), data = dat, n.chains = 3)
r = coda.samples(jm, c('mu','sigma','pi'), 1e3)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Convergence diagnostics}
<<jags_convergence, dependson='jags_run', echo=TRUE>>=
gelman.diag(r)
gelman.diag(r, multivariate=FALSE)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Convergence diagnostics (2)}
<<jags_convergence2, dependson='jags_run', echo=TRUE>>=
plot(r, density=FALSE)
@
\end{frame}

\begin{frame}
\frametitle{Prior distributions}

The parameters of the model are unidentified due to \alert{label-switching}, \pause i.e. 
\[ Y_i \stackrel{ind}{\sim} \sum_{h=1}^H \pi_h N(\mu_h, \sigma_h^2) \stackrel{d}{=} \sum_{h'=1}^H \pi_{h'} N(\mu_{h'}, \sigma_{h'}^2) \]
for some permutation $h'$. 

\vspace{0.2in} \pause

One way to resolve this issue is to enforce identifiability in the prior. \pause For example, in one-dimension, we can order the component means: $\mu_1<\mu_2<\cdots<\mu_H$. 

\vspace{0.2in} \pause

To ensure the posterior is proper
\begin{itemize}
\item Maintain proper prior for $\pi$
\item Ensure proper prior for ratios of variances 

(perhaps by ensuring prior is proper for variances themselves)
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{Two conditionally conjugate prior options}

Option 1:
\[ 
Dir(\pi;a) \I(\mu_1<\cdots<\mu_H) \prod_{h=1}^H N(\mu_h;m_h,v_h^2\phantom{\sigma_h^2}) IG(\sigma_h^2; c_h, d_h) 
\]

\vspace{0.2in} \pause

Option 2:
\[
Dir(\pi;a) \I(\mu_1<\cdots<\mu_H)  \prod_{h=1}^H N(\mu_h;m_h,v_h^2\sigma_h^2) IG(\sigma_h^2; c_h, d_h)
\]

\end{frame}




\subsection{JAGS}
\begin{frame}[fragile]
<<jags2, echo=TRUE>>=
library("rjags")
jags_model = "
model {
  for (i in 1:n) {
    y[i] ~ dnorm(mu[zeta[i]], tau[zeta[i]])
    zeta[i] ~ dcat(pi[])
  }

  for (i in 1:H) {
    mu0[i] ~ dnorm(0,1e-5)
    tau[i] ~ dgamma(1,1)
    sigma[i] <- 1/sqrt(tau[i])
  }

  mu[1:H] <- sort(mu0)
  pi ~ ddirich(a)
}"
@

<<jags2_run, dependson=c('jags_data', 'jags2'), echo=TRUE, results='hide'>>=
jm = jags.model(textConnection(jags_model), data = dat, n.chains = 3)
r = coda.samples(jm, c('mu','sigma','pi'), 1e3)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Convergence diagnostics}
<<jags2_convergence, dependson='jags2_run', echo=TRUE>>=
gelman.diag(r)
gelman.diag(r, multivariate=FALSE)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Convergence diagnostics (2)}
<<jags2_convergence2, dependson='jags2_run', echo=TRUE>>=
plot(r, density=FALSE)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Posterior on data density}
<<posterior_density, dependson='jags2_run'>>=
extract_draws <- function(x) {
  data.frame(iteration = start(r):end(r),
             chain = rep(1:3, each=end(r)-start(r)+1),
             value = unlist(r[,x$parameter])) 
}

draws <- data.frame(parameter = varnames(r)) %>% 
  group_by(parameter) %>%
  do(extract_draws(.)) %>%
  pivot_wider(
    names_from = parameter,
    values_from = value
  )


get_density <- function(x) {
  xx = seq(-3, 10, by=0.1)
  data.frame(xx = xx,
             yy = 
               x$`pi[1]`*dnorm(xx, x$`mu[1]`, x$`sigma[1]`) + 
               x$`pi[2]`*dnorm(xx, x$`mu[2]`, x$`sigma[2]`) + 
               x$`pi[3]`*dnorm(xx, x$`mu[3]`, x$`sigma[3]`))
}

d <- draws %>%
  group_by(iteration,chain) %>%
  do(get_density(.))

sm = ddply(d, .(xx), summarize, 
           mean=mean(yy), sd=sd(yy),
           lb = quantile(yy, .025),
           ub = quantile(yy, .975))
@

<<posterior_density_plot, dependson=c('posterior_density','jags_data'), results='hide'>>=
ggplot(data.frame(x=c(-3,10)), aes(x)) + 
  geom_histogram(data=tmp, aes(x=phi, y=..density..), 
                 binwidth=0.2, alpha=0.5) +
  geom_ribbon(data=sm, aes(x=xx, ymin=lb, ymax=ub), 
              fill='blue', alpha=0.5) +
  #  geom_line() +
  labs(x='x', y='density') +
  theme_bw()
@
\end{frame}



\subsection{Clustering}
\begin{frame}[fragile]
\frametitle{Group membership}

Group membership can be obtained using the $\zeta_i$, e.g. 
\[
P(\mbox{gene $i$ in cluster $h$}) = P(\zeta_i=h|y) \approx \sum_{m=1}^M \I\left(\zeta_i^{(m)} = h\right).
\]

<<clustering, dependson=c('jags2_run'), results='hide'>>=
jm = jags.model(textConnection(jags_model), data = dat, n.chains = 3)
r = coda.samples(jm, c('zeta'), 1e3)
@

<<clustering_results, dependson='clustering'>>=
draws = ddply(data.frame(parameter=varnames(r)), .(parameter), function(x) {
  data.frame(iteration = start(r):end(r),
             chain = rep(1:3, each=end(r)-start(r)+1),
             value = unlist(r[,x$parameter])) 
}) 

sm = ddply(draws, .(parameter), function(x) {
  data.frame(p1 = mean(x$value==1), 
             p2 = mean(x$value==2),
             p3 = mean(x$value==3))
})
head(sm)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Clustering}

Genes can then be clustered by assigning them to a group based on their posterior probabilities of group membership, i.e. for gene $i$, we assign the group according to 
\[
\mbox{argmax}_h P(\zeta_i=h|y).
\]

<<clustering_final, dependson=c("clustering_results"),fig.height=3>>=
cluster = ddply(sm, .(parameter), function(x) data.frame(group=which.max(x[,-1])))
library(gtools)
ordr = mixedorder(as.character(cluster$parameter))
# ggplot(cbind(cluster[ordr,], tmp), aes(x=phi,y=..density..)) +
#   geom_histogram(binwidth=0.2, alpha=0.5) +
#   facet_wrap(~group) +
#   theme_bw()
@

\vspace{0.2in} \pause

Unfortunately clustering is extremely sensitive to the parametric model chosen, e.g. normal in this example, and the cluster could change dramatically with a different choice, e.g. $t$. 
\end{frame}


\subsection{Choosing $H$}
\begin{frame}
\frametitle{Choosing $H$}
When using finite mixture models one of the key choices is to choose $H$, 
the number of clusters. 

\begin{itemize}
\item A Bayesian approach would place a prior on $H$, e.g. a Poisson or truncated Poisson, and then use reversible jump MCMC to estimate it. \pause
\item A more pragmatic approach is to start with a small $H$ and then determine whether there is some feature of the data that is not being adequately addressed\pause, e.g. via posterior predictive pvalues.  \pause
\item An empirical Bayes finds an MLE (or MAP) via 
\[ 
\hat{H} = \mbox{argmax}_H p(y|H) = \int p(y|\pi,\mu,\sigma^2,H) p(\pi,\mu,\sigma^2|H) d\pi d\mu d\sigma^2
\]
and then condition on $\hat{H}$ in the analysis. \pause Typically this MLE (or MAP) is found via the EM algorithm.
\end{itemize}

\end{frame}




\subsection{Multivariate density estimation}
\begin{frame}
\frametitle{Multivariate density estimation}
<<phi_psi_plot, dependson='rnaseq_plot', message=FALSE>>=
do.call("grid.arrange", c(plots[c(1,5,6,25)], ncol=2))
@
\end{frame}



\subsection{Finite mixture}
\begin{frame}
\frametitle{Finite mixture}

A model for the joint distribution for $Y_i=(\phi_i,\psi)^\top$ is

\[ 
Y_i \stackrel{ind}{\sim} \sum_{h=1}^H \pi_h N(\mu_h, \mySigma_h)
\]
where $\sum_{h=1}^H \pi_h = 1$. 

\vspace{0.2in} \pause

Alternatively, we can introduce a latent variable $\zeta_i=h$ if observation $i$ came from group $h$. \pause
Then 
\[ \begin{array}{rl}
Y_i|\zeta_i=z &\stackrel{ind}{\sim} N(\mu_{z},\mySigma_{z}) \\ 
\zeta_i &\stackrel{ind}{\sim} Cat(H,\pi)
\end{array} \]
where $\zeta\sim Cat(H,\pi)$ is a categorical random variable with $P(\zeta=h) = \pi_h$ for $h=1,\ldots,H$ and $\pi=(\pi_1,\ldots,\pi_H)$. 

\end{frame}



\begin{frame}
\frametitle{A possible prior}

Let's assume 
\[ \begin{array}{rl}
\pi &\sim Dir(a) \\
\mu_h|\Sigma_h & \stackrel{ind}{\sim} N_p(m_h, v_h^2\mySigma_h) \\
\mySigma_h &\stackrel{ind}{\sim} IW(D_h,c_h)
\end{array} \]
where $c_h>p-1$ is the degrees of freedom and $D$ is the scale matrix. \pause The mean of this distribution is $D_h/(c_h-p-1)$ for $c_h>p+1$. 

% \vspace{0.2in} \pause
% 
% Commonly, we have $m_h=m$, $v_h=v$, $c_h=c$, and $D_h=D$. \pause If the data have been standardized (scaled and centered), a reasonable default prior is 
% \begin{itemize}
% \item $m=0$, 
% \item $v=1$, 
% \item $c=2$, 
% \item $d=4$, (BDA3 pg 535) and
% \item $a$ is $1/H$ (BDA3 pg 536). 
% \end{itemize}

\end{frame}



\begin{frame}
\frametitle{MCMC}
\tiny

\vspace{-0.05in}

The steps of a Gibbs sampler with stationary distribution 
\[ 
p(\pi,\mu,\mySigma,\zeta|y) \propto p(y|\zeta,\mu,\mySigma) p(\zeta|\pi) p(\mu|\mySigma) p(\mySigma) p(\pi)
\]
has steps 

\begin{enumerate}
\item For $i=1,\ldots,n$, sample $\zeta_i$ from its full conditional
\[ 
P(\zeta_i=h|\ldots) \propto \pi_h N(y_i; \mu_h, \mySigma_h) 
\]
\item Jointly sample $\pi$ and $\mu,\sigma^2$ as they are conditionally independent.
  \begin{enumerate}\tiny
  \item Sample $\pi \sim Dir(a+Z)$ where $Z=(Z_1,\ldots,Z_H)$ and $Z_h = \sum_{i=1}^n \I(\zeta_i=h)$. 
  \item For $h=1,\ldots,H$, sample $\mu_h,\mySigma_h$ from their full conditional
\[ 
\mySigma_h \stackrel{ind}{\sim} IW(D_h',c_h')
\quad
\mu_h|\mySigma_h \stackrel{ind}{\sim} N(m_h', v_h'^2\mySigma_h)
\]
where 
\[ \begin{array}{rl}
v_h'^2 &= (1/v_h^2 + Z_h)^{-1} \\
m_h' &= v_h'^2(m_h/v_h^2 + Z_h\overline{y}_h) \\
c_h' &= c_d + Z_h \\
D_h' &= D_h + \sum_{i:\zeta_i=h} (y_i-\overline{y}_h)(y_i-\overline{y}_h)^\top + \left(\frac{Z_h}{1+Z_h/v_h^2}\right)(\overline{y}_h-\mu_h)(\overline{y}_h-\mu_h)^\top  \\
\overline{y}_h &= \frac{1}{Z_h} \sum_{i:\zeta_i=h} y_i 
\end{array} \]
\end{enumerate}
\end{enumerate}

\end{frame}


\begin{frame}[fragile]
<<jags_joint, echo=TRUE>>=
library("rjags")
joint_mixture_model = "
model {
  for (i in 1:n) {
    y[i,1:p] ~ dmnorm(mu[,zeta[i]], Tau[,,zeta[i]])
    zeta[i] ~ dcat(pi[])
  }

  for (h in 1:H) {
    mu[1:p,h] ~ dmnorm(mu0,Tau[,,h])
    Tau[1:p,1:p,h] ~ dwish(D[,],c)
    Sigma[1:p,1:p,h] <- inverse(Tau[,,h])
  }

  pi ~ ddirich(a)
}"
@

<<jags_joint_data, dependson=c('rnaseq'), echo=TRUE, results='hide'>>=
tmp = hat[sample(nrow(hat), 1000),]
dat = list(n=nrow(tmp), y = tmp[,c('phi','psi')], p=2, H=10)
dat$a = rep(1/dat$H, dat$H)
dat$D = diag(1, dat$p)
dat$c = dat$p+1
dat$mu0 = c(3,0)
@

<<jags_joint_run, dependson=c('jags_joint','jags_joint_data'), echo=TRUE, results='hide', eval=FALSE>>=
jm = jags.model(textConnection(joint_mixture_model), 
                data = dat, 
                n.chains = 3)
r = coda.samples(jm, c('pi','mu','Sigma'), 1e3)
@
\end{frame}



% \begin{frame}
% \frametitle{Approximation to the joint distribution}
% 
% <<joint_density_estimation, dependson=c('jags_joint_run','jags_joint_data')>>=
% draws = ddply(data.frame(parameter=varnames(r)), .(parameter), function(x) {
%   data.frame(iteration = start(r):end(r),
%              chain = rep(1:3, each=end(r)-start(r)+1),
%              value = unlist(r[,x$parameter])) 
% }) %>%
%   dcast(iteration+chain~parameter)
% 
% library(mvtnorm)
% dmixnorm = function(x,pi,mu,Sigma) {
%   density = 0
%   for (i in 1:length(pi)) 
%     density = density+pi[i]*dmvnorm(x,mu[i],Sigma[,,i])
%   return(density)
% }
% pi_col = grep('pi', names(draws))
% mu_col = grep('mu', names(draws))
% Sigma_col = grep('Sigma', names(draws))
% 
% grid = expand.grid(phi = seq(-3,9,length.out=101), 
%                    psi=c(-4, 1, length.out=101))
% 
% ddply(data.frame(iteration=1:nrow(draws)), .(iteration), function(x) {
%   pi = as.numeric(draws[x$iteration,pi_col])
%   mu = as.matrix(draws[x$iteration,mu_col], ncol=2, nrow=10, byrow=TRUE)
%   Sigma = as.array(draws[x$iteration,Sigma_col], dim=c(dat$g, dat$g, dat$H))
%   
%   ddply(grid, .(phi,psi), function(g) {
%     data.frame(density = dmixnorm(c(g$phi,g$psi), pi, mu, Sigma))
%   })
% }) 
% @
% 
% \end{frame}


\end{document}

