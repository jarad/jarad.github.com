---
title: "Random Forests"
author: "Jarad Niemi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
layout: page
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE
)
```

[R code](29-random_forests.R)

```{r packages}
library("tidyverse")
theme_set(theme_bw())
library("rpart")
library("rpart.plot")
library("randomForest")
library("DT")
```

In these slides, we'll introduce classification and regression trees
(CART). 

For simplicity in the analyses, 
I will use a subset of the diamonds data set where we randomly select
100 observations and eliminate (for simplicity) the categorical variables. 

```{r}
set.seed(20230425) # This matches what was used in a previous set of slides
n <- 100
d <- diamonds %>%
  dplyr::select(-cut, -color, -clarity) %>%
  rename(lprice = price) %>%
  mutate(lprice = log(lprice))

train <- d %>% sample_n(n)
test <- d %>% sample_n(n)
```

Load up previous error file

```{r}
error <- read_csv("../28-penalty/error.csv")
```


# Classification and Regression Trees (CART)

Tree-based methods utilize a regression approach with dummy variables created
from continuous variables being above or below a cutoff. 
Then in each split, variables will be split again. 
This methodology can be utilized for many regression problems including a 
binary response and continuous response.
Here we will investigate the continuous response. 

## Fit

```{r}
m <- rpart(lprice ~ ., data = train)
```

## Tree

Trees (in the real world) are constructed of a trunk, branches, and leaves. 
CART trees utilize the same structure where the trunk is composed of all 
observations, the branches split those observations, and (eventually) 
result in leaves. 

These leaves are composed of a collection of observations. 
The mean of those observations is the estimated and predicted mean for any
observations that fall into that leaf. 

The plot below provides the dummy variables that split the observations. 
The top number in each box is the mean of those observations.
The percentage in the each box is the percentage of all observations that 
end up in that branch or leaf. 

```{r}
rpart.plot(m, uniform = TRUE)
```

This regression tree utilizes only the explanatory variables carat, x, and z. 
These variables are related to the size of the diamond:
carat is weight, x is depth, and z is height. 
Thus, this tree is ordered from left (smallest) to right (largest) diamonds and 
the log price increases from left to right. 

### Regression model

Recall that a regression model is specified by a set of explanatory variables
$X_1,\ldots,X_p$. Here those explanatory variables are the product of 
dummy variables that lead to a leaf. 
Based on the tree above, the explanatory variables are

- $X_1 = \mathrm{I}(x < 5.7)\mathrm{I}(z < 3.1)\mathrm{I}(x < 4.4)$
- $X_2 = \mathrm{I}(x < 5.7)\mathrm{I}(z < 3.1)\mathrm{I}(x \ge 4.4)$
- $X_3 = \mathrm{I}(x < 5.7)\mathrm{I}(z \ge 3.1)$
- $X_4 = \mathrm{I}(x \ge 5.7)\mathrm{I}(carat < 1.2)\mathrm{I}(x < 6.1)$
- $X_5 = \mathrm{I}(x \ge 5.7)\mathrm{I}(carat < 1.2)\mathrm{I}(x \ge 6.1)$
- $X_6 = \mathrm{I}(x \ge 5.7)\mathrm{I}(carat \ge 1.2)\mathrm{I}(x < 7.3)$
- $X_7 = \mathrm{I}(x \ge 5.7)\mathrm{I}(carat \ge 1.2)\mathrm{I}(x \ge 7.3)$

Recall that for these data `lprice` is our response. 

Some of these dummy variables can be simplified, e.g. 

- $X_1 = \mathrm{I}(z < 3.1)\mathrm{I}(x < 4.4)$

The resulting regression model has 
\[ 
E[Y_i] = \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5 + \beta_6X_6
\]
where $\beta_j$ is the mean for the observations with $X_j = 1$. 

We can verify that this is what is going on by computing group means 

but in order to do so we need more accurate cut points

```{r}
m
```

```{r}
train %>%
  mutate(
    X1 = (x < 5.705)*(z < 3.07)*(x < 4.37),
    X2 = (x < 5.705)*(z < 3.07)*(x >= 4.37),
    X3 = (x < 5.705)*(z >= 3.07),
    X4 = (x >= 5.705)*(carat < 1.18)*(x < 6.105),
    X5 = (x >= 5.705)*(carat < 1.18)*(x >= 6.105),
    X6 = (x >= 5.705)*(carat >= 1.18)*(x < 7.345),
    X7 = (x >= 5.705)*(carat >= 1.18)*(x >= 7.345)
  ) %>%
  summarize(
    mean1 = sum(lprice*X1)/sum(X1),
    mean2 = sum(lprice*X2)/sum(X2),
    mean3 = sum(lprice*X3)/sum(X3),
    mean4 = sum(lprice*X4)/sum(X4),
    mean5 = sum(lprice*X5)/sum(X5),
    mean6 = sum(lprice*X6)/sum(X6),
    mean7 = sum(lprice*X7)/sum(X7)
  ) %>%
  round(1) %>%
  pivot_longer(everything())
```



## Iterative construction

We can take a look at the iterative construction of the model using

```{r, include=FALSE}
summary(m)
```


## Predictions

```{r}
p <- bind_rows(
  test  %>% mutate(p = predict(m, newdata = test),  type = "test"),
  train %>% mutate(p = predict(m, newdata = train), type = "train")
)

ggplot(p, aes(x = p, y = lprice, shape = type, color = type)) + 
  geom_abline(intercept = 0, slope = 1, color = "gray") + 
  geom_point(position = position_dodge(width = 0.1)) 
```

```{r}
p_train <- predict(m, newdata = train)
p_test  <- predict(m, newdata = test)

error <- bind_rows(
  error,
  data.frame(
    group         = "Tree",
    method        = "default",
    in_sample     = mean((p_train - train$lprice)^2),
    out_of_sample = mean((p_test  -  test$lprice)^2)
  )
)
```


## Tuning parameters

```{r}
args(rpart.control)
```
Each argument can tune the CART model to underfit or overfit. 
For example, 

- minsplit
    - low values lead to overfitting
    - high values lead to underfitting
- minbucket
    - low values lead to overfitting
    - high values lead to underfitting
- cp
    - low values lead to overfitting
    - high values lead to underfitting
    
    
### Underfit

```{r}
m <- rpart(lprice ~ ., data = train,
           control = rpart.control(
             minsplit = 40,
             minbucket = 20,
             cp = 0.1
           ))
```

```{r}
rpart.plot(m)
```

```{r}
p_train <- predict(m, newdata = train)
p_test  <- predict(m, newdata = test)

error <- bind_rows(
  error,
  data.frame(
    group         = "Tree",
    method        = "underfit",
    in_sample     = mean((p_train - train$lprice)^2),
    out_of_sample = mean((p_test  -  test$lprice)^2)
  )
)
```

### Overfit

```{r}
m <- rpart(lprice ~ ., data = train,
           control = rpart.control(
             minsplit = 10,
             minbucket = 5,
             cp = 0.001
           ))
```

```{r}
rpart.plot(m)
```



```{r}
p_train <- predict(m, newdata = train)
p_test  <- predict(m, newdata = test)

error <- bind_rows(
  error,
  data.frame(
    group         = "Tree",
    method        = "overfit",
    in_sample     = mean((p_train - train$lprice)^2),
    out_of_sample = mean((p_test  -  test$lprice)^2)
  )
)
```






# Random forests

The idea behind random forests is encapsulated in the name.

`Forests` means that we will create a collection of tree models, 
i.e. a forest. 
Since we have a collection of trees, we will use a model averaged 
prediction from those trees. 
Since the trees are all interchangeable, this average will be an unweighted
average. 

`Random` indicates that randomness will be employed in the process to 
construct a variety of trees. 
This randomness will be included in two steps: 

1. each time we construct a tree, we will sample from the data with replacement and
1. at every split, we will randomly choose a subset of the explanatory variables to split on. 

In the first step, we will fit the model with the resampled data. 
Since some observations will not be included in a particular resample, 
these data can be used to evaluate out-of-sample error. 
In random forests, this is called the out-of-bag error. 

## Fit

```{r}
m <- randomForest(lprice ~ ., data = train)
m
```

Out-of-bag error versus number of trees. 

```{r}
plot(m)
```


## Prediction


```{r}
p_train <- predict(m, newdata = train)
p_test  <- predict(m, newdata = test)

error <- bind_rows(
  error,
  data.frame(
    group         = "Random forest",
    method        = "default",
    in_sample     = mean((p_train - train$lprice)^2),
    out_of_sample = mean((p_test  -  test$lprice)^2)
  )
)
```


## Arguments

There are a number of arguments to the random forest algorithm that can be 
tuned that will affect predictive performance.

```{r, eval=FALSE}
?randomForest
```

- sampsize: number of resampled observations to train on
- mtry: number of explanatory variable to try at each step
- nodesize: number of observations in each leaf
- ntree: number of trees to create
- replace: whether to sample with replacement or not


## Overfit

We can overfit by using a larger portion of our data, 
making sampling without replacement, 
trying all explanatory variables at every split, and
making the minimum number of observations in each split as small as possible.

This should lead to every tree being exactly the same, but perhaps there are 
additional tuning parameters that result in this not being the case. 

```{r}
m <- randomForest(lprice ~ ., data = train,
                  sampsize = nrow(train),
                  replace = FALSE,
                  mtry = 5,
                  nodesize = 1 
                  )
m

p_train <- predict(m, newdata = train)
p_test  <- predict(m, newdata = test)

error <- bind_rows(
  error,
  data.frame(
    group         = "Random forest",
    method        = "overfit",
    in_sample     = mean((p_train - train$lprice)^2),
    out_of_sample = mean((p_test  -  test$lprice)^2)
  )
)
```


## Underfit

To underfit, we can reverse all the settings in the previous section.
Here we will make the sample for each tree small, 
try only 1 explanatory variable for each split, 
require a relatively large number of obserations in each leaf, and 
use a small number of trees. 

```{r}
m <- randomForest(lprice ~ ., data = train,
                  sampsize = 0.3*nrow(train), # training set is same size as data
                  mtry = 1, # number of variables to try at each split
                  nodesize = 10, # number of observations in each leaf
                  ntree = 20
                  )
m

p_train <- predict(m, newdata = train)
p_test  <- predict(m, newdata = test)

error <- bind_rows(
  error,
  data.frame(
    group         = "Random forest",
    method        = "underfit",
    in_sample     = mean((p_train - train$lprice)^2),
    out_of_sample = mean((p_test  -  test$lprice)^2)
  )
)
```



# Summary

## Implementations

There are a number of different packages that implement tree based approaches
to regression modeling. 
Many of those approaches and additional machine and statistical learning 
approaches exist on the 
[CRAN Task View: Machine Learning and Statisical Learning](https://cran.r-project.org/web/views/MachineLearning.html).

## Probabilistic predictions

Tree models are just a particular type of regression model and thus 
prediction intervals should be straight-forward, but apparently are not 
implemented in `predict.rpart`. 
Prediction intervals construction from random forests models is an active area
of research. 
The [piRF](https://cran.r-project.org/web/packages/piRF/) package aims to 
combine multiple approaches for regression models in a single package. 

## Comparison

Below is a table comparing performance for a number of methods for the log
price of diamonds based on continuous explanatory variables. 
While the results here indicating similar performance amongst all these methods,
we should be careful in drawing too many conclusions from this analysis. 
As a reminder, for computational reasons, we are only using 100 training and 
testing data points. 
In addition, the relationship between the explanatory variables and log price
seem reasonably linear. 
For more complicated relationships and more data, 
the more flexible methods will likely perform better. 

```{r}
error %>%
  datatable(
    rownames = FALSE,
    caption = "In and out-of-sample error for various prediction methods",
    filter = "top"
  ) %>%
  formatRound(columns = c("in_sample","out_of_sample"), digits = 3)
```