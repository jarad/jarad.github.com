---
layout: page
title: "Linear regression"
author: "Jarad Niemi"
date: "`r Sys.Date()`"
header-includes:
- \usepackage{blkarray}
- \usepackage{amsmath}
output: 
  html_document:
      toc: true
      toc_float: true
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[R code](19-linear_regression.R)

```{r}
library("tidyverse"); theme_set(theme_bw())
library("Sleuth3")
library("ggResidpanel")
library("emmeans")
```

# Background

As a general approach, regression allows the response variable mean 
(or expectation) to depend on categorical and continuous explanatory variables 
in complex patterns. 

## Energy expenditure 

Scientific question: 
How does echolocation affect energy expenditure?

### Two-group analysis

```{r}
d <- case1002 %>%
  filter(grepl("bats", Type)) %>%
  mutate(echolocating = Type == "echolocating bats")

d %>% 
  group_by(echolocating) %>%
  summarize(
    n = n(),
    mean = mean(Energy),
    sd = sd(Energy))

t.test(Energy ~ echolocating, data = d, var.equal = FALSE)
```

```{r}
# Maybe should use logarithms
d %>% 
  group_by(echolocating) %>%
  summarize(
    n = n(),
    mean = mean(log(Energy)),
    sd = sd(log(Energy)))

t.test(log(Energy) ~ echolocating, data = d, var.equal = FALSE)
```

### Three-group analysis

```{r}
case1002 %>% 
  group_by(Type) %>%
  summarize(
    n = n(),
    mean = mean(log(Energy)),
    sd = sd(log(Energy)))

m <- lm(log(Energy) ~ Type, data = case1002)
anova(m)
summary(m)
```

Why does there seem to be a difference? 
Is it due to echolocation or something else, e.g. Mass?

### Regression

```{r}
ggplot(case1002, aes(x = Mass, y = Energy, color = Type, shape = Type)) +
  geom_point() + 
  scale_y_log10() + 
  scale_x_log10()
```

```{r}
m <- lm(log(Energy) ~ log(Mass) + Type, data = case1002)
anova(m)
```
After we control/adjust for Mass, Type does not appear to improve the model.
Thus, after controlling for Mass, echolocation does not appear to increase 
energy expenditure. 

Of course, now we might like to know why echolocation is primarily in low 
mass species. 



## Regression Model

The general structure of a regression model is 
\[ 
Y_i \stackrel{ind}{\sim} N(\beta_0 + \beta_1X_{i,1} + \cdots + \beta_{p-1} X_{i,p-1}, \sigma^2)
\]
for $i=1,\ldots,n$
or, equivalently,
\[ 
Y_i = \beta_0 + \beta_1X_{i,1} + \cdots + \beta_{p-1} X_{i,p-1}, \qquad \epsilon_i \stackrel{ind}{\sim} N(0, \sigma^2).
\]

where, for observation $i$, 

- $Y_i$ is the value of the response variable,
- $X_{i,j}$ is the value of the $j$th explanatory variable, and
- $\epsilon_i$ is the error.

You have a lot of flexibility in your choice for the response and explanatory
variables.



### Matrix notation

An alternative way to represent the model uses matrix notation and the 
multivariate normal distribution. 
\[
Y = X\beta + \epsilon, \qquad \epsilon \sim N_n(0,\sigma^2 \mathrm{I})
\]
where 

- $Y = (Y_1,\ldots,Y_n)^\top$ is an $n\times 1$ response variable vector
- $X$ is an $n\times p$ *design matrix*
  - each row $X_{r,\cdot}$ contains the explanatory variables values for one observation
  - each column $X_{\cdot, c}$ contains the one explanatory variable values for all observations
- $\beta = (\beta_0,\beta_1,\ldots,\beta_{p-1})$ is a $p\times 1$ coefficient parameter vector
- $\epsilon = (\epsilon_1,\ldots,\epsilon_n)$ is an $n\times 1$ error vector
- $N_n(\mu,\Sigma)$ is a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)
with 
  - mean $\mu$ and
  - covariance matrix $\Sigma$ where 
    - $\Sigma_{i,j}$ is the covariance between $\epsilon_i$ and $\epsilon_j$
- $\mathrm{I}$ is the $n\times n$ identity matrix

If you are interested in learning more about the multivariate normal distribution
and its uses, look for a course in multivariate data analyses, 
e.g. STAT 475.


### Parameter estimates

Use of matrix notation and linear algebra eases parameter estimation:

- estimated coefficients $\hat\beta = (X^\top X)^{-1} X^\top y$
- fitted/predicted values $\hat{y} = X\hat\beta$
- residuals $\hat{\epsilon} = y - \hat{y}$
- estimated error variance $\hat\sigma^2 = \frac{1}{n-p}\hat{e}^\top \hat{e}$
- coefficient of determination ($R^2$) $1-\hat{e}^\top \hat{e}/(y-\overline{y})^\top (y-\overline{y})$

```{r}
m <- lm(log(Energy) ~ log(Mass) * Type, data = case1002)
s <- summary(m)

# Coefficient estimates and confidence/credible intervals
coef(m)            # or m$coefficients
confint(m)

# SD estimate
s$sigma

# Coefficint of determination
s$r.squared

# Fitted and residuals
head(fitted(m))    # or head(m$fitted.values)
head(residuals(m)) # or head(m$residuals)

# A reasonable display of most of this
summary(m)
```



## Assumptions

The assumptions in every regression model are 

- errors are independent,
- errors are normally distributed,
- errors have constant variance, 

and 

- the expected response, $E[Y_i]$, depends on the explanatory variables according
to a linear function (of the parameters).

We generally use graphical techniques to assess these assumptions. 
In particular, we utilize 

- residuals v fitted
- qqplot of residuals
- residuals v explanatory
- residuals v index

### base R graphics

```{r}
opar = par(mfrow = c(2,3))
plot(m, 1:6, ask = FALSE)
par(opar)
```

```{r}
d <- case1002 %>%
  mutate(residuals = residuals(m))

plot(d$residuals) # residuals v index
plot(residuals ~ Type, data = d, type = "p")
plot(residuals ~ log(Mass), data = d)
```

### ggResidpanel

This package is created by ISU Statistics PhD Alumni and utilizes ggplot2. 

```{r}
resid_panel(m, plots = "all")

resid_panel(m, plots = "all", 
            qqbands = TRUE, smoother = TRUE)

resid_panel(m, plots = c("resid","index","qq","cookd"), 
            qqbands = TRUE, smoother = TRUE)

resid_xpanel(m, smoother = TRUE)
```


## Interpretation

In a linear regression model there are $p$ coefficients $\beta_0,\ldots,\beta_{p-1}$
and one variance $\sigma^2$. 
Generally, we are most interested in understanding the effect of explanatory 
variables by interpreting (a transformation of) the coefficients.

The general interpretation is 

    a one unit increase in the explanatory variable is associated with a $\beta$
    increase in the mean of the response when holding all other variables constant
    
There are several issues with this interpretation

- is the one-unit increase relevant?
- is the increase in mean relevant?
- can you hold the other variables constant?

In this class, we will focus on graphical interpretations. 


## Tests

### T-tests for regression coefficients

If a regression coefficient is exactly zero, 
this means that explanatory variable has no effect on the mean of the 
response. 
Thus constructing a hypothesis test with the null value being zero is reasonable.
If the p-value for this test is small, 
it only tells us there is something wrong with the model that includes this
coefficient being zero. 
But this small p-value could arise due to any (or multiple) model assumptions. 
Thus, the analyst needs to evaluate all model assumptions. 

Recall that each regression coefficient can be tested using the following
t statistic
\[ 
T = \frac{\hat\beta - b_0}{SE(\hat\beta)}
\]
where $b_0$ is the hypothesized value. 
Then this statistic is compared to a t-distribution with $n-p$ degrees of 
freedom. 

```{r}
coef(s) # or s$coefficients
```

This table provides $\hat\beta$, $SE(\hat\beta)$, the t statistic with 
$b_0, and the two-sided p-value. 


### F-tests

Two models are *nested* when setting some parameters in one of the models to 
specific values (usually 0) produces the other model. 
The second model is called the *simpler* model. 

When we would like to compare two nested models we can use an F-test. 
The null model for this comparison is the simpler model. 
A small p-value provides evidence against the simpler model. 

In the standard regression output, R provides an F-test to compare the 
intercept only model to the full model. 

```{r}
summary(m)
```

We can recreate this F-test using the ANOVA function. 

```{r}
m  <- lm(log(Energy) ~ log(Mass) * Type, data = case1002)
m0 <- lm(log(Energy) ~ 1, data = case1002)
anova(m0, m)
```
But, generally this F-test is not very interesting. 
It may help us to consider adding terms to the model sequentially. 

```{r}
anova(m)
```

This is a sequential comparison, i.e. 

1. comparing log(Mass) to intercept-only model
1. comparing Type + log(Mass) to log(Mass)
1. comparing Type + log(Mass) + Type:log(Mass) to Type + log(Mass)

If we just want to look at the highest order terms, 
we can use `drop()`.


```{r}
drop1(m, test = "F")
```


### Contrasts

Contrasts are linear combinations of means who coefficients sum to zero. 
They are useful in comparing the means of groups of groups to each other. 
Extensions of these ideas can be used to compare group means at fixed values of
other explanatory variables. 

First, imagine that we wanted to perform all pairwise comparisons. 

```{r}
m <- lm(log(Energy) ~ Type, data = case1002)
e <- emmeans(m, pairwise~Type)
e
```

Suppose we wanted to compare echolocating bats vs 
(non-echolocating bats AND non-echolocating birds). 
One way to pose this question is to compare the mean of the echolocating bats
to the average of the means for the non-echolocating bats and 
non-echolocating birds. 

```{r}
contrast(e, list("echo v non-echo" = c(-1,.5,.5)))
```
Suppose instead you wanted to consider the model that has a different 
slope for each type and you wanted to compare the slopes amongst the 
different types. 

```{r}
m <- lm(log(Energy) ~ log(Mass)*Type, data = case1002)
emtrends(m, pairwise ~ Type, var = "log(Mass)")
```
Differences between groups at a particular value of a continuous variable

```{r}
ggplot(case1002, aes(x = Mass, y = Energy, 
       color = Type, linetype = Type, shape = Type)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  scale_x_log10() + 
  scale_y_log10() +
  geom_vline(xintercept = 400)
```

```{r}
m <- lm(log(Energy) ~ log(Mass)*Type, data = case1002)
em <- emmeans(m, pairwise ~ Type, at = list(Mass = 400))
em
co <- contrast(em, type = "response")
confint(co)
```

The tools provided in the `emmeans` package are tremendously helpful to 
real-world statistical analysis. 
They allow you to fit a single model and ask many scientific questions. 


## Model construction

There are various ways to construct regression models in R. 

### Multiple explanatory variables

If we want to include multiple explanatory variables, 
we simply use the `+` symbol.

```{r}
lm(Energy ~ Mass + Type, data = case1002)
```

### Transformations

Transformations of the response and explanatory variables can generally be 
included directly in the call. 

```{r}
lm(log(Energy) ~ sqrt(Mass) + Type, data = case1002)
```

The most common transformation is the `log()`. 

#### I()

If we want to include a polynomial term, e.g. quadratic, we need to 
encapsulate it in `I()`. 

```{r}
lm(log(Energy) ~ Mass + I(Mass^2) + Type, data = case1002)
```
This can also be used to "Shift the intercept" to provide an interpretable
intercept. 

```{r}
meanMass <- mean(case1002$Mass)
lm(Energy ~ I(Mass - meanMass), data = case1002)
```


### Interactions

Interactions between variables can be included by using `:`.

```{r}
lm(log(Energy) ~ log(Mass) + Type + log(Mass):Type, data = case1002)
```

But a convenient shorthand is to use `*` which will include the main effects
as well as the interaction. 

```{r}
lm(log(Energy) ~ log(Mass) * Type, data = case1002)
```

### Include everything

We can include everything in the data.frame using the `.`. 

```{r}
lm(log(Energy) ~ ., data = case1002)
```

Interactions (and main effects) can be included using `.^2`. 

```{r}
lm(log(Energy) ~ .^2, data = case1002)
```

If there are more explanatory variables, this can be used to include higher
order interactions. 


### Remove terms

```{r}
lm(log(Energy) ~ log(Mass) + Type - 1, data = case1002) # remove intercept
```

```{r}
lm(log(Energy) ~ log(Mass) * Type - Type, data = case1002)
```




# Examples

## Simple linear regression

```{r}
ggplot(case0701, aes(x = Velocity, y = Distance)) +
  geom_point() + 
  geom_smooth(method = "lm", formula = y ~ x)
```

An alternative, more general approach, utilizes the `predict()` function to
create data for the lines. 

```{r}
m <- lm(Distance ~ Velocity, data = case0701)

d <- data.frame(Velocity = range(case0701$Velocity)) %>%
  mutate(prediction = predict(m, newdata = .))

ggplot(case0701, aes(x = Velocity)) +
  geom_point(aes(y = Distance)) + 
  geom_line(
    data = d,
    aes(y = prediction), color = "blue")
```

We can obtain the equation for the line using the `summary()` output.

```{r}
summary(m)
```

Take a look at diagnostics

```{r}
resid_panel(m, plots = c("resid","index","qq","cookd"))
resid_xpanel(m)
```


## One categorical variable

```{r}
ggplot(case0501, aes(x = Diet, y = Lifetime)) + 
  geom_jitter(width = 0.1)
```

```{r}
m <- lm(Lifetime ~ Diet, data = case0501)
```

```{r}
resid_panel(m, plots = c("resid","index","qq","cookd"))
resid_xpanel(m)
```

Visual representation

```{r}
d <- data.frame(Diet = unique(case0501$Diet)) 
p <- d %>% mutate(prediction = predict(m, d))

ggplot() + 
  geom_jitter(
    data = case0501,
    aes(x = Diet, y = Lifetime),
    width = 0.1) + 
  geom_point(
    data = p,
    aes(x = Diet, y = prediction),
    color = "red",
    size = 3)
```


## Polynomial

```{r}
ggplot(Sleuth3::case1001, aes(Height, Distance)) +
  geom_point() +
  theme_bw()
```

```{r}
m1 <- lm(Distance ~ Height,                             case1001)
m2 <- lm(Distance ~ Height + I(Height^2),               case1001)
m3 <- lm(Distance ~ Height + I(Height^2) + I(Height^3), case1001)
```

```{r}
g <- ggplot(case1001, aes(x=Height, y=Distance)) + geom_point(size=3)

g 
g + stat_smooth(method="lm", formula = y ~ x)                                   
g + stat_smooth(method="lm", formula = y ~ x + I(x^2))          
g + stat_smooth(method="lm", formula = y ~ x + I(x^2) + I(x^3)) 
```

Recall that error variance will ALWAYS decrease as you add more explanatory
variables. 
So reduced uncertainty in the line does not ensure a better out-of-sample
fit. 

```{r}
m <- lm(Distance ~ Height + I(Height^2) + I(Height^3) + I(Height^4), case1001)
anova(m)
```

This suggests that that model with a quadratic term is insufficient while the 
model with a cubic does not raise any red flags. 
But, you should still check model assumptions. 


## Transformations

```{r}
m <- lm(log(Energy) ~ log(Mass) + Type, data = case1002)

d <- expand.grid(
  Mass = seq(min(case1002$Mass), max(case1002$Mass), length = 1001),
  Type = unique(case1002$Type)
)

p <- d %>%
  mutate(Energy = exp(predict(m, newdata = d))) # note the exp()

g <- ggplot() + 
  geom_point(
    data = case1002, aes(x = Mass, y = Energy, color = Type, shape    = Type)) + 
  geom_line(
    data = p,        aes(x = Mass, y = Energy, color = Type, linetype = Type)) 

g
```

These lines will look straight (and parallel) on log-log axes

```{r}
g + 
  scale_x_log10() + 
  scale_y_log10()
```



## Interactions

```{r}
m <- lm(log(Energy) ~ log(Mass) * Type, data = case1002) # note the *

d <- expand.grid(
  Mass = seq(min(case1002$Mass), max(case1002$Mass), length = 1001),
  Type = unique(case1002$Type)
)

p <- d %>%
  mutate(Energy = exp(predict(m, newdata = d))) # note the exp()

g <- ggplot() + 
  geom_point(
    data = case1002, aes(x = Mass, y = Energy, color = Type, shape    = Type)) + 
  geom_line(
    data = p,        aes(x = Mass, y = Energy, color = Type, linetype = Type)) 

g
```

These lines will look straight (and parallel) on log-log axes

```{r}
g + 
  scale_x_log10() + 
  scale_y_log10()
```