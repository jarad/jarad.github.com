---
title: "Predictions"
author: "Jarad Niemi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
layout: page
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE
)
options(width = 120)
```

[R code](30-prediction.R)

```{r packages, message=FALSE, warning=FALSE}
library("tidyverse")
theme_set(theme_bw())
library("randomForest")
library("nnet")
library("xgboost")
library("DT")
```

Let's take a stab at predicting 
[Wild Blueberry Yield](https://www.kaggle.com/competitions/playground-series-s3e14/overview). 
Here I will follow the process I used to take build a method for prediction
of Wild Blueberry Yield. 

# Data

Let's read the training data and get an understanding of it.
I am caching this chunk so that I can depend on it in future chunks. 
I am also making the caching depend on the file. 

```{r data, cache=TRUE, cache.extra=tools::md5sum("data/train.csv")}
d <- read_csv("data/train.csv",

  # Good practice to formally define variable types.
  col_types = cols(
    id          = col_character(),
    .default    = col_double()
  )
)
```

Check to see if there are any missing values. 
Please note that this may depend on how "not available" data are recorded in the
data set. 

```{r, dependson="data"}
anyNA(d)
```

It appears there are no missing data here. 


The training data have `r nrow(d)` observations on 
`r ncol(d)` variables.
The variable names are 

```{r variables, dependson="data"}
names(d)
```

which includes the response variable `yield` and the unique id `id`. 
This `id` is not going to be important in the prediction but is simply just 
an identifier. 
I infer this because the `id` is just sequential numbers

```{r, dependson="data"}
all(diff(as.numeric(d$id)) == 1)
```

## Yield

Let's take a look at yield first since it is our 

```{r, dependson="data"}
ggplot(d, aes(x = yield)) +
  geom_histogram(aes(y = after_stat(density)),
    fill = "gray"
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(d$yield),
      sd   = sd(d$yield)
    ),
    color = "blue",
    linewidth = 2
  )
```

This histogram looks slightly skewed to the left. 
With the left-skewness and the ratio of the max to min not greater than 10,
I am not initially considering using the logarithm of yield as my response 
variable. 
But, I may contemplate it in the future. 

Apparently multiple observations have the same min/max yield

```{r, dependson="data"}
d %>% filter(yield == min(yield))
d %>% filter(yield == max(yield))
```

Thus these seem like forced min/max values. 
When it comes to prediction, we should probably ensure that we never have 
values outside this range. 



## Explanatory variables

The [original blueberry yield contest](https://www.kaggle.com/datasets/shashwatwork/wild-blueberry-yield-prediction-dataset)
contains more information on the explanatory variables. The variables can be
grouped by type

- clonesize: average size of shrub in the field (I think)
- bees: density of bees in the field
    - honeybee
    - bumbles
    - andrena
    - osmia
- temperature: summaries of temperatures during the season
    - MaxOfUpperTRange 
    - MinOfUpperTRange 
    - AverageOfUpperTRange
    - MaxOfLowerTRange 
    - MinOfLowerTRange 
    - AverageOfLowerTRange  
- rain: 
    - RainingDays: the number of days when rain was greater than 0
    - AverageRainingDays: average amount of rain on rainy days (I think)

The original data did not have the other variables, 
but these variables are included in the `test` data set and therefore are 
features that can be used in the prediction. 
Other than the names, I have no more information about these variables, 
but they all seem related to the fruit (since seeds are inside the fruit).

- fruit set
- fruit mass
- seeds

### Clonesize

```{r clonesize, dependson="data"}
ggplot(d, aes(x = clonesize, y = yield)) +
  geom_point(position = position_jitter(width = 0.5))
```

I thought this was a double, but apparently not. 
I went back and changed how I read the data in. 

```{r, dependson="data"}
d %>%
  group_by(clonesize) %>%
  summarize(
    n = n(),
    mean = mean(yield),
    sd = sd(yield),
    max = max(yield),
    min = min(yield)
  )
```

It seems a bit weird that multiple observations have exactly the max/min values. 
I went back and pointed this out in the [yield](#yield) section. 


### Bee

```{r bee, cache=TRUE, dependson="data"}
bee <- d %>% select(honeybee:osmia, yield)
```

```{r, dependson="bee"}
summary(bee)
```

```{r bee-long, cache=TRUE, dependson="bee"}
bee_long <- bee %>%
  pivot_longer(honeybee:osmia)

ggplot(bee_long, aes(x = value)) +
  geom_histogram() +
  facet_wrap(~name, scales = "free")
```
There is clear discreteness in these data. Presumably this is to due to counting 
the number of individual bees observed and dividing by the number of plants in
the field (or dividing by the number of days). 
Perhaps there were 50 plants, but, in some fields, some plants died?

```{r, dependson="bee_long"}
sort(unique(bee_long$value)) * 50
```
Some of the honeybee observations are much larger than the rest. 

```{r, dependson="bee"}
bee %>% filter(honeybee > 15)
```


```{r, cache=TRUE, dependson="bee"}
pairs(bee)
```

```{r, dependson="bee"}
cor(bee) %>% round(3)
```

### Temperature

Let's take a look at the temperature variables. 

```{r temp, cache=TRUE, dependson="data"}
temp <- d %>% select(MaxOfUpperTRange:AverageOfLowerTRange, yield)
```

```{r, dependson="temp"}
temp_long <- temp %>%
  select(-yield) %>%
  pivot_longer(everything())

ggplot(temp_long, aes(x = value)) +
  geom_histogram() +
  facet_wrap(~name)
```

Again some clear discretization. Basically there are 4-5 unique temperature values
for each temperature variable. 
Probably the fields are in 4-5 spatial locations with the same temperature 
stations. 


```{r, dependson="temp"}
with(d, table(MaxOfUpperTRange, MinOfUpperTRange))
```
Are the temperatures with only a few counts real or typos?

```{r, dependson="temp"}
temp %>%
  # select(MaxOfUpperTRange:AverageOfLowerTRange) %>%
  cor() %>%
  round(3)
```

These are highly correlated and have low correlation with yield. 

Perhaps we should be constructing a variable for `region` of the field based on
temperature. But then we should make sure these same temperatures are 
observed in the `test` data set. 

### Fruit

Let's take a look at the remaining variables. 

```{r fruit, cache=TRUE, dependson="data"}
fruit <- d %>% select(fruitset:yield)
```

```{r, dependson="fruit"}
fruit_long <- fruit %>%
  pivot_longer(-yield)

ggplot(fruit_long, aes(x = value)) +
  geom_histogram() +
  facet_wrap(~name, scales = "free")
```

```{r, dependson="fruit"}
fruit %>%
  cor() %>%
  round(3)
```

```{r, cache=TRUE, dependson="fruit"}
pairs(fruit)
```


# Modeling

I wanted to use the 
[caret package](https://cran.r-project.org/web/packages/caret/)
as it provides a consistent interface to fit a number of different methods.
It also provides automatic parameter tuning. 
Unfortunately, for some methods, it was taking too long to run. 
So, where possible, this package was used, but elsewhere other packages were
utilized with their default settings, i.e. no tuning. 

Here we will set up a train/test split based on the kaggle training data. 
We will utilize 80% of the observations for training and the remaining 20% for 
testing. 

```{r train, cache=TRUE, dependson="data"}
set.seed(20230503)

# Construct our own train and test
u <- sample(c(TRUE, FALSE),
  size = nrow(d),
  prob = c(.8, .2), # 80% for training, 20% for testing
  replace = TRUE
)

train <- d[u, ] %>% select(-id) # remove id to exclude it as an explanatory variable
test <- d[!u, ] %>% select(-id)
```

We can also set up a function to calculate our metric to compare models.
Since the [competition is using mean absolute deviation/error](https://www.kaggle.com/competitions/playground-series-s3e14/overview/evaluation),
we will use this as our metric. 

```{r mad, cache=TRUE, dependson="train"}
mad <- function(p) {
  mean(abs(p - test$yield))
}
```


## LASSO

For all of the methods, we will train in one code chunk and cache the results.

```{r lasso-train, cache=TRUE, dependson="train"}
m_lasso <- caret::train(yield ~ ., data = train, method = "lasso")
```

Then we will predict in another code chunk. 

```{r lasso-predict, cache=TRUE, dependson="lasso-train"}
p_lasso <- predict(m_lasso, newdata = test)
```


## Random forest

```{r rf-train, cache=TRUE, dependson="train"}
m_rf <- randomForest(yield ~ ., data = train)
```

```{r rf-predict, cache=TRUE, dependson="rf-train"}
p_rf <- predict(m_rf, newdata = test)
```


## Neural network



```{r nn-train, cache=TRUE, dependson="train"}
m_nnet <- nnet(yield ~ .,
  data = train,
  size = 5,
  decay = 5e-4,
  rang = .01
) # rang*max(abs(x)) ~= 1

# Code modified from
# https://stats.stackexchange.com/questions/209678/nnet-package-is-it-neccessary-to-scale-data
# m_nnet <- caret::train(yield ~ ., data = train, method = "nnet",
#                        preProcess = c("center","scale"),
#                          trace = FALSE,
#                         tuneGrid = expand.grid(size=1:8, decay=3**(-6:1)),
#                         trControl = trainControl(method = 'repeatedcv',
#                                                 number = 10,
#                                                 repeats = 10,
#                                                 returnResamp = 'final'))
```

```{r nn-predict, cache=TRUE, dependson=c("nn-train","test_normalized")}
# Need to unscale predictions
p_nnet <- predict(m_nnet, newdata = test)
unique(p_nnet)
```
The predictions from this neural network are constant. 
I tried a variety of approaches and, so far, have failed to get anything other
than a constant prediction. 


## xgbTree

```{r xgbtree-train, cache=TRUE, dependson="train", message=FALSE}
m_xgbtree <- xgboost(
  data = train %>% select(-yield) %>% as.matrix(),
  label = train$yield,
  params = list(booster = "gbtree"),
  nrounds = 20, # Increasing this reduces train RMSE
  verbose = 0
)

# This code only increased MAD by 1
# m_xgbtree <- caret::train(yield ~ ., data = train, method = "xgbTree",
#                        verbose = 0)
```

```{r xgbtree-predict, cache=TRUE, dependson="xgbtree-train"}
p_xgbtree <- predict(m_xgbtree, newdata = as.matrix(test %>% select(-yield)))
```

## xgbLinear

```{r xgblinear-train, cache=TRUE, cache=TRUE, dependson="train", message=FALSE}
# m_xgblinear <- xgboost(data = train %>% select(-yield) %>% as.matrix,
#                      label = train$yield,
#                      params = list(booster = "gblinear"),
#                      nrounds = 10, # Increasing this reduces train RMSE
#                      verbose = 0)

m_xgblinear <- caret::train(yield ~ .,
  data = train, method = "xgbLinear",
  verbose = 0
)
```
```{r xgblinear-predict, cache=TRUE, dependson="xgblinear-train"}
p_xgblinear <- predict(m_xgblinear, newdata = as.matrix(test %>% select(-yield)))
```

# Summary

## Comparison

```{r mae, cache=TRUE, dependson=c("mad",paste0(c("lasso","rf","nnet","xgbtree","xgblinear"),"-predict"))}
error <- tribble(
  ~method, ~`test-mad`,
  "lasso", mad(p_lasso),
  "rf", mad(p_rf),
  "nnet", mad(p_nnet),
  "xgbtree", mad(p_xgbtree),
  "xgblinear", mad(p_xgblinear)
)
```

```{r comparison, dependson="mae"}
error |>
  datatable(
    rownames = FALSE,
    caption = "Test Mean Absolute Deviation (MAD)"
  ) |>
  formatRound(columns = c("test-mad"), digits = 0)
```

It seems that tree based (rf and xgbtree) methods out-performed linear methods
(lasso and xgblinear). 
This could be due to some large values in the explanatory variables, 
e.g. honeyee, that would be hugely influential in any regression type of 
analysis. 



## kaggle competition entry

For the kaggle competition, 
we'll take our best performing method and fit it using the entire training 
data and then predict for the test data. 

```{r kaggle-train, cache=TRUE, cache.extra=tools::md5sum("data/train.csv")}
train <- read_csv("data/train.csv") %>% select(-id)

m <- xgboost(
  data = train %>% select(-yield) %>% as.matrix(),
  label = train$yield,
  params = list(booster = "gbtree"),
  nrounds = 20, # Increasing this reduces train RMSE
  verbose = 0
)
```

```{r, kaggle-test, cache=TRUE, dependson="kaggle-train", cache.extra=tools::md5sum("data/test.csv")}
test <- read_csv("data/test.csv")

prediction <- test %>%
  mutate(
    yield = predict(m, newdata = as.matrix(test %>% select(-id))),

    # Since the training yield data seem to have a fixed min/max
    # truncate data to that range
    yield = ifelse(yield > max(train$yield), max(train$yield), yield),
    yield = ifelse(yield < min(train$yield), min(train$yield), yield)
  )

write_csv(prediction %>% select(id, yield),
  file = "submission.csv"
)
```


Our out-of-sample error is estimated to be around 360 based on our study above
and our observed error on the 20% (see below) is around 354. 
This is much larger than other predictions on the 
[leaderboard](https://www.kaggle.com/competitions/playground-series-s3e14/leaderboard?).
Note that the leaderboard is calculated with approximately 20% of the test 
data and the final results will be based on the other 80%. 
Thus, most likely the top predictions on the leaderboard are individuals 
overfitting that 20% and therefore they will perform poorly on the remaining
80%. 


## Feature creation

For these slides, we did not modify the explanatory variables (features) at all. 
But we could consider

1. removing highly correlated features
1. creating additional features

The temperature variables have correlation that is close to 1 and thus we could
eliminate all but one of these variables. 

We could consider adding additional features, e.g. 

- principal components analysis
    - temperatures
    - fruit
- differences in max/min temperature
- total bee density
- total rain
- logarithms of explanatory variables



## Tuning

Some of the methods we tuned parameters to fit our held out test set better. 
For the other methods, we could probably improve our performance if we tuned
parameters. 
This was not done due to time constraints. 

