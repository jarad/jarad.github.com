---
layout: page
title: "Logistic regression"
author: "Jarad Niemi"
date: "`r Sys.Date()`"
header-includes:
- \usepackage{blkarray}
- \usepackage{amsmath}
output: 
  html_document:
      toc: true
      toc_float: true
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[R code](20-logistic_regression.R)

```{r}
library("tidyverse"); theme_set(theme_bw())
library("Sleuth3")
library("ggResidpanel")
library("emmeans")
```

# Logistic Regression

## Model

Let $Y_i$ be a Bernoulli response variable.
Assume
\[ 
Y_i \stackrel{ind}{\sim} Ber(\theta_i), \quad
\eta_i = \mbox{logit}(\theta_i) = \log\left(\frac{\theta_i}{1-\theta_i} \right) = 
\beta_0 + \beta_1X_{i,1} + \cdots + \beta_{p-1} X_{i,p-1}
\]
where $\frac{\theta_i}{1-\theta_i}$ is the *odds*. 

Sometimes observations can be grouped according to common explanatory varaible
values. 
When this is the case, the model can be written using a binomial distribution:
\[ 
Y_i \stackrel{ind}{\sim} Bin(n_i,\theta_i), \quad
\mbox{logit}(\theta_i) = \log\left(\frac{\theta_i}{1-\theta_i} \right) = 
\beta_0 + \beta_1X_{i,1} + \cdots + \beta_{p-1} X_{i,p-1}
\]
Remember that a binomial is simply the sum of a set of Bernoulli random 
variables that and **independent** and have the same probability of success.
Thus these two models are equivalent. 

The inverse of the logistic, i.e. logit, function will be used later:

```{r}
expit <- function(eta) 1/(1+exp(-eta)) # inverse logitistic
```


### glm()

In R, you can fit these models both ways. 
The following data set is recorded in a grouped fashion.

```{r}
case1802
```

Thus, it is natural to fit binomial model

```{r}
# NoCold is success here
m_binomial <- glm(cbind(NoCold, Cold) ~ Treatment, 
                  data = case1802,
                  family = "binomial")
```

Let's wrangle the data into the other format.

```{r}
case1802_individual <- case1802 %>%
  tidyr::pivot_longer(-Treatment, names_to = "Result", values_to = "n") %>%
  tidyr::uncount(n) %>%
  mutate(Result = factor(Result))

table(case1802_individual)
```
Now we can analyze this data set

```{r}
m_bernoulli <- glm(Result ~ Treatment,
                   data = case1802_individual,
                   family = "binomial")
```


## Parameter estimation

The maximum likelihood estimator for this model is not available in closed form
and thus we use an iterative algorithm to find the answers. 
But we don't have to worry about these details. 

```{r}
coef(m_bernoulli)
```
This MLE can be used to determine asymptotic confidence intervals.

```{r}
confint(m_bernoulli)
```
The fact that the interval for the coefficient for the dummy variable for 
vitamin C does not include 0 provides evidence that,
if the remainder of the model is correct, 
that it is unlikely the coefficient is 0. 


## Testing

We can also compute p-values for a single parameter being zero or a collection
of parameters being zero. 

```{r}
summary(m_bernoulli)$coefficients
```

A small p-value here indicates evidence against the model that has that 
coefficient equal to 0. 
But remember that other aspects of the model could be problematic, 
e.g. independence, constant probability (within a group), etc. 

The p-value above is based on a Wald test. 
An alternative is based on a likelihood ratio test. 

```{r}
anova(m_bernoulli, test = "LRT")
```

Unlike with the linear regression (which is based on the normal distribution)
the p-values here don't match exactly. 



## Predictions

When computing predictions, we need to be explicit about whether we want 
predictions on the linear part, i.e. $\eta_i$, or the response, 
i.e. $\theta_i = E[Y_i]$. 
By default, the `predict()` function provides predictions on the linear part. 

```{r}
nd <- data.frame(Treatment = unique(case1802_individual$Treatment))

p <- nd %>%
  mutate(prediction = predict(m_bernoulli, newdata = .))
p
```

We can manually compute predictions on the response. 

```{r}
p %>% mutate(probability = expit(prediction))
```

Alternatively, we can use `predict()` to compute these probabilities 
directly. 

```{r}
p %>% mutate(probability = predict(m_bernoulli, newdata = ., type = "response"))
```

You can also obtain standard errors which can be used to construct asymptotic
confidence intervals. 

```{r}
cbind(nd, predict(m_bernoulli, newdata = nd, se.fit = TRUE)) %>%
  mutate(
    lcl = fit - qnorm(.975)*se.fit,
    ucl = fit + qnorm(.975)*se.fit,
    
    # convert to probability
    prob  = expit(fit),
    p.lcl = expit(lcl),
    p.ucl = expit(ucl)
  ) %>%
  select(-residual.scale)
```



### emmeans()

Another alternative is to use the `emmeans::emmeans()` function. 

```{r}
emmeans(m_bernoulli, ~ Treatment, type = "response")
```

This can also be used to perform the comparisons, e.g. 

```{r}
emmeans(m_bernoulli, pairwise ~ Treatment, type = "response")
```
The odds ratio is the simplest interpretation of a logistic regression model. 


## Diagnostics

It is much more difficult to assess model assumptions in logistic regression 
models.

```{r}
resid_panel(m_binomial)
resid_panel(m_bernoulli)
```



# Examples

## O-ring Failure

```{r}
summary(ex2011)
```

```{r}
m <- glm(Failure ~ Temperature, 
         data = ex2011,
         family = "binomial")
```

```{r}
summary(m)
```

```{r}
p <- bind_cols(ex2011, 
               predict(m, newdata = ex2011, se.fit = TRUE)) %>%
  mutate(
    lcl = fit - qnorm(.975)*se.fit,
    ucl = fit + qnorm(.975)*se.fit
  )
```

```{r}
ggplot(p, aes(x = Temperature, y = fit, ymin = lcl, ymax = ucl)) + 
  geom_ribbon(alpha = 0.5) +
  geom_line()
```

This prediction is on the linear predictor scale, i.e. $\eta$. 
But this is not very interpretable. 
Thus, we should convert to the probability scale. 

```{r}
p <- p %>%
  mutate(
    fit = expit(fit),
    lcl = expit(lcl),
    ucl = expit(ucl)
  )

ggplot(p, aes(x = Temperature, y = fit, ymin = lcl, ymax = ucl)) + 
  geom_ribbon(alpha = 0.5, fill = "gray") +
  geom_line(color = "blue") +
  labs(
    y = "Probability",
    title = "Probability of O-ring Failure vs Temperature"
  ) +
  geom_jitter(aes(y = 1*(Failure == "Yes")),
              height = 0)
```


## Bird keeping

This analysis intends to understand the relationship between bird keeping and 
lung cancer. 
But since we know smoking is related to lung cancer, 
we need to adjust (or control for) smoking to understand the effect of bird
keeping.

The actual data here are from a case-control study where cases are found that
(in some sense) are a match to the controls.

```{r}
summary(case2002)
```
In logistic regression models, it is often helpful to construct a binary variable
so that it is clear what "success" means. 

```{r}
d <- case2002 %>%
  mutate(LungCancer   = LC == "LungCancer",
         
         BirdKeeping  = ifelse(BK == "Bird", "Yes", "No"),
         BirdKeeping  = factor(BirdKeeping, levels = c("Yes","No")),
         
         SmokingYears = YR) %>%
  
  select(LungCancer, BirdKeeping, SmokingYears)

summary(d)
```



```{r}
m <- glm(LungCancer ~ BirdKeeping * SmokingYears, 
         data = d,
         family = "binomial")
```

Attempt to assess assumptions

```{r}
resid_panel(m)
```

Model selection

```{r}
summary(m)
anova(m, test = "Chi")
```

Remove interaction

```{r}
m <- glm(LungCancer ~ BirdKeeping + SmokingYears, 
         data = d,
         family = "binomial")

summary(m)
```

```{r}
em <- emmeans(m, pairwise ~ BirdKeeping, type = "response")
confint(em)
```
Construct a plot to help understand the relationship between 
bird keeping and years of smoking on the probability of lung cancer. 

```{r}
nd <- expand.grid(
  BirdKeeping  = unique(d$BirdKeeping),
  SmokingYears = seq(min(d$SmokingYears), max(d$SmokingYears), length = 1001)
)

p <- bind_cols(nd, predict(m, newdata = nd, se.fit = TRUE)) %>%
  mutate(
    lcl = fit - qnorm(.975)*se.fit,
    ucl = fit + qnorm(.975)*se.fit,
    
    # Convert to probability scale
    prob = expit(fit),
    lcl  = expit(lcl),
    ucl  = expit(ucl)
  )

ggplot(p, aes(x = SmokingYears, 
              y = prob, ymin = lcl, ymax = ucl,
              fill = BirdKeeping, color = BirdKeeping)) + 
  geom_ribbon(alpha = 0.5) + 
  geom_line() +
  ylim(0,1) +
  labs(y = "Probability of lung lancer",
        x = "Number of years of smoking",
        title = "Effect of Bird Keeping on Lung Cancer",
        subtitle = "Controlling for Years of Smoking",
       color = "Bird keeping",
       fill = "Bird keeping") +
  theme(legend.position = "bottom")
```

This is not necessarily the end of the analysis as there are a number of other
variables in this data set that could be considered. 
A quick analysis suggests that maybe we have considered the most important 
variables. 

```{r}
m <- glm(LC ~ ., data = case2002, family = "binomial")
drop1(m, test = "LRT")
```



## Vehicle accidents

```{r}
m <- glm(Cause ~ Make*Passengers*VehicleAge, data = ex2018, family = "binomial")
anova(m, test = "LRT")
```

```{r}
m <- glm(Cause ~ Make * Passengers + VehicleAge, 
         data = ex2018,
         family = "binomial")

nd <- expand.grid(
  Make  = unique(ex2018$Make),
  Passengers = seq(min(ex2018$Passengers), 
                   max(ex2018$Passengers)),
  VehicleAge = seq(min(ex2018$VehicleAge),
                   max(ex2018$VehicleAge))
  )

p <- bind_cols(nd, 
               predict(m, newdata = nd, se.fit = TRUE)) %>%
  mutate(
    lcl = fit - qnorm(.975)*se.fit,
    ucl = fit + qnorm(.975)*se.fit,
    
    fit = expit(fit),
    lcl = expit(lcl),
    ucl = expit(ucl)
  )

ggplot(p, aes(x = Passengers, 
              y = fit, 
              ymin = lcl,
              ymax = ucl,
              fill = Make,
              color = Make,
              linetype = Make)) + 
  geom_ribbon(alpha = 0.5) +
  geom_line() + 
  facet_wrap(~VehicleAge, 
             labeller = label_both) +
  theme(legend.position = "bottom") + 
  labs(
    y = "Probability",
    title = "Probability a Car Accident is Caused by Tire Failure"
  )
```
