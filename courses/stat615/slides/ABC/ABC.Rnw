\documentclass[handout]{beamer}

\usepackage{verbatim,chemarr}

\graphicspath{{figs/}}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}



\title{Approximate Bayesian Computation}
\author{Jarad Niemi}
\institute[ISU]{Iowa State University}
\date{\today}

\begin{document}

%\section{Temp??} \begin{comment}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE,
               cache=TRUE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE>>=
library(plyr)
library(dplyr)
library(reshape2)
library(ggplot2)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@



\frame{\maketitle}

\frame{\frametitle{Outline}
	\begin{itemize}
%	\item Likelihood-based inference
	\item Stochastic kinetic models
%	\item Computational statistics
	\item Approximate Bayesian computation
	\end{itemize}
}


\section{Likelihood-based inference}

\begin{comment}

\subsection{Probability functions}
\frame{\frametitle{}
	Mathematical statistics is primarily concerned with random variables. \pause
	
	\begin{definition}
	A \alert{random variable} is a variable whose value is not known, but whose probability distribution is known. \pause
	\end{definition}
	\begin{definition}
	A \alert{probability distribution} describes the range and relative probabilities of a realization of a random variable. \pause 
	\begin{itemize}
	\item A \alert{probability mass function} describes the probability of each value of a discrete random variable. \pause
	\item A \alert{probability density function} describes the probability for each range of a continuous random variable. 
	\end{itemize}
	\end{definition}
}

\subsection{Examples}
\frame{\frametitle{}
	%\begin{tabular}{cc}
	%\includegraphics{Dice}
	%\end{tabular}
	For example, \pause 
	\begin{itemize}
	\item An unbiased six-sided die, has a uniform discrete distribution over which number is rolled. \pause
	\[ p(d) = P(D=d) =1/6, \quad d=1,2,3,4,5,6 \]
	
	\vspace{0.1in} \pause
	
	\item Which reaction occurs in a Gillespie simulation, has a discrete distribution over which reaction occurs.\pause
	\[ p(k) = P(K=k) =a_k(x)/a_0(x), \quad k=1,2,\ldots,M \]
	
	\vspace{0.1in} \pause
	
	\item Pseudo-random number generators provide pseudo-random uniform values in the interval (0,1). \pause
	\[ p(u) =\mathrm{I}(0<u<1) \]
	
	\vspace{0.1in} \pause
	
	\item The time to the next reaction $\tau$ in the Gillespie algorithm has an exponential distribution. \pause
	\[ p(\tau) =\lambda\exp(-\lambda \tau) , \quad x>0 \]
	\end{itemize}
}

\subsection{Bayesian inference}
\frame{\frametitle{Likelihood-based inference}
	\begin{itemize}
	\item Input: 
		\begin{itemize}
		\item<2-> Data: $y$, 
		\item<3-> Model: $p(y|\theta)$, likelihood: $L(\theta)\propto p(y|\theta)$
		\item<5-> Prior: $p(\theta)$
		\end{itemize}
	\item<4-> Maximum-likelihood estimation
	\[ \hat{\theta}=\mbox{argmax}_\theta L(\theta)=\mbox{argmax}_\theta p(y|\theta)  \]
	\item<6-> Bayesian inference
	 	\begin{itemize}
		\item<7-> Posterior: $p(\theta|y)$
		\item<8-> Bayes' Theorem gives:
		\[ p(\theta|y) = \frac{p(y|\theta) p(\theta)}{p(y)} \propto p(y|\theta) p(\theta) \]
		\end{itemize}
	\end{itemize}
}

\end{comment}

\section{Stochastic kinetic models}
\subsection{Terminology}
\begin{frame}
\frametitle{Stochastic kinetic models}
	\setkeys{Gin}{width=0.8\textwidth}
	Imagine a \emph{well-mixed} system in \emph{thermal equilibrium} with
	\begin{itemize}[<+->]
	\item $N$ species: $S_1,\ldots,S_N$ with
	\item number of molecules $X_1,\ldots,X_N$ with elements $X_j\in\mathbb{Z}^+$
	\item which change according to $M$ reactions: $R_1,\ldots,R_M$ with
	\item propensities $a_1(x),\ldots,a_M(x)$.  
	\item The propensities are given by $a_j(x) = \theta_j h_j(x)$ 
	\item where $h_j(x)$ is a known function of the system state. 
	\item If reaction $j$ occurs, the state is updated by the stoichiometry $\nu_j$ with 
	\item elements $\nu_{ij}\in\{-2,-1,0,1,2\}$, i.e. reaction orders 0,1, and 2.
	\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Michaelis-Menton System}

The Michaelis-Menton system has $N=4$ species: 
\begin{itemize}
\item Substrate (S), 
\item Enzyme (E),
\item Substrate-Enzyme Complex (SE), and 
\item Product (P).
\end{itemize}
\pause
The $M=3$ reactions as well as their propensities and stoichiometries are 

\vspace{0.1in} 

\begin{center}
\begin{tabular}{rll|rrrr}
&&& \multicolumn{4}{c}{Stoichiometry} \\
\multicolumn{2}{c}{Reaction} & Propensity & S & E & SE & P \\
\hline
S + E &\longrightarrow SE & \theta_1 $X_S X_E$ & -1 & -1 & 1 & \\
SE & \longrightarrow S + E & \theta_2 $X_{SE}$ & 1 & 1 & -1 & \\
SE & \longrightarrow P+E & \theta_3 $X_{SE}$ &  & 1 & -1 & 1 \\
\hline
\end{tabular}
\end{center}

\vspace{0.1in} \pause

where $\theta = (\theta_1,\theta_2,\theta_3)$ is the parameter of interest.

\end{frame}


\begin{frame}
\frametitle{Michaelis-Menton snapshot}
<<>>=
species = c('Substrate',
            'Enzyme',
            'Substrate-Enzyme Complex',
            'Product')

d = ddply(data.frame(species=factor(species, levels=species),
                     number = c(100,5,5,2)),
          .(species), function(d) 
          {
            data.frame(x=runif(d$n), y=runif(d$n))
          })

ggplot(d, aes(x,y,col=species, shape=species)) +
  geom_point() + 
  theme(legend.position='bottom') +
  scale_x_continuous(expand=c(0,0)) + scale_y_continuous(expand=c(0,0))
@
\end{frame}



\subsection{Gillespie algorithm}
\begin{frame}
\frametitle{Gillespie algorithm}
	\begin{itemize}[<+->]
	\item If reaction $j\in\{1,\ldots,M\}$ has the following probability
	\[ \lim_{dt\to 0} P(\mbox{reaction $j$ within the interval $(t,t+dt)$}|X_t) = a_j(X_t) dt,  \]
	\item[] then this defines a \alert{continuous-time Markov jump process}.
	\item Then a realization from this model can be obtained using the Gillespie algorithm:
		\begin{enumerate}
		\item For $j\in\{1,\ldots,M\}$, calculate $a_j(X_t)$.
		\item Calculate $a_0(X_t) = \sum_{j=1}^M a_j(X_t)$.
		\item Simulate a reaction time $\tau \sim Exp(a_0(X_t))$ 
		\item Simulate a reaction id $k\in\{1,\ldots,M\}$ with probability $a_k(X_t)/a_0(X_t)$
    \item Update $X$ according to $v_k$ and time by $\tau$. 
		\end{enumerate}
	\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Michaelis-Menton Gillespie Simulation}
<<michaelis_mention_gillespie>>=
library('smfsb')
data(spnModels)
# simulate a realisation of the process and plot it
out = gillespie(MM,500)
out$t = c(0,out$t)
colnames(out$x) = names(MM$M)

d = cbind(time=out$t, as.data.frame(out$x))
ggplot(melt(d, id.var='time', variable.name='species', value.name='count'),
       aes(time,count,color=species,linetype=species)) + 
  geom_step() +
  theme(legend.position='bottom') 
@
\end{frame}



\subsection{Complete observations}
\begin{frame}
\frametitle{Complete observations}
	Suppose you observe all system transitions:\pause
	\begin{itemize}
	\item $n$ reactions occur in the interval $[0,T]$ \pause
	\item $t_1,\ldots,t_n$ are the reaction times \pause
	\item $r_1,\ldots,r_n$ are the reaction indicators, $r_i\in\{1,\ldots,M\}$
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	Then inference can be performed based on the likelihood
	\[ 
	L(\theta) \propto \prod_{j=1}^M \theta_j^{n_j} \exp\left(-\theta_j I_j \right) 
	\]
	\pause where
	\[ \begin{array}{lll}
	n_j &= \sum_{i=1}^n \mathrm{I}(r_i=j) & \mbox{\# of $j$ reactions} \\
	\pause \\
	I_j &= \int_0^T h_j(X_t) dt \pause &= \sum_{i=1}^n h_j(X_{t_{i-1}}) (t_i-t_{i-1}) \pause + h_j(X_{t_n})[T- t_n]
	\end{array} \]
\end{frame}

\subsection{Inference}
\begin{frame}
\frametitle{Inference}
	\begin{itemize}
	\item Maximum likelihood estimation \pause
	\[ \hat{\theta}_j = \frac{n_j}{I_j} \]
	\item \pause Conjugate Bayesian inference \pause
	\[ \begin{array}{ll}
	p(\theta) &= \prod_{j=1}^M Ga(\theta_j; \alpha_j,\beta_j) \\
	\pause \\
	p(\theta|X) &= \prod_{j=1}^M Ga(\theta_j; \alpha_j+n_j, \beta_j+I_j) \\
	\pause \\
	E[\theta_j|X] &= \frac{\alpha_j+n_j}{\beta_j+I_j}
	\end{array} \]
	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Michaelis-Menton Complete Data Inference}
<<michaelis_mention_complete_data_inference, dependson='michaelis_mention_gillespie'>>=
# These simulations end after n events, so therefore there is no 
# time at the end of the simulation where no event happens
n_rxn = function(x, sys) {
  stoich = sys$Post - sys$Pre
  n_rxn = rep(0,nrow(stoich))
  
  for (i in 2:nrow(x)) {
    v = x[i,] - x[i-1,]
    for (j in 1:nrow(stoich)) 
      if (all(v == stoich[j,])) n_rxn[j] = n_rxn[j] + 1
  }
  return(n_rxn)
}

d = data.frame(reaction = c('S+E->E','S+E<-SE', 'SE->P'),
               a = 1+n_rxn(out$x, MM),
               b = 1+colSums(adply(out$x, 1, MM$h)[,-1]))
p = ddply(d, .(reaction), function(dd) {
  data.frame(x=seq(0,.2,by=.001)) %>% 
    mutate(density = dgamma(x, dd$a, dd$b))
})

ggplot(p, aes(x, density, color=reaction, linetype=reaction)) + 
  geom_line() +
  theme(legend.position='bottom')
@
\end{frame}

\subsection{Discrete observations}
\begin{frame}
\frametitle{Discrete observations}
	Suppose you only observe the system at discrete-times: \pause
	\begin{itemize}
	\item For simplicity, observe the system at times $t=1,2,\ldots,T$. \pause
	\item At these times, we observe $y_t=X_t$ the system state. \pause
	\item But do not observe the system between these times. \pause
	\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Michaelis-Mention discrete observations}
<<michaelis_mention_discrete_observations, dependson='michaelis_mention_gillespie'>>=
d = as.data.frame(discretise(out,dt=1))
names(d) = names(MM$M)
d$time = as.numeric(rownames(d))-1

g = ggplot(melt(d, id.var='time', variable.name='species', value.name='count'), 
       aes(time, count, color=species, shape=species)) + 
  geom_point() + 
  theme_bw() + 
  theme(legend.position='bottom')
print(g)
@
\end{frame}


\subsection{Inference}
\begin{frame}
\frametitle{Inference}
	Inference is still performed based on the likelihood
	\[ L(\theta) = p(y|\theta) \pause = p(t,y)  \]
	\pause but this is the solution to the \alert{chemical master equation}
	\[
	\frac{\partial}{\partial t}p(t,y) = \sum_{j=1}^M \big(a_j(y-v_m)p(t,y-v_m)- a_j(y)p(t,y)\big)
	\]
	
	\vspace{0.2in}
	
	\pause For constitutive production $h(X_t)=1$ and $a(X_t)=\theta$, \pause
	we still have 
	\[ L(\theta) \propto  \theta^{n} \exp\left(-\theta I \right) \]
	\pause with 	\[ n= y_T-y_0 \qquad I = \int_0^T 1 dt = T \]
\end{frame}




\subsection{Reversible isomerization}
\begin{frame}
\frametitle{Reversible isomerization}

<<fig.height=4, warning=FALSE>>=
g + xlim(30,31)
@

	
	\begin{itemize}
	\item How many reactions occurred in the interval $[30,31]$? \pause
	\item What is $\int_{30}^{31} X_{SE} dt$?
	\end{itemize}
\end{frame}

\subsection{Summary}
\begin{frame}
\frametitle{Summary}
	\begin{itemize}
	\item	With complete observations and independent gamma priors, the posterior is
	\[ p(\theta|X) = \prod_{j=1}^M Ga(\theta_j; \alpha_j+n_j, \beta_j+I_j) \]
	where 
	\[ \begin{array}{ll}
	n_j &= \sum_{i=1}^n \mathrm{I}(r_i=j) \\
	I_j &= \int_0^T h_j(X_t) dt = \sum_{i=1}^n h_j(X_{t_{i-1}}) (t_i-t_{i-1}) + h_j(X_{t_n})[T- t_n]
	\end{array} \]
	
	\vspace{0.2in} \pause
	
	\item For discrete observations, the likelihood is analytically intractable and therefore no closed form exists for the posterior (or MLEs). 
	\end{itemize}
\end{frame}

\section{Sampling methods}
\begin{frame}
\frametitle{The idea}
	\begin{itemize}[<+->]
	\item But we can simulate from the model using the Gillespie algorithm!!
	\item Intuitively, if we 
		\begin{enumerate}
		\item pick a set of parameters,
		\item simulate a realization using these parameters,
		\item and it matches our data, 
		\item then these parameters should be reasonable.
		\end{enumerate}
  \end{itemize}
  
  \vspace{0.2in} \pause
  
  Our goal is to formalize this through
  \begin{enumerate}
  \item Rejection sampling
  \item Gibbs sampling
  \end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Simulations from the prior}
<<simulations_from_prior, dependson='<michaelis_mention_discrete_observations',warning=FALSE>>=
r = rdply(10, {
  g = gillespie(MM,300)
  cbind(data.frame(time=c(0,g$t), g$x))
}) 
names(r)[3:6] = names(MM$M)
r$.n = as.factor(r$.n)

g = ggplot(subset(r,time<=1), 
       aes(time, P)) +
  geom_line(aes(linetype=.n,group=.n)) + 
  geom_point(data = subset(d, time<=1), 
             aes(time, P,color='orange')) +
  theme_bw() + 
  theme(legend.position='none')
print(g)
@
\end{frame}



\subsection{Rejection sampling}
\begin{frame}
\frametitle{Rejection sampling}
	Our objective is samples from the posterior 
	\[ \begin{array}{rl} 
  p(\theta|y) \pause &=\int p(\theta,X|y) dX \pause \propto \int p(y|X)p(X|\theta)p(\theta) dX \pause \\
	&= \int \prod_{t=1}^n \mathrm{I}(y_t=X_t)  p(X|\theta)p(\theta) dX
	\end{array} \]
  
	\vspace{0.2in} \pause 
  
  A rejection sampling procedure is \pause
	\begin{enumerate}
	\item Sample $\theta\sim p(\theta)$. \pause
	\item Sample $X\sim p(X|\theta)$ a.k.a. Gillespie \pause
	\item If $y_t=X_t$ for $t=1,2,\ldots,T$, then \pause
	\item $\theta$ is a sample from $p(\theta|y)$ and \pause
	\item $\theta,X$ is a sample from $p(\theta,X|y)$. 
	\end{enumerate}
\end{frame}

\subsection{Gibbs sampling}
\begin{frame}
\frametitle{Gibbs sampling}
	Our objective is samples from the posterior 
	\[ p(\theta|y) =\int p(\theta,X|y) dX \propto \int p(y|X)p(X|\theta)p(\theta) dX \]
	
  \vspace{0.2in} \pause 
  
  A Gibbs sampling procedure is \pause
	\begin{enumerate}
	\item Start with $\theta^{(0)},X^{(0)}$ \pause
	\item For $k=1,\ldots,K$, 
		\begin{enumerate}[a.]
		\item Sample $\theta^{(k)}\sim p(\theta|X^{(k-1)})$ \pause
		\item Sample $X^{(k)}\sim p(X|\theta^{(k)},y)$ a.k.a. rejection sampling \pause
		\end{enumerate}
	\end{enumerate}
	
	\vspace{0.2in} \pause
	
	$\theta^{(k)},X^{(k)}$ converge to samples from $p(\theta,X|y)$ \pause
\end{frame}






\section{Approximate Bayesian computation}
\begin{frame}
\frametitle{An approximate posterior}
	\begin{itemize}
	\item Intuitively, if we 
		\begin{enumerate}
		\item pick a set of parameters,
		\item simulate a realization using these parameters,
		\item and it \alert{is similar to} our data, 
		\item then these parameters should be reasonable. \pause \pause
		\end{enumerate}

	\vspace{0.2in} \pause
		
	\item We can formalize this using
		\begin{itemize}
		\item Approximate Bayesian computation
		\end{itemize}
	\end{itemize}
\end{frame}




\subsection{The Approximation}
\frame{\frametitle{Approximate Bayesian computation (ABC)}
	Our \only<2->{\alert{approximate }}objective is samples from the posterior 
	\[ p(\theta|\alt<1>{y}{\rho\le \epsilon}) \pause\pause =\int p(\theta,X|\rho\le \epsilon) dX \pause \propto \int \mathrm{I}(\rho\le \epsilon)p(X|\theta)p(\theta) dX \]
	\pause where $\rho=\rho(y,X)$ is a measure of the difference between your data $y$ and simulations $X$.
	
	\vspace{0.2in} \pause
	
	\begin{itemize}[<+->]
	\item Choice of $\epsilon$ reflects tension between computability and accuracy.
		\begin{itemize}
		\item As $\epsilon\to\infty$, 
			\begin{itemize}
			\item $p(\theta|\rho\le\epsilon)\stackrel{d}{\to} p(\theta)$
			\item acceptance probability converges to 1
			\end{itemize}
		\item As $\epsilon\to0$, 
			\begin{itemize}
			\item $p(\theta|\rho\le\epsilon)\stackrel{d}{\to}p(\theta|y)$
			\item acceptance probability decreases
			\end{itemize}
		\end{itemize}
	\end{itemize}
}




\subsection{ABC rejection sampling}
\begin{frame}
\frametitle{ABC rejection sampling}
	\setkeys{Gin}{width=\textwidth}
	Let $\rho=\sum_{t=1}^n |y_t-X_t|$ and $\epsilon = n$, 
	

	\vspace{0.2in} \pause 
  
  An ABC rejection sampling procedure is \pause
	\begin{enumerate}
	\item Sample $\theta\sim p(\theta)$ \pause
	\item Sample $X\sim p(X|\theta)$ a.k.a. Gillespie \pause
	\item If $\rho(y,X)\le \epsilon$, then \pause
	\item $\theta$ is a sample from $p(\theta|\rho\le\epsilon)$ and \pause
	\item $\theta,X$ is a sample from $p(\theta,X|\rho\le\epsilon)$. \pause
	\end{enumerate}
\end{frame}







\subsection{ABC Gibbs sampling}
\begin{frame}
\frametitle{ABC Gibbs sampling}
	Let $\rho=\sum_{t=1}^n |y_t-X_t|$ and $\epsilon = n$, 


	\vspace{0.2in} \pause 
  
  A Gibbs sampling procedure is \pause
	\begin{enumerate}
	\item Start with $\theta^{(0)},X^{(0)}$ \pause
	\item For $k=1,\ldots,K$, 
		\begin{enumerate}[a.]
		\item Sample $\theta^{(k)}\sim p(\theta|X^{(k-1)})$ \pause
		\item Sample $X^{(k)}\sim p(X|\theta^{(k)},\rho\le\epsilon)$ a.k.a. rejection sampling \pause
		\end{enumerate}
	\end{enumerate}
	
	\vspace{0.2in}
	
	$\theta^{(k)},X^{(k)}$ converge to samples from $p(\theta,X|\rho\le\epsilon)$ \pause
	
\end{frame}




\subsection{Gibbs sampling example}
\frame{\frametitle{Michaelis-Menton system}
\[
E + S \xrightleftharpoons[\theta_2]{\theta_1} ES \stackrel{\theta_3}{\longrightarrow} E + P 
\]

\pause 

\begin{table}[htb]
\begin{center}
\begin{minipage}{0.85\textwidth}
\caption{Measurements taken from a simulated Michaelis-Mention system with parameters $\theta_1=0.001$, $\theta_2=0.2$, and $\theta_3=0.1$.}
\label{tab:data}
\end{minipage}
\begin{tabular}{|l|rrrrrrrrrrr|}
\hline
Time & 0 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\
\hline
$E$ & 120 & 71 & 76 & 81 & 80 & 90 & 90& 104 & 103 & 109 & 109 \\
$S$ & 301 & 219 & 180 & 150 & 108 & 86 & 61 & 52 & 35 & 29 & 22 \\
\hline
\end{tabular}
\end{center}
\end{table}
}

\frame{\frametitle{}
	\setkeys{Gin}{width=\textwidth}
	With $\epsilon=0$ (i.e. draws from $p(\theta|y)$),
	
	\vspace{0.2in}
	
	\begin{columns}
	\begin{column}{0.5\textwidth}
	\includegraphics{Michaelis-Menton-parameter-posteriors} \pause
	\end{column}
	\begin{column}{0.5\textwidth}
	\includegraphics{Michaelis-Menton-trajectory-posteriors}
	\end{column}
	\end{columns}
}

\frame{\frametitle{}
	\setkeys{Gin}{width=0.8\textwidth}
	Since rejection sampling is inherently parallel, run this algorithm on a graphical processing unit: \pause 
	
	\vspace{0.2in}
	
	\begin{center}
	\includegraphics{Michaelis-Menton-CPUvsGPU}
	\end{center}
}

\section{Summary}
\frame{\frametitle{Summary}
	\begin{itemize}[<+->]
	\item Bayesian inference in discretely observed SCKMs
		\begin{itemize}
		\item Goal: $p(\theta|y)\propto p(y|\theta)p(\theta)$
		\item Likelihood, $L(\theta)=p(y|\theta)$, is analytically intractable 
		\item Sampling methods are required, e.g. rejection and/or Gibbs 
		\item Acceptance rate can be unacceptably low
		\end{itemize}
	\item Approximate Bayesian computation (ABC) in SCKMs
		\begin{itemize}
		\item Goal: $p(\theta|\rho\le\epsilon)\propto p(\rho\le\epsilon|\theta)p(\theta)$
		\item $\rho=\rho(y,X)$ measures the difference between data and a simulation
		\item $\epsilon$ balances computability with accuracy
		\item Readily accommodates bounded errors, e.g. $y_t=X_t\pm \epsilon$
		\end{itemize}
	\item ABC generally
		\begin{itemize}
		\item More general than SKMs, e.g. phylogenetic trees
		\item Building $\rho$ is an art, often use sufficient statistics of the data
		\item Not useful for unbounded errors, e.g. $y_t=X_t+\epsilon_t, \epsilon_t\sim N(0,\sigma^2)$
		\item Current debate about usefulness for model selection
		\end{itemize}
	\end{itemize}
}


\end{document}


