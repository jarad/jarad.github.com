\documentclass[handout,aspectratio=169]{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}
\usepackage{animate}
\usepackage{verbatim}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Hamiltonian Monte Carlo}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}
\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=10, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE,
               cache=TRUE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE>>=
library(animation)
library(plyr)
library(ggplot2)
library(reshape2)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle
Adapted from \href{https://arxiv.org/abs/1206.1901}{Radford Neal's MCMC Using Hamltonian Dynamics} in Handbook of Markov Chain Monte Carlo (2011).}

\section{Hamiltonian dynamics}
\begin{frame}
\frametitle{Hamiltonian system}

Considering a body in a frictionless 1-dimensional environment with  
\begin{itemize}
\item mass $m$,
\item location $\theta$, and
\item momentum $\omega = mv$ where $v$ is velocity.
\end{itemize}
\pause
The mass has 
\begin{itemize}
\item potential energy $U(\theta)$, proportional to its height, and
\item kinetic energy $K(\omega)= \frac{1}{2}mv^2 =\frac{1\omega^2}{2m}$. 
\end{itemize}
\end{frame}


\subsection{Sir William Rowan Hamilton's equations}
\begin{frame}
\frametitle{Hamilton's equations}

Extending this to $d$ dimensions, 
we have 
\begin{itemize}
\item position vector $\theta$ and 
\item momentum vector $\omega$.
\end{itemize}
\pause
The Hamiltonian $H(\theta,\omega)$ describes the time evolution of the system through 
\[ \begin{array}{rl}
\frac{d\theta_i}{dt} &= \phantom{-}\frac{\partial H}{\partial \omega_i} \\ \\
\frac{d\omega_i}{dt} &= -\frac{\partial H}{\partial \theta_i}
\end{array} \]
for $i=1,\ldots,d$. 
\end{frame}


\begin{frame}
\frametitle{Potential and kinetic energy}

\small

For Hamiltonian Monte Carlo, 
we usually use Hamiltonian functions that can be written as follows:
\[ 
H(\theta,\omega) = U(\theta) + K(\omega)
\]
\pause
where 
\begin{itemize}
\item $U(\theta)$ is called the potential energy and will be defined to be minus the log probability density of the distribution for $\theta$ (plus any constant that is 
convenient) and 
\item $K(\omega)$ is called the kinetic energy and is usually defined as 
\[ 
K(\omega) = \omega^\top M^{-1} \omega / 2
\]
where $M$ is a symmetric, positive-definite ``mass matrix'', 
\pause which is 
typically diagonal, and is often a scalar multiple of the identity matrix.
This form for $K(\omega)$ corresponds to minus the log probability density 
(plus a constant) of the zero-mean Gaussian distribution with covariance
matrix $M$.
\end{itemize}

\pause

The resulting Hamilton's equations are 
\[ 
\frac{d\theta_i}{dt} = [M^{-1}\omega]_i, \qquad
\frac{d\omega_i}{dt} = -\frac{\partial U}{\partial \theta_i}.
\]
\end{frame}



\subsection{One-dimensional example}
\begin{frame}
\frametitle{One-dimensional example}

Suppose 
\[ 
H(\theta,\omega) = U(\theta) + K(\omega), \quad U(\theta) = \theta^2/2, \quad K(\omega) = \omega^2/2
\]
\pause
The dynamics resulting from this Hamiltonian are
\[ 
\frac{d\theta}{dt} = \omega, \quad \frac{d\omega}{dt}=-\theta.
\]
\pause
Solutions are of the form
\[ 
\theta(t) = r\, cos(a+t),\quad \omega(t) = -r\, sin(a+t)
\]
for some constants $r$ and $a$ that depend on initial conditions.
\end{frame}


\begin{frame}[fragile]
\frametitle{One-dimensional example simulation}

<<oned_sim, fig.show='animate', interval=.1>>=
tt <- seq(0,2*pi, length=21)
theta <- cos(tt); omega = -sin(tt)

draw_step = function(i, e=.01) {
  opar = par(mfrow=c(1,2))
  curve(x^2/2, -1, 1, xlab='Position', ylab='Potential Energy')
  points(theta[i], theta[i]^2/2+e, pch=19)
  curve(x^2/2, -1, 1, xlab='Momentum', ylab='Kinetic Energy')
  points(omega[i], omega[i]^2/2+e, pch=19)
  par(opar)
}

# 
for (i in 1:length(tt)) {
  draw_step(i,0)
}
@
\end{frame}


\subsection{Conservation of the Hamiltonian}
\begin{frame}
\frametitle{Conservation of the Hamiltonian}

This simple model clearly conserves the Hamiltonian:
\[ \begin{array}{rl}
H(\theta,\omega) &= U(\theta) + K(\omega) \pause \\
&= \frac{\theta^2}{2} + \frac{\omega^2}{2} \pause \\
&= \frac{[r\, cos(a+t)]^2}{2} + \frac{[-r\, sin(a+t)]^2}{2} \pause \\
&= \frac{r^2}{2} \left[ cos^2(a+t) + sin^2(a+t) \right] \pause \\
&= r^2/2 
\end{array} \]
\end{frame}

\begin{frame}[fragile]
\frametitle{Conservation of the Hamiltonian}

\vspace{-0.5in}

<<oned_volume, fig.show='animate', interval=.1, fig.asp=1, fig.width=4, out.width='.5\\linewidth'>>=
t3 <- seq(0,2*pi, length=1000)
d <- data.frame(x = cos(t3), y=-sin(t3))
for (i in 1:length(tt)) {
  plot(y~x, d, type='l', col='gray', xlab='Location', ylab='Momentum')
  points(theta[i], omega[i], pch=19)
}
@
\end{frame}


\begin{frame}
\frametitle{Conservation of the Hamiltonian}

The dynamics conserve the Hamiltonian since
\[ \begin{array}{rl}
\frac{dH}{dt} &= 
\sum_{i=1}^d \left[ \frac{d\theta_i}{dt}\frac{\partial H}{\partial \theta_i}
+ \frac{d\omega_i}{dt} \frac{\partial H}{\partial \omega_i}\right] \\ \\
&= 
\sum_{i=1}^d \left[ \frac{\partial H}{\partial \omega_i}\frac{\partial H}{\partial \theta_i}
-\frac{\partial H}{\partial \theta_i}\frac{\partial H}{\partial \omega_i} \right]
\end{array} \]

\pause
If $H$ is conserved, then the acceptance probability based on Hamiltonian 
dynamics is 1. 
\pause
Simulating most Hamiltonian systems, 
we can only make $H$ approximately conserved.

\end{frame}


\subsection{Reversibility}
\begin{frame}
\frametitle{Reversibility}
Hamiltonian dynamics is reversible, 
i.e. the mapping $T_s$ from the state at time $t$, $(\theta(t),\omega(t))$, to the state 
at time $t+s$, $(\theta(t+s),p(t+s))$, is one-to-one, and hence as an inverse,
$T_{-s}$.
\pause
Under our usual assumptions for HMC, the inverse mapping can be obtained by
negating $\omega$, applying $T_s$, and then negating $\omega$ again.
\pause
The reversibility of Hamiltonian dynamics is important for showing convergence
of HMC.
\end{frame}




\subsection{Volume preservation}
\begin{frame}
\frametitle{Volume preservation}

If we apply the mapping $T_s$ to points in some region $R$ in $(\theta,\omega)$ space with
volume $V$, the image of $R$ under $T_s$ will also have volume $V$. 
\pause 
This feature simplifies calculation of the acceptance probability for 
Metropolis updates.

\end{frame}


\section{Approximating Hamiltonian dynamics}
\subsection{Euler's method}
\begin{frame}
\frametitle{Euler's method}

For simplicity, assume 
\[ 
H(\theta,\omega) = U(\theta) + K(\omega), \qquad K(\omega) = \sum_{i=1}^d \frac{\omega_i^2}{2m_i}.
\]
\pause
One way to simulate Hamiltonian dynamics is to discretize time into increments
of $e$, \pause i.e.
\[ \begin{array}{rll}
\omega_i(t+e) 
&= \omega_i(t) + e \frac{d\omega_i}{dt}(t) 
&= \omega_i(t) - e \frac{\partial U}{\partial \theta_i}(\theta(t)) \\
\theta_i(t+e) 
&= \theta_i(t) + e\frac{d\theta_i}{dt}(t) 
&= \theta_i(t) + e\frac{\omega_i(t)}{m_i}
\end{array}\]

\end{frame}


\subsection{Leapfrog method}
\begin{frame}
\frametitle{Leapfrog method}

An improved approach is the \alert{leapfrog} method which has the following 
updates:
\[ \begin{array}{rl}
\omega_i(t+e/2) 
&=\omega_i(t) - (e/2)\frac{\partial U}{\partial \theta_i}(\theta(t)) \pause \\
\theta_i(t+e) &= \theta_i(t) + e \frac{\omega_i(t+e/2)}{m_i} \pause \\
\omega_i(t+e) &= \omega_i(t+e/2)- (e/2)\frac{\partial U}{\partial \theta_i}(\theta(t+e))
\end{array} \]
\pause
The leapfrog method is reversible and preserves volume exactly.
\end{frame}




\begin{frame}[fragile]
\frametitle{Leap-frog simulator}
<<echo=TRUE>>=
leap_frog = function(U, grad_U, e, L, theta, omega) {
  omega = omega - e/2 * grad_U(theta) 
    
    for (l in 1:L) {
      theta = theta + e * omega
      if (l<L) omega = omega - e * grad_U(theta)
    }
    omega = omega - e/2 * grad_U(theta)
  return(list(theta=theta,omega=omega))
}
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Leap-frog simulator}
<<leapfrog-animation, fig.show='animate', interval=.1>>=
  # Create the data
  set.seed(20150915)
n_steps = 63
theta = rep(2.5, n_steps)
omega = rep(0, n_steps)
for (i in 2:n_steps) {
  tmp = leap_frog(function(x) x^2/2, function(x) x, .1, 1, theta[i-1], omega[i-1])
  theta[i] = tmp$theta
  omega[i] = tmp$omega
}

draw_step = function(i, e=.01) {
  opar = par(mfrow=c(1,2))
  curve(x^2/2, -3, 3, main='Position', ylab='Potential Energy')
  points(theta[i], theta[i]^2/2+e, pch=19)
  curve(x^2/2, -3, 3, main='Momentum', ylab='Kinetic Energy')
  points(omega[i], omega[i]^2/2+e, pch=19)
  par(opar)
}

# 
for (i in 1:n_steps) {
  draw_step(i)
}
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Conservation of the Hamiltonian}

\vspace{-0.5in}

<<leapfrog-volume, fig.show='animate', interval=.1, fig.asp=1, fig.width=5, out.width='.6\\linewidth'>>=
r = sqrt(theta[1]^2 + omega[1]^2)
n = 1001
d = data.frame(x = c(seq(-r,r,length=n), seq(r, -r, length=n)))
d$y = sqrt(r^2-d$x^2)
d$y[1:n] = -d$y[1:n]
for (i in 1:length(theta)) {
  plot(y~x, d, type='l', col='gray', xlab='Location', ylab='Momentum')
  points(theta[i], omega[i], pch=19)
}
@
\end{frame}



% \begin{frame}
% \frametitle{Parameter augmentation}
% 
% Suppose we are interested in sampling from a posterior distribution for $\theta\in \mathbb{R}^d$
% \[ 
% p(\theta|y) \propto p(y|\theta)p(\theta) . 
% \]
% 
% \pause
% Now augment $\theta$ with moment variable $\omega\sim N_d(0,D)$ independent of $\theta|y$ such that 
% \[ 
% p(\theta|y) 
% = \int p(\theta|\omega,y)p(\omega)d\omega \pause 
% = \int p(\theta|y) p(\omega) d\omega
% \]
% \pause
% To compare with Neal (2010), we have $\theta=\theta$, $p=\omega$, 
% \[ 
% U(\theta) = -\log[p(y|\theta)p(\theta)] = -\log p(y|\theta) -\log p(\theta),
% \]
% \pause
% and 
% \[
% K(\omega) = -\log p(\omega).
% \]
% 
% \end{frame}

\section{Hamiltonian Monte Carlo}
\begin{frame}
\frametitle{Probability distributions}

The Hamiltonian is an energy function for the joint state of ``position'', 
$\theta$,
and ``momentum'', $\omega$, 
and so defines a joint distribution for them, via
\[
p(\theta,\omega) = \frac{1}{Z} \exp\left(-H(\theta,\omega)\right)
\]
where $Z$ is the normalizing constant.
\pause

If $H(\theta,\omega) = U(\theta)+K(\omega)$, the joint density is 
\[
p(\theta,\omega) = \frac{1}{Z} \exp\left(-U(\theta)\right)\exp\left(-K(\omega)\right).
\]
\pause

If we are interested in a posterior distribution, we set 
\[
U(\theta) = -\log\left[p(y|\theta)p(\theta)\right].
\]

\end{frame}




\begin{frame}
\frametitle{Hamiltonian Monte Carlo algorithm}

\small

Set tuning parameters 
\begin{itemize}
\item $L$: the number of steps
\item $e$: stepsize
\item $D=\{d_i\}$: covariance matrix for $\omega$
\end{itemize}
\pause

Let $\theta^{(i)}$ be the current value of the parameter $\theta$. 
\pause 
The leap-frog Hamiltonian Monte Carlo algorithm is 
\begin{enumerate}
\item Sample $\omega \sim N_d(0,D)$. \pause
\item Simulate Hamiltonian dynamics on location $\theta^{(i)}$ and momentum  $\omega$ via the leapfrog method (or any reversible method that preserves volume)
for $L$ steps with stepsize $e$. \pause 
Call these updated values $\theta^*$ and $-\omega^*$. \pause
\item Set $\theta^{(i+1)} = \theta^*$ with probability $\min\{1,\rho(\theta^{(i)},\theta^*)\}$ 
\pause
where 
\[ 
\rho(\theta^{(i)},\theta^*) = 
\frac{p(\theta^*|y)}{p(\theta^{(i)}|y)}
\frac{p(\omega^*)}{p(\omega^{(i)})} \pause = 
\frac{p(y|\theta^*)p(\theta^*)}{p(y|\theta^{(i)})p(\theta^{(i)})} 
\frac{N_d(\omega^*;0,D)}{N_d(\omega^{(i)};0,D)} 
\]
\pause
otherwise set $\theta^{(i+1)} = \theta^{(i)}$.
\end{enumerate}
\end{frame}



% \begin{frame}
% \frametitle{Leap-frog simulation of Hamiltonian dynamics}
% Given a current location $\theta(0)$ and momentum $\omega(0)$ at time $0$, the leap-frog method can be used to approximate simulating Hamiltonian dynamics up to time $Le$ using a series of $L$ steps each of time $e$. 
% 
% \vspace{0.2in} 
% 
% \pause The algorithm is 
% \begin{enumerate}
% \item For $\ell = 1,\ldots,L$, 
% \begin{enumerate}
% \item For $i=1,\ldots, d$, $\omega_i\left(\left[\ell-\frac{1}{2}\right]e\right) = \omega_i([\ell-1]e)-\frac{e}{2}\frac{\partial U}{\partial \theta_i} (\theta([\ell-1]e))$ \pause
% \item For $i=1,\ldots, d$, $\theta_i(\ell e) = \theta_i([\ell-1]e) + e \frac{\omega_i\left(\left[\ell-\frac{1}{2}\right]e\right)}{d_i}$ \pause
% \item For $i=1,\ldots, d$, $\omega_i(\ell e) = \omega_i\left(\left[\ell-\frac{1}{2}\right]e\right)-\frac{e}{2}\frac{\partial U}{\partial \theta_i} (\theta(\ell e))$ 
% \end{enumerate}
% \end{enumerate}
% \pause
% where $\theta_i$ and $\omega_i$ are the $i^{th}$ element of the location and momentum, respectively. 
% 
% \end{frame}




\begin{frame}
\frametitle{Reversibility}

Reversibility for the leapfrog means that
\begin{itemize}
\item if you simulate from $(\theta,\omega)$ to $(\theta^*,\omega^*)$ for some step size $e$ and number of steps $L$ \pause then
\item if you simulate from $(\theta^*,-\omega^*)$ for the same $e$ and $L$, you will end up at $(\theta,-\omega)$.
\end{itemize}
\pause
If we use $q$ to denote our simulation ``density'', then reversibility means
\[
q(\theta^*,\omega^*|\theta,\omega) = q(\theta,-\omega|\theta^*,-\omega^*)
\]
\pause
and thus in the Metropolis-Hastings calculation, the proposal is symmetric.
\pause
In order to ensure reversibility of our proposal, 
we need to negate momentum after we complete the leap-frog simulation.
\pause 
So long as $q(\omega) =q(-\omega)$, 
which is true for a multivariate normal centered at 0, 
this will not affect our acceptance probability.

\end{frame}






\begin{frame}
\frametitle{Conservation of Hamiltonian results in perfect acceptance}

The Hamiltonian is conserved if $H(\theta,\omega)=H(\theta^*,\omega^*)$
which implies
\[ \begin{array}{rl}
p(\theta^*|y)p(\omega^*)
&= \exp\left(-H(\theta^*,\omega^*)\right) \\
&= \exp\left(-H(\theta  ,\omega  )\right) \\
&= p(\theta|y)p(\omega) 
\end{array} \]
\pause
and thus the Metropolis-Hastings acceptance probability is
\[ 
\rho(\theta^{(i)},\theta^*) = 
\frac{p(\theta^*|y)p(\omega^*)}{p(\theta^{(i)}|y)p(\omega^{(i)})} = 1.
\]
\pause 
This will only be the case if the simulation is perfect! 
\pause 
But we have discretization error. 
\pause
The acceptance probability accounts for this error. 
\end{frame}





\begin{frame}[fragile]
<<echo=TRUE>>=
HMC_neal = function(U, grad_U, e, L, current_theta) {
  theta = current_theta
  omega = rnorm(length(theta),0,1)
  current_omega = omega
  
  omega = omega - e * grad_U(theta) / 2
  
  for (i in 1:L) {
    theta = theta + e * omega
    if (i!=L) omega = omega - e * grad_U(theta)
  }
  omega = omega - e * grad_U(theta) / 2
  
  omega = -omega
  
  current_U  = U(current_theta)
  current_K  = sum(current_omega^2)/2
  proposed_U = U(theta)
  proposed_K = sum(omega^2)/2
  
  # cat(paste(e,L,i,theta,omega,"\n"))
  
  if (runif(1) < exp(current_U-proposed_U+current_K-proposed_K))
  {
    return(theta)
  }
  else {
    return(current_theta)
  }
}
@
\end{frame}


\begin{frame}[fragile]
<<echo=TRUE>>=
HMC = function(n_reps, log_density, grad_log_density, tuning, initial) {
  theta = rep(0, n_reps)
  theta[1] = initial$theta
  
  for (i in 2:n_reps) theta[i] = HMC_neal(U = function(x) -log_density(x), 
                                          grad_U = function(x) -grad_log_density(x),
                                          e = tuning$e, 
                                          L = tuning$L, 
                                          theta[i-1])
  theta
}
@
\end{frame}

\begin{frame}[fragile]
<<echo=TRUE>>=
theta = HMC(1e4, function(x) -x^2/2, function(x) -x, list(e=1,L=1), list(theta=0))
hist(theta, freq=F, 100)
curve(dnorm, add=TRUE, col='red', lwd=2)
@
\end{frame}



\begin{frame}
\frametitle{Tuning parameters}
There are three tuning parameters:
\begin{itemize}
\item $e$: step size
\item $L$: number of steps
\item $D$: covariance matrix for momentum
\end{itemize}

\vspace{0.2in} \pause

Let $\mySigma=V(\theta|y)$, then an optimal normal distribution for $\omega$ is $N(0,\mySigma^{-1})$. 
\pause
Typically, we do not know $\mySigma$, but we can estimate it using posterior samples. 
\pause
We can update this estimate throughout burn-in (or warm-up). 
\end{frame}




\begin{frame}[fragile]
\frametitle{Effect of $e$ and $L$}
<<tuning, cache=TRUE, echo=TRUE>>=
n_reps = 1e4
d = expand.grid(e=10^c(-2:2), L=10^c(-2:0))
r = ddply(d, .(e,L), function(xx) {
  data.frame(
    iteration = 1:n_reps,
    theta = HMC(n_reps, function(x) -x^2/2, function(x) -x, list(e=xx$e,L=xx$L), list(theta=0)))
})
@
\end{frame}

\begin{frame}
<<dependson='tuning'>>=
ggplot(r, aes(iteration,theta)) + 
  geom_line() + 
  facet_grid(L~e) +
  theme_bw() +
  labs(title = "Traceplots for HMC runs",
       y = expression(theta))
@
\end{frame}

\begin{frame}
<<dependson='tuning'>>=
ggplot(r, aes(theta)) + 
  geom_histogram(aes(y=..density..), binwidth=.1) + 
  facet_grid(L~e, scales='free_y') + 
  stat_function(fun = dnorm, colour = "red") +
  theme_bw()
@
\end{frame}

\begin{frame}
<<e, dependson='tuning'>>=
s = ddply(r, .(e,L), summarize, 
          acceptance_rate = length(unique(theta))/length(theta),
          autocorrelation = as.numeric(acf(theta, lag.max=1, plot=FALSE)[1]$acf))
m = melt(s, id.var=c('e','L'), variable.name='statistic')
ggplot(m, aes(e, value, color=L, shape=as.factor(L))) + 
  geom_point() + 
  facet_wrap(~statistic) +
  scale_x_log10() +
  theme_bw()
@
\end{frame}

\begin{frame}
\frametitle{Random-walk vs HMC}

\url{https://www.youtube.com/watch?v=Vv3f0QNWvWQ}

\end{frame}


\section{Summary}
\begin{frame}
\frametitle{Summary}

Hamiltonian Monte Carlo (HMC) is a Metropolis-Hastings method using parameter augmentation and a sophisticated proposal distribution based on Hamiltonian dynamics such that 
\begin{itemize}
\item the acceptance probability can be kept near 1 \pause
\item while still efficiently exploring the posterior. 
\end{itemize}
\pause
HMC still requires us to set tuning parameters 
\begin{itemize}
\item $e$: step size
\item $L$: number of steps
\item $D$: covariance matrix for momentum
\end{itemize}
\pause
and can only be run in models with continuous parameters in $\mathbb{R}^d$ (or transformed to $\mathbb{R}^d$).

\end{frame}

\end{document}
