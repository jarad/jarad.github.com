\documentclass[handout]{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Bayesian model averaging}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}
\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE>>=
library("dplyr")
library("BMA")
library("BMS")
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle}

\section{Bayesian model averaging}
\begin{frame}
\frametitle{Bayesian model averaging}

Let $\{M_\gamma: \gamma \in \mG\}$ indicate a set of models for a particular data set $y$. \pause If $\Delta$ is a quantity of interest, e.g. effect size, a future observable, or the utility of a course of action, \pause then its posterior distribution is 
\[ 
p(\Delta|y) = \sum_{\gamma\in\mG} p(\Delta|M_\gamma, y) p(M_\gamma|y)  
\]
\pause
where 
\[ 
p(M_\gamma|y) 
= \frac{p(y|M_\gamma)p(M_\gamma)}{p(y)} \pause
= \frac{p(y|M_\gamma)p(M_\gamma)}{\sum_{\lambda\in \mG} p(y|M_\lambda)p(M_\lambda)}
\]
is the \alert{posterior model probability}
\pause
and 
\[
p(y|M_\gamma) = \int p(y|\theta_\gamma,M_\gamma) p(\theta_\gamma|M_\gamma) d\theta_\gamma
\]
\pause
is the \alert{marginal likelihood for model $M_\gamma$} and 
$\theta_\gamma$ is the set of parameters in model $M_\gamma$. 

\end{frame}


\begin{frame}
\frametitle{Bayesian model averaged moments}

Since $p(\Delta|y)$ is a discrete mixture, we may be interested in simplifying inference concerning $\Delta$ to a couple of moments. 
\pause
Let $\hat{\Delta}_\gamma = E[\Delta|y,M_\gamma]$. 
\pause 
Then the expectation is 
\[ 
E[\Delta|y] \pause= \sum_{\gamma\in\mG} \hat{\Delta}_\gamma p(M_\gamma|y) 
\]
\pause
and the variance is 
\[ 
Var[\Delta|y] \pause = \left[ \sum_{\gamma\in\mG} (Var[\Delta|y,M_\gamma) + \hat{\Delta}_\gamma^2) p(M_\gamma|y) \right] - E[\Delta|y]^2
\]
\pause
The appealing aspect here is that the moments only depend on the moments from each individual model and the posterior model probability.

\end{frame}


\begin{frame}
\frametitle{Difficulties with BMA}

\begin{itemize}[<+->]
\item Evaluating the summation can be difficult since $|\mG|$, the cardinality of $\mG$, might be huge.
\item Calculating the marginal likelihood.
\item Specifying the prior over models.
\item Choosing the class of models to average over.
\end{itemize}
\end{frame}



\subsection{Reducing cardinality}
\begin{frame}
\frametitle{Reducing cardinality}

If $|\mG|$ is small enough, we can enumerate all models and perform model averaging exactly. But if $|\mG|$ is too large, we will need some parsimony. 

\vspace{0.2in} \pause

Rather than summing over $\mG$, we can only include those models whose posterior probability is sufficiently large 
\[
\mathcal{A} = \left\{ M_\gamma: \frac{\max_\lambda p(M_\lambda|y)}{p(M_\gamma|y)} = \frac{\max_\lambda p(y|M_\lambda)p(M_\lambda)}{p(y|M_\gamma)p(M_\gamma)} \le C\right\}
\]
\pause
relative to other models where $C$ is chosen by the researcher. \pause Also, appealing to Occam's razor, we should exclude complex models which receive less support than sub-models of that complex model\pause, i.e. 
\[ 
\mathcal{B} = \left\{ M_\gamma: \forall M_\lambda \in \mathcal{A}, M_\lambda \subset M_\gamma, \frac{p(M_\lambda|y)}{p(M_\gamma|y)} < 1\right\}
\]
\pause 
So, we typically sum over the smaller set of models $\mG' = \mathcal{B}$. 
\end{frame}



\begin{frame}
\frametitle{Searching through models}

One approach is to search through models and keep a list of the best models. \pause To speed up the search the following criteria can be used to decide what models should be kept in $\mG'$: \pause

\begin{itemize}
\item When comparing two nested models, if a simpler model is rejected, then all submodels of the simpler model are rejected. \pause
\item When comparing two non-nested models, we calculate the ratio of posterior model probabilities 
\[ \frac{p(M_\gamma|y)}{p(M_{\gamma'}|y)} \]
if this quantity is less than $O_L$, we reject $M_\gamma$ and if it is greater than $O_R$ we reject $M_{\gamma'}$. 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Using MCMC to search through models}

Construct a neighborhood around $M^{(i)}$ (the current model in the chain), call it $nbh(M^{(i)})$. \pause Now propose a draw $M^*$ from the following proposal distribution 
\[ 
q(M^*|M^{(i)}) = \left\{ \begin{array}{cl}
0 & \forall M^* \notin nbh(M^{(i)}) \\
\frac{1}{|nbh(M^{(i)})|} & \forall M^* \in nbh(M^{(i)})
\end{array}\right. 
\]
\pause
Set $M^{(i+1)} = M^*$ with probability $\min\{1,\rho(M^{(i)},M^*)\}$ where 
\[ 
\rho(M^{(i)},M^*) = \frac{p(M^*|y)}{p(M^{(i)}|y)} \frac{|nbh(M^{(i)})|}{|nbh(M^*)|}
\]
and otherwise set $M^{(i+1)} = M^{(i)}$. \pause This Markov chain converges to draws from $p(M_\gamma|y)$ and therefore can estimate posterior model probabilities. 

\end{frame}


\subsection{Evaluating integrals}
\begin{frame}
\frametitle{Evaluating the marginal likelihoods}

\small

Recall that as the sample size $n$ increases, the posterior converges to a normal distribution. \pause Let 
\[ 
g(\theta) = \log(p(y|\theta,M)p(\theta|M)) \pause = \log p(y|\theta,M) + \log p(\theta|M)
\]
\pause
Let $\hat{\theta}_{MAP}$ be the MAP for $\theta$ in model $M$. \pause Taking a Taylor series expansion of $g(\theta)$ around $\hat{\theta}_{MAP}$, we have 
\[ 
g(\theta) \approx g(\hat{\theta}_{MAP}) - \frac{1}{2}(\theta-\hat{\theta}_{MAP}) A (\theta-\hat{\theta}_{MAP})^\top
\]
where $A$ is the negative Hession of $g(\theta)$ evaluated at $\hat{\theta}_{MAP}$. \pause Combining this with the first equation and exponentiating, \pause we have 
\[ 
p(y|\theta,M)p(\theta|M) \approx p(y|\hat{\theta}_{MAP},M)p(\hat{\theta}_{MAP}) \exp\left( - \frac{1}{2}(\theta-\hat{\theta}_{MAP}) A (\theta-\hat{\theta}_{MAP})^\top  \right)
\]
\pause
Hence, the approximation to $p(\theta|y,M) \propto p(y|\theta,M)p(\theta|M)$ is normal. 

\end{frame}



\begin{frame}
\frametitle{Evaluating the marginal likelihoods (cont.)}

\small

If we take the integral over $\theta$ of both sides and take the logarithm, we have 
\[ 
\log p(y|M) \approx \log p(y|\hat{\theta}_{MAP},M) + \log p(\hat{\theta}_{MAP}|M) + \frac{p}{2} \log(2\pi) - \frac{1}{2} \log|A|
\]
where $p$ is the dimension of $\theta$, i.e. the number of parameters. \pause We call this approximation the \alert{Laplace approximation}. 

\vspace{0.2in} \pause

Another approximation that is more computationally efficient but less accurate is to only retain terms that increase with $n$: \pause
\begin{itemize}
\item $\log p(y|\hat{\theta},M)$ increases linearly with $n$ \pause
\item $\log |A|$ increases as $p\log n$ \pause
\end{itemize}
As $n$ gets large $\hat{\theta}_{MAP} \to \hat{\theta}_{MLE}$. \pause Taking these two together we have 
\[ 
\log p(y|M) \approx \log p(y|\hat{\theta}_{MLE},M) - \frac{p}{2} \log n
\]
\pause
Multiplying by -2, we obtain Schwarz's Bayesian Information Criterion (BIC)
\[
BIC = -2 \log p(y|\hat{\theta}_{MLE},M) + p \log n
\]
\end{frame}


\subsection{Priors over models}
\begin{frame}
\frametitle{Priors over models}

For data-based comparisons of models, you can use Bayes Factors directly since 
\[ 
BF(M_\gamma:M_{\gamma'}) 
= \frac{p(y|M_\gamma)}{p(y|M_{\gamma'})} \pause 
= \frac{\int p(y|\theta_\gamma)p(\theta_\gamma|M_\gamma) d \theta_\gamma}{\int p(y|\theta_{\gamma'})p(\theta_{\gamma'}|M_{\gamma'}) d \theta_{\gamma'}}
\]
where the last equality is a reminder that priors over parameters still matter.

\vspace{0.2in} \pause

For model averaging, you need to calculate posterior model probabilities which require specification of the prior probabability of each model. \pause One possible prior for regression models is 
\[
p(M_\gamma) = \prod_{i=1}^p w_i^{1-\gamma_i} (1-w_i)^{\gamma_i}
\]
\pause
Setting $w_i = 0.5$ corresponds to a uniform prior over the model space. 
\end{frame}


\begin{frame}
\frametitle{BMA output}

The quantities of interest from BMA are typically
\begin{itemize}
\item Posterior model probabilities $p(M_\gamma|y)$ \pause
\item Posterior inclusions probabilities (for regression)
\[ 
p(\mbox{including explanatory variable }i|y) = \sum_{\gamma\in\mG} p(M_\gamma|y) \I(\gamma_i=1) 
\]
\pause
which provides an overall assessment of whether explanatory variable $i$ is important or not. \pause
\item Posterior distributions, means, and variances for ``parameters'', e.g. 
\[ 
E(\theta_i|y) = \sum_{\gamma\in\mG} p(M_\gamma|y) E[\theta_{\gamma,i}|y] 
\]
\pause
But does this make any sense? What happened to $\theta_\gamma$? \pause
\item Predictions:
\[
p(\tilde{y}|y) = \sum_{\gamma\in\mG} p(M_\gamma|y) p(\tilde{y}|M_\gamma,y) 
\]
\end{itemize}

\end{frame}


\section{BMA in R}
\begin{frame}
\frametitle{R packages for BMA}

There are two main packages for Bayesian model average in R \pause
\begin{itemize}
\item \href{https://cran.r-project.org/web/packages/BMA/index.html}{BMA}: glm model averaging using BIC \pause
\item \href{https://cran.r-project.org/web/packages/BMS/index.html}{BMS}: lm model averaging using g-priors and (possibly) MCMC \pause
\end{itemize}

\vspace{0.2in} \pause

Until recently there was another package
\begin{itemize}
\item \href{https://cran.r-project.org/web/packages/BAS/index.html}{BAS}: lm model averaging with a variety of priors and (possibly) MCMC (additionally performed sampling without replacement)
\end{itemize}
\end{frame}




\subsection{BMA}
\begin{frame}[fragile]
\frametitle{BMA in R}

<<BMA, message=FALSE, echo=TRUE>>=
library(BMA)
UScrime <- MASS::UScrime

# Set up data
x = UScrime[,-16]
y = log(UScrime[,16])
x[,-2] = log(x[,-2])

# Run BMA using BIC
lma = bicreg(x, y, 
             strict = TRUE, # remove submodels that are less likely 
             OR = 20)       # maximum BF ratio
@
\end{frame}


\begin{frame}[fragile]
<<BMA2, echo=TRUE>>= 
summary(lma)
@
\end{frame}



\begin{frame}[fragile]
Does this make any sense?

<<BMA3, echo=TRUE>>=
plot(lma)
@
\end{frame}


\begin{frame}[fragile]
<<BMA4, echo=TRUE>>=
imageplot.bma(lma)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{BMA for GLMs in R}

<<BMA_glm, message=FALSE, echo=TRUE>>=
# Set up data
y <- MASS::birthwt$low                  # 1 indicates birthweight < 2.5 kg

x <- MASS::birthwt %>%
  dplyr::select(-low,-bwt) %>%
  mutate(race  = factor(race),    # mother's race (1 = white, 2 = black, 3 = other)
         smoke = factor(smoke),   # smoking status during pregnancy.
         ptl   = factor(ptl),     # number of previous premature labours
         ht    = factor(ht>=1),   # history of hypertension
         ui    = factor(ui))      # presence of uterine irritability

# Use BIC-based BMA
lma <- bic.glm(x, y, strict = TRUE, OR = 20, 
                     glm.family="binomial", 
                     factor.type=TRUE) # remove all levels of a factor
@
\end{frame}


\begin{frame}[fragile]
<<BMA_glm4>>=
summary(lma)
@
\end{frame}


\begin{frame}[fragile]
<<BMA_glm4_plot>>=
imageplot.bma(lma)
@
\end{frame}




\begin{frame}[fragile]
\frametitle{Predictions}

<<npkBMA, echo=TRUE>>=
npkBMA = bicreg( x = npk[, c("block","N","P","K")], y = npk$yield)
summary(npkBMA)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Predictions}

<<BMA_predictions, echo=TRUE>>=
p = predict( npkBMA, newdata = npk, quantiles = c(.5,.05,.95))
bind_cols(npk, as.data.frame(p$quantiles)) %>% head(20)
@
\end{frame}



\section{MCMC for sampling $\theta$ and $M$}
\begin{frame}
\frametitle{MCMC for sampling $\theta$ and $M$}

Suppose, you construct a Markov chain to sample jointly from $p(M_\gamma,\theta_\gamma|y)$. \pause An issue here is that when you move $M_\gamma \to M_{\gamma'}$, there is a chance that you change the dimension of $\theta$, i.e. $\sum_{i=1}^p \gamma_i \ne \sum_{i=1}^p \gamma_i'$. \pause This can be done via Metropolis-Hastings where the change of dimension is taken into account and this approach is called \alert{reversible jump MCMC}. 

\vspace{0.2in} \pause

An alternative is to fully incorporate $\gamma$ as a parameter in the model. \pause For example, 

\[ \begin{array}{rl}
y_{ij} &\ind N(\gamma_i\theta_i, \sigma^2) \\
\theta_i &\ind N(\mu,\tau) \\
\gamma_i &\ind Ber(w_i)
\end{array} \]
\pause
This is essentially a way to implement the point-mass prior. 
\end{frame}



\begin{frame}
\frametitle{MCMC for Model averaging for GLMs}

\small

We can implement a similar MCMC to perform model averaging in Bayesian GLMs. \pause Let $\theta_i = E[y_i|\theta_i]$and $\phi$ as a dispersion parameter, then we can define a GLM as 
\[ \begin{array}{rl}
y_i &\sim p(y_i|\theta_i, \psi) \\
\theta_i &= g^{-1}(X_i'\beta) \\
\beta_j &= \gamma_j \phi_j \\
\phi_j &\ind N(\mu,\tau) \\
\gamma_j &\ind Ber(w_j)
\end{array} \]

\vspace{0.2in} \pause

For probit and ordinal regression, we can augment the model with parameters $\zeta_i$, e.g. for probit regression 
\[ 
y_i = \I(\zeta_i>0) \mbox{ and } \zeta_i \ind N(\theta_i,\psi).
\]
There is a similar augmentation for logistic regression, see the {\tt BayesLogit} and references therein. 
\pause
For these models and linear regression, we can construct an MCMC entirely using Gibbs steps. 
\pause
For other models, e.g. Poisson regression, sampling $\phi_j$ results in a non-Gibbs step.

\end{frame}




\subsection{BMS}
\begin{frame}[fragile]
\frametitle{BMS}
<<BMS, echo=TRUE>>=
library(BMS) # Bayesian model sampling
data(datafls)
dim(datafls)
bma1 = bms(datafls,
           burn=1000, 
           iter=2000, 
           g="EBL",          # Local empirical Bayes 
           mprior="uniform", # model over priors (extremely flexible)
           user.int = interactive()) 
@
\end{frame}


\begin{frame}[fragile]
<<BMS_print, echo=TRUE>>=
print(bma1)
@
\end{frame}


\begin{frame}[fragile]
<<BMS_summary, echo=TRUE>>=
summary(bma1)
@
\end{frame}


\begin{frame}[fragile]
<<BMS2a, echo=TRUE>>=
plot(bma1)
@
\end{frame}


\begin{frame}[fragile]
<<BMS3, echo=TRUE>>=
image(bma1)
@
\end{frame}






\begin{frame}[fragile]
<<BMS5,echo=TRUE>>=
p1 = predict(bma1)             # fitted values based on MCMM frequencies
p2 = predict(bma1, exact=TRUE) # fitted values based on best models
plot(p1,p2); abline(0,1)
@
\end{frame}




% \subsection{Bayesian regression}
% \begin{frame}
% \frametitle{Bayesian regression}
% 
% Consider the model $M_\gamma$:
% \[ y = X_\gamma\beta_\gamma + \epsilon \]
% with 
% \[ \epsilon \sim N(0,\sigma^2 \I) \]
% where 
% \begin{itemize}
% \item $y$ is a vector of length $n$
% \item $\gamma$ is a binary vector indicating which explanatory variables are included in model $M_\gamma$
% \item $\beta_\gamma$ is an unknown vector of length $p_\gamma$
% \item $X_\gamma$ is a known $n\times p_\gamma$ design matrix including those columns indicated by $\gamma$
% \item $\sigma^2$ is an unknown scalar
% \end{itemize}
% \end{frame}
% 
% 
% \begin{frame}
% \frametitle{BMA in regression}
% 
% 
% 
% \end{frame}

\begin{frame}[fragile]
\frametitle{Model averaging of regression coefficients}

<<echo = TRUE>>=
set.seed(20171007)
n <- 100
x1 <- rnorm(n)
y <- x1 + rnorm(n)
x2 <- x1 + rnorm(n,0,.01)

library(BMA)
m <- bicreg(cbind(x1,x2), y)

summary(m)
@

\end{frame}

\end{document}
