\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\graphicspath{{figures/}{../StateSpaceModels/figures/}}


\title{Dynamic linear models}

\begin{document}

%\section{Temp??} \begin{comment}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, message=FALSE, warning=FALSE>>=
library("ggplot2")
library(plyr)
library("dplyr")
library(reshape2)
library("dlm")
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle}

\section{Dynamic linear models}

\subsection{Structure, notation, and terminology}
\frame{\frametitle{Structure}
 \[ \begin{array}{rl@{\qquad}l}
 Y_t &= F_t\theta_t + v_t &v_t\sim N_m(0,V_t) \\
 \theta_t &= G_t\theta_{t-1}+w_t & w_t\sim N_p(0,W_t) \\
 \theta_0 &\sim N_p(m_0,C_0)
 \end{array} \] \pause

 \vspace{0.1in}
 
 \begin{center}
 \includegraphics{stateSpaceModel}
 \end{center}
}


\subsection{Local level model}
\frame{\frametitle{Local level model}
\setkeys{Gin}{width=\textwidth}
  \[ \begin{array}{rl@{\qquad}l}
 Y_t &= \theta_t + v_t &v_t\sim N_1(0,V) \\
 \theta_t &= \theta_{t-1}+w_t & w_t\sim N_1(0,W) \\
 \theta_0 &\sim N_1(m_0,C_0)
 \end{array} \]

 \vspace{0.1in} \pause

 Signal-to-noise, $r=W/V$.

 \vspace{0.1in} \pause

 \begin{center}
 \includegraphics{signalToNoise1}
 \end{center}
}

\frame{\frametitle{Local level model}
\setkeys{Gin}{width=\textwidth}
  \[ \begin{array}{rl@{\qquad}l}
 Y_t &= \theta_t + v_t &v_t\sim N(0,V) \\
 \theta_t &= \theta_{t-1}+w_t & w_t\sim N(0,W) \\
 p(\theta_0) &= N(m_0,C_0)
 \end{array} \]

 \vspace{0.1in}

 Signal-to-noise, $r=W/V$.

 \vspace{0.1in}

 \begin{center}
 \includegraphics{signalToNoise2}
 \end{center}
}

\section{Kalman filtering}
\subsection{Idea}
\frame{\frametitle{Kalman filter idea}
 {\small
 Goal: obtain $p(\theta_t|y_{1:t})$  \pause

 \vspace{0.1in}

 Recursive procedure: \pause
 \begin{itemize}
 \item Assume $p(\theta_{t-1}|y_{1:t-1})$ \pause \uncover<10->{$\qquad p(\theta_{t-1}|y_{1:t-1}) = N(m_{t-1},C_{t-1})$}
 \item Prior for $\theta_t$ \pause
 \begin{eqnarray*} p(\theta_t|y_{1:t-1}) = \int p(\theta_t|\theta_{t-1})p(\theta_{t-1}|y_{1:t-1}) d\theta_{t-1}
 \end{eqnarray*} \pause
 \item One-step ahead predictive distribution for $y_t$ \pause
 \begin{eqnarray*} p(y_t|y_{1:t-1}) = \int p(y_t|\theta_t)p(\theta_t|y_{1:t-1})d\theta_t
 \end{eqnarray*} \pause
 \item Filtered distribution for $\theta_t$ \pause
 \[ p(\theta_t|y_{1:t}) = \frac{p(y_t|\theta_t)p(\theta_t|y_{1:t-1})}{p(y_t|y_{1:t-1})}\] \pause
 \end{itemize}
 }
}

\subsection{The hard way}
\frame{\frametitle{}
{\tiny
 \begin{itemize}
 \item Prior for $\theta_t$ \pause
 \begin{eqnarray*} p(\theta_t|y_{1:t-1}) &=& \int N(\theta_t;G_t\theta_{t-1},W_t) N(\theta_{t-1};m_{t-1},C_{t-1}) d\theta_{t-1} \\
 &=& \int \frac{1}{(2\pi)^{p/2}|W_t|^{1/2}}\exp\left(-\frac{1}{2}(\theta_t-G_t\theta_{t-1})^\top W_t^{-1}(\theta_t-G_t\theta_{t-1})\right) \\ && \quad \frac{1}{(2\pi)^{p/2}|C_{t-1}|^{1/2}}\exp\left(-\frac{1}{2}(\theta_{t-1}-m_{t-1})^\top C_{t-1}^{-1}(\theta_{t-1}-m_{t-1})\right) d\theta_{t-1} \\ \pause
 &=& N(a_t,R_t)
 \end{eqnarray*} \pause
 \item One-step ahead predictive distribution for $y_t$ \pause
 \begin{eqnarray*} p(y_t|y_{1:t-1}) &=& \int p(y_t|\theta_t)p(\theta_t|y_{1:t-1})d\theta_t \\
 &=& \int N(y_t;F_t\theta_t,V_t)N(\theta_t;a_t,R_t)d\theta_t \\
 &=& \int \frac{1}{(2\pi)^{m/2}|V_t|^{1/2}}\exp\left(-\frac{1}{2}(y_t-F_t\theta_t)^\top V_t^{-1}(y_t-F_t\theta_t)\right) \\
 && \quad \frac{1}{(2\pi)^{p/2}|R_t|^{1/2}}\exp\left(-\frac{1}{2}(\theta_t-a_t)^\top R_t^{-1}(\theta_t-a_t)\right) d\theta_t \\ \pause
 &=& N(f_t,Q_t)
 \end{eqnarray*} \pause
 \item Filtered distribution for $\theta_t$ \pause
 \begin{eqnarray*} p(\theta_t|y_{1:t}) &=& \frac{p(y_t|\theta_t)p(\theta_t|y_{1:t-1})}{p(y_t|y_{1:t-1})}
  = \frac{N(y_t;F_t\theta_t,V_t)N(\theta_t;a_t,R_t)}{N(y_t;f_t,Q_t)} \\ \pause
 &=& N(m_t,C_t) \end{eqnarray*}
 \end{itemize}
}
}

\subsection{Kalman filter}
\frame{\frametitle{Latent state prior}
 Assume $p(\theta_{t-1}|y_{1:t-1}) = N(m_{t-1},C_{t-1})$. Find $p(\theta_t|y_{1:t-1})$.

 \vspace{0.5in} \pause

 Evolution equation: $\theta_t = G_t\theta_{t-1}+w_t$ where $w_t\sim N_p(0,W_t)$. \pause

 \vspace{0.2in}

 \begin{itemize}
 \item CONAN $\implies \theta_t|y_{1:t-1}$ is normal \pause
 \item $E[\theta_t|y_{1:t-1}] \pause = G_tm_{t-1} \pause = a_t$ \pause
 \item $Var[\theta_t|y_{1:t-1}] \pause = G_tC_{t-1}G_t^\top +W_t \pause = R_t$ \pause
 \item $p(\theta_t|y_{1:t-1}) = N(a_t,R_t)$.
 \end{itemize}
}

\frame{\frametitle{One-step ahead prediction}
 Prior is $p(\theta_t|y_{1:t-1}) = N(a_t,R_t)$. Find $p(y_t|y_{1:t-1})$.

 \vspace{0.5in} \pause

 Observation equation: $y_t = F_t\theta_t+v_t$ where $v_t\sim N_m(0,V_t)$. \pause

 \vspace{0.2in}

 \begin{itemize}
 \item CONAN $\implies y_t|y_{1:t-1}$ is normal \pause
 \item $E[y_t|y_{1:t-1}] \pause = F_ta_t \pause = f_t$ \pause
 \item $Var[y_t|y_{1:t-1}] \pause = F_tR_tF_t^\top +V_t \pause = Q_t$ \pause
 \item $p(y_t|y_{1:t-1}) = N(f_t,Q_t)$.
 \end{itemize}
}

\frame{\frametitle{Latent state posterior - linear regression approach}
 We have $p(\theta_t|y_{1:t-1}) = N(a_t,R_t)$. Find $p(\theta_t|y_{1:t})$.

 \vspace{0.5in} \pause

 Observation equation: $y_t = F_t\theta_t+v_t$ where $v_t\sim N_m(0,V_t)$. \pause

 \vspace{0.2in}

 \begin{itemize}
 \item Linear regression $\implies \theta_t|y_{1:t}$ is normal \pause
 \item $Var[\theta_t|y_{1:t}] \pause = (R_t^{-1}+F_t^\top V_t^{-1}F_t)^{-1} \pause = C_t$ \pause
 \item $E[\theta_t|y_{1:t}] \pause = C_t(R_t^{-1}a_t+F_t^\top V_t^{-1}y_t) \pause = m_t$ \pause
 \item $p(\theta_t|y_{1:t}) = N(m_t,C_t)$.
 \end{itemize}
}

\frame{\frametitle{Latent state posterior - multivariate normal approach}
 Prior is $p(\theta_t|y_{1:t-1}) = N(a_t,R_t)$ and $p(y_t|y_{1:t-1}) = N(f_t,Q_t)$. Find $p(\theta_t|y_{1:t})$.

 \vspace{0.5in} \pause

 Consider \[ p\left(\left. \left[ \begin{array}{c} y_t \\ \theta_t \end{array} \right] \right|y_{1:t-1}\right)\pause = N\left(\left[\begin{array}{c} f_t \\ a_t \end{array}\right], \left[\begin{array}{cc} Q_t & \uncover<4->{F_tR_t} \\ \uncover<4->{R_tF_t^\top } & R_t \end{array} \right]\right) \]

 \vspace{0.2in} \pause \pause

 \begin{itemize}
 \item MVN theory $\implies \theta_t|y_{1:t-1},y_t$ is normal \pause
 \item $E[\theta_t|y_{1:t}] \pause = a_t+R_tF_t^\top Q_t^{-1}(y_t-f_t)
  \pause = m_t$ \pause
 \item $Var[\theta_t|y_{1:t}] \pause = R_t-R_tF_t^\top Q_t^{-1}F_tR_t \pause = C_t$ \pause
 \item $p(\theta_t|y_{1:t}) = N(m_t,C_t)$.
 \end{itemize}
}

\frame{\frametitle{Kalman filter}
 \begin{itemize}
 \item Assume $p(\theta_{t-1}|y_{1:t-1})=N(m_{t-1},C_{t-1})$.
 \item Obtain prior $p(\theta_t|y_{1:t-1}) = N(a_t,R_t)$ where
 \[ a_t = G_tm_{t-1} \quad \mbox{and} \quad R_t =G_tC_{t-1}G_t^\top +W_t.\]
 \item Obtain one step ahead predictive $p(y_t|y_{1:t-1}) = N(f_t,Q_t)$ where
 \[ f_t = F_ta_t \quad \mbox{and} \quad Q_t = F_tR_tF_t^\top +V_t.\]
 \item Obtain posterior $p(\theta_t|y_{1:t}) = N(m_t,C_t)$ where
 \[ \begin{array}{rcl@{\quad \mbox{and} \quad}rcl}
 m_t &=& a_t+K_te_t & C_t &=& R_t-K_tQ_tK_t^\top  \\
 e_t &=& y_t-f_t  & K_t &=& R_tF_t^\top Q_t^{-1}
 \end{array} \]
 \end{itemize} \pause
 $K_t$ is the \alert{Kalman gain} or \alert{adaptive coefficient}.
}

\subsection{Kalman filter example}
\frame{\frametitle{Local level model}
\setkeys{Gin}{width=\textwidth}
  \[ \begin{array}{rl@{\qquad}l}
 Y_t &= \theta_t + v_t &v_t\sim N(0,V) \\
 \theta_t &= \theta_{t-1}+w_t & w_t\sim N(0,W) \\
 p(\theta_0) &= N(m_0,C_0)
 \end{array} \]

 \vspace{0.1in}

 Signal-to-noise, $r=W/V$.

 \vspace{0.1in}

 \begin{center}
 \includegraphics{signalToNoise}
 \end{center}
}

\frame{\frametitle{Local level model}
\setkeys{Gin}{width=\textwidth}
  \[ \begin{array}{rl@{\qquad}l}
 Y_t &= \theta_t + v_t &v_t\sim N(0,V) \\
 \theta_t &= \theta_{t-1}+w_t & w_t\sim N(0,W) \\
 p(\theta_0) &= N(m_0,C_0)
 \end{array} \]

 \vspace{0.1in}

 Signal-to-noise, $r=W/V$.

 \vspace{0.1in}

 \begin{center}
 \includegraphics{kalmanFilter}
 \end{center}
}

\frame{\frametitle{Kalman gain (adaptive coefficient)}
\setkeys{Gin}{width=\textwidth}
\vspace{-0.2in}
 \begin{center}
 \includegraphics{kalmanFilter2}
 \end{center}
}

\subsection{Missing observations}
\frame{\frametitle{Kalman filter}
 \begin{itemize}
 \item Assume $p(\theta_{t-1}|y_{1:t-1})=N(m_{t-1},C_{t-1})$.
 \item Obtain prior $p(\theta_t|y_{1:t-1}) = N(a_t,R_t)$ where
 \[ a_t = G_tm_{t-1} \quad \mbox{and} \quad R_t =G_tC_{t-1}G_t^\top +W_t.\]
 \item Obtain one step ahead predictive $p(y_t|y_{1:t-1}) = N(f_t,Q_t)$ where
 \[ f_t = F_ta_t \quad \mbox{and} \quad Q_t = F_tR_tF_t^\top +V_t.\]
 \item Obtain posterior $p(\theta_t|y_{1:t}) = N(m_t,C_t)$ where
 \[ \begin{array}{rcl@{\quad \mbox{and} \quad}rcl}
 m_t &=& a_t+K_te_t & C_t &=& R_t-K_tQ_tK_t^\top  \\
 e_t &=& y_t-f_t  & K_t &=& R_tF_t^\top Q_t^{-1}
 \end{array} \]
 \end{itemize}
}

\frame{\frametitle{Kalman filter \alert{with missing data}}
 \begin{itemize}
 \item Assume $p(\theta_{t-1}|y_{1:t-1})=N(m_{t-1},C_{t-1})$.
 \item Obtain prior $p(\theta_t|y_{1:t-1}) = N(a_t,R_t)$ where
 \[ a_t = G_tm_{t-1} \quad \mbox{and} \quad R_t =G_tC_{t-1}G_t^\top +W_t.\]
 \item Obtain one step ahead predictive $p(y_t|y_{1:t-1}) = N(f_t,Q_t)$ where
 \[ f_t = F_ta_t \quad \mbox{and} \quad Q_t = F_tR_tF_t^\top +V_t.\]
 \item Obtain posterior $p(\theta_t|y_{1:t}) = N(m_t,C_t)$ where
  \begin{itemize}
  \item \alert{If $y_t$ is not observed, $m_t=a_t$ and $C_t=R_t$.}
  \item If $y_t$ is observed,
 \[ \begin{array}{rcl@{\quad \mbox{and} \quad}rcl}
 m_t &=& a_t+K_te_t & C_t &=& R_t-K_tQ_tK_t^\top  \\
 e_t &=& y_t-f_t  & K_t &=& R_tF_t^\top Q_t^{-1}
 \end{array} \]
  \end{itemize}
 \end{itemize}
}

\subsection{Forecasting}
\begin{frame}
\frametitle{Forecasting}

Forecasting is simply the Kalman filter with missing observations.
\pause
So, 
\[ \begin{array}{rl}
\left( \begin{array}{c} \theta_{t+k} \\ Y_{t+k} \end{array} \right) \sim
N\left( \left[ \begin{array}{c} a_t(k) \\ f_t(k) \end{array}\right], 
\left[ \begin{array}{cc} R_t(k) & F_{t+k}R_t(k) \\ R_t(k)F_{t+k}^\top & Q_t(k) \end{array} \right]\right)
\end{array} \]
\pause 
where 
\[ \begin{array}{rl}
a_t(k) &= G_{t+k}a_t(k-1) \\
R_t(k) &= G_{t+k} R_{t}(k-1)G_{t+k}^\top +W_{t+k} \\
f_t(k) &= F_{t+k}a_t(k) \\
Q_t(k) &= F_{t+k}R_t(k)R_{t+k}^\top + V_t
\end{array} \]
\pause 
with $a_t(0) = m_t$ and $R_t(0) = C_t$.

\end{frame}

\subsection{Kalman Smoother}
\begin{frame}
\frametitle{Kalman Smoother}

Smoothing can be accomplished in a manner similar to the Kalman filter via
the Kalman smoother.
\pause
If we have $\theta_{t+1}|y_{1:T} \sim N(s_{t+1},S_{t+1})$, 
then $\theta_t|y_{1:T} \sim N(s_t,S_t)$ where
\[ \begin{array}{rl}
s_t &= m_t + C_tG_{t+1}^\top R_{t+1}^{-1}(s_{t+1}-a_{t+1}) \\
S_t &= C_t - C_tG_{t+1}^\top R_{t+1}^{-1}(R_{t+1}-S_{t+1})R_{t+1}^{-1}G_{t+1}C_t.
\end{array} \]

\end{frame}


\subsection{Backward sampling}
\begin{frame}
\frametitle{Backward sampling}
Recall
\begin{itemize}[<+->]
\item $p(\theta_t|y_{1:t})=N(m_t,C_t)$ is available for all $t$ from filtering and 
\item $p(\theta_t|\theta_{t+1},y_{1:T})=N(h_t,H_T)$ with
\end{itemize}
\uncover<3>{
\begin{eqnarray*}
H_t &=& (C_t^{-1}+G_{t+1}^\top W_{t+1}^{-1}G_{t+1})^{-1} \\
h_t &=& H_t(C_t^{-1}m_t+G_{t+1}^\top W_{t+1}^{-1}\theta_{t+1})
\end{eqnarray*}
} \pause
\uncover<4->{The algorithm is then} \pause
\begin{itemize}[<+->]
\item Forward filter to obtain $p(\theta_t|y_{1:t})=N(m_t,C_t)$ for all $t$. 
\item Sample $\theta_T \sim H(m_T,C_T)$. 
\item For $t=T-1,T-2,\ldots,1,0$,
 \begin{itemize}
 \item Calculate $h_t$ and $H_t$ based on $\theta_{t+1}$.
 \item Draw $\theta_t\sim N(h_t,H_T)$.
 \end{itemize}
\end{itemize}
This is then a joint draw of 
$\theta_{0:T} \sim p(\theta_0,\ldots,\theta_T|y_{1:T})$.
\end{frame}


\begin{frame}
\frametitle{Inference questions?}

Any questions on performing inference on the latent states in a DLM?

\end{frame}


\section{Model components}
\frame{\frametitle{Decomposition of time series}
Consider a univariate series $Y_t$. Think of this series has being the sum of \alert{independent} components
\[ Y_t = Y_{1,t}+\cdots + Y_{h,t} = \sum_{i=1}^h Y_{i,t} \]

\vspace{0.1in} \pause

where each component has its own independent DLM (dynamic linear model),
\[ \begin{array}{rl@{\qquad}l}
Y_{i,t} &= F_{i,t}\theta_{i,t}+v_{i,t} & v_{i,t} \sim N(0,V_{i,t}) \\
\theta_{i,t} &= G_{i,t}\theta_{i,t-1}+w_{i,t} & w_{i,t}\sim N(0,W_{i,t})
\end{array} \]

\vspace{0.1in}\pause

Then
\[ \begin{array}{rl@{\qquad}l}
Y_{t} &= F_{t}\theta_{t}+v_{t} & v_{t} \sim N(0,V_{t}) \\
\theta_{t} &= G_{t}\theta_{t-1}+w_{t} & w_{t}\sim N(0,W_{t})
\end{array} \]
}


\frame{\frametitle{Observation equation}
Recall
\[ \begin{array}{rl@{\qquad}l}
Y_t &= Y_{1,t}+\cdots + Y_{h,t} \\
\\
Y_{i,t} &= F_{i,t}\theta_{i,t}+v_{i,t} & v_{i,t} \sim N(0,V_{i,t}) \\
\end{array} \]

\vspace{0.1in}\pause

\[ \begin{array}{rlll}
Y_t &= \sum_{i=1}^h Y_{i,t}  \\ \pause
 &= \sum_{i=1}^h \big[ F_{i,t}\theta_{i,t}+v_{i,t} \big] \\ \pause
 &= \sum_{i=1}^h F_{i,t}\theta_{i,t}+\sum_{i=1}^h v_{i,t} \\ \pause
 &= \sum_{i=1}^h F_{i,t}\theta_{i,t}+v_t & v_t\sim N(0,V_t) \quad V_t = \sum_{i=1}^h V_{i,t} \\ \pause
 &= F_t\theta_t + v_t
\end{array} \]
where
\[ \theta_t = \left[ \begin{array}{c} \theta_{1,t} \\ \vdots \\ \theta_{h,t} \end{array} \right] \qquad \pause
F_t = [F_{1,t}|\cdots |F_{h,t}] \]
}

\frame{\frametitle{Evolution equation}
Recall
\begin{eqnarray*}
\theta_t &= \left[ \begin{array}{c} \theta_{1,t} \\ \vdots \\ \theta_{h,t} \end{array} \right] \pause
= \left[ \begin{array}{c} G_{1,t}\theta_{1,t-1}+w_{1,t} \\ \vdots \\ G_{h,t}\theta_{h,t-1}+w_{h,t} \end{array} \right]  \pause
&= \left[ \begin{array}{c} G_{1,t}\theta_{1,t-1} \\ \vdots \\ G_{h,t}\theta_{h,t-1} \end{array} \right]+\left[ \begin{array}{c} w_{1,t} \\ \vdots \\ w_{h,t} \end{array} \right] \\ \\ \pause
&= \left[ \begin{array}{c} G_{1,t}\theta_{1,t-1} \\ \vdots \\ G_{h,t}\theta_{h,t-1} \end{array} \right] + w_t \pause
&= G_t\theta_{t-1}+w_t \pause
\end{eqnarray*} 
where $w_t\sim N(0,W_t)$ and
\[ W_t = \left[ \begin{array}{ccc} W_{1,t} \\ & \ddots \\ && W_{h,t} \end{array}\right]  \qquad G_t = \left[ \begin{array}{ccc} G_{1,t} \\ & \ddots \\ && G_{h,t} \end{array} \right] \pause
 \]
}

\frame{\frametitle{Combining model components}
Consider 
\[ \begin{array}{rl@{\qquad}l}
Y_{t} &= F_{t}\theta_{t}+v_{t} & v_{t} \sim N(0,V_{t}) \\
\theta_{t} &= G_{t}\theta_{t-1}+w_{t} & w_{t}\sim N(0,W_{t})
\end{array} \] \pause
where $V_t = \sum_{i=1}^h V_{i,t}$, 

\[ \theta_t = \left[ \begin{array}{c} \theta_{1,t} \\ \vdots \\ \theta_{h,t} \end{array} \right] \qquad 
F_t = [F_{1,t}|\cdots |F_{h,t}] \]
and
\[ W_t = \left[ \begin{array}{ccc} W_{1,t} \\ & \ddots \\ && W_{h,t} \end{array}\right]  \qquad G_t = \left[ \begin{array}{ccc} G_{1,t} \\ & \ddots \\ && G_{h,t} \end{array} \right] \]
}

\section{Polynomial trend models}
\subsection{Introduction}
\frame{\frametitle{Polynomial trend model definition}
\begin{quote}
A polynomial model of order $n$ is a DLM with constant [and specified] matrices $F_t=F$ and $G_t=G$, and a forecast function of the form 
\[ f_t(k) = E(Y_{t+k}|y_{1:t}) = a_{t,0}+a_{t,1}k+\cdots+a_{t,n-1}k^{n-1}, \, k\ge 0 \]
where $a_{t,0},\ldots,a_{t,n-1}$ are linear functions of $m_t=E(\theta_t|y_{1:t})$ and are independent of $k$. Thus, the forecast function is a polynomial or order $n-1$ in $k$.
\end{quote}

\vspace{0.2in} \pause

In practice we use, 
\begin{itemize}
\item Local level model ($n=1$).
\item Linear trend model ($n=2$).
\item Exponential trends are accommodated by taking logs and then using a linear trend model.
\end{itemize}
}

\subsection{Local level model}
\frame{\frametitle{Local level model}
  \[ \begin{array}{rl@{\qquad}l}
 Y_t &= \theta_t + v_t &v_t\sim N(0,V) \\
 \theta_t &= \theta_{t-1}+w_t & w_t\sim N(0,W) \\
 p(\theta_0) &= N(m_0,C_0)
 \end{array} \]

 \vspace{0.2in} \pause

 where $F_t=F=1$, $G_t=G=1$, $V_t=V$, and $W_t=W$.

 \vspace{0.2in} \pause

 What is the forecast function? $f_t(k) = E(Y_{t+k}|y_{1:t}) \pause = m_t$.
}

\frame{\frametitle{Lake Superior data}
 \begin{center}
 \includegraphics{lakeSuperior}
 \end{center}
}

\frame{\frametitle{Lake Superior data}
 \begin{center}
 \includegraphics{lakeSuperior2}
 \end{center}
}

\frame{\frametitle{Lake Superior data}
 \begin{center}
 \includegraphics{lakeSuperior3}
 \end{center}
}



\subsection{Linear trend model}
\frame{\frametitle{Linear trend model}
  \[ \begin{array}{rl@{\qquad}l}
 Y_t &= \mu_t + v_t &v_t\sim N(0,V) \\
 \mu_t &= \mu_{t-1}+\beta_{t-1}+w_{t,1} & w_{t,1}\sim N(0,\sigma^2_\mu) \\
 \beta_t &= \beta_{t-1}+w_{t,2} & w_{t,2}\sim N(0,\sigma^2_\beta) \\
 p(\theta_0) &= N(m_0,C_0)
 \end{array} \]
 
 \vspace{0.1in} \pause
 
 \begin{itemize}
 \item $F_t \pause = F= (1,0)$ \pause
 \item $\theta_t \pause = (\mu_t,\beta_t)^\top $ \pause
 \item \[ G_t \pause = \left[ \begin{array}{cc} 1 & 1 \\ 0 & 1 \end{array} \right] \] \pause
 \item Forecast function $f_t(k)=E[Y_{t+k}|y_{1:t}]\pause = \hat{\mu}_t+k\hat{\beta}_t$ 
 \end{itemize}
}

\frame{\frametitle{Lake Superior data}
 \begin{center}
 \includegraphics{spain}
 \end{center}
}

\frame{\frametitle{Polynomial trend models}
 \[ \begin{array}{rl@{\qquad}l}
 Y_t &= F_t\theta_t + v_t &v_t\sim N_m(0,V_t) \\
 \theta_t &= G_t\theta_{t-1}+w_t & w_t\sim N_p(0,W_t) \\
 p(\theta_0) &= N(m_0,C_0)
 \end{array} \]
 
 \vspace{0.1in} \pause
 
 \begin{itemize}
 \item $F_t=F=(1,0,\ldots,0)$ \pause
 \item 
 \[ G_t = G =\left[ \begin{array}{cccccc} 1 & 1 & 0 & 0 & \cdots & 0 \\
 0 & 1 & 1 & 0 & \cdots & 0 \\
 \vdots &&&&\ddots & \vdots \\
 0 & & \cdots & 0 & 1 & 1 \\
 0 & & \cdots & & 0 & 1 \end{array} \right] \] \pause
 \item $W_t = W=\mbox{diag}(W_1,\ldots,W_n)$
 \end{itemize}
}



\begin{frame}[fragile]
\frametitle{Specifying a local level model in R}

<<eval=FALSE>>=
library("dlm")
local.level = dlmModPoly(order=1) # ?dlmModPoly

# Set variances
V(local.level) = 9.465
W(local.level) = 0.121

# Or set variances during model construction
modLSup = dlmModPoly(1, dV = 9.465, dW = 0.121)
@

\end{frame}




\section{Seasonal models}
\subsection{Example time series}
% \begin{frame}
% \frametitle{Example seasonal time series}
% 
% <<>>=
% 
% @
% \end{frame}




\frame{\frametitle{Example seasonal time series}
\setkeys{Gin}{width=0.7\textwidth}

\vspace{-0.1in}

\begin{center}
\includegraphics{nottem}
\end{center}
}

\frame[containsverbatim]{\frametitle{Regression - air temp on month}
{\scriptsize
\begin{verbatim}
Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  46.2900     0.5176  89.433  < 2e-16 ***
xAug         14.2300     0.7320  19.440  < 2e-16 ***
xDec         -6.7600     0.7320  -9.235  < 2e-16 ***
xFeb         -7.1000     0.7320  -9.700  < 2e-16 ***
xJan         -6.5950     0.7320  -9.010  < 2e-16 ***
xJul         15.6100     0.7320  21.325  < 2e-16 ***
xJun         11.7500     0.7320  16.052  < 2e-16 ***
xMar         -4.0950     0.7320  -5.594 6.32e-08 ***
xMay          6.2700     0.7320   8.566 1.62e-15 ***
xNov         -3.7100     0.7320  -5.068 8.29e-07 ***
xOct          3.2050     0.7320   4.378 1.82e-05 ***
xSep         10.1900     0.7320  13.921  < 2e-16 ***
---
Signif. codes:  0 ë***í 0.001 ë**í 0.01 ë*í 0.05 ë.í 0.1 ë í 1

Residual standard error: 2.315 on 228 degrees of freedom
Multiple R-squared: 0.9304,     Adjusted R-squared: 0.9271
F-statistic: 277.3 on 11 and 228 DF,  p-value: < 2.2e-16
\end{verbatim}
}
}

\frame[containsverbatim]{\frametitle{No intercept regression - air temp on month}
{\scriptsize
\begin{verbatim}
Coefficients:
     Estimate Std. Error t value Pr(>|t|)
xApr  46.2900     0.5176   89.43   <2e-16 ***
xAug  60.5200     0.5176  116.93   <2e-16 ***
xDec  39.5300     0.5176   76.37   <2e-16 ***
xFeb  39.1900     0.5176   75.72   <2e-16 ***
xJan  39.6950     0.5176   76.69   <2e-16 ***
xJul  61.9000     0.5176  119.59   <2e-16 ***
xJun  58.0400     0.5176  112.13   <2e-16 ***
xMar  42.1950     0.5176   81.52   <2e-16 ***
xMay  52.5600     0.5176  101.55   <2e-16 ***
xNov  42.5800     0.5176   82.27   <2e-16 ***
xOct  49.4950     0.5176   95.62   <2e-16 ***
xSep  56.4800     0.5176  109.12   <2e-16 ***
---
Signif. codes:  0 ë***í 0.001 ë**í 0.01 ë*í 0.05 ë.í 0.1 ë í 1

Residual standard error: 2.315 on 228 degrees of freedom
Multiple R-squared: 0.9979,     Adjusted R-squared: 0.9978
F-statistic:  9231 on 12 and 228 DF,  p-value: < 2.2e-16
\end{verbatim}
}
}

\frame{\frametitle{Time series decomposition}
\setkeys{Gin}{width=0.6\textwidth}
$>$ \texttt{plot(decompose(nottem))}

\vspace{-0.1in}

\begin{center}
\includegraphics{nottem-decompose}
\end{center}
}

\frame{\frametitle{Example seasonal time series}
\setkeys{Gin}{width=0.7\textwidth}

\vspace{-0.1in}

\begin{center}
\includegraphics{nottem}
\end{center}
}

\frame{\frametitle{Zero-mean seasonal time series}
\setkeys{Gin}{width=0.7\textwidth}

\vspace{-0.1in}

\begin{center}
\includegraphics{nottem2}
\end{center}
}

\subsection{Seasonal factor models}
\frame{\frametitle{Seasonal factors}
Suppose all $s$ seasons have a factor $\alpha_i$. Then
{\small
\begin{columns}
\column{0.5\textwidth}
\begin{eqnarray*}
Y_1 &=& \alpha_1 + v_1 \\
Y_2 &=& \alpha_2 + v_2 \\
&\vdots & \\
Y_s &=& \alpha_s +v_s \\ \pause
Y_{s+1} &=& \alpha_1 + v_{s+1} \\ \pause
Y_{s+2} &=& \alpha_2 + v_{s+2} \\
&\vdots & \\
Y_{2s} &=& \alpha_s +v_{2s} \\ \pause
Y_{2s+1} &=& \alpha_1 + v_{2s+1} \\
Y_{2s+2} &=& \alpha_2 + v_{2s+2} \\
&\vdots &
\end{eqnarray*} \pause
\column{0.5\textwidth}
\begin{eqnarray*}
Y_t &=& F_t\theta_t + v_t \\
\theta_t &=& G_t\theta_{t-1} + w_t
\end{eqnarray*} \pause
where
\begin{eqnarray*}
\theta_1 &=& \pause (\alpha_1,\alpha_2,\ldots,\alpha_s)^\top  \\
F_t &=& F = \pause (1,0,0,\ldots,0) \\ \pause
G_t &=& G= \pause \left[ \begin{array}{ccccc}
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1 \\
1 & 0 & 0 & \vdots & 0
\end{array} \right] \\ \pause
W_t &=& W = \pause 0 \quad \pause \mbox{(maybe)}
\end{eqnarray*}
\end{columns}
}
}

\frame{\frametitle{Rotating factors}
If $\theta_1 = (\alpha_1,\alpha_2,\ldots,\alpha_s)^\top $ and $W_t=0$, what is $\theta_t$?

\vspace{0.1in} \pause

\begin{eqnarray*}
\theta_2 &=& G\theta_1 = \pause (\alpha_2,\alpha_3,\alpha_4,\ldots,\alpha_s,\alpha_1)^\top  \\ \pause
\theta_3 &=& G\theta_2 = \pause (\alpha_3,\alpha_4,\ldots,\alpha_s,\alpha_1,\alpha_2)^\top  \\ \pause
&\vdots & \\ \pause
\theta_t &=& G\theta_{t-1} = \pause (\alpha_{j},\alpha_{j+1},\ldots,\alpha_s,\alpha_1,\ldots,\alpha_{j-1})^\top  \\ \pause
\end{eqnarray*}
where $j=t \mbox{ mod } s$ ($j\%\%s$ in R).
}

\frame{\frametitle{Alternative DLM for seasonal factors}
Suppose all $s$ seasons have a factor $\alpha_i$. Then
{\small
\begin{columns}
\column{0.5\textwidth}
\begin{eqnarray*}
Y_1 &=& \alpha_1 + v_1 \\
Y_2 &=& \alpha_2 + v_2 \\
&\vdots & \\
Y_s &=& \alpha_s +v_s \\
Y_{s+1} &=& \alpha_1 + v_{s+1} \\
Y_{s+2} &=& \alpha_2 + v_{s+2} \\
&\vdots & \\
Y_{2s} &=& \alpha_s +v_{2s} \\
Y_{2s+1} &=& \alpha_1 + v_{2s+1} \\
Y_{2s+2} &=& \alpha_2 + v_{2s+2} \\
&\vdots &
\end{eqnarray*}
\column{0.5\textwidth}
\begin{eqnarray*}
Y_t &=& F_t\theta_t + v_t \\
\theta_t &=& G_t\theta_{t-1} + w_t
\end{eqnarray*} \pause
where
\begin{eqnarray*}
\theta_1 &=&  (\alpha_1,\alpha_s,\alpha_{s-1},\ldots,\alpha_2)^\top  \\
F_t &=& F =  (1,0,0,\ldots,0) \\
G_t &=& G=  \left[ \begin{array}{ccccc}
0 & 0 & \cdots & 0 & 1 \\
1 & 0 & \cdots & 0 & 0 \\
0 & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \vdots & 1& 0
\end{array} \right] \\
W_t &=& W =  0 \quad  \mbox{(maybe)}
\end{eqnarray*}
\end{columns}
}
}

\frame{\frametitle{Identifiability in seasonal factor models}
\begin{itemize}
\item Modeling mean separately
\item Looking at deviations from the mean \pause
 \begin{itemize}
 \item[$\implies$] parameter identifiability issue
 \end{itemize}

\vspace{0.2in} \pause

\item Identifiability constraints: \pause
 \begin{itemize}
 \item Set $\alpha_j=0$ for some $j\in\{1,2,\ldots,s\}$. \pause
 \item Sum-to-zero constraint, i.e. $\sum_{i=1}^s \alpha_i=0$.
 \end{itemize}
\end{itemize}
}

\frame{\frametitle{Parsimonious seasonal factor model}
{\tiny
\begin{columns}
\column{0.5\textwidth}
\begin{eqnarray*}
Y_t &=& F_t\theta_t + v_t \\
\theta_t &=& G_t\theta_{t-1} + w_t
\end{eqnarray*} \pause
where
\begin{eqnarray*}
\theta_1 &=&  (\alpha_1,\alpha_s,\alpha_{s-1},\ldots,\alpha_3)^\top  \\
F_t &=& F =  (1,0,\ldots,0) \\
G_t &=& G=  \left[ \begin{array}{ccccc}
-1 & -1 & \cdots & -1 & -1 \\
1 & 0 & \cdots & 0 & 0 \\
0 & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \vdots & 1& 0
\end{array} \right] \\
W_t &=& W =  0 \quad  \mbox{(maybe)}
\end{eqnarray*} \pause
\column{0.5\textwidth}
What is $\theta_2$ if $W=0$?
\begin{eqnarray*}
\theta_2 &=& G\theta_1 = \pause (\alpha_2,\alpha_1,\alpha_s,\alpha_{s-1},\ldots,\alpha_4)^\top  \\ \pause
\theta_3 &=& G\theta_2 = \pause (\alpha_3,\alpha_2,\alpha_1,\alpha_s,\alpha_{s-1},\ldots,\alpha_5)^\top  \\ \pause
&\vdots & \\ \pause
\theta_t &=& G\theta_{t-1} \\
&=& \pause (\alpha_{j},\alpha_{j-1},\ldots,\alpha_1,\alpha_s,\alpha_{s-1},\ldots,\alpha_{j+2})^\top  \\ \pause
\end{eqnarray*}
where $j=t \mbox{ mod } s$.
\end{columns}
}
}

\begin{frame}[fragile]
\frametitle{Seasonal factor model in R}
<<lm_model, cache=TRUE>>=
y = as.numeric(nottem)
months = c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")
x = factor(rep(months,length(y)/12), levels=months)
summary(m <- lm(y-mean(y)~x-1))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Seasonal factor model in R}
<<seasonal_factor_model, dependson='lm_model', cache=TRUE, echo=TRUE>>=
library(dlm)
nottem.model = dlmModSeas(12,dV=2.315^2,dW=rep(0,11))
nottem.filter = dlmFilter(y-mean(y),nottem.model)
n = length(y) + 1 # Due to theta_0
data.frame(mean = c(-sum(nottem.filter$m[n,]), rev(nottem.filter$m[n,])),
           lm_mean = m$coefficients)
@

<<>>=
# V = unlist(dlmSvd2var(nottem.filter$U.C[[n]], nottem.filter$D.C[n,]))
@
\end{frame}



\section{Fourier form seasonal models}
\subsection{Basis}
\frame{\frametitle{Canonical basis}
Suppose all $s$ seasons have a factor $\alpha_i$. 
Then, think of $\alpha$ 
as a linear combination of basis vectors
\[ \alpha= (\alpha_1,\ldots,\alpha_s) = \sum_{i=1}^s \alpha_i u_i \]
where $u_i$ is the $s$-dimensional vector having the $i$th component equal to 
one and all other elements zero.

\vspace{0.2in} \pause

\begin{itemize}
\item This representation lacks
 \begin{itemize}[<+->]
 \item Interpretation
 \item Smoothness differentiation
 \item Parsimony
 \end{itemize}
\end{itemize}
}

\frame{\frametitle{Fourier frequencies}
Let
\[ \omega_j = 2\pi \frac{j}{s}, \quad j=0,1,\ldots,\frac{s}{2}\]
where $s$ is the length of the period.

\vspace{0.1in} \pause

Consider
{\scriptsize
\[ \begin{array}{rl}
e_0 &= (1,1,\ldots,1)^\top  \pause \\
c_1 &= (\cos \omega_1, \cos 2\omega_1, \ldots \cos s\omega_1)^\top  \\
s_1 &= (\sin \omega_1, \sin 2\omega_1, \ldots \sin s\omega_1)^\top  \pause \\
&\vdots \\
c_j &= (\cos \omega_j, \cos 2\omega_j, \ldots \cos s\omega_j)^\top  \\
s_j &= (\sin \omega_j, \sin 2\omega_j, \ldots \sin s\omega_j)^\top  \pause \\
&\vdots \\
c_{s/2} &= (\cos \omega_{s/2}, \cos 2\omega_{s/2}, \ldots \cos s\omega_{s/2})^\top 
\end{array} \]
}

\pause

$s_{s/2}$ is all zeros
}


% 
% \begin{frame}
% \frametitle{Fourier frequencies for period of length 12}
% 
% <<>>=
% s = 12 # period; must be an even number or you may break the code
% js = 0:(s/2)
% d <- expand.grid(j = js, i=1:s) %>%
%   mutate(omega = j/2,
%          c = cos(i*omega),
%          s = sin(i*omega)) %>%
%   tidyr::gather(tmp, value, c, s) %>%
%   mutate(basis = factor(paste0(tmp, j), 
%                         levels = paste0(c("c","s"), rep(js, each=2)))) %>%
%   filter(!(grepl("0", basis)))
% 
% ggplot(d, aes(i, value)) + 
%   geom_point() + 
%   facet_grid(basis~.) +
%   scale_y_continuous(breaks=c(-1,1)) + 
%   theme_bw() +
%   theme(panel.spacing = unit(0, "lines"))
% @
% 
% \end{frame}


\frame{\frametitle{Plotting Fourier basis}

\vspace{-0.1in}

\setkeys{Gin}{width=0.6\textwidth}
\begin{center}
\includegraphics{trigBasisFunctions}
\end{center}
}




\begin{frame}
\frametitle{Fourier basis}
Basis for $\mathbb{R}^s$

\[ \alpha = a_0e_0+\sum_{j=1}^{s/2-1} (a_jc_j+b_js_j)+a_{s/2}c_{s/2}. \]
\pause


\vspace{0.2in} \pause

Assume $a_0=0$ since the mean will be modeled separately, e.g. through a 
polynomial trend model. 
\pause
For $j=1,2,\ldots,s/2$, the $j$th \alert{harmonic} is 
\[ \begin{array}{rl}
S_j(t) &= a_j \cos(t \omega_j) + b_j \sin (t \omega_j) \\
&= A_j \cos (t \omega_j+ \gamma_j)
\end{array} \]
where $b_{s/2}=0$, \pause
$A_j = \sqrt{a_j^2+b_j^2}$ is the amplitude, 
and $\gamma_j = \mbox{arctan}(-b_j/a_j)$ is the phase.

% \item Naturally periodic, e.g. $s_j(t+ks)=s_j(t)$ for $k=1,2,\ldots$ \pause since
% \[ \sin \frac{2\pi(t+ks)j}{s} = \sin\left( \frac{2\pi tj}{s} + 2\pi kj\right) = \sin \frac{2\pi tj}{s}.\]
\end{frame}









\subsection{Building a DLM}
\begin{frame}
\frametitle{Evolving a harmonic}
For $j=1,2,\ldots,s/2$, the $j$th \alert{harmonic} is 
\[
S_j(t) = a_j \cos(t \omega_j) + b_j \sin (t \omega_j) 
\]
\pause
\alert{What is $S_j(t+1)$?} 
\pause
It is 
\[ 
S_j(t+1) = a_j \cos([t+1] \omega_j) + b_j \sin ([t+1] \omega_j).
\]
\pause
For $j<s/2$, 
if we only know the value of $S_j(t)$, 
i.e. we don't know the value of $a_j$ and $b_j$ individual, 
we cannot determine $S_j(t+1)$. 
\pause
But we can find that 
\[ \begin{array}{rl}
S_j(t+1) =& a_j \cos([t+1] \omega_j) + b_j \sin ([t+1] \omega_j) \\
&\vdots \\
=& (a_j \cos(t \omega_j) + b_j \sin (t \omega_j)) \cos(\omega_j) + \\
&+ (-a_j \sin(t \omega_j) + b_j \cos(t \omega_j)) \sin(\omega_j) \\ \\
=& S_j(t) \cos(\omega_j) + S_j^*(t) \sin(\omega_j)
\end{array} \]

\end{frame}

\frame{\frametitle{Building a DLM - constructing G}
For $j=1,2,\ldots,s/2$, the $j$th \alert{harmonic} is 
\[
S_j(t) = a_j \cos(t \omega_j) + b_j \sin (t \omega_j) 
\]
\pause 
% Then
% \[ g_t = \sum_{j=1}^{s/2} S_j(t).\]
% \pause 
The \alert{conjugate harmonic} is
\[ S_j^*(t) = -a_j \sin(t \omega_j) + b_j \cos (t \omega_j).\]
\pause
And the $j$th harmonic evolves in time according to 
\[ 
\left[ \begin{array}{c} S_j(t+1) \\ S_j^*(t+1) \end{array}\right] =
\left[ \begin{array}{cc} \phantom{-}\cos \omega_j & \phantom{-}\sin \omega_j \\ -\sin \omega_j & \phantom{-}\cos \omega_j \end{array} \right]
\left[ \begin{array}{c} S_j(t) \\ S_j^*(t) \end{array}\right]
\]
and 
\[ S_{s/2}(t+1) = -S_{s/2}(t) \]
}






\frame{\frametitle{Period $j$ DLMs}
Consider DLMs containing only seasonality of frequency $j<s/2$.
\[ \left[\begin{array}{c} S_j(t+1) \\ S_j^*(t+1) \end{array} \right] = \left[ \begin{array}{rr} \cos \omega_j & \sin \omega_j \\ -\sin \omega_j & \cos \omega_j \end{array} \right] \left[\begin{array}{c} S_j(t) \\ S_j^*(t) \end{array} \right].\] \pause
So this is a DLM with evolution matrix
\[ H_j = \left[ \begin{array}{rr} \cos \omega_j & \sin \omega_j \\ -\sin \omega_j & \cos \omega_j \end{array} \right], \]
state vector $(S_j(t),S_j^*(t))^\top $, and observation matrix $F=[1\, 0]$.

\vspace{0.1in} \pause

If $j=s/2$, we have
\[ S_{s/2}(t+1) = \cos((t+1)\pi)=-\cos(t\pi)=-S_{s/2}(t) \] \pause
which is a DLM with state vector $S_{s/2}(t)$, evolution matrix $H_{s/2}=[-1]$, and observation matrix $F=[1]$.
}

\frame{\frametitle{Fourier form seasonal DLM}
Combine these period $j$ DLMs using our combining rules:
\[ \theta_t = (S_1(t),S_1^*(t),\ldots,S_{\frac{s}{2}-1}(t),S_{\frac{s}{2}-1}^*(t),S_{\frac{s}{2}}(t))^\top  \]
with evolution matrix
\[ G = \mbox{blockdiag}(H_1,\ldots,H_{\frac{s}{2}}) \]
and observation matrix
\[ F=[1\, 0\, 1\, 0\, \ldots\, 0\, 1]\]
and initial vector
\[ \theta_0 = (a_1,b_1,\ldots,a_{\frac{s}{2}-1},b_{\frac{s}{2}-1},a_{\frac{s}{2}})^\top . \]
\pause
The seasonal process can be made more smooth by dropping higher harmonics.
}


\begin{frame}[fragile]
\frametitle{Nottingham data}
<<echo=TRUE>>=
d = ddply(data.frame(harmonics=1:6), .(harmonics), function(x) {
  nottem.model = dlmModTrig(12,x$harmonics,dV=2.315^2,dW=rep(0,11))
  nottem.filter = dlmFilter(y-mean(y),nottem.model)
  forecast = dlmForecast(nottem.filter, 12)
  data.frame(month = factor(months, levels=months),
             effect = forecast$f)
})
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Nottingham data}
<<echo=TRUE, out.width='.7\\linewidth'>>=
ggplot(subset(d, harmonics==6), aes(month, effect, group=1)) + 
  geom_line() +
  theme_bw()
@
\end{frame}


\frame{\frametitle{Plotting Harmonics for Nottingham data}

\vspace{-0.1in}

\setkeys{Gin}{width=0.6\textwidth}
\begin{center}
\includegraphics{nottemHarmonics}
\end{center}
}


\begin{frame}[fragile]
\frametitle{Nottingham data - fewer harmonics}
<<echo=TRUE,out.width='.7\\linewidth'>>=
d$harmonics_f = factor(d$harmonics)
ggplot(d, aes(month, effect, color=harmonics_f, shape=harmonics_f, group=harmonics_f)) + 
  geom_point() + 
  geom_line() +
  theme_bw()
@
\end{frame}




\subsection{General periodic components}
\begin{frame}[fragile]
\frametitle{Sunspots}

The previous development works for even period $s$ and odd period $s$ with the 
slight change that $j=0,\ldots,(s-1)/2$. 
\pause
A similar development can be constructed for periodic observations with a 
non-integer period. 
\pause
In this example, the period for sunspots was estimated to be 130.51 months.
This quantity and the variances were estimated via MLEs.

<<sunspots, cache=TRUE, echo=TRUE>>=
mod <- dlmModTrig(q = 2, tau = 130.51, dV = 0,
                  dW = rep(c(1765e-2, 3102e-4), each = 2)) +
        dlmModPoly(1, dV = 0.7452, dW = 0.1606)

sspots <- sqrt(sunspots)
sspots.smooth <- dlmSmooth(sspots, mod)
y <- cbind(sspots,
           tcrossprod(dropFirst(sspots.smooth$s[, c(1, 3, 5)]),
                      matrix(c(0, 0, 1, 1, 1, 0), nr = 2,
                             byrow = TRUE)))
colnames(y) <- c("Sunspots", "Level", "Periodic")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Sunspots}
<<sunspots_plot, dependson='sunspots', echo=TRUE>>=
plot(y, yax.flip = TRUE, oma.multi = c(2, 0, 1, 0))
@
\end{frame}




\section{ARMA models}
\subsection{ARMA model definition}
\frame{\frametitle{ARMA(p,q) [Box-Jenkins models]}
Assuming $\mu=0$, the autoregressive moving average model is :
\begin{eqnarray*} 
Y_t &=& \sum_{j=1}^p \phi_jY_{t-j} + \sum_{j=1}^q \psi_j \epsilon_{t-j}+\epsilon_t \\
\\
\\ \pause
\\
Y_t &=& \sum_{j=1}^r \phi_jY_{t-j} + \sum_{j=1}^{r-1} \psi_j \epsilon_{t-j}+\epsilon_t 
\end{eqnarray*}
where $r=\max\{p,q+1\}$ with $\phi_j=0$ for $j>p$ and $\psi_j=0$ for $j>q$ and 
$\epsilon_t\ind N(0,\sigma^2)$. 
}

\subsection{DLM representation of an ARMA model}
\frame{\frametitle{DLM representation of ARMA model}
Then an ARMA(p,q) is a DLM with
\begin{eqnarray*}
Y_t &=& F\theta_t \\
\theta_{t} &=& G\theta_{t-1} +R\epsilon_t
\end{eqnarray*}
with 

\vspace{-0.4in} \pause

\begin{eqnarray*}
V &=& 0 \\ \pause
W &=& RR^\top \sigma^2 \\ \pause
\theta_t &=& (\theta_{1,t},\ldots,\theta_{r,t})^\top  \\ \pause
F &=& (1,0,\ldots,0) \\ \pause 
G &=& \left[ \begin{array}{ccccc} 
\phi_1 & 1 & 0 & \cdots & 0 \\
\phi_2 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & & \ddots & \vdots \\
\phi_{r-1} & 0 & \cdots & 0 & 1 \\
\phi_r & 0 & \cdots & 0 & 0
\end{array}\right] \\ \pause
R &=& (1, \psi_1,\ldots,\psi_{r-2},\psi_{r-1})^\top  
\end{eqnarray*}
}

\frame{\frametitle{Verification of DLM representation of an ARMA model}
\vspace{-0.3in}
{\small
\begin{eqnarray*}
Y_t &=& \theta_{1,t} \\ \pause
\theta_{1,t} &=& \phi_1 \theta_{1,t-1}+\theta_{2,t-1}+\epsilon_t \\ \pause
\theta_{2,t} &=& \phi_2 \theta_{1,t-1}+\theta_{3,t-1}+\psi_1\epsilon_{t} \\ \pause
&\vdots & \\ 
\theta_{r-1,t} &=& \phi_{r-1} \theta_{1,t-1}+\theta_{r,t-1}+\psi_{r-2}\epsilon_t \\ \pause
\theta_{r,t} &=& \phi_{r} \theta_{1,t-1}+\psi_{r-1}\epsilon_t \\ 
\\
\phantom{\theta_{r-1,t-(r-2)}} &\phantom{\implies}& \phantom{\phi_1\theta_{1,t-1}+\cdots+\phi_r\theta_{1,t-r}+\psi_1\epsilon_{t-1}+\cdots+\psi_{r-1}\epsilon_{t-r-1}+\epsilon_t} \\ && \phantom{\sum_{j=1}^r \phi_jY_{1,t-j}+\sum_{j=1}^{r-1} \psi_j\epsilon_{t-j}+\epsilon_t}\\
 \\
\\
\end{eqnarray*}
}
}

\frame{\frametitle{Verification of DLM representation of an ARMA model}
\vspace{-0.3in}
{\small
\begin{eqnarray*}
Y_t &=& \theta_{1,t} \\
\theta_{1,t} &=& \phi_1 \theta_{1,t-1}+\theta_{2,t-1}+\epsilon_t \\
\theta_{2,t-1} &=& \phi_2 \theta_{1,t-2}+\theta_{3,t-2}+\psi_1\epsilon_{t-1} \\
&\vdots & \\
\theta_{r-1,t-(r-2)} &=& \phi_{r-1} \theta_{1,t-(r-1)}+\theta_{r,t-(r-1)}+\psi_{r-2}\epsilon_{t-(r-2)} \\
\theta_{r,t-(r-1)} &=& \phi_{r} \theta_{1,t-r}+\psi_{r-1}\epsilon_{t-(r-1)} \\ \pause
&\implies \\
\theta_{1,t} &=& \phi_1\theta_{1,t-1}+\phi_2\theta_{1,t-2}+\theta_{3,t-2}+\psi_1\epsilon_{t-1}+\epsilon_t \\ \pause
&\implies \\
\theta_{1,t} &=& \phi_1\theta_{1,t-1}+\cdots+\phi_r\theta_{1,t-r}+\psi_1\epsilon_{t-1}+\cdots+\psi_{r-1}\epsilon_{t-(r-1)}+\epsilon_t \\ \pause
Y_{t} &=& \phi_1Y_{t-1}+\cdots+\phi_rY_{t-r}+\psi_1\epsilon_{t-1}+\cdots+\psi_{r-1}\epsilon_{t-(r-1)}+\epsilon_t \\ \pause
Y_{t} &=& \sum_{j=1}^r \phi_jY_{1,t-j}+\sum_{j=1}^{r-1} \psi_j\epsilon_{t-j}+\epsilon_t
\end{eqnarray*}
}
}

\subsection{ARIMA models}
\frame{\frametitle{ARIMA(p,d,q)}
\begin{itemize}[<+->]
\item An ARIMA(p,d,q) can be fit by taking the $d$th order difference of the data and then applying an ARMA(p,q) model.
 \begin{itemize}
 \item Clearly we can do the previous with the DLM representation.
 \end{itemize}
\item Alternatively, ARIMA models have a direct DLM representation (see section 3.2.5 in Petris)
\item ARIMA models are typically used for non-stationary time series.
\item In the DLM framework, modeling non-stationarity through a polynomial trend or seasonal model is more common than using the ARIMA framework.
\end{itemize}
}

\subsection{Output gap example}
\frame{\frametitle{Log US GDP}
\vspace{-0.2in}
\begin{center}
\includegraphics{logusgdp}
\end{center}
}

\frame{\frametitle{A model for Log US GDP}
\[ Y_t = Y_t^{(p)}+Y_t^{(g)} \] \pause
\begin{columns}
\column{0.5\textwidth}
\begin{eqnarray*}
Y_t ^{(p)} &=& F^{(p)}\theta_t^{(p)} \\
\theta_t^{(p)} &=& G^{(p)}\theta_{t-1}^{(p)} + w_t^{(p)} \\
\\
\theta_t^{(p)} &=& (Y_t^{(p)},\delta_t)^\top  \\
F^{(p)} &=& (1,0)^\top  \\
G^{(p)} &=& \left[ \begin{array}{cc} 1 & 1 \\ 0 & 1 \end{array} \right] \\
W^{(p)} &=& \mbox{diag}(\sigma^2_\epsilon,\sigma^2_z) 
\end{eqnarray*}
\column{0.5\textwidth} \pause
\begin{eqnarray*}
Y_t ^{(g)} &=& F^{(g)}\theta_t^{(g)} \\
\theta_t^{(g)} &=& G^{(g)}\theta_{t-1}^{(g)} + w_t^{(g)} \\
\\
\theta_t^{(g)} &=& (Y_t^{(g)},\theta_{t,2}^{(g)})^\top  \\
F^{(g)} &=& (1,0)^\top  \\
G^{(g)} &=& \left[ \begin{array}{cc} \phi_1 & 1 \\ \phi_2 & 0 \end{array} \right] \\
W^{(g)} &=& \mbox{diag}(\sigma^2_u,0)
\end{eqnarray*}
\end{columns}
}

\frame{\frametitle{A model for Log US GDP}

\vspace{-0.1in}

\begin{eqnarray*}
Y_t &=& F\theta_t +v_t\\
\theta_t &=& G\theta_{t-1} + w_t
\end{eqnarray*}
\pause

\vspace{0.1in}

\begin{eqnarray*}
\theta_t &=& (Y_t^{(p)},\delta_t,Y_t^{(g)},\theta_{t,2}^{(g)})^\top  \\
F &=& (1,0,1,0) \\
G &=& \left[ \begin{array}{cccc} 
1 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & \phi_1 & 1 \\
0 & 0 & \phi_2 & 0 \end{array} \right] \\
V &=& 0 \\
W &=& \mbox{diag}(\sigma^2_\epsilon,\sigma^2_z,\sigma^2_u,0)
\end{eqnarray*}
}

\frame[containsverbatim]{\frametitle{MLE parameter estimates in Log US GDP model}
%\begin{columns}
%\column{0.5\textwidth}
{\small
\begin{verbatim}
> level0 <- Lgdp[1]
> slope0 <- mean(diff(Lgdp))
> buildGap <- function(u) {
    trend <- dlmModPoly(dV = 1e-7, dW = exp(u[1:2]),
                        m0 = c(level0, slope0), C0 = 2 * diag(2))
    gap <- dlmModARMA(ar = u[4:5], sigma2 = exp(u[3]))
    return(trend + gap)}
> init <- c(-3, -1, -3, .4, .4)
> outMLE <- dlmMLE(Lgdp, init, buildGap)
> dlmGap <- buildGap(outMLE$par)
> sqrt(diag(W(dlmGap))[1:3])
[1] 0.0057817835 0.0000763763 0.0061453639
> GG(dlmGap)[3:4, 3]
[1]  1.4806256 -0.5468107
\end{verbatim}
}
}

\frame[containsverbatim]{\frametitle{MLE parameter estimates in Log US GDP model}
\setkeys{Gin}{width=0.5\textwidth}
\begin{verbatim}
gdpSmooth <- dlmSmooth(Lgdp, dlmGap)
\end{verbatim}

\begin{center}
\includegraphics{gdpSmooth}
\end{center}
}





\section{DLM Regression models}
\subsection{DLM Regression models}
\begin{frame}
\frametitle{Standard regression model}

Consider a temporal regression problem with $y_t \in \mathbb{R}$ and 
$x_t \in \mathbb{R}^p$.
\pause
A standard regression model assumes 
\[
y_t = x_t^\top\theta + \epsilon_t, \qquad \epsilon_t \ind N(0,\sigma^2).
\]
\pause
But, if the effect of $x_t$ on $y_t$ changes with $t$, 
\pause
we may want to consider the model 
\[
y_t = x_t^\top\theta_t + \epsilon_t, \qquad \epsilon_t \ind N(0,\sigma^2).
\]
\pause
And put some kind of evolution on $\theta_t$, e.g. 
\[ 
\theta_t = G_t \theta_{t-1} + \omega_t,\qquad \omega_t \ind N_p(0,W_t).
\]
\pause
\alert{Outstanding problem: How can we differentiate dynamic coefficients from
autocorrelated errors?}
\end{frame}




\begin{frame}
\frametitle{Dynamic regression model}
This dynamic regression model written as a DLM is 
\[ \begin{array}{rll}
Y_t &= F_t \theta_t + \onoise_t & \onoise_t \ind N(0,\sigma_t^2) \\
\theta_t &= G_t \theta_{t-1} + w_t & w_t \ind N_p(0,W_t) 
\end{array} \]
\pause
where 
\begin{itemize}
\item $F_t = x_t^\top$,
\item (typically) $G_t = I_p$, and 
\item (typically) $W_t$ is diagonal.
\end{itemize}
\end{frame}





\subsection{Capital asset pricing model}
\frame{\frametitle{Background}
Use the following notation:
\begin{tabular}{ll}
$r_t$ &: return at time $t$ of the asset under study \\
$r_t^{(M)}$ &: return at time $t$ of the market \\
$r_t^{(f)}$ &: return at time $t$ of a risk free asset
\end{tabular}

\vspace{0.2in} \pause

Let 
\[ \begin{array}{ll}
y_t &= r_t - r_t^{(f)} \\
x_t &= r_t^{(M)}-r_t^{(f)} 
\end{array} \]

\vspace{0.2in} \pause

A univariate capital asset pricing model (CAPM) assumes that 
\[ y_t = \alpha+\beta x_t +v_t, \qquad v_t \ind N(0,\sigma^2). \]
}



\begin{frame}
<<>>=
# see page 123 in Petris, et. al.
capm <- structure(list(MOBIL = c(-0.046, -0.017, 0.049, 0.077, -0.011, 
-0.043, 0.028, 0.056, 0.064, -0.069, 0.037, 0.041, 0.061, -0.002, 
0.029, 0.079, -0.086, 0.088, 0.018, 0.111, 0.18, -0.031, 0.051, 
0.063, 0.075, 0.366, -0.176, 0.119, 0.003, -0.024, 0.054, -0.059, 
0.007, 0.059, 0.193, -0.081, -0.082, -0.067, -0.042, -0.025, 
-0.092, 0.053, 0.021, -0.057, -0.106, 0.025, 0.038, -0.09, -0.016, 
-0.016, -0.038, -0.011, 0.092, -0.038, -0.073, 0.157, 0.043, 
0.02, -0.04, 0.069, 0.055, 0.009, 0.095, 0.091, -0.052, 0.077, 
-0.06, 0.118, -0.05, -0.032, -0.033, 0.009, 0.084, 0.024, -0.028, 
0.029, -0.146, 0.01, -0.092, 0.261, 0.013, 0.014, -0.042, -0.052, 
0.053, 0.071, 0, 0.027, 0.029, -0.032, 0.002, -0.013, 0.004, 
0.1, -0.008, -0.04, -0.021, -0.003, -0.026, 0.042, 0.082, 0.012, 
-0.022, 0.173, 0.053, 0.02, 0.044, 0.019, 0.093, -0.022, 0.124, 
-0.007, -0.003, 0.091, 0.032, 0.03, -0.082, -0.178, -0.15, 0.159
), IBM = c(-0.029, -0.043, -0.063, 0.13, -0.018, -0.004, 0.092, 
0.049, -0.051, -0.046, 0.031, 0.108, 0.034, -0.017, 0.052, -0.004, 
-0.022, -0.035, -0.049, 0.016, -0.032, -0.079, 0.06, -0.013, 
0.066, -0.062, -0.122, -0.016, 0.025, 0.061, 0.111, 0.017, -0.021, 
0.039, 0.035, -0.004, -0.052, 0.011, -0.029, -0.06, 0.017, -0.015, 
-0.03, -0.002, -0.018, -0.048, 0.075, 0.044, 0.119, -0.014, -0.034, 
0.075, -0.029, -0.014, 0.082, 0.087, 0.041, 0.089, 0.094, 0.113, 
0.027, 0.01, 0.028, 0.15, -0.041, 0.081, 0.001, 0.001, 0.062, 
-0.001, -0.066, 0.039, -0.065, -0.026, 0.034, -0.002, -0.044, 
-0.019, 0.047, 0.127, 0.004, 0.012, -0.023, 0.011, 0.108, -0.009, 
-0.052, -0.004, 0.025, -0.038, 0.062, -0.028, -0.022, 0.048, 
0.085, 0.113, -0.026, 0.003, 0.004, 0.031, -0.018, -0.039, -0.096, 
0.055, -0.031, -0.081, 0.037, -0.056, 0.073, 0.092, 0.076, 0.067, 
0.006, 0.016, -0.009, 0.053, -0.105, -0.187, -0.087, 0.043), 
    WEYER = c(-0.116, -0.135, 0.084, 0.144, -0.031, 0.005, 0.164, 
    0.039, -0.021, -0.09, -0.033, -0.034, 0.203, -0.038, 0.097, 
    -0.069, -0.013, 0.053, 0, 0.165, -0.015, -0.083, -0.065, 
    0.104, 0.069, 0.033, -0.129, 0.027, 0.089, -0.026, 0.14, 
    -0.041, -0.064, 0.017, 0.015, 0.007, 0.028, 0.025, 0.088, 
    -0.05, -0.031, 0.021, -0.081, -0.061, -0.113, -0.02, 0.179, 
    -0.072, -0.079, 0.014, -0.009, 0.059, -0.086, -0.015, -0.012, 
    0.221, -0.029, 0.15, 0.141, -0.04, 0.023, 0.065, -0.023, 
    0.091, -0.067, -0.013, -0.071, -0.011, -0.033, -0.046, 0.151, 
    -0.069, -0.039, -0.093, 0.094, -0.088, -0.087, 0.019, 0.036, 
    0.055, -0.069, 0.035, 0.032, 0.026, 0.084, -0.016, -0.081, 
    0.003, 0.031, -0.004, 0.02, -0.013, -0.074, 0.008, 0.171, 
    -0.004, 0.072, 0.123, 0.051, -0.037, 0.01, -0.061, -0.048, 
    0.122, -0.058, 0.135, 0.006, -0.041, 0.27, 0.094, 0.089, 
    -0.027, -0.107, 0.026, 0.021, 0.081, -0.054, -0.271, -0.066, 
    0.103), CITCRP = c(-0.115, -0.019, 0.059, 0.127, 0.005, 0.007, 
    0.032, 0.088, 0.011, -0.071, -0.005, -0.019, 0.043, -0.082, 
    0.026, 0, 0.022, 0.095, -0.075, 0.065, -0.017, -0.125, 0.03, 
    0.113, -0.079, -0.08, -0.069, 0.048, 0.104, 0.058, -0.023, 
    0.029, -0.068, -0.049, 0.123, 0.131, -0.062, -0.005, 0.045, 
    0.086, 0.099, -0.013, -0.019, -0.108, 0.032, 0.052, 0.045, 
    -0.028, 0.035, 0, 0.007, 0.101, -0.101, -0.003, -0.025, 0.077, 
    0.059, 0.318, 0.007, -0.098, 0.085, 0.039, 0.132, 0.104, 
    -0.102, -0.016, -0.079, -0.007, 0.006, -0.118, 0.162, 0.023, 
    0.024, -0.039, -0.054, -0.004, -0.148, 0.078, -0.029, 0.164, 
    0.076, -0.027, 0, 0.098, 0.097, -0.015, 0.046, 0.012, 0.094, 
    0.043, -0.03, -0.063, -0.085, 0.09, 0.062, 0.065, 0.005, 
    0.101, 0.153, -0.042, 0.038, -0.036, -0.117, 0.082, -0.111, 
    0.04, 0.01, 0.019, 0.087, -0.066, -0.052, 0.07, 0.052, 0.051, 
    0.041, 0.033, -0.086, -0.282, -0.136, 0.064), MARKET = c(-0.045, 
    0.01, 0.05, 0.063, 0.067, 0.007, 0.071, 0.079, 0.002, -0.189, 
    0.084, 0.015, 0.058, 0.011, 0.123, 0.026, 0.014, 0.075, -0.013, 
    0.095, 0.039, -0.097, 0.116, 0.086, 0.124, 0.112, -0.243, 
    0.08, 0.062, 0.086, 0.065, 0.025, 0.015, 0.006, 0.092, -0.056, 
    -0.014, -0.009, 0.067, -0.008, 0.064, -0.003, -0.033, -0.031, 
    -0.164, 0.062, 0.069, -0.039, -0.079, -0.101, -0.028, 0.041, 
    0.003, -0.078, -0.006, 0.122, 0.008, 0.136, 0.049, 0.014, 
    0.065, 0.028, 0.043, 0.097, 0.08, 0.048, -0.017, -0.034, 
    0, -0.082, 0.066, -0.012, -0.029, -0.03, 0.003, -0.003, -0.058, 
    0.005, -0.058, 0.146, 0, -0.035, -0.019, -0.001, 0.097, 0.012, 
    0.008, -0.01, 0.019, -0.003, 0.012, 0.005, -0.055, 0.026, 
    0.059, 0.013, -0.009, 0.049, 0.048, -0.009, 0.049, 0.004, 
    -0.076, 0.049, -0.047, 0.018, 0, -0.005, 0.148, 0.065, 0.037, 
    -0.025, 0.004, 0.038, 0.055, 0.015, -0.015, -0.26, -0.07, 
    0.073), RKFREE = c(0.00487, 0.00494, 0.00526, 0.00491, 0.00513, 
    0.00527, 0.00528, 0.00607, 0.00645, 0.00685, 0.00719, 0.0069, 
    0.00761, 0.00761, 0.00769, 0.00764, 0.00772, 0.00715, 0.00728, 
    0.00789, 0.00802, 0.00913, 0.00819, 0.00747, 0.00883, 0.01073, 
    0.01181, 0.00753, 0.0063, 0.00503, 0.00602, 0.00731, 0.0086, 
    0.00895, 0.01137, 0.00977, 0.01092, 0.01096, 0.01025, 0.01084, 
    0.01255, 0.01128, 0.01154, 0.01169, 0.01054, 0.01003, 0.00816, 
    0.0074, 0.00949, 0.00946, 0.01067, 0.00972, 0.00908, 0.00914, 
    0.00714, 0.00503, 0.00563, 0.0062, 0.00614, 0.00648, 0.00646, 
    0.00599, 0.00686, 0.00652, 0.00649, 0.00673, 0.00714, 0.00668, 
    0.00702, 0.00678, 0.00683, 0.00693, 0.00712, 0.00672, 0.00763, 
    0.00741, 0.00627, 0.00748, 0.00771, 0.00852, 0.0083, 0.00688, 
    0.00602, 0.00612, 0.00606, 0.00586, 0.0065, 0.00601, 0.00512, 
    0.00536, 0.00562, 0.00545, 0.00571, 0.00577, 0.0054, 0.00479, 
    0.00548, 0.00523, 0.00508, 0.00444, 0.00469, 0.00478, 0.00458, 
    0.00343, 0.00416, 0.00418, 0.0042, 0.00382, 0.00454, 0.00437, 
    0.00423, 0.00207, 0.00438, 0.00402, 0.00455, 0.0046, 0.0052, 
    0.00358, 0.00288, 0.00277)), .Names = c("MOBIL", "IBM", "WEYER", 
"CITCRP", "MARKET", "RKFREE"), class = "data.frame", row.names = c("1978.01", 
"1978.02", "1978.03", "1978.04", "1978.05", "1978.06", "1978.07", 
"1978.08", "1978.09", "1978.10", "1978.11", "1978.12", "1979.01", 
"1979.02", "1979.03", "1979.04", "1979.05", "1979.06", "1979.07", 
"1979.08", "1979.09", "1979.10", "1979.11", "1979.12", "1980.01", 
"1980.02", "1980.03", "1980.04", "1980.05", "1980.06", "1980.07", 
"1980.08", "1980.09", "1980.10", "1980.11", "1980.12", "1981.01", 
"1981.02", "1981.03", "1981.04", "1981.05", "1981.06", "1981.07", 
"1981.08", "1981.09", "1981.10", "1981.11", "1981.12", "1982.01", 
"1982.02", "1982.03", "1982.04", "1982.05", "1982.06", "1982.07", 
"1982.08", "1982.09", "1982.10", "1982.11", "1982.12", "1983.01", 
"1983.02", "1983.03", "1983.04", "1983.05", "1983.06", "1983.07", 
"1983.08", "1983.09", "1983.10", "1983.11", "1983.12", "1984.01", 
"1984.02", "1984.03", "1984.04", "1984.05", "1984.06", "1984.07", 
"1984.08", "1984.09", "1984.10", "1984.11", "1984.12", "1985.01", 
"1985.02", "1985.03", "1985.04", "1985.05", "1985.06", "1985.07", 
"1985.08", "1985.09", "1985.10", "1985.11", "1985.12", "1986.01", 
"1986.02", "1986.03", "1986.04", "1986.05", "1986.06", "1986.07", 
"1986.08", "1986.09", "1986.10", "1986.11", "1986.12", "1987.01", 
"1987.02", "1987.03", "1987.04", "1987.05", "1987.06", "1987.07", 
"1987.08", "1987.09", "1987.10", "1987.11", "1987.12")) %>%
  
  mutate(date = seq(from = as.Date("1978/1/1"), 
                    by = "month", 
                    length.out = n()))

ggplot(capm %>% tidyr::gather(stock, value, -date),
       aes(date, value, group=stock)) + 
  geom_line() + 
  facet_wrap(~stock) +
  theme_bw()
@
\end{frame}



% \frame{\frametitle{CAPM example}
% \begin{center}
% \includegraphics{returns}
% \end{center}
% }

% \begin{frame}
% \frametitle{Static regression of IBM returns}
% <<>>=
% m <- lm(IBM ~ RKFREE, capm)
% m$coeff
% 
% m2 <- dlmModReg(capm$RKFREE, dV = summary(m)$sigma^2) 
% mF <- dlmFilter(capm$IBM, m2)
% mF$m[1 + length(capm$IBM), ]
% @
% \end{frame}

\frame[containsverbatim]{\frametitle{Static regression of IBM returns}
 
\begin{verbatim}
> outLM <- lm(IBM ~ x)
> outLM$coef
  (Intercept)             x
-0.0004895937  0.4568207721 
>
> mod <- dlmModReg(x, dV = 0.00254, m0 = c(0, 0),
+                  C0 = diag(c(1e+07, 1e+07)))
> outF <- dlmFilter(IBM, mod)
> outF$m[1 + length(IBM), ]
[1] -0.0004895937  0.4568207719
\end{verbatim}

\vspace{0.2in}

Since the estimate for $\beta<1$ the stock would be considered conservative. 
}

\frame[containsverbatim]{\frametitle{Dynamic regression of IBM returns}
\begin{verbatim}
> buildCapm <- function(u) {
+     dlmModReg(x, dV = exp(u[1]), dW = exp(u[2 : 3]))
+ }
> outMLE <- dlmMLE(IBM, parm = rep(0, 3), buildCapm)
> exp(outMLE$par)
[1] 2.328402e-03 1.100214e-05 6.495784e-04
> outMLE$value
[1] -276.7014
> mod <- buildCapm(outMLE$par)
> outS <- dlmSmooth(IBM, mod)
> plot(as.zoo(dropFirst(outS$s)), main = "", 
       mar = c(0, 2.1, 0, 1.1),
       oma = c(2.1,0,.1,.1), cex.axis = 0.5)
\end{verbatim}
}

\frame{\frametitle{CAPM example}
\begin{center}
\includegraphics{dynamicReg}
\end{center}
}
\section{DLMs for longitudinal data}
\subsection{Longitudinal data}
\frame{\frametitle{Longitudinal data}
Suppose at each time point, you have observations from multiple units, e.g. people, countries, stocks, etc. The data can be represented as a matrix. \pause

\[
\begin{array}{|c|cccc|}
\hline
& \multicolumn{4}{c|}{\mbox{Time}} \\
\mbox{Item} & 1 & 2 & \cdots & T \pause \\
\hline
1 & Y_{1,1} & Y_{1,2} & \cdots & Y_{1,T} \pause \\
2 & Y_{2,1} & Y_{2,2} & \cdots & Y_{2,T} \pause \\
\vdots & \multicolumn{4}{c|}{\vdots} \\
m & Y_{m,1} & Y_{m,2} & \cdots & Y_{m,T} \\
\hline
\end{array} \]
}

\subsection{Individual univariate DLMs}
\frame{\frametitle{Individual univariate DLMs}
Suppose for each unit $i=1,\ldots,m$, we can write down a univariate DLM:
\[ \begin{array}{rl@{\qquad}l}
Y_{i,t} &= F\theta_t^{(i)}+v_{i,t}, &v_{i,t}\sim N(0,V_i) \\
\theta_t^{(i)} &= G\theta_{t-1}^{(i)}+w_t^{(i)}, & w_t^{(i)}\sim N_p(0,W_i)
\end{array} \]

\vspace{0.2in} \pause

\begin{itemize}[<+->]
\item $F$ and $G$ are constant for all $i$ and $t$
\item $V_i$ and $W_i$ are constant for all $t$
\item $\theta_t^{(i)}$ are still vectors, thus the notation
\item Is there a relationship between $V_i$ and $V_j$ or $W_i$ and $W_j$ for $i\ne j$?
\item How about $Cov(v_{i,t},v_{j,t})$ or $Cov(w_t^{(i)},w_t^{(j)})$ for $i\ne j$?
\end{itemize}
}





\section{Seemingly unrelated time series}
\subsection{An $m=2$ example}
\frame{\frametitle{Linear trend models}
Suppose you have a linear trend model for each unit $i$:
 \[ \begin{array}{rl@{\qquad}l}
 Y_{i,t} &= F_{i,t}\theta_t^{(i)} + v_{i,t} &v_{i,t}\sim N_m(0,V_{i,t}) \\
 \theta_{i,t} &= G_{i,t}\theta_{t-1}^{(i)}+w_{t}^{(i)} & w_t^{(i)}\sim N_p(0,W_{i,t})
 \end{array} \]

 \vspace{0.2in} \pause

 \begin{itemize}
 \item What is $F_{i,t}$? \pause $F_{i,t}=(1,0)^\top $ \pause
 \item What is $G_{i,t}$? \pause \[ G_{i,t} = \left[ \begin{array}{cc} 1 & 1 \\ 0 & 1 \end{array} \right] \] \pause
 \item What is $V_{i,t}$? \pause $V_{i,t} = V_i$ \pause
 \item What is $W_{i,t}$? \pause $W_{i,t} = W_i$ \pause
 \item What is $\theta_t^{(i)}$? \pause $\theta_t^{(i)} = (\mu_{i,t},\beta_{i,t})$
 \end{itemize}
}

\frame{\frametitle{Linear trend models}
{\small
Combine these into a DLM with $m=2$:
 \[ \begin{array}{rl@{\qquad}l}
 Y_{t} &= F_{t}\theta_{t} + v_{t} &v_{t}\sim N_m(0,V_{t}) \\
 \theta_{t} &= G_{t}\theta_{t-1}+w_{t} & w_{t}\sim N_p(0,W_{t})
 \end{array} \]

 \vspace{0.1in} \pause

\begin{columns}
\column{0.5\textwidth}
 \begin{itemize}
 \item $\theta_{t} = (\mu_{1,t},\mu_{2,t},\beta_{1,t},\beta_{2,t})$ \pause
 \item What is $F_{t}$? \pause \[ F_t = \left[ \begin{array}{cccc} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \end{array} \right] \] \pause
 \item What is $G_{t}$? \pause \[ G_{t} = \left[ \begin{array}{cccc} 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{array} \right] \] \pause
 \end{itemize}
 \column{0.5\textwidth}
 \begin{itemize}
 \item What is $V_{t}$? \pause \[ V_{t} = V \pause = \left[ \begin{array}{cc} V_1 & ? \\ ? & V_2 \end{array} \right] \]  \pause
 \item What is $W_{t}$? \pause \[ W_{t} = W \pause = \left[ \begin{array}{cc} W_\mu & 0 \\ 0 & W_\beta \end{array} \right] \] \pause
 \item What are $W_\mu$ and $W_\beta$? \pause If diagonal?
 \end{itemize}
\end{columns}
}
}

\subsection{In general}
\frame{\frametitle{Kronecker products}
Given two matrices $A$ and $B$, the Kronecker product $A\otimes B$ is defined as
\[ \left[ \begin{array}{ccc}
a_{1,1} B & \cdots & a_{1,n}B \\
\vdots & \vdots & \vdots \\
a_{m,1} B & \cdots & a_{m,n} B
\end{array} \right] \]

\vspace{0.2in} \pause

\begin{itemize}
\item If $A$ is 2x2, what is $A\otimes I_2$?
\end{itemize}
}

\frame{\frametitle{SUTSE model}
\[ \begin{array}{rl@{\qquad}l}
 Y_{t} &= (F\otimes I_m) \theta_{t} + v_{t} &v_{t}\sim N_m(0,V) \\
 \theta_{t} &= (G\otimes I_m)\theta_{t-1}+w_{t} & w_{t}\sim N_p(0,W)
 \end{array} \]

 \vspace{0.1in} \pause

where

 \vspace{0.1in}

The covariance matrices $V$ and $W$ are typically somewhat sparse. 
\pause
Often 
\begin{itemize}
\item off-diagonal elements of $V$ are zero
\item off-diagonal elements of $W$ are zero
\item some diagonal elements of $W$ are zero
\end{itemize}
}

\subsection{SUTSE example}
\frame{\frametitle{The data}
\begin{center}
\includegraphics{invest}
\end{center}
}


\frame[containsverbatim]{\frametitle{Model setup}

\vspace{-0.1in}

\begin{verbatim}
> mod <- dlmModPoly(2)
> mod$FF <- mod$FF %x% diag(2)
> mod$GG <- mod$GG %x% diag(2)
>
> W1 <- matrix(0, 2, 2)
> W2 <- diag(c(49, 437266))
> W2[1, 2] <- W2[2, 1] <- 155
> mod$W <- bdiag(W1, W2)
>
> V <- diag(c(72, 14353))
> Var[1, 2] <- Var[2, 1] <- 1018
> mod$V <- V
>
> mod$m0 <- rep(0, 4)
> mod$C0 <- diag(4) * 1e7
> mod <- as.dlm(mod)
\end{verbatim}
}

\frame[containsverbatim]{\frametitle{Filtering and one-step ahead prediction errors}

\vspace{-0.1in}

\begin{verbatim}
> investFilt <- dlmFilter(invest, mod)
>
> sdev <- residuals(investFilt)$sd
> lwr <- investFilt$f + qnorm(0.025) * sdev
> upr <- investFilt$f - qnorm(0.025) * sdev
\end{verbatim}
}

\frame{\frametitle{The data}
\begin{center}
\includegraphics{invest-filter}
\end{center}
}


\section{Seemingly unrelated regressions model}
% \frame{\frametitle{Longitudinal data}
% Suppose at each time point, you have observations from multiple units, e.g. people, countries, stocks, etc., with a common covariate. The data can be represented as a matrix. \pause
% 
% \[
% \begin{array}{|c|cccc|}
% \hline
% & \multicolumn{4}{c|}{\mbox{Time}} \\
% \mbox{Item} & 1 & 2 & \cdots & T  \\
% \hline
% 1 & Y_{1,1} & Y_{1,2} & \cdots & Y_{1,T}  \\
% 2 & Y_{2,1} & Y_{2,2} & \cdots & Y_{2,T}  \\
% \vdots & \multicolumn{4}{c|}{\vdots} \\
% m & Y_{m,1} & Y_{m,2} & \cdots & Y_{m,T} \\
% \hline
%  & x_1 & x_2 & \vdots & x_T \\
% \hline
% \end{array} \]
% }

\subsection{Individual univariate dynamic regressions}
\frame{\frametitle{Individual univariate dynamic regressions}
Suppose for each unit $i$, we have a univariate dynamic regression model:
\[ \begin{array}{rl@{\qquad}l}
Y_{i,t} &= F_t^{(i)}\theta_t^{(i)}+v_{i,t}, &v_{i,t}\sim N(0,V_i) \\
\theta_t^{(i)} &= G_{i,t}\theta_{t-1}^{(i)}+w_t^{(i)}, & w_{t}^{(i)}\sim N_p(0,W_i)
\end{array} \]

 \vspace{0.2in} \pause

 \begin{itemize}
 \item What is $F_t^{(i)}$? \pause $F_t^{(i)}=(x_{1,t},\ldots,x_{p,t})$  if covariates are common to all series, as they will be in the example to follow \pause
 \item What is $G_{i,t}$? \pause $G_{i,t} = I_p$ \pause
 \item What is $V_{i,t}$? \pause $V_{i,t} = V_i$ \pause
 \item What is $W_{i,t}$? \pause $W_{i,t} = I_p$ \pause
 \item What is $\theta_t^{(i)}$? \pause $\theta_t^{(i)} = (\beta_{1,t}^{(i)},\ldots,\beta_{p,t}^{(i)})^\top $
 \end{itemize}
}

\subsection{Model}
\frame{\frametitle{Simple multivariate dynamic regression}
\[ \begin{array}{rl@{\quad}l}
y_t &= (F_t \otimes I_m)\theta_t + v_t & v_t \ind N(0,V) \\
\theta_t &= (G\otimes I_m)\theta_{t-1} + w_t & w_t \ind N(0,W)
\end{array} \]
where \pause
\[ y_t = \left[ \begin{array}{c} y_{1,t} \\ \vdots \\ y_{m,t} \end{array} \right], \quad \pause
\theta_t = \left[ \begin{array}{c} \alpha_{1,t} \\ \vdots \\ \alpha_{m,t} \\ \beta_{1,t} \\ \vdots \\ \beta_{m,t} \end{array} \right], \quad \pause v_t = \left[ \begin{array}{c} v_{1,t} \\ \vdots \\ v_{m,t} \end{array} \right], \quad \pause
w_t = \left[ \begin{array}{c} w_{1,t} \\ \vdots \\ w_{m,t} \\ w_{m+1,t} \\ \vdots \\ w_{2m,t} \end{array} \right] \] \pause
with $F_t=(1,x_t)^\top , G=I_2,$ and $W=\mbox{blockdiag}(W_\alpha,W_\beta)$.
}

\subsection{An example}
\frame{\frametitle{Data}
\begin{center}
\includegraphics{returns}
\end{center}
}

\frame[containsverbatim]{\frametitle{Model setup}

\vspace{-0.1in}

{\small
\begin{verbatim}
> CAPM <- dlmModReg(market)
> CAPM$FF <- CAPM$FF %x% diag(m)
> CAPM$GG <- CAPM$GG %x% diag(m)
> CAPM$JFF <- CAPM$JFF %x% diag(m)
> CAPM$W <- CAPM$W %x% matrix(0, m, m)
> CAPM$W[-(1 : m), -(1 : m)] <-
+     c(8.153e-07,  -3.172e-05, -4.267e-05, -6.649e-05,
+       -3.172e-05, 0.001377,   0.001852,   0.002884,
+       -4.267e-05, 0.001852,   0.002498,   0.003884,
+       -6.649e-05, 0.002884,   0.003884,   0.006057)
> CAPM$V <- CAPM$V %x% matrix(0, m, m)
> CAPM$Var[] <- c(41.06,     0.01571, -0.9504, -2.328,
+                0.01571, 24.23,     5.783,   3.376,
+               -0.9504,   5.783,   39.2,     8.145,
+               -2.328,    3.376,    8.145,  39.29)
> CAPM$m0 <- rep(0, 2 * m)
> CAPM$C0 <- diag(1e7, nr = 2 * m)
\end{verbatim}
}
}

\frame[containsverbatim]{\frametitle{Smoothing inference}

\vspace{-0.1in}

\begin{verbatim}
> CAPMsmooth <- dlmSmooth(y, CAPM)
\end{verbatim}
}

\frame{\frametitle{Smoothed estimates}
\begin{center}
\includegraphics{returns-smooth}
\end{center}
}



\section{More DLMs for longitudinal data}
\frame{\frametitle{Longitudinal data}
Suppose at each time point, you have observations from multiple units, e.g. people, countries, stocks, etc. The data can be represented as a matrix.

\[
\begin{array}{|c|cccc|}
\hline
& \multicolumn{4}{c|}{\mbox{Time}} \\
\mbox{Item} & 1 & 2 & \cdots & T \\
\hline
1 & Y_{1,1} & Y_{1,2} & \cdots & Y_{1,T} \\
2 & Y_{2,1} & Y_{2,2} & \cdots & Y_{2,T} \\
\vdots & \multicolumn{4}{c|}{\vdots} \\
m & Y_{m,1} & Y_{m,2} & \cdots & Y_{m,T} \\
\hline
\end{array} \]
}

\subsection{Hierarchical DLMs}
\frame{\frametitle{Individual DLMs}
\[ Y_{i,t} = F_{i,t}\theta_{i,t}+v_{i,t}, \quad v_{i,t} \sim N(0,\sigma_{i,t}^2) \]

\vspace{0.1in}

\uncover<2>{
SUTSE: Necessary to estimate blocks of $m\times m$ matrices in the covariance matrix $W$.
}
\uncover<3->{
Instead:
\[ \begin{array}{rl@{\qquad}l}
\theta_{i,t} &= \lambda_t +\epsilon_{i,t}, & \epsilon_{i,t}\sim N_k(0,\Sigma_t) \\
\uncover<4->{\lambda_t &= G\lambda_{t-1} + w_t, & w_t \sim N_k(0,W_t)}
\end{array} \]

\vspace{0.1in}

\uncover<5->{
Hierarchical DLM:
\[ \begin{array}{rl@{\qquad}l}
Y_{t} &= F_{y,t}\theta_{t}+v_{t}, & v_{t} \sim N_m(0,V_{y,t}) \\
\theta_{t} &= F_{\theta,t}\lambda_t +\epsilon_{t}, & \epsilon_{t}\sim N_P(0,V_{\theta,t}) \\
\lambda_t &= G_t\lambda_{t-1} + w_t, & w_t \sim N_k(0,W_t)
\end{array} \]
}
}
}

\frame{\frametitle{Hierarchical DLMs}
Hierarchical DLM:
\[ \begin{array}{rl@{\qquad}l}
Y_{t} &= F_{y,t}\theta_{t}+v_{t}, & v_{t} \sim N_m(0,V_{y,t}) \\
\theta_{t} &= F_{\theta,t}\lambda_t +\epsilon_{t}, & \epsilon_{t}\sim N_P(0,V_{\theta,t}) \\
\lambda_t &= G_t\lambda_{t-1} + w_t, & w_t \sim N_k(0,W_t)
\end{array} \]

\vspace{0.1in}

where
\[ \begin{array}{rl@{\qquad}l}
\theta_t &= (\theta_{1,t}^\top ,\ldots,\theta_{m,t}^\top )^\top  \\
F_{y,t} &= \mbox{blockdiag}(F_{i,t}) \\
F_{\theta,t} &= [I_p|\cdots|I_p]^\top  \\
V_{y,t} &= \mbox{diag}(\sigma_{i,t}^2) \\
V_{\theta,t} &= \mbox{blockdiag}(\Sigma_t) \\
G_t &= G
\end{array} \]
}

\frame{\frametitle{Integrating out $\theta_t$}
Let's write this as a standard DLM \pause

\vspace{0.3in}

\[ \begin{array}{rl@{\qquad}l}
Y_{t} &= F_{y,t}F_{\theta,t}\lambda_{t}+v_{t}^*, & v_{t}^* \sim N_m(0,F_{y,t}V_{\theta,t}F_{y,t}^\top +V_{y,t}) \\
\lambda_t &= G_t\lambda_{t-1} + w_t, & w_t \sim N_k(0,W_t)
\end{array} \]
}

\subsection{Dynamic nonparametric regression}
\frame{\frametitle{Longitudinal data with common covariates}
Suppose at each time point, you have observations from multiple units, e.g. people, countries, stocks, etc., with a common covariate. The data can be represented as a matrix. 

\[
\begin{array}{|c|cccc|c|}
\hline
& \multicolumn{4}{c|}{\mbox{Time}} &\\
\mbox{Item} & 1 & 2 & \cdots & T &  Covariate  \\
\hline
1 & Y_{1,1} & Y_{1,2} & \cdots & Y_{1,T} & x_1 \\
2 & Y_{2,1} & Y_{2,2} & \cdots & Y_{2,T} & x_2  \\
\vdots & \multicolumn{4}{c|}{\vdots} & \\
m & Y_{m,1} & Y_{m,2} & \cdots & Y_{m,T} & x_m \\
\hline

\end{array} \]
}

\subsection{Non-parametric regression}
\frame{\frametitle{}
{\small
Let's allow flexibility in how $Y$ relates to $x$: \pause
\[ E(Y_t|x) = \sum_{j=1}^k \beta_{j,t} h_j(x) \] \pause
where $h_j(x)$ may be, e.g.
\[ \begin{array}{ll}
\mbox{powers} &: h_j(x) = x^j \\
\mbox{trig functions} &: h_j(x) = a_j\sin(b_j x) \\
\mbox{cubic splines} &: h_j(x) =a_j+b_j(x-x_j)+c_j(x-x_j)^2+d_j(x-x_j)^3 \\
\end{array}\] \pause
Then
\[ Y_{i,t} = \sum_{j=1}^k \beta_{j,t} h_j(x_i) +\epsilon_{i,t}, \qquad \epsilon_{i,t}\sim N(0,\sigma^2). \] \pause
In matrix notation,
\[ Y_t = F\beta_t+\epsilon_t, \qquad \epsilon_t\sim N_m(0,V). \] \pause
What are $F$ and $V$?
}
}

\subsection{Dynamic nonparametric regression}
\frame{\frametitle{Dynamic nonparametric regression}
\[ \begin{array}{rl@{\qquad}l}
Y_t &= F\theta_t+\epsilon_t, & \epsilon_t\sim N_m(0,V) \\
\theta_t &= G\theta_{t-1}+w_t, & w_t \sim N_p(0,W_t)
\end{array} \]

\vspace{0.1in}

where
\[ \begin{array}{rl@{\qquad}l}
\theta_t &= \beta_t \\
\\
F &= \left[ \begin{array}{ccc}
h_1(x_1) & \cdots & h_p(x_1) \\
\vdots && \vdots \\
h_1(x_m) & \cdots & h_p(x_m)
\end{array}\right] \\
\\
V &= \sigma^2I_m
\end{array} \]
}

\subsection{Dynamic factor models}
\frame{\frametitle{Dynamic factor model}
\[ \begin{array}{rl@{\qquad}l}
Y_t &= A\mu_t+v_t, & v_t\sim N_m(0,V) \\
\mu_t &= \mu_{t-1}+w_t, & w_t \sim N_p(0,W)
\end{array} \]

\vspace{0.1in} \pause 

where $A$ is a fixed $m\times p$ matrix of factor loadings with $p<m$. 

\vspace{0.2in} \pause

Identifiability of $A$. Suppose $H$ is a $p\times p$ invertible matrix. Then 
\[ \begin{array}{rl@{\qquad}l}
Y_t &= \tilde{A}\tilde{\mu}_t+v_t, & v_t\sim N_m(0,V) \\
\tilde{\mu}_t &= \tilde{\mu}_{t-1}+\tilde{w}_t, & \tilde{w}_t \sim N_p(0,HWH^\top )
\end{array} \]\pause
with $\tilde{\mu}_t = H\mu_t$ and $\tilde{A}=AH^{-1}$. 

\vspace{0.2in} \pause

\begin{itemize}
\item How many parameters in $A$? \pause $mp$ \pause
\item How many parameters in $W$? \pause $\frac{1}{2}p(p+1)$ \pause
\item Number of free parameters is $mp-\frac{1}{2}p(p-1)$. 
\end{itemize}
}

\frame{\frametitle{Identifiable dynamic factor models}
One method of enforcing identifiability is \pause
\begin{itemize}
\item Let $W=I_p$,
\item Let $A=\left[ \begin{array}{c} L \\ B \end{array} \right]$ where $L$ is a lower triangular matrix and $B$ can be anything.
\item Total parameters are \pause $mp-\frac{1}{2}p(p-1)$.
\end{itemize}

\vspace{0.2in} \pause

Another method is 
\begin{itemize}
\item Let $W$ be diagonal
\item Let $A_{i,i}=1$ and $A_{i,j}=0$ for $j>i$.
\end{itemize}
}


\section{Unknown parameters in DLMs}
\subsection{Polynomial trend models}
\frame{\frametitle{Unknown parameters in polynomial trend models}
What is known? \pause
\begin{itemize}
\item $F_t = (1, 0, \ldots, 0)$
\item
\[ G_t = G =\left[ \begin{array}{cccccc} 1 & 1 & 0 & 0 & \cdots & 0 \\
 0 & 1 & 1 & 0 & \cdots & 0 \\
 \vdots &&&&\ddots & \vdots \\
 0 & & \cdots & 0 & 1 & 1 \\
 0 & & \cdots & & 0 & 1 \end{array} \right] \]
\end{itemize}

\vspace{0.2in} \pause

What are the unknown parameters? \pause
\begin{itemize}[<+->]
\item $\theta_t$
\item $V_t\stackrel{?}{=}V$
\item $W_t\stackrel{?}{=}W$
\end{itemize}
}

\subsection{Seasonal models}
\frame{\frametitle{Unknown parameters in seasonal models}
What is known? \pause
\begin{itemize}
\item $F_t$
 \begin{itemize}
 \item Seasonal factor: $F_t = (1, 0, \ldots, 0)$
 \item Fourier form: $F_t = (1, 0, 1, 0, \ldots, 1, 0)$
 \end{itemize}
\item $G_t=G$
 \begin{itemize}
 \item Seasonal factor: rotation matrix
 \item Fourier form: block diagonal with blocks $H_j$
 \[ H_j = \left[ \begin{array}{rr} \cos \omega_j & \sin \omega_j \\ -\sin \omega_j & \cos \omega_j \end{array}\right] \]
 \end{itemize}
\end{itemize}

\vspace{0.2in} \pause


What are the unknown parameters? \pause
\begin{itemize}
\item $\theta_t$
\item $V_t\stackrel{?}{=}V$
\item $W_t\stackrel{?}{=}W\stackrel{?}{=}0$
\end{itemize}
}

\subsection{Dynamic regression models}
\frame{\frametitle{Unknown parameters in dynamic regression models}
What is known? \pause
\begin{itemize}
\item $F_t=x_t$
\item $G_t \stackrel{?}{=} G \stackrel{?}{=} I$
\end{itemize}

\vspace{0.2in} \pause

What are the unknown parameters? \pause
\begin{itemize}
\item $\theta_t$
\item $V_t\stackrel{?}{=}V \stackrel{?}{=} \sigma^2 I$ or $\sigma^2 D$
\item $W_t\stackrel{?}{=}W\stackrel{?}{=}D$
\end{itemize}
}

\subsection{Bottom line}
\frame{\frametitle{The bottom line is...}
\begin{itemize}[<+-| alert@+>]
\item In all of these univariate models,
 \begin{itemize}
 \item the unknowns are $\theta_t$, $W_t$, and $V_t$,
 \item $\theta_t$ has always been unknown
 \item and often, $W_t=W$ and $V_t=V$.
 \end{itemize}
\item In our multivariate models,
 \begin{itemize}
 \item commonly $W_t=W$ and $V_t=V$, but now they are (block-diagonal) matrices.
 \end{itemize}
\end{itemize}
}

\section{Maximum likelihood estimation}
\subsection{General MLE approach}
\frame{\frametitle{}
For a parameter vector $\psi$ and data vector $y$, the likelihood function \pause
\[ L(\psi) \propto p(y|\psi).\]

\vspace{0.2in} \pause

The maximum likelihood estimate is  \pause
\[ \hat{\psi}=\mbox{argmax}_\psi L(\psi) .\]

\vspace{0.2in} \pause

Which is equivalent to
\[ \hat{\psi}=\mbox{argmax}_\psi \ell(\psi) \]
where $\ell(\psi) = \log L(\psi)$.
}

\subsection{DLM likelihood function}
\frame{\frametitle{Likelihood function for DLMs}
If $\psi=(W, V)$, what is $L(\psi)$ for a general DLM?

\vspace{0.1in} \pause

What do we know? \pause
\begin{eqnarray*}
p(y_t|\theta_t,V) &\pause =& N(y_t;F_t \theta_t, V) \\ \pause
p(\theta_t|\theta_{t-1},W) &\pause =& N(\theta_t; G_t \theta_{t-1}, W) \\ \pause
p(\theta_0) &\pause=& N(m_0,C_0) \\ \pause
\\
p(\theta_t|y_{1:t-1},\psi) &\pause = & N(a_t, R_t) \\ \pause
p(y_t|y_{1:t-1},\psi) &\pause = & N(f_t, Q_t) \\ \pause
\\
p(y|\psi) &\pause = & \prod_{t=1}^n p(y_t|y_{1:t-1},\psi)
\end{eqnarray*}
}

\subsection{MLEs in DLMs}
\frame{\frametitle{Finding MLEs for DLMs}
If $y_t$ is multivariate, the likelihood function is \pause
\[ L(\psi) \propto \prod_{t=1}^n \frac{1}{(2\pi)^{k/2} |Q_t|^{1/2}} \exp\left( -\frac{1}{2} (y_t-f_t)^\top Q_t^{-1}(y_t-f_t)\right).\]

\vspace{0.2in} \pause

Log-likelihood function \pause
\[ \ell(\psi) = C + -\frac{1}{2}\sum_{t=1}^n \log |Q_t|-\frac{1}{2}\sum_{t=1}^n (y_t-f_t)^\top Q_t^{-1}(y_t-f_t).\]

\vspace{0.2in} \pause

The MLE is then
\[ \hat{\psi}=\mbox{argmax}_\psi \ell(\psi) \]

\vspace{0.2in} \pause

The R function \texttt{dlmMLE} does all of this for you.
}

%\subsection{Prediction using MLEs}
%\frame{\frametitle{Local level model}
%Suppose our model is
%\[ \begin{array}{rl@{\qquad}l}
%y_t &= \theta_t +v_t & v_t \sim N(0,\sigma^2) \\
%\theta_t &= \theta_{t-1} + w_t & w_t \sim N(0,1)
%\end{array} \]
%and you have observed data up to time $n$ and your job is to create a 95\% interval for $y_{n+1}$. How do you do it?
%
%\vspace{0.2in} \pause
%
%\begin{itemize}
%\item Find $\widehat{\sigma^2}$
%\item Find $p(y_{t+n}|y_{1:n},\widehat{\sigma^2}) = N(\widehat{f_{t+1}},\widehat{Q_{t+1}})$
%\item Calculate 95\% intervals from this distribution.
%\end{itemize}
%}
%
%\frame{\frametitle{Local level model (cont.)}
%
%}

\section{Conjugate Bayesian inference}
\subsection{Review}
\frame{\frametitle{Bayesian inference}
What do we have to specify to perform Bayesian inference, i.e. parameter estimation, for data $y$? \pause
\begin{itemize}
\item A statistical model $p(y|\psi)$
\item A prior $p(\psi)$
\end{itemize}

\vspace{0.2in} \pause

What is the objective of Bayesian inference? \pause
\begin{itemize}
\item The posterior $p(\psi|y) \propto p(y|\psi)p(\psi)$.
\end{itemize}
}

\frame{\frametitle{Conjugacy}
Conjugate Bayesian inference is one where if
\[ \psi \sim f(\alpha) \implies \psi|y \sim f(\alpha'). \]

\vspace{0.2in} \pause

Remember the examples
\begin{itemize}
\item $y \sim N(\mu,I), \mu \sim N(\cdot, \cdot) \implies \mu|y \sim N(\cdot, \cdot)$
\item $y \sim N(0,\phi^{-1}I), \phi \sim Ga(\cdot, \cdot) \implies \phi|y \sim Ga(\cdot, \cdot)$
\item $y \sim N(\mu,\phi^{-1}I), \mu,\phi \sim NG(\cdot) \implies \mu, \phi|y \sim NG(\cdot)$
\item $y \sim N(X\beta,\phi^{-1}I), \beta,\phi \sim NG(\cdot) \implies \beta, \phi|y \sim NG(\cdot)$
\item $y \sim Bin(n,p), p\sim Be(\cdot,\cdot) \implies p|y \sim Be(\cdot,\cdot)$
\end{itemize}
}

\subsection{Conjugate Bayesian inference in DLMs}
\frame{\frametitle{What are the unknowns in DLMs?}
So for $\psi = (F_{1:n}, G_{1:n}, W_{1:n}, V_{1:n})$, we are looking for
\[ \psi \sim f(\alpha) \implies \psi|y \sim f(\alpha'). \]

\vspace{0.2in} \pause

This only happens in simple examples. Today, we will discuss
\begin{itemize}[<+->]
\item $V_t=\phi^{-1}\tilde{V}_t, W_t=\phi^{-1}\tilde{W}_t, C_0=\phi^{-1}\tilde{C}_0$
\item $W_t$ specified by a discount factor
\item Evolving $\phi=1/\sigma^2$
\end{itemize}
}

\subsection{common $\phi^{-1}$}
\frame{\frametitle{common $\phi^{-1}$}
\[ \begin{array}{rl@{\qquad}l}
Y_t &= F_t\theta_t + v_t &v_t\sim N_m(0,\phi^{-1}\tilde{V}_t) \\
\theta_t &= G_t\theta_{t-1}+w_t & w_t\sim N_p(0,\phi^{-1}\tilde{W}_t) \\
p(\theta_0) &= N(m_0,\phi^{-1}\tilde{C}_0) \\
\phi &\sim  Ga(\alpha_0, \beta_0)
\end{array} \]

\vspace{0.2in} \pause

Everything is known except
\begin{itemize}
\item $\theta_t$ for all $t$
\item $\phi$
\end{itemize}
}

\frame{\frametitle{}
{\footnotesize
Starting with
\[ \theta_{t-1},\phi|y_{1:t-1} \sim NG(m_{t-1}, \tilde{C}_{t-1}, \alpha_{t-1}, \beta_{t-1}) \]

\vspace{0.1in} \pause

One step ahead prior
\[ \theta_{t},\phi|y_{1:t-1} \sim NG(a_t, \tilde{R}_{t}, \alpha_{t-1}, \beta_{t-1}) \]
where $a_t = G_tm_{t-1}$ and $\tilde{R}_t = G_t \tilde{C}_{t-1} G_t^\top +\tilde{W}_t$.

\vspace{0.1in} \pause

One step ahead predictive density
\[ Y_t|y_{1:t-1} \sim t_{2\alpha_{t-1}}(f_t, \tilde{Q}_t\beta_{t-1}/\alpha_{t-1})\]
with $f_t=F_ta_t$ and $\tilde{Q}_t = F_t\tilde{R}_tF_t^\top +\tilde{V}_t$.

\vspace{0.1in} \pause

Filtering density
\[ \theta_t,\phi|y_{1:t}  \sim NG(m_{t}, \tilde{C}_{t}, \alpha_{t}, \beta_{t}) \]
with
\begin{eqnarray*}
m_t &=& a_t + \tilde{R}_tF_t \tilde{Q}_t^{-1}(y_t-f_t) \\
\tilde{C}_t &=& \tilde{R}_t-\tilde{R}_tF_t^\top \tilde{Q}_t^{-1}\tilde{R}_t^\top  \\
\alpha_t &=& \alpha_{t-1} +\frac{m}{2} \\
\beta_t &=& \beta_{t-1}+\frac{1}{2}(y_t-f_t)^\top \tilde{Q}_t^{-1}(y_t-f_t)
\end{eqnarray*}
}
}

\subsection{$W_t$ specified by discount factor}
\frame{\frametitle{Discount factor}
Let's specify how adaptive we want our model to be.
\begin{itemize}
\item Do this by specifying $W_t$ relative to $V_t$ and $C_t$ using a discount factor $\delta\in(0,1]$.
\item $\delta=1$ means no loss of information, i.e. $W_t=0$
\item $\delta=0$ means no information retained
\item Often $\delta>0.9$
\end{itemize}

\vspace{0.2in}

To implement, set \[ W_t = \frac{1-\delta}{\delta} G_t^\top  C_{t-1} G_t\] or \[ \tilde{W}_t = \frac{1-\delta}{\delta} G_t^\top  \tilde{C}_{t-1} G_t\]
if using a common $\sigma^2$.
}

\frame{\frametitle{Discount factor effect}
\begin{center}
\includegraphics{cdiscount}
\end{center}
}


\frame[containsverbatim]{\frametitle{Choosing the discount factor}
Specify $\delta$ based on one-step ahead prediction errors.

\begin{center}
\begin{verbatim}
  DF MAPE  MAD   MSE sigma2
 1.0 0.10 3.02 21.54 12.00
 0.9 0.09 2.86 19.92  9.64
 0.8 0.10 2.87 20.29  8.94
 0.3 0.11 3.42 25.12  5.07
\end{verbatim}

Last column is posterior expectation for $\sigma^2$, 
i.e. $E[\sigma^2|y_{1:187}]$.

\end{center}
}

\frame{\frametitle{Inference for $\delta=0.95$ on Lake Superior Data}
\begin{center}
\includegraphics{wdiscount}
\end{center}
}


\subsection{Evolving $\phi_t$}
\frame{\frametitle{Evolving $\phi_t$}
Choose $\delta^*\in(0,1)$
\begin{itemize}
\item $\phi_{t-1}|y_{1:t-1} \sim Ga(\alpha_{t-1},\beta_{t-1})$ \pause
\item $\phi_t|y_{1:t-1} \sim Ga(\delta^*\alpha_{t-1},\delta^*\beta_{t-1})$
\end{itemize}

\vspace{0.2in} \pause

What is
\begin{itemize}
\item $E[\phi_t|y_{1:t-1}] \pause = E[\phi_{t-1}|y_{1:t-1}]$ \pause
\item $Var[\phi_t|y_{1:t-1}] \pause = \frac{1}{\delta^*} Var[\phi_{t-1}|y_{1:t-1}]$
\end{itemize}
}

\frame{\frametitle{Evolving $\phi_t$ for Lake Superior data}
\begin{center}
\includegraphics{evolve}
\end{center}
}

\subsection{Bottom line}
\frame{\frametitle{}
The situations for conjugate Bayesian analysis are small, therefore we need more advanced techniques.
}




\section{Review}
\subsection{Gibbs sampling}
\frame{\frametitle{Gibbs sampling algorithm}
Start with an initial guess for all parameters and call it $\psi^{(0)}$.
Set $j=1$. \pause

\begin{enumerate}
\item Sample $\psi_1^{(j)}\sim p\left(\psi_1|\psi_2^{(j-1)},\ldots,\psi_K^{(j-1)},y\right)$ \pause
\item Sample $\psi_2^{(j)}\sim p\left(\psi_2|\psi_1^{(j)},\psi_3^{(j-1)},\ldots,\psi_K^{(j-1)},y\right)$ \pause
\item $\vdots$ \pause
\item Sample $\psi_k^{(j)}\sim p\left(\psi_k|\psi_1^{(j)},\ldots,\psi_{k-1}^{(j)},\psi_{k+1}^{(j-1)},\ldots,\psi_K^{(j-1)},y\right)$ \pause
\item $\vdots$ \pause
\item Sample $\psi_{K-1}^{(j)}\sim p\left(\psi_{K-1}|\psi_1^{(j)},\ldots,\psi_{K-2}^{(j)},\psi_{K}^{(j-1)},y\right)$ \pause
\item Sample $\psi_K^{(j)}\sim p\left(\psi_K|\psi_1^{(j)},\ldots,\psi_{K-1}^{(j)},y\right)$ \pause
\item If $j<J$, $j=j+1$ and return to step 1.
\end{enumerate}
}%
%
%\frame{\frametitle{Bivariate normal example}
%\setkeys{Gin}{width=0.6\textwidth}
%Suppose we have the model
%\[ y_i \ind p N(\mu_0, \sigma_0^2) + (1-p) N(\mu_1, \sigma_1^2). \]
%\begin{center}
%\includegraphics{mixdens}
%\end{center}
%Our goal is to estimate $\psi = (p, \mu_0, \sigma_0^2, \mu_1, \sigma_1^2)$ after observing $y_1,\ldots,y_n$.
%}
%
%\frame{\frametitle{Bivariate normal priors}
%Suppose we have the model
%\[ y_i \ind p N(\mu_0, \sigma_0^2) + (1-p) N(\mu_1, \sigma_1^2). \]
%
%\vspace{0.2in}
%
%Priors:
%\begin{itemize}
%\item $p\pause \sim Be(1,1)$ \pause
%\item $\mu_0\pause  \propto 1$ \pause
%\item $\mu_1\pause  \propto 1$ \pause
%\item $\sigma_0^2\pause  \propto 1/\sigma_0^2$ \pause
%\item $\sigma_1^2\pause  \propto 1/\sigma_1^2$
%\end{itemize}
%}
%
%\frame{\frametitle{Bivariate normal example (cont.)}
%
%\vspace{-0.1in}
%
%What is $p(y|\psi)$? \pause
%\[ p(y|\psi) = \prod_{i=1}^n \left[ p N(y_i; \mu_0, \sigma_0^2) + (1-p) N(y_i; \mu_1, \sigma_1^2)\right]. \]
%
%\vspace{0.2in} \pause
%
%What are the full conditional distributions?
%\[ \begin{array}{rl}
%p(p|\ldots)&\propto p(y|\psi)\phantom{\frac{1}{\sigma_0^2}} \\
%p(\mu_0|\ldots)&\propto p(y|\psi) \\
%p(\sigma_0^2|\ldots)&\propto p(y|\psi)\frac{1}{\sigma_0^2} \\
%p(\mu_1|\ldots)&\propto p(y|\psi)\phantom{\frac{1}{\sigma_0^2}} \\
%p(\sigma_1^2|\ldots)&\propto p(y|\psi)\frac{1}{\sigma_1^2}
%\end{array} \]
%
%\vspace{0.2in} \pause
%
%\alert{None of these are available in closed form.}
%}
%
%\frame{\frametitle{Data augmentation}
%For this model, there exists a relevant latent variable, namely group membership. \pause
%\[ z_i =\left\{ \begin{array}{ll} 1 & \mbox{if observation $i$ comes from group 1} \\ 2 & \mbox{if observation $i$ comes from group 2} \end{array} \right. \]
%
%\vspace{0.2in} \pause
%
%What is a reasonable prior for $z_i$? \pause $p(z_i=1)=p$
%
%\vspace{0.2in} \pause
%
%The model can now be written
%\begin{eqnarray*}
%y_i|z_i &\stackrel{ind}{\sim}& N(\mu_{z_i}, \sigma_{z_i}^2) \\
%z_i &\ind & Ber(p)
%\end{eqnarray*}
%}
%
%\frame{\frametitle{Full data likelihood}
%\[ \begin{array}{ll} p(y|\psi,z) &\propto \prod_{i=1}^n N(y_i; \mu_{z_i}, \sigma_{z_i}^2) p^{z_i}(1-p)^{1-z_i} \\
%\\ \pause
%&= \prod_{z_i=0} N(y_i; \mu_0, \sigma_0^2) \, \prod_{z_i=1} N(y_i; \mu_1, \sigma_1^2) \, p^{n\bar{z}} (1-p)^{n(1-\bar{z})} \end{array}\]
%where $\bar{z} = \frac{1}{n} \sum_{i=1}^n z_i$.
%}

\section{Univariate Gibbs sampling}
\subsection{for states in DLMs}
\frame{\frametitle{What full conditionals are required?}
Suppose our goal is to draw from $p(\theta_{0:T}|y_{1:T})$ using univariate Gibbs sampling. We will implicitly assume conditioning on any other unknown parameters. 

\vspace{0.2in} \pause

\begin{itemize}[<+->]
\item What are the required full condition distributions?
 \begin{itemize}
 \item $p(\theta_0|\theta_{1:T},y_{1:T})$
 \item $p(\theta_t|\theta_{-t},y_{1:T})$ where $\theta_{-t}$ is $\theta_{0:T}$ with the $t^{th}$ element removed
 \item $p(\theta_T|\theta_{0:T-1},y_{1:T})$
 \end{itemize}
\end{itemize}
}

\frame{\frametitle{DLMs}

\vspace{-0.1in}

 \[ \begin{array}{rl@{\qquad}l}
 Y_t &= F_t\theta_t + v_t &v_t\sim N_m(0,V_t) \\
 \theta_t &= G_t\theta_{t-1}+w_t & w_t\sim N_p(0,W_t) \\
 p(\theta_0) &= N(m_0,C_0)
 \end{array} \]

 \vspace{0.4in}

 \begin{center}
 \includegraphics{stateSpaceModel}
 \end{center}
}

\frame{\frametitle{What are the full conditionals?}

\vspace{-0.2in}

{\scriptsize
\begin{eqnarray*}
p(\theta_0|\ldots) \pause &=& p(\theta_0|\theta_1) \\ \pause 
&\propto & N(\theta_1; G_1\theta_0, W_1) N(\theta_0; m_0, C_0) \\ \pause
&\propto & N(\theta_0; k_0, K_0) \\ \pause
K_0 &=& (C_0^{-1}+G_1^\top W_1^{-1}G_1)^{-1} \\ \pause
k_0 &=& K_0(C_0^{-1}m_0 +G_1^\top W_1^{-1}\theta_1) \\ \pause
\\
p(\theta_T|\ldots) \pause &=& p(\theta_T|\theta_{T-1},y_T) \\ \pause
&\propto & N(y_T; F_T\theta_T, V_T) N(\theta_T;G_T\theta_{T-1},W_T) \\ \pause
&\propto & N(\theta_T; k_T, K_T) \\ \pause
K_T &=& (W_T^{-1}+F_T^\top V_T^{-1}F_T)^{-1} \\ \pause
k_T &=& K_T(W_T^{-1}G_T\theta_{T-1} + F_T^\top V_T^{-1}y_T) \\ \pause
\\
p(\theta_t|\ldots) \pause &=& p(\theta_t|\theta_{t-1},\theta_{t+1},y_t) \\ \pause
&\propto & N(y_t; F_t\theta_t, V_t) N(\theta_{t+1}; G_{t+1}\theta_t, W_t) N(\theta_t; G_t\theta_{t-1}, W_{t+1})\\ \pause
&\propto & N(\theta_t; k_t, K_t) \\ \pause
K_t &=& (W_t^{-1}+F_t^\top V_t^{-1}F_t+G_{t+1}^\top W_{t+1}^{-1}G_{t+1})^{-1} \\ \pause
k_t &=& K_t(W_t^{-1}G_t\theta_{t-1}+F_t^\top V_t^{-1}y_t+G_{t+1}^\top W_{t+1}^{-1}\theta_{t+1})
\end{eqnarray*}
}
}

\subsection{Local level model example}
\frame{\frametitle{Consider the local level model with $V=1$ and $W=0.01^2$.}
\begin{center}
\includegraphics{localLevelData}
\end{center}
}

\frame{\frametitle{Univariate Gibbs sampling for states}
\begin{center}
\includegraphics{localLevelGibbs}
\end{center}
}

\frame{\frametitle{Exact quantiles for states}
\begin{center}
\includegraphics{localLevelExact}
\end{center}
}

\frame{\frametitle{True underlying state}
\begin{center}
\includegraphics{localLevelTruth}
\end{center}
}

\section{Smoothing}
\subsection{In state-space models}
\frame{\frametitle{Filtering}
{\tiny
 Goal: $ p(\theta_t|y_{1:t})$ where $y_{1:t} = (y_1,y_2,\ldots,y_t)$  (filtered distribution)

 \vspace{0.1in}

 Recursive procedure:
 \begin{itemize}
 \item Assume $p(\theta_{t-1}|y_{1:t-1})$
 \item Prior for $\theta_t$
 \begin{eqnarray*} p(\theta_t|y_{1:t-1})  &=& \int p(\theta_t,\theta_{t-1}|y_{1:t-1}) d\theta_{t-1} \\
  &=& \int p(\theta_t|\theta_{t-1},y_{1:t-1})p(\theta_{t-1}|y_{1:t-1}) d\theta_{t-1} \\
  &=& \int p(\theta_t|\theta_{t-1})p(\theta_{t-1}|y_{1:t-1}) d\theta_{t-1} \\
 \end{eqnarray*}
 \item One-step ahead predictive distribution for $y_t$
 \begin{eqnarray*} p(y_t|y_{1:t-1})  &=&  \int p(y_t,\theta_t|y_{1:t-1}) d\theta_t \\
  &=& \int p(y_t|\theta_t,y_{1:t-1})p(\theta_t|y_{1:t-1})d\theta_t \\
  &=& \int p(y_t|\theta_t)p(\theta_t|y_{1:t-1})d\theta_t \\
 \end{eqnarray*}
 \item Filtered distribution for $\theta_t$
 \[ p(\theta_t|y_{1:t})  = \frac{p(y_t|\theta_t,y_{1:t-1})p(\theta_t|y_{1:t-1})}{p(y_t|y_{1:t-1})}  = \frac{p(y_t|\theta_t)p(\theta_t|y_{1:t-1})}{p(y_t|y_{1:t-1})}\]
 \end{itemize}
 }
}

\frame{\frametitle{Smoothing}
 Goal: $p(\theta_{t}|y_{1:T})$ for $t<T$

 \vspace{0.1in} \pause
{\tiny
 \begin{itemize}
 \item Backward transition probability $p(\theta_t|\theta_{t+1},y_{1:T})$ \pause
 \begin{eqnarray*}
 p(\theta_t|\theta_{t+1},y_{1:T}) \pause &=& p(\theta_t|\theta_{t+1},y_{1:t}) \\ \pause
 &=& \frac{p(\theta_{t+1}|\theta_t,y_{1:t})p(\theta_t|y_{1:t})}{p(\theta_{t+1}|y_{1:t})} \\ \pause
 &=& \frac{p(\theta_{t+1}|\theta_t)p(\theta_t|y_{1:t})}{p(\theta_{t+1}|y_{1:t})} \pause
 \end{eqnarray*}

 \item Recursive smoothing distributions $p(\theta_t|y_{1:T})$ assuming we know $p(\theta_{t+1}|y_{1:T})$ \pause
 \begin{eqnarray*}
 p(\theta_t|y_{1:T}) \pause &=& \int p(\theta_t,\theta_{t+1}|y_{1:T}) d\theta_{t+1} \\ \pause
 &=& \int p(\theta_{t+1}|y_{1:T})\alert{p(\theta_t|\theta_{t+1},y_{1:T})}d\theta_{t+1} \\ \pause
   &=& \int p(\theta_{t+1}|y_{1:T}) \frac{p(\theta_{t+1}|\theta_t)p(\theta_t|y_{1:t})}{p(\theta_{t+1}|y_{1:t})} d\theta_{t+1} \\ \pause
   &=& p(\theta_t|y_{1:t}) \int \frac{p(\theta_{t+1}|\theta_t)}{p(\theta_{t+1}|y_{1:t})} p(\theta_{t+1}|y_{1:T}) d \theta_{t+1}\pause
   \end{eqnarray*}
   Start from $p(\theta_T|y_{1:T})$.
 \end{itemize}
 }
}


\subsection{In DLMs}
\frame{\frametitle{Kalman smoother}
If $p(\theta_{t+1}|y_{1:T}) = N(s_{t+1},S_{t+1})$, then
\begin{eqnarray*}
p(\theta_t|\theta_{t+1},y_{1:T}) &=& p(\theta_t|\theta_{t+1},y_{1:t}) \\
&\propto & p(\theta_{t+1}|\theta_t,y_{1:t})p(\theta_t|y_{1:t}) \\
&=& N(\theta_{t+1}; G_{t+1}\theta_t,W_{t+1}) N(\theta_t;m_t,C_t) \\
&\propto & N(\theta_t; h_t, H_t) \\
H_t &=& (C_t^{-1}+G_{t+1}^\top W_{t+1}^{-1}G_{t+1})^{-1} \\
h_t &=& H_t(C_t^{-1}m_t+G_{t+1}^\top W_{t+1}^{-1}\theta_{t+1}) \\
\\
p(\theta_t|y_{1:T}) &=& \int p(\theta_t|\theta_{t+1},y_{1:T})p(\theta_{t+1}|y_{1:T}) d\theta_{t+1} \\ \pause
&=& N(\theta_t; s_t, S_t) \\
S_t &=& C_t-C_tG_{t+1}^\top R_{t+1}^{-1}(R_{t+1}-S_{t+1})R_{t+1}^{-1}G_{t+1}C_t \\
s_t &=& m_t+C_tG_{t+1}^\top R_{t+1}^{-1}(s_{t+1}-a_{t+1})
\end{eqnarray*}
}

\frame{\frametitle{True underlying state}
\begin{center}
\includegraphics{localLevelTruth}
\end{center}
}

\section{Forward filtering backward sampling}
\subsection{MCMC in DLMs}
\frame{\frametitle{}
Let $\psi$ represent any unknown, non-dynamic model parameters such that the data follows a DLM conditional on $\psi$.\pause
 \[ \begin{array}{rl@{\qquad}l}
 Y_t &= F_t(\psi)\theta_t + v_t &v_t\sim N_m(0,V_t(\psi)) \\
 \theta_t &= G_t(\psi)\theta_{t-1}+w_t & w_t\sim N_p(0,W_t(\psi)) \\
 p(\theta_0) &= N(m_0(\psi),C_0(\psi))
 \end{array} \]\pause
For example, $\psi = (V,W)$ where $V_t(\psi)=V$ and $W_t(\psi)=W$ \pause while $F_t(\psi)=F$ and $G_t(\psi)=G$ are known, \pause as in polynomial trend, seasonal factor, and dynamic regression models.

\vspace{0.2in} \pause

\begin{itemize}[<+->]
\item The Bayesian inferential objective is then $p(\theta_{0:T},\psi|y_{1:T})$. 
\item While $p(\theta_{0:T}|y_{1:T},\psi)$ is known analytically, generally $p(\theta_{0:T},\psi|y_{1:T})$ is not.
\item So resort to numerical methods, most often MCMC
\end{itemize}
}

\subsection{MCMC Schemes}
\frame{\frametitle{MCMC Schemes}
Scheme I - all univariate samples
\begin{itemize}
\item For $t\in\{0,1,\ldots,T\}$ sample $p(\theta_t|\ldots)$.
\item For $j\in\{1,\ldots,J\}$ sample $p(\psi_j|\ldots)$ for $J$ parameters.
\end{itemize}

\vspace{0.2in} \pause 

Scheme II - block sampling of states
\begin{itemize}
\item Sample $p(\theta_{0:T}|\ldots)$.
\item For $j\in\{1,\ldots,J\}$ sample $p(\psi_j|\ldots)$ for $J$ parameters.
\end{itemize}
}

\frame{\frametitle{MCMC Schemes (cont.)}
Scheme III - block sampling of parameters
\begin{itemize}
\item Sample $p(\theta_{0:T}|\ldots)$.
\item Sample $p(\psi|\ldots)$.
\end{itemize}
e.g. polynomial trend, seasonal factor, and dynamic regression models

\vspace{0.2in} \pause

Scheme IV - hybrid
\begin{itemize}
\item Sample $p(\psi_{J'}|\psi_{J\setminus J'}, y_{1:T})$ for some subset $J'$ of parameters.
\item Sample $p(\theta_{0:T}|\ldots)$.
\item Sample $p(\psi_{J\setminus J'}|\ldots)$.
\end{itemize}
}

\frame{\frametitle{MCMC Schemes}
Generally better to jointly sampling unknowns, a.k.a. block sampling.
\begin{itemize}
\item Scheme I has all univariate draws
\item Scheme II samples latent state jointly
\item Scheme III samples latent state jointly and parameters jointly
\item Scheme IV samples some parameters $\psi_{J'}$ and all latent states jointly and then samples remaining parameters jointly
\end{itemize}

\vspace{0.2in}

Bottom line: if parameters are highly correlated in the posterior, it is better to sample those parameters jointly.
}

\subsection{Algorithm}
\frame{\frametitle{Forward filtering backward sampling (FFBS)}
Recall
\begin{itemize}[<+->]
\item $p(\theta_T|y_{1:T})=N(m_T,C_T)$ is available from filtering
\item $p(\theta_t|\theta_{t+1},y_{1:T})=N(h_t,H_T)$ is available from smoothing
\end{itemize}
\uncover<3>{
\begin{eqnarray*}
H_t &=& (C_t^{-1}+G_{t+1}^\top W_{t+1}^{-1}G_{t+1})^{-1} \\
h_t &=& H_t(C_t^{-1}m_t+G_{t+1}^\top W_{t+1}^{-1}\theta_{t+1})
\end{eqnarray*}
} \pause
\uncover<4->{The algorithm is then} \pause
\begin{itemize}[<+->]
\item Forward filter to obtain $p(\theta_t|y_{1:t})=N(m_t,C_t)$ for all $t$. 
\item Sample $\theta_T \sim H(m_T,C_T)$. 
\item For $t\in\{T-1,T-2,\ldots,1,0\}$,
 \begin{itemize}
 \item Calculate $h_t$ and $H_t$ based on $\theta_{t+1}$.
 \item Draw $\theta_t\sim N(h_t,H_T)$.
 \end{itemize}
\end{itemize} 
}

\subsection{Examples}
\frame{\frametitle{Local level model}
\begin{center}
\includegraphics{localLevelTruth}
\end{center}
}


\frame{\frametitle{Local level model}
\begin{center}
\includegraphics{localLevelFFBS}
\end{center}
}

\frame{\frametitle{Local level model - unknown variances}
 \[ \begin{array}{rl@{\qquad}l}
 Y_t &= \theta_t + v_t &v_t\sim N_m(0,V) \\
 \theta_t &= \theta_{t-1}+w_t & w_t\sim N_p(0,W) \\
 p(\theta_0) &= N(m_0,C_0)
 \end{array} \]
 
\vspace{0.2in}

MCMC Scheme:
\begin{itemize}
\item Sample $p(\theta_{0:T}|\ldots)$ using FFBS
\item Sample $p(V,W|\ldots)$ 
\end{itemize}
}

\frame{\frametitle{Nile river level}
\begin{center}
\includegraphics{nileData}
\end{center}
}

\frame{\frametitle{Nile river level}
\begin{center}
\includegraphics{nileParams}
\end{center}
}

\frame{\frametitle{Nile river level}
\begin{center}
\includegraphics{nileStates}
\end{center}
}


\section{MCMC}
\subsection{DLMs}
\frame{\frametitle{MCMC in DLMs}
Recall the inferential objective of the Bayesian approach in DLMs:
\[ p(\theta_{0:n},\psi|y_{1:n}) \]

\vspace{0.2in} \pause

Since $p(\theta_{0:n},\psi|y_{1:n})$ is not typically available analytically, we commonly use Markov chain Monte Carlo. \pause These  approaches sample from \alert{full conditional distributions}, \pause e.g.
\begin{itemize}
\item $p(\theta_t|\theta_{-t},\psi, y_{1:n})$ for $t=0,1,2,\ldots,n$. \pause
\item $p(\psi_j|\theta_{0:n},\psi_{-j},y_{1:n})$ for $j=1,2,\ldots,J$.
\end{itemize}
\pause These draws could be Gibbs or Metropolis-Hastings.
}

\frame{\frametitle{MCMC Univariate Sampling Activity}
Fill in \alert{?} with $i$ or $i-1$.
\[ \begin{array}{lrcl}
&\theta_0^{\alt<1>{(\alert{?})}{(i)}} &\sim& p(\theta_0|\theta_{1}^{\alt<1-2>{(\alert{?})}{(i-1)}},\ldots,\theta_n^{\alt<1-2>{(\alert{?})}{(i-1)}},\psi^{\alt<1-2>{(\alert{?})}{(i-1)}},y_{1:n}) \\
\\
\multicolumn{4}{l}{\mbox{For $t\in 1,\ldots,n-1$, sample from }} \\
&\theta_t^{\alt<1>{(\alert{?})}{(i)}} &\sim& p(\theta_t| \theta_0^{\alt<1-2>{(\alert{?})}{(i)}},\ldots,\theta_{t-1}^{\alt<1-2>{(\alert{?})}{(i)}},\theta_{t+1}^{\alt<1-2>{(\alert{?})}{(i-1)}},\ldots,\theta_n^{\alt<1-2>{(\alert{?})}{(i-1)}},\psi^{\alt<1-2>{(\alert{?})}{(i-1)}},y_{1:n}) \\ \\
&\theta_n^{\alt<1>{(\alert{?})}{(i)}} &\sim& p(\theta_n| \theta_0^{\alt<1-2>{(\alert{?})}{(i)}},\ldots,\theta_{n-1}^{\alt<1-2>{(\alert{?})}{(i)}},\psi^{\alt<1-2>{(\alert{?})}{(i-1)}},y_{1:n}) \\ \\ \\
&\psi_1^{\alt<1>{(\alert{?})}{(i)}} &\sim& p(\psi_1|\theta^{\alt<1-2>{(\alert{?})}{(i)}}, \psi_{2}^{\alt<1-2>{(\alert{?})}{(i-1)}},\ldots,\psi_{J}^{\alt<1-2>{(\alert{?})}{(i-1)}},y_{1:n}) \\ \\
\multicolumn{4}{l}{\mbox{For $j\in 2,\ldots,J-1$, sample from }} \\
&\psi_j^{\alt<1>{(\alert{?})}{(i)}} &\sim& p(\psi_j|\theta^{\alt<1-2>{(\alert{?})}{(i)}}, \psi_{1}^{\alt<1-2>{(\alert{?})}{(i)}},\ldots,\psi_{j-1}^{\alt<1-2>{(\alert{?})}{(i)}},\psi_{j+1}^{\alt<1-2>{(\alert{?})}{(i-1)}},\ldots,\psi_{J}^{\alt<1-2>{(\alert{?})}{(i-1)}},y_{1:n}) \\
&\psi_J^{\alt<1>{(\alert{?})}{(i)}} &\sim& p(\psi_J|\theta^{\alt<1-2>{(\alert{?})}{(i)}}, \psi_{1}^{\alt<1-2>{(\alert{?})}{(i)}},\ldots,\psi_{J-1}^{\alt<1-2>{(\alert{?})}{(i)}},y_{1:n})
\end{array} \] \pause \pause
}

\frame{\frametitle{Convergence to stationary distribution}
The samples $(\theta^{(i)},\psi^{(i)})$ converge to samples from $p(\theta_{0:n},\psi|y_{1:n})$, regardless of what $(\theta^{(0)},\psi^{(0)})$ was. 

\vspace{0.2in} \pause

Let's look at an example: local level model.

 \[ \begin{array}{rl@{\qquad}l}
 Y_t &= \theta_t + v_t &v_t\sim N(0,2) \\
 \theta_t &= \theta_{t-1}+w_t & w_t\sim N(0,0.5) \\
 p(\theta_0) &= N(0,1)
 \end{array} \]
 
 \vspace{0.1in} \pause
 
with $y_1=1$. \pause The objective is $p(\theta_0,\theta_1|y_1)$. 
}

\frame{\frametitle{Local level convergence example}
\setkeys{Gin}{width=0.5\textwidth}
\begin{columns}
\column{0.5\textwidth}
\only< 1   >{\includegraphics{joint0}}
\only< 2   >{\includegraphics{joint1-2a}}
\only< 3- 5>{\includegraphics{joint1-2b}}
\only< 6   >{\includegraphics{joint1-2c}}
\only< 7   >{\includegraphics{joint1-2d}}
\only< 8   >{\includegraphics{joint2-2a}}
\only< 9-11>{\includegraphics{joint2-2b}}
\only<12   >{\includegraphics{joint2-2c}}
\only<13   >{\includegraphics{joint2-2d}}

\only<14   >{\includegraphics{joint1-3a}}
\only<15-17>{\includegraphics{joint1-3b}}
\only<18   >{\includegraphics{joint1-3c}}
\only<19   >{\includegraphics{joint1-3d}}
\only<20   >{\includegraphics{joint2-3a}}
\only<21-23>{\includegraphics{joint2-3b}}
\only<24   >{\includegraphics{joint2-3c}}
\only<25   >{\includegraphics{joint2-3d}}

\only<26   >{\includegraphics{joint1-4a}}
\only<27-29>{\includegraphics{joint1-4b}}
\only<30   >{\includegraphics{joint1-4c}}
\only<31   >{\includegraphics{joint1-4d}}
\only<32   >{\includegraphics{joint2-4a}}
\only<33-35>{\includegraphics{joint2-4b}}
\only<36   >{\includegraphics{joint2-4c}}
\only<37   >{\includegraphics{joint2-4d}}

\only<38   >{\includegraphics{joint1-5a}}
\only<39-41>{\includegraphics{joint1-5b}}
\only<42   >{\includegraphics{joint1-5c}}
\only<43   >{\includegraphics{joint1-5d}}
\only<44   >{\includegraphics{joint2-5a}}
\only<45-47>{\includegraphics{joint2-5b}}
\only<48   >{\includegraphics{joint2-5c}}
\only<49   >{\includegraphics{joint2-5d}}

\only<50   >{\includegraphics{joint1-6a}}
\only<51-53>{\includegraphics{joint1-6b}}
\only<54   >{\includegraphics{joint1-6c}}
\only<55   >{\includegraphics{joint1-6d}}
\only<56   >{\includegraphics{joint2-6a}}
\only<57-59>{\includegraphics{joint2-6b}}
\only<60   >{\includegraphics{joint2-6c}}
\only<61   >{\includegraphics{joint2-6d}}

\only<62   >{\includegraphics{joint1-7a}}
\only<63-65>{\includegraphics{joint1-7b}}
\only<66   >{\includegraphics{joint1-7c}}
\only<67   >{\includegraphics{joint1-7d}}
\only<68   >{\includegraphics{joint2-7a}}
\only<69-71>{\includegraphics{joint2-7b}}
\only<72   >{\includegraphics{joint2-7c}}
\only<73   >{\includegraphics{joint2-7d}}

\only<74   >{\includegraphics{joint1-8a}}
\only<75-77>{\includegraphics{joint1-8b}}
\only<78   >{\includegraphics{joint1-8c}}
\only<79   >{\includegraphics{joint1-8d}}
\only<80   >{\includegraphics{joint2-8a}}
\only<81-83>{\includegraphics{joint2-8b}}
\only<84   >{\includegraphics{joint2-8c}}
\only<85   >{\includegraphics{joint2-8d}}

\only<86   >{\includegraphics{joint1-9a}}
\only<87-89>{\includegraphics{joint1-9b}}
\only<90   >{\includegraphics{joint1-9c}}
\only<91   >{\includegraphics{joint1-9d}}
\only<92   >{\includegraphics{joint2-9a}}
\only<93-95>{\includegraphics{joint2-9b}}
\only<96   >{\includegraphics{joint2-9c}}
\only<97   >{\includegraphics{joint2-9d}}

\only<98   >{\includegraphics{joint1-10a}}
\only<99-101>{\includegraphics{joint1-10b}}
\only<102   >{\includegraphics{joint1-10c}}
\only<103   >{\includegraphics{joint1-10d}}
\only<104   >{\includegraphics{joint2-10a}}
\only<105-107>{\includegraphics{joint2-10b}}
\only<108   >{\includegraphics{joint2-10c}}
\only<109   >{\includegraphics{joint2-10d}}
\only<110   >{\includegraphics{joint-samples}}

\column{0.5\textwidth}
\only< 4   >{\includegraphics{cond1-2a}}
\only< 5- 6>{\includegraphics{cond1-2b}}
\only<10   >{\includegraphics{cond2-2a}}
\only<11-12>{\includegraphics{cond2-2b}}

\only<16   >{\includegraphics{cond1-3a}}
\only<17-18>{\includegraphics{cond1-3b}}
\only<22   >{\includegraphics{cond2-3a}}
\only<23-24>{\includegraphics{cond2-3b}}

\only<28   >{\includegraphics{cond1-4a}}
\only<29-30>{\includegraphics{cond1-4b}}
\only<34   >{\includegraphics{cond2-4a}}
\only<35-36>{\includegraphics{cond2-4b}}

\only<40   >{\includegraphics{cond1-5a}}
\only<41-42>{\includegraphics{cond1-5b}}
\only<46   >{\includegraphics{cond2-5a}}
\only<47-48>{\includegraphics{cond2-5b}}

\only<52   >{\includegraphics{cond1-6a}}
\only<51-52>{\includegraphics{cond1-6b}}
\only<58   >{\includegraphics{cond2-6a}}
\only<59-60>{\includegraphics{cond2-6b}}

\only<64   >{\includegraphics{cond1-7a}}
\only<65-66>{\includegraphics{cond1-7b}}
\only<70   >{\includegraphics{cond2-7a}}
\only<71-72>{\includegraphics{cond2-7b}}

\only<76   >{\includegraphics{cond1-8a}}
\only<77-78>{\includegraphics{cond1-8b}}
\only<82   >{\includegraphics{cond2-8a}}
\only<83-84>{\includegraphics{cond2-8b}}

\only<88   >{\includegraphics{cond1-9a}}
\only<89-90>{\includegraphics{cond1-9b}}
\only<94   >{\includegraphics{cond2-9a}}
\only<95-96>{\includegraphics{cond2-9b}}

\only<100   >{\includegraphics{cond1-10a}}
\only<101-102>{\includegraphics{cond1-10b}}
\only<106   >{\includegraphics{cond2-10a}}
\only<107-108>{\includegraphics{cond2-10b}}
\end{columns}
}

\frame{\frametitle{Traceplots}
\setkeys{Gin}{width=\textwidth}
\begin{columns}
\column{0.5\textwidth}
\includegraphics{marg1}
\column{0.5\textwidth}
\includegraphics{marg2}
\end{columns}
}

\frame{\frametitle{Running average}
\setkeys{Gin}{width=\textwidth}
\begin{columns}
\column{0.5\textwidth}
\includegraphics{cusum1}
\column{0.5\textwidth}
\includegraphics{cusum2}
\end{columns}
}

\frame{\frametitle{Auto-correlation plots}
\setkeys{Gin}{width=\textwidth}
\begin{columns}
\column{0.5\textwidth}
\includegraphics{acf1}
\column{0.5\textwidth}
\includegraphics{acf2}
\end{columns}
}

\section{MCMC Diagnostics}
\subsection{Overview}
\frame{\frametitle{MCMC Convergence diagnostics}
\begin{itemize}
\item Graphical techniques
 \begin{itemize}
 \item \alert{Traceplots}
 \item \alert{Ergodic mean}
 \end{itemize}
\item Non-graphical techniques
 \begin{itemize}
 \item Geweke diagnostic - single chain
 \item \alert{Gelman/Rubin diagnostic} - multiple chains
 \end{itemize}
\end{itemize}
}

\subsection{Lack of convergence}
\frame{\frametitle{Lack of convergence}
We can \alert{never} know if our chain has converged. 

\vspace{0.2in} \pause

All convergence diagnostics detect a lack of convergence. 

\vspace{0.2in} \pause

So instead of saying `\alert{the chain has converged}' you should be saying `\alert{the chain shows no lack of convergence}'.
}

\subsection{Burn-in}
\frame{\frametitle{Burn-in}
\begin{definition}
\alert{Burn-in} is the number of MCMC iterations before the chain shows no lack of convergence.
\end{definition}

\vspace{0.2in} \pause

Burn-in is thrown-out to eliminate the bias associated with the starting point.

\vspace{0.3in} 

\invisible{If the starting point is crucial, why not start multiple chains in different locations? With the local level model, start chain 1 at (-2000, 3000) and start chain 2 at (3000, -2000).}
}

\frame{\frametitle{Burn-in example}
\begin{center}
\alt<1>{\includegraphics{burnin1}}{\includegraphics{burnin2}}
\end{center}
\pause
}

\frame{\frametitle{Burn-in}
\begin{definition}
\alert{Burn-in} is the period of time before the chain has converged.
\end{definition}

\vspace{0.2in} 

Burn-in is thrown-out to eliminate the bias associated with the starting point.

\vspace{0.3in}

If the starting point is crucial, why not start multiple chains in different locations? With the local level model, start chain 1 at (-2000, 3000) and start chain 2 at (3000, -2000).
}

\frame{\frametitle{Multiple chains}
\begin{center}
\alt<1>{\includegraphics{mult1}}{\includegraphics{mult2}}
\end{center}
\pause
}

\subsection{Gelman-Rubin diagnostic}
\frame{\frametitle{Gelman-Rubin diagnostic}
\begin{itemize}
\item Start multiple chains at locations that are overdispersed relative to the posterior. 
\item ANOVA comparison
 \begin{itemize}
 \item Within-chain versus between-chain variances
 \item Represented as a scale reduction factor such that values around 1 indicate no lack of convergence.
 \end{itemize}
\end{itemize}
}

\frame{\frametitle{Local level model example}
\begin{center}
\includegraphics{overdispersed}
\end{center}
}

\frame[containsverbatim]{\frametitle{Local level model example - 100 iterations}
\setkeys{Gin}{width=\textwidth}
\begin{columns}
\column{0.5\textwidth}
{\small
In package \texttt{coda}, use function \texttt{gelman.diag}.

\begin{verbatim}
Potential scale reduction factors:

     Point est. 97.5% quantile
[1,]       1.12           1.45
[2,]       1.13           1.48

Multivariate psrf

1.09
\end{verbatim}

Values substantially above 1 indicate lack of convergence. 
}
\column{0.5\textwidth}
\includegraphics{gelman2}
\end{columns}
}

\frame[containsverbatim]{\frametitle{Local level model example - 1000 iterations}
\setkeys{Gin}{width=\textwidth}
\begin{columns}
\column{0.5\textwidth}
{\small
In package \texttt{coda}, use function \texttt{gelman.diag}.

\begin{verbatim}
Potential scale reduction factors:

     Point est. 97.5% quantile
[1,]          1           1.00
[2,]          1           1.00

Multivariate psrf

1.00
\end{verbatim}

Values substantially above 1 indicate lack of convergence.
}
\column{0.5\textwidth}
\includegraphics{gelman2}
\end{columns}
}

\subsection{Length of chain}
\frame[containsverbatim]{\frametitle{Iterations for inference}
\setkeys{Gin}{width=\textwidth}
Now that no lack of convergence is apparent, how long should I run my chain?

\begin{itemize}
\item The longer you run the chain, the lower your Monte Carlo error.
\item Monte Carlo error reduces by the $\sqrt{N}$ where $N$ is the number of MCMC iterations.
\item So, if you want a $10$-fold decrease in Monte Carlo error, you need to run $10^2$ times your current number of iterations.
\end{itemize}
}

\frame{\frametitle{Simple Monte Carlo example}
Consider the model $y_i \ind N(\mu,1)$ and our goal is to estimate $E[y_i]=\mu$. The Monte Carlo approximation is 
\[ \mu \approx \mu_{MC} = \frac{1}{n} \sum_{i=1}^n y_i \]
with the variance of this approximation given by 
\[ \mbox{se}(\mu_{MC}) \approx \sqrt{\frac{1}{n(n-1)} \sum_{i=1}^n (y_i-\mu_{MC})^2} = \frac{1}{\sqrt{n}} sd_{y}\]
where $sd_y$ is the standard deviation of the sample $y=(y_1,y_2,\ldots,y_n)$. Since this standard deviation converges to 1, by our model assumption above, the standard error of the Monte Carlo estimate decrease by the square root of $n$.  
}

\frame{\frametitle{Simple Monte Carlo example}
\setkeys{Gin}{width=\textwidth}
At 176 simulations, the standard error of $\mu_{MC}$ is $\sim 0.07$. To decrease this to $0.007$ (an order of magnitude increase in accuracy), we would need to take a total of $176\cdot 10^2=17600$ simulations.
\begin{center}
\includegraphics{monteCarlo}
\end{center}
}

\frame[containsverbatim]{\frametitle{Iterations for inference}
\setkeys{Gin}{width=\textwidth}
\begin{columns}
\column{0.55\textwidth}
\begin{center}
\includegraphics{acfs}
\end{center}
\column{0.45\textwidth}
Use \texttt{effectiveSize}
\begin{verbatim}
    var1     var2
169.6231 165.3914
\end{verbatim}
\end{columns}
}


\section{Bayesian analysis}
\subsection{Work flow}
\frame{\frametitle{Work flow}
\begin{itemize}
\item Exploratory data analysis
\item Define a model with priors
\item Fit the model using MLE techniques
\item Inference
 \begin{itemize}
 \item Fit in WinBUGS
 \item Code it up in R/C
 	\begin{itemize}
	 \item Choose an MCMC scheme
 	\item Find the full conditional distributions (if available)
	\end{itemize}
 \item Monitor chain convergence
 \item Summarize the posterior
 \end{itemize}
\item Model checking
	\begin{itemize}
	\item Diagnostic plots to evaluate model assumptions, e.g. one-step head forecasts
	\end{itemize} 
\end{itemize}
}

\subsection{Examples}
\frame{\frametitle{Examples}
\begin{itemize}
\item Nile flow - local level model
\item Spain/Denmark investments - SUTSE
\end{itemize}
}

\section{Example I - Nile river level}

\subsection{The data}
\frame{\frametitle{Nile river level}
\begin{center}
\includegraphics{nile}
\end{center}
}
\frame{\frametitle{Nile river level}
\begin{center}
\includegraphics{nileacf}
\end{center}
}

\subsection{The model}
\frame{\frametitle{Local level model}
\[ \begin{array}{rl@{\qquad}l}
Y_t &= \theta_t + v_t & v_t \sim N(0,V) \\
\theta_t &= \theta_{t-1} + w_t & w_t \sim N(0,W) \\ \pause
\\
V & \sim IG(a_V,b_V) \\
W & \sim IG(a_W,b_W) \\
\theta_0 & \sim N(m_0,C_0)
\end{array} \] \pause
where $p(V,W,\theta_0) = p(V)p(W)p(\theta_0)$ and $v_t$ and $w_t$ are independent across time and mutually independent of each other as well as independent of $\theta_0$.
}
\frame{\frametitle{Non-informative priors}
\[ \begin{array}{rl@{\qquad}l}
V & \sim IG(a_V,b_V) \\
W & \sim IG(a_W,b_W) \\
\theta_0 & \sim N(m_0,C_0)
\end{array} \]

\vspace{0.3in} \pause

Non-informative prior


\[ \begin{array}{rll}
V& \propto 1/V &\implies a_V = b_V = 0 \pause \\
W& \propto 1/W &\implies a_W = b_W = 0 \pause\\
\theta_0 &\propto 1 &\implies m_0 = 0, C_0 = \infty
\end{array} \]
}

\frame{\frametitle{Informative variance priors}
Informative prior for $V$ (or $W$), $E[V]=5$ with varying accuracy\pause
\begin{center}
\includegraphics{vprior}
\end{center}
}

\frame{\frametitle{Informative state prior}
``On average, the Nile flow is around 920 $\pm$ 340.'' With what probability? \pause
\begin{center}
\includegraphics{theta0prior}
\end{center}
}

\subsection{Inference}
\frame{\frametitle{MCMC scheme}
In DLMs, conditional on unknown parameters, we can sample from the joint state vector at all times using FFBS. \pause

\begin{itemize}
\item $p(\theta_{0:T}| V, W, y_{1:T})$ \pause (for references see page 161 of Petris et al.) \pause
\item $p(V|\theta_{0:T}, W, y_{1:T})$
\item $p(W|\theta_{0:T}, V, y_{1:T})$
\end{itemize}
}

\frame{\frametitle{Full conditional distributions - the hard way}
\[ \begin{array}{rl}
\multicolumn{2}{l}{p(V|\theta_{0:T}, W, y_{1:T})} \\ \pause
&\propto \prod_{t=1}^T p(y_t|\theta_t,V) p(\theta_t|\theta_{t-1},W) p(V)p(W)p(\theta_0) \\ \pause
&\propto \prod_{t=1}^T p(y_t|\theta_t,V) p(V) \\ \pause
&=\prod_{t=1}^T N(y_t; \theta_t, V) IG(V; a_V, b_V) \\ \pause
&\propto V^{-T/2}\exp\left(-\frac{1}{2V} \sum_{t=1}^T (y_t-\theta_t)^2 \right) V^{-a_V-1} \exp\left(-b_V/V\right) \\ \pause
&= V^{-(a_V+T/2)-1} \exp\left(-\left[b_V+\frac{1}{2} \sum_{t=1}^T (y_t-\theta_t)^2\right]/V \right) \\ \pause
&\propto IG\left(a_V+T/2, b_V+\frac{1}{2} \sum_{t=1}^T (y_t-\theta_t)^2 \right)
\end{array} \]
}
\frame{\frametitle{Full conditional distributions - the hard way}
\[ \begin{array}{rl}
\multicolumn{2}{l}{p(W|\theta_{0:T}, V, y_{1:T})} \\ \pause
&\propto \prod_{t=1}^T p(y_t|\theta_t,V) p(\theta_t|\theta_{t-1},W) p(V)p(W)p(\theta_0) \\ \pause
&\propto \prod_{t=1}^T p(\theta_t|\theta_{t-1},W) p(W) \\ \pause
&=\prod_{t=1}^T N(\theta_t; \theta_{t-1}, W) IG(W; a_W, b_W) \\ \pause
&\propto W^{-T/2}\exp\left(-\frac{1}{2W} \sum_{t=1}^T (\theta_t-\theta_{t-1})^2 \right) W^{-a_W-1} \exp\left(-b_W/W\right) \\ \pause
&= V^{-(a_W+T/2)-1} \exp\left(-\left[b_W+\frac{1}{2} \sum_{t=1}^T (\theta_t-\theta_{t-1})^2\right]/W \right) \\ \pause
&\propto IG\left(a_W+T/2, b_W+\frac{1}{2} \sum_{t=1}^T (\theta_t-\theta_{t-1})^2 \right)
\end{array} \]
}
\frame{\frametitle{Full conditional distributions - the easy way}
Recall from HW 1b, if $\sigma^2 \sim IG(a,b)$ and $x_i\ind N(0,\sigma^2)$, \pause then
\[ p(\sigma^2|x_1,x_2,\ldots,x_n) = IG\left(a+n/2, b+\frac{1}{2} \sum_{t=1}^n x_t^2\right).\]

\vspace{0.2in} \pause

Notice
\[ \begin{array}{rl}
V&\sim IG(a_V,b_V) \\ \pause
v_t = y_t-\theta_t &\ind N(0,V) \\ \pause
p(V|y_{1:T}, \theta_{0:T}, W) &= p(V|y_{1:T},\theta_{1:T}) \\ \pause
&= IG(a_V+T/2, b_V+\frac{1}{2} \sum_{t=1}^T v_t^2) \\ \pause
\\
W &\sim IG(a_W, b_W) \\ \pause
w_t = \theta_t-\theta_{t-1} &\ind N(0,W) \\ \pause
p(W|y_{1:T}, \theta_{0:T}, V) &= p(W|\theta_{1:T}) \\ \pause
&= IG(a_W+T/2, b_W+\frac{1}{2} \sum_{t=1}^T w_t^2)
\end{array} \]
}

\frame{\frametitle{MCMC scheme revisited}
\begin{itemize}
\item $p(\theta_{0:T}|\ldots)$ using FFBS
\item $p(V|\ldots)=IG(a_V+T/2, b_V+\frac{1}{2} \sum_{t=1}^T v_t^2)$
\item $p(W|\ldots)= IG(a_W+T/2, b_W+\frac{1}{2} \sum_{t=1}^T w_t^2)$
\end{itemize}

\vspace{0.2in} \pause

Notice that $p(V|\ldots)$ doesn't depend on $W$ and $p(W|\ldots)$ doesn't depend on $V$. \pause So our scheme is actually
\begin{itemize}
\item $p(\theta_{0:T}|\ldots)$ using FFBS
\item $p(V,W|\ldots)=IG(a_V+T/2, b_V+\frac{1}{2} \sum_{t=1}^T v_t^2)IG(a_W+T/2, b_W+\frac{1}{2} \sum_{t=1}^T w_t^2)$
\end{itemize}
}

\frame[containsverbatim]{\frametitle{Coding it up}
Begin by creating a function to draw from the posterior of a conjugate inverse gamma
\begin{verbatim}
drawIGpost <- function(x, a=0, b=0) {
  return(rinvgamma(1, a+length(x)/2, b+sum(x^2)/2))
}
\end{verbatim}
}
\frame[containsverbatim]{\frametitle{Coding it up}
Begin by creating a function to draw from the posterior of a conjugate inverse gamma
{\small
\begin{verbatim}
for (i in 1:n.reps) {
  cat(i,"\n")
  # Sample states
  mod   <- dlmModPoly(1, dV=V, dW=W)
  filt  <- dlmFilter(Nile, mod)
  theta <- dlmBSample(filt)

  # Sample V and W
  V <- drawIGpost(y-theta[-1])
  W <- drawIGpost(theta[-1]-theta[-n])

  # Save iterations
  V.reps[i] <- V
  W.reps[i] <- W
  theta.reps[i,] = theta
}
\end{verbatim}
}
}

\frame{\frametitle{Running the MCMC}
\begin{itemize}
\item Run 1
 \begin{itemize}
 \item Run 1 chain starting from the MLEs
 \item Check traceplots for this run
 \item Obtain posterior summaries for model parameters
 \item Choose initial values that are $<$ minimum and $>$ maximum for each model parameter
 \end{itemize}
\item Multi-runs
 \begin{itemize}
 \item Start multiple chains from combinations of these values
 \item Check traceplots and Gelman-Rubin diagnostic for these chains
 \item Discard burn-in and produce posterior summaries on remaining iterations
 \item If more iterations are needed, initialize new chains from the last iteration of the old chains
 \end{itemize}
\end{itemize}
}

\frame{\frametitle{Monitoring convergence}
Use \texttt{plot.mcmc} in \texttt{coda} package.
\begin{center}
\includegraphics[page=1]{Nileresults}
\end{center}
}
\frame{\frametitle{Monitoring convergence}
Use \texttt{plot.mcmc} in \texttt{coda} package.
\begin{center}
\includegraphics[page=2]{Nileresults}
\end{center}
}
\frame{\frametitle{Monitoring convergence}
Use \texttt{plot.mcmc} in \texttt{coda} package.
\begin{center}
\includegraphics[page=3]{Nileresults}
\end{center}
}

\frame[containsverbatim]{\frametitle{Gelman-Rubin diagnostic}
\begin{verbatim}
> gelman.diag(window(mcmc.results,1,4000))
Potential scale reduction factors:

       Point est. 97.5% quantile
V.reps       1.00           1.00
W.reps       1.00           1.00
             1.00           1.00
             1.00           1.00

             1.00           1.01
             1.00           1.00

Multivariate psrf

1.01
\end{verbatim}
}

\frame[containsverbatim]{\frametitle{Posterior summaries}

\vspace{-0.1in}

{\scriptsize
\begin{verbatim}
> summary(window(mcmc.results,4001,5000))

Iterations = 4001:5000
Thinning interval = 1
Number of chains = 4
Sample size per chain = 1000

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

          Mean      SD Naive SE Time-series SE
V.reps 15642.8 3187.29  50.3955       125.8969
W.reps  1630.4 1449.03  22.9111       100.2639
        1107.9   73.84   1.1676         1.3148
        1108.7   62.93   0.9951         1.1196

2. Quantiles for each variable:

         2.5%     25%     50%     75%   97.5%
V.reps 9854.5 13459.2 15450.6 17640.9 22337.6
W.reps  241.2   651.7  1183.1  2092.0  5529.9
        964.9  1060.0  1106.7  1153.6  1261.0
        987.1  1066.5  1107.5  1149.0  1238.8
\end{verbatim}
}
}

\frame{\frametitle{Posterior summaries}
\begin{center}
\includegraphics{VWpost}
\end{center}
}

\frame{\frametitle{Posterior summaries}
\begin{center}
\includegraphics[page=1]{thetapost}
\end{center}
}

\frame{\frametitle{Posterior summaries}
\begin{center}
\includegraphics[page=2]{thetapost}
\end{center}
}

\frame{\frametitle{Summaries of functions of parameters}
The posterior for $f(\psi)$ is available using the MCMC simulations by plugging our iterations $\psi^{(i)}$ into $f(\cdot)$ and calculating desired quantities, \pause e.g.
\[ E[f(\psi)] \approx \frac{1}{n} \sum_{i=1}^n f(\psi^{(i)}).\]

\vspace{0.2in} \pause

For example,
\begin{itemize}
\item $f(\theta_{0:T},V,W) = \sqrt(V)$
\item $f(\theta_{0:T},V,W) = \sqrt(W)$
\item $f(\theta_{0:T},V,W) = W/V$ (signal-to-noise ratio)
\item $f(\theta_{0:T},V,W) = P(W/V<1)$
\end{itemize}
}

\frame{\frametitle{Standard deviations}
\begin{center}
\includegraphics{sdpost}
\end{center}
}

\frame{\frametitle{Signal-to-noise ratio}

\vspace{-0.2in}

\begin{center}
\includegraphics{rpost}
\end{center}
\pause
\[ P(r<1) = 0.998.\]
}

\section{$d$ inverse gamma priors}
\subsection{Model}
\frame{\frametitle{The model}
\[ \begin{array}{rl@{\qquad}l}
Y_t &= F_t\theta_t + v_t & v_t \sim N(0,V) \\
\theta_t &= G_t\theta_{t-1} + w_t & w_t \sim N_p(0,W)
\end{array} \]
where $V$ is scalar and $W$ is diagonal with elements $W_i$ \pause and assumed priors
\[ \begin{array}{rl@{\qquad}l}
p(V,W_1,\ldots,W_p, \theta_0) &= p(V)p(\theta_0) \prod_{i=1}^p p(W_i) \\
V & \sim IG(a_V,b_V) \\
W_i & \sim IG(a_{W_i},b_{W_i}) \\
\theta_0 & \sim N(m_0,C_0)
\end{array} \]
}
\subsection{Linear trend example}
\frame{\frametitle{Linear trend model}
For example
\[ \begin{array}{rl@{\qquad}l}
Y_t &= F\theta_t + v_t & v_t \sim N(0,V) \\
\theta_t &= G\theta_{t-1} + w_t & w_t \sim N_p(0,W)
\end{array} \]
where $F=(1, 0)$, $G[1,1]=G[1,2]=G[2,2]=1$, $G[2,1]=0$, and $W$ is diagonal with elements $W_i$ \pause and assumed priors
\[ \begin{array}{rl@{\qquad}l}
p(V,W_1,\ldots,W_p, \theta_0) &= p(V)p(\theta_0)p(W_1)p(W_2) \\
V & \sim IG(a_V,b_V) \\
W_1 & \sim IG(a_{W_1},b_{W_1}) \\
W_2 & \sim IG(a_{W_2},b_{W_2}) \\
\theta_0 & \sim N(m_0,C_0)
\end{array} \]
}
\frame{\frametitle{Rewrite the linear trend model}
\[ \begin{array}{rl@{\qquad}l}
Y_t &= \mu_t + v_t & v_t \sim N(0,\sigma^2) \\
\mu_t &= \mu_{t-1}+\beta_t + w_{t,1} & w_{t,1} \sim N(0,\sigma_{\mu}^2) \\
\beta_t &= \beta_{t-1} + w_{t,2} & w_{t,2} \sim N(0,\sigma_{\beta}^2)
\end{array} \]
where $w_{t,1}$ and $w_{t,2}$ are independent.

\vspace{0.2in} \pause

What are the full conditionals for $\sigma^2, \sigma_{\mu}^2,$ and $\sigma_{\beta}^2$? \pause
\begin{itemize}
\item $p(\sigma^2|\ldots) \pause = IG\left(a_{\sigma^2} + T/2, b_{\sigma^2}+\frac{1}{2} \sum_{t=1}^T v_t^2\right)$ \pause
\item $p(\sigma_\mu^2|\ldots) \pause = IG\left(a_{\sigma_\mu^2} + T/2, b_{\sigma_\mu^2}+\frac{1}{2} \sum_{t=1}^T w_{t,1}^2\right)$ \pause
\item $p(\sigma_\beta^2|\ldots) \pause = IG\left(a_{\sigma_\beta^2} + T/2, b_{\sigma_\beta^2}+\frac{1}{2} \sum_{t=1}^T w_{t,2}^2\right)$ \pause
\end{itemize}
and importantly, they are independent!
}
\subsection{General models}
\frame{\frametitle{More generally}
\[ \begin{array}{rl@{\qquad}l}
Y_t &= F_t\theta_t + v_t & v_t \sim N(0,V) \\
\theta_t &= G_t\theta_{t-1} + w_t & w_t \sim N_p(0,W)
\end{array} \]
where $W$ is diagonal with elements $W_i$ and all variances have independent inverse gamma priors.

\vspace{0.2in} \pause

The full conditionals for parameters are
\begin{itemize}
\item $p(V|\ldots) = IG\left(a_{V} + T/2, b_{V}+\frac{1}{2} \sum_{t=1}^T v_t^2\right)$
\item $p(W_i|\ldots) = IG\left(a_{W_i} + T/2, b_{W_i}+\frac{1}{2} \sum_{t=1}^T w_{t,i}^2\right)$
\end{itemize}
and again, they are independent!
}
\subsection{MCMC scheme}
\frame{\frametitle{MCMC scheme for models with $d$ inverse gamma priors}
Two-stage Gibbs sampler
\begin{itemize}
\item Use FFBS to sample from $p(\theta_{0:T}|\ldots)$
\item Jointly sample $V,W_1,\ldots,W_p$ by sampling their full conditionals
 \begin{itemize}
 \item $p(V|\ldots)$
 \item $p(W_i|\ldots)$ for $i\in(1,2,\ldots,p)$.
 \end{itemize}
\end{itemize}

\vspace{0.2in} \pause

Implemented in \texttt{dlmGibbsDIG}.
}



\section{Unknown covariance matrices}
\subsection{Model}
\frame{\frametitle{}
Suppose we assume the model
\[ \begin{array}{rl@{\qquad}l}
Y_t &= F_t\theta_t + v_t & v_t \sim N_m\left(0,\mathrm\Phi_0^{-1}\right) \\
\theta_t &= G_t\theta_{t-1} + w_t & w_t \sim N_p\left(0,\mathrm\Phi_1^{-1}\right)
\end{array} \]
where $\mathrm\Phi_0$ is an $m\times m$ observation precision matrix and $\mathrm\Phi_1$ is a $p\times p$ evolution precision matrix. \pause It will be convenient to choose independent Wishart distributions for the prior for these precision matrices, \pause i.e. 
\[p\left(\mathrm\Phi_0,\mathrm\Phi_1\right)=p\left(\mathrm\Phi_0\right)p\left(\mathrm\Phi_1\right)=\mathcal{W}\left(\mathrm\Phi_0; \nu_0, S_0\right)\mathcal{W}\left(\mathrm\Phi_1; \nu_1, S_1\right) \] \pause
where

\[ \mathcal{W}(P; \nu, S) = \frac{|S|^{\nu}|P|^{\frac{\nu-p-1}{2}}}{\Gamma_p(\nu)}\exp\left( - tr(SP)\right) \]
is a distribution on symmetric, positive definite matrices $P$ with parameters $\nu>(p-1)/2$ and $S$, symmetric non-singular matrix.
}

\subsection{Full conditionals}
\frame{\frametitle{Full conditionals for the precision matrices}
Wishart distributions are conditionally conjugate in this model: \pause
\[ p\left(\mathrm\Phi_0|\ldots\right) = \mathcal{W}\left(\nu_0+T/2, S_0+\frac{1}{2}SS_y\right) \]
where $SS_y = \sum_{t=1}^T \left(y_t-F_t\theta_t\right)\left(y_t-F_t\theta_t\right)^\top $.

\vspace{0.2in} \pause

\[ p\left(\mathrm\Phi_1|\ldots\right) = \mathcal{W}\left(\nu_1+T/2, S_1+\frac{1}{2}SS_\theta\right) \]
where $SS_\theta = \sum_{t=1}^T \left(\theta_t-G_t\theta_{t-1}\right)\left(\theta_t-G_t\theta_{t-1}\right)^\top $.

\vspace{0.2in} \pause

Again, $p\left(\mathrm\Phi_0,\mathrm\Phi_1|\ldots\right)=p\left(\mathrm\Phi_0|\ldots\right)p\left(\mathrm\Phi_1|\ldots\right)$.

\vspace{0.2in} \pause

To draw from these distributions, use \texttt{rwishart} in \texttt{dlm} package which has arguments degrees of freedom $\delta$ and scale matrix $V_0^{-1}$ where $\mathcal{W}(\delta/2, V_0/2)$. 
}

\section{$d$ inverse Wisharts}
\subsection{The model}
\frame{\frametitle{The model}
Consider the model with block-diagonal evolution covariance:

\vspace{0.2in}

\[ \begin{array}{rl@{\qquad}l}
Y_t &= F_t\theta_t + v_t & v_t \sim N_m(0,\mathrm\Phi_0^{-1}) \\
\theta_t &= G_t\theta_{t-1} + w_t & w_t \sim N_{p*}(0,W)
\end{array} \]

\vspace{0.2in}

where $W$ is block-diagonal with elements $W_i$.\pause  Set $\mathrm\Phi_i^{-1} = W_i$ and give $\mathrm\Phi_0, \mathrm\Phi_1, \ldots, \mathrm\Phi_d$ independent Wishart priors $\mathrm\Phi_i\sim \mathcal{W}(\nu_i, S_i).$
}

\frame{\frametitle{Rewritten univariate model}
For combining individual components, e.g. polynomial trend, seasonal, dynamic regression, $G_t$ is block diagonal with elements $G_{i,t}$ relating to $W_i$ and the model can be re-written 
\[ \begin{array}{rl@{\qquad}l}
Y_t &= F_t\theta_t + v_t & v_t \sim N(0,\mathrm\Phi_0^{-1}) \\
\theta_{1,t} &= G_{1,t}\theta_{1,t-1} + w_{1,t} & w_{1,t} \sim N_{p_1}(0,\mathrm\Phi^{-1}_1) \\
& \vdots \\
\theta_{i,t} &= G_{i,t}\theta_{i,t-1} + w_{i,t} & w_{i,t} \sim N_{p_i}(0,\mathrm\Phi^{-1}_i) \\
& \vdots \\
\theta_{p,t} &= G_{p,t}\theta_{p,t-1} + w_{p,t} & w_{p,t} \sim N_{p_d}(0,\mathrm\Phi^{-1}_d) \\
\end{array} \]
where $w_{i,t}$ are independent across $i$. Then 
\[ SS_{ii,t} = (\theta_{i,t}-G_{i,t}\theta_{i,t-1})(\theta_{i,t}-G_{i,t}\theta_{i,t-1})^\top .\]
}

\frame{\frametitle{Multivariate models}
Let
\[ SS_t = (\theta_t-G_t\theta_{t-1})(\theta_t-G_t\theta_{t-1})^\top  \]
and partition it according to
\[ SS_t = \left[ \begin{array}{ccc} SS_{11,t} & \cdots & SS_{1d,t} \\
\vdots & \ddots & \vdots \\ SS_{d1,t} & \cdots & S_{dd,t} \end{array} \right] \]
where the partition is according to the partition in $\mathrm\Phi=\mbox{blockdiag}(\mathrm\Phi_1,\ldots,\mathrm\Phi_d)$.
}

\subsection{Full conditional distributions}
\frame{\frametitle{Full conditional distributions}
\[ p\left(\mathrm\Phi_0^{-1}|\ldots\right) = \mathcal{W}\left(\nu_0+T/2, S_0+\frac{1}{2}SS_y\right) \]
where $SS_y = \sum_{t=1}^T \left(y_t+F_t\theta_t\right)\left(y_t-F_t\theta_t\right)^\top $.

\vspace{0.2in} \pause

\[ p\left(\mathrm\Phi_i^{-1}|\ldots\right) = \mathcal{W}\left(\nu_i+T/2, S_i+\frac{1}{2}SS_{\theta_i}\right) \]
where $SS_{\theta_i} = \sum_{t=1}^T SS_{ii,t}$ given on the previous page.

\vspace{0.2in} \pause

Once again, $p\left(\mathrm\Phi_0^{-1},\mathrm\Phi_1^{-1},\ldots,\mathrm\Phi_d^{-1}|\ldots\right)=p\left(\mathrm\Phi_0^{-1}|\ldots\right)\prod_{i=1}^d p\left(\mathrm\Phi_i^{-1}|\ldots\right)$.
}

\section{SUTSE example}
\subsection{Data}
\frame{\frametitle{Denmark and Spain investments}
\begin{center}
\includegraphics{invest}
\end{center}
}
\subsection{Model}
\frame{\frametitle{SUTSE model}
\[ \begin{array}{rl@{\qquad}l}
Y_t &= (F \otimes I_2)\theta_t + v_t & v_t \sim N_2(0,\mathrm\Phi_0^{-1}) \\
\theta_t &= (G \otimes I_2) \theta_{t-1} + w_t & w_t \sim N_4(0,W)
\end{array} \]
where $W=\mbox{blockdiag}(W_1,W_2)$, $\mathrm\Phi_1^{-1}=W_1$, and $\mathrm\Phi_2^{-1}=W_2$.

\vspace{0.2in}\pause

Assume independent Wishart priors
\[ \begin{array}{rl@{\qquad}l}
p(\mathrm\Phi_0) &= \mathcal{W}\left(\frac{\delta_0+1}{2}, \frac{1}{2}V_0\right) & V_0 = (\delta_0-2) \left[ \begin{array}{cc} 10^2 & 0 \\ 0 & 500^2 \end{array} \right] \\
p(\mathrm\Phi_1) &= \mathcal{W}\left(\frac{\delta_1+1}{2},\frac{1}{2}W_{\mu,0}\right) & W_{\mu,0} = (\delta_1-2) \left[ \begin{array}{cc} 0.01^2 & 0 \\ 0 & 0.01^2 \end{array} \right]\\
p(\mathrm\Phi_2) &= \mathcal{W}\left(\frac{\delta_2+1}{2},\frac{1}{2}W_{\beta,0}\right) & W_{\beta,0} = (\delta_2-2) \left[ \begin{array}{cc} 5^2 & 0 \\ 0 & 100^2 \end{array} \right]
\end{array} \]
where $\delta_0=\delta_2=3$ and $\delta_1=100$.
}

\subsection{MCMC}
\frame{\frametitle{MCMC sampling}
MCMC Scheme:
\begin{itemize}
\item Sample $\theta_{0:T}\sim p(\theta_{0:T}|\ldots)$ using FFBS
\item Sample $p(\mathrm\Phi_0,\mathrm\Phi_1,\mathrm\Phi_2|\ldots)$ jointly
\[ \begin{array}{rl@{\qquad}l}
p(\mathrm\Phi_0|\ldots) &= \mathcal{W}\left(\frac{\delta_0+1+T}{2}, \frac{1}{2}(V_0+SS_y)\right) \\
p(\mathrm\Phi_1|\ldots) &= \mathcal{W}\left(\frac{\delta_1+1+T}{2},\frac{1}{2}(W_{\mu,0}+SS_{1\cdot})\right) \\
p(\mathrm\Phi_2|\ldots) &= \mathcal{W}\left(\frac{\delta_2+1+T}{2},\frac{1}{2}(W_{\beta,0}+SS_{2\cdot})\right)
\end{array} \]
where
\[
SS_{i\cdot} = \sum_{t=1}^T SS_{ii,t}.
 \]
provided earlier.
\end{itemize}
}

\frame{\frametitle{Convergence and autocorrelation}
\begin{center}
\includegraphics[page=1]{inv-conv}
\end{center}
}
\frame{\frametitle{Convergence}
\begin{center}
\includegraphics[page=2]{inv-conv}
\end{center}
}
\frame{\frametitle{Convergence}
\begin{center}
\includegraphics[page=3]{inv-conv}
\end{center}
}

\subsection{Inference}
\frame{\frametitle{Posterior covariance expectations}
\[ \begin{array}{rl} E[V|y_{1:T}] &= \phantom{1e-5}\left[ \begin{array}{cccc}
86 &(1) & 1026 &(23) \\
&& 59340 &(807)
\end{array} \right]
\\
\\
E[W_\mu|y_{1:T}] &= 1e-5\left[ \begin{array}{cccc}
9.97  &(0.02) & 0.016 & (0.014) \\
&& 10.04 & (0.02)
\end{array} \right]
\\
\\
E[W_\beta|y_{1:T}] &= \phantom{1e-5}\left[ \begin{array}{cccc}
38.3  &(0.8) & 305 & (41) \\
&& 311073 & (2346)
\end{array} \right]
\end{array} \]
}
\frame{\frametitle{Posterior $\mu_t$}
\begin{center}
\includegraphics{inv-states}
\end{center}
}



\section{Missing data}
\subsection{Types}
\frame{\frametitle{Types of missing data}

Complete data $Y_{i,t}$ and missing indicator $M_{i,t}$ where $M_{i,t}=1$ if observation $Y_{it}$ is missing and $0$ otherwise. \pause Let $Y_{\mbox{obs}}$ contain all the data that is observed while $Y_{\mbox{mis}}$ contains all the data that is missing with $Y = (Y_{\mbox{obs}},Y_{\mbox{mis}})$.  \pause Then several types of missing-ness are possible:

\vspace{0.2in} \pause

\begin{itemize}
\item Missing completely at random (MCAR)
\[ p(M|Y,\phi) = p(M|\phi). \] \pause
\item Missing at random (MAR)
\[ p(M|Y,\phi) = p(M|Y_{obs},\phi). \] \pause
\item Not missing at random
\[ p(M|Y,\phi) \mbox{ depends on } Y_{\mbox{mis}} .\] \pause
\end{itemize}

We will only consider $MCAR$.
}

\frame{\frametitle{No missing data}
\begin{center}
\includegraphics[page=1]{missing}
\end{center}
}
\frame{\frametitle{Missing completely at random}
\begin{center}
\includegraphics[page=2]{missing}
\end{center}
}
\frame{\frametitle{Not missing at random}
\begin{center}
\includegraphics[page=3]{missing}
\end{center}
}

\subsection{in DLMs}
\frame{\frametitle{Missing data in multivariate DLMs}
Two situations:
\begin{itemize}
\item Totally missing: at time $t$, $Y_t$ is completely missing
\item Partially missing: at time $t$, part of $Y_t$ is observed
\end{itemize}
}

\frame{\frametitle{Totally missing}
Recall `Kalman filter' lecture: missing data are handled trivially while filtering.
\[ m_t = a_t \qquad C_t = R_t.\]

\vspace{0.2in} 

Unknown fixed parameters are sampled without these data, e.g. scalar $V$
\[ \begin{array}{rl}
p(\phi_V|\ldots) &= G\left(a+\frac{T'}{2}, b+\sum_{t\in \mbox{obs}} (y_t -F_t\theta_t)^2\right)
\end{array} \]
where `obs' is a vector of times when the data are observed and $T'\le T$ is the length of obs.

\vspace{0.1in}
e.g. matrix $V$
\[ \begin{array}{rl}
p(\mathrm\Phi_V|\ldots) &= W\left(a+\frac{T'}{2}, b+\sum_{t\in \mbox{obs}} (y_t -F_t\theta_t)(y_t -F_t\theta_t)^\top \right) .
\end{array} \]
}

\frame{\frametitle{Partially missing when filtering}
Suppose $M_t$ is the matrix that is built by taking an identity matrix and removing the rows of any missing observations in $y_t$. \pause Then $\tilde{y}_t = M_ty_t$ contains only the observed data. The correct observation equation to consider is 
\[ \tilde{y}_t = \tilde{F}_t\theta_t + \tilde{v}_t \qquad \tilde{v}_t \sim N(0,\tilde{V}_t). \]

\vspace{0.2in} \pause

What are $\tilde{F}_t$ and $\tilde{V}_t$?

\vspace{0.1in} \pause 

\begin{itemize}
\item $\tilde{F}_t = M_tF_t$
\item $\tilde{V}_t = M_tV_tM_t^\top $
\end{itemize}
}

\frame{\frametitle{Partially missing in MCMC}
Let $Y = (Y_{\mbox{obs}},Y_{\mbox{mis}})$. \pause If we build an MCMC with only the observed data, then our scheme will look like

\vspace{0.2in}

\begin{itemize}
\item Sample $p(\theta|Y_{\mbox{obs}},\psi)$ via FFBS
\item Sample $p(\psi|Y_{\mbox{obs}},\theta)$.
\end{itemize}

\vspace{0.2in} \pause

For example, consider the observation precision matrix $\mathrm\Phi_V$ as the only unknown parameter. What is it's full conditional distribution?
\[ p(\mathrm\Phi_V|Y_{\mbox{obs}}, \theta) \propto p(Y_{\mbox{obs}}|\mathrm\Phi_V,\theta)p(\mathrm\Phi_V)\]
Who knows?
}

\frame{\frametitle{Partially missing in MCMC}
Let $Y = (Y_{\mbox{obs}},Y_{\mbox{mis}})$. \pause Augment the MCMC to simulate the missing values, then our scheme will look like

\vspace{0.2in}

\begin{itemize}
\item Sample $p(\theta|Y,\psi)$ via FFBS
\item Sample $p(\psi|Y,\theta)$
\item Sample $p(Y_{\mbox{mis}}|Y_{\mbox{obs}},\theta,\psi)$.
\end{itemize}

\vspace{0.2in} \pause

This works since 
\[ p(\theta,\psi|Y_{\mbox{obs}}) = \int p(\theta,\psi,Y_{\mbox{mis}}|Y_{\mbox{obs}}) dY_{\mbox{mis}} .\]
}

\frame{\frametitle{Partially missing in MCMC}
How to simulate $p(Y_{\mbox{mis}}|Y_{\mbox{obs}},\theta,\psi)$?

\vspace{0.2in} \pause

First note, 
\[ p(Y_{\mbox{mis}}|Y_{\mbox{obs}},\theta,\psi) = \prod_{t=1}^T p(Y_{\mbox{mis},t}|Y_{\mbox{obs},t},\theta_t,\psi).\]

\vspace{0.2in} \pause

Second note,
\[ \left( \begin{array}{c} Y_{\mbox{mis},t} \\ Y_{\mbox{obs},t}\end{array} \right) \pause \sim N\left(\pause \left[\begin{array}{c} F\theta_{\mbox{mis},t} \\ F\theta_{\mbox{obs},t} \end{array} \right] ,\left[ \begin{array}{cc}V_{\mbox{mis}} & V_{\mbox{m,o}} \\ V_{\mbox{o,m}} & V_{\mbox{obs}}\end{array}\right]
\right). \]
}

\section{Forecasting}
\frame{\frametitle{Goal}
With \alert{all fixed parameters known}: \pause
\[ \begin{array}{rl}
p(y_{t+k},\theta_{t+k}|y_{1:t}) \pause &= \int p(y_{t+k},\theta_{t+k},\theta_{t+(k-1)}|y_{1:t}) d\theta_{t+(k-1)} \\ \pause
&= \int p(y_{t+k},\theta_{t+k}|\theta_{t+(k-1)})p(\theta_{t+(k-1)}|y_{1:t}) d\theta_{t+(k-1)}
\end{array} \] \pause
To get $p(y_{t+k},\theta_{t+k}|\theta_t)$ just use the Kalman filter with missing data from $y_{t+1}$ up to $y_{t+(k-1)}$.
\vspace{0.2in} \pause

With \alert{unknown fixed parameters}: \pause
\[ \begin{array}{rl}
\multicolumn{2}{l}{p(y_{t+k},\theta_{t+k}|y_{1:t}) \pause =} \\
 &=\int p(y_{t+k},\theta_{t+k}|\theta_{t+(k-1)},\psi) p(\theta_{t+(k-1)},\psi|y_{1:t}) d\theta_{t+(k-1)} d\psi.
 \end{array} \] \pause
Now we can't just use the Kalman filter due to the unknown fixed parameters. Instead, we need to integrate over their posteriors.
}

\subsection{Integrating over Kalman filters}
\frame{\frametitle{MCMC Forecasting}
After completing the MCMC, follow this procedure
\begin{itemize}[<+->]
\item For each iteration $j=1,2,\ldots,J$ in the MCMC chain post burn-in:
 \begin{itemize}
 \item Run a Kalman filter (\texttt{dlmFilter}) on your data using $\psi^{(j)}$ to obtain $p(\theta_t|y_{1:t},\psi^{(j)})=N(m_t^{(j)}, C_t^{(j)})$.
 \item Forecast ahead (\texttt{dlmForecast}) to obtain $p(y_{t+k}|y_{1:t},\psi^{(j)}) = N(f_t(k)^{(j)}, Q_t(k)^{(j)})$ (see section 2.8 in Petris)
 \item Calculate mean and 95\% intervals for $p(y_{t+k}|y_{1:t},\psi^{(j)})$, i.e. $f_t(k)^{(j)}\, (f_t(k)^{(j)}-1.96\sqrt{Q_t(k)^{(j)}}, f_t(k)^{(j)}+1.96\sqrt{Q_t(k)^{(j)})}$ if $Q$ is scalar, otherwise do this component-wise.
 \end{itemize}
\end{itemize}

\vspace{0.2in} \pause

This provides a set of means and 95\% intervals, one for each MCMC iteration $j$.
}

\frame{\frametitle{MCMC Forecasting}
To find the marginal mean and 95\% interval, average these means and 95\% intervals for all $j$, i.e.
\begin{eqnarray*}
E[y_{t+k}|y_{1:t}] &\approx& \frac{1}{J} \sum_{j=1}^J f_t(k)^{(j)} \\
Q_{2.5\%}[y_{t+k}|y_{1:t}] &\approx& \frac{1}{J} \sum_{j=1}^J f_t(k)^{(j)}-1.96\sqrt{Q_t(k)^{(j)}} \\
Q_{97.5\%}[y_{t+k}|y_{1:t}] &\approx& \frac{1}{J} \sum_{j=1}^J f_t(k)^{(j)}+1.96\sqrt{Q_t(k)^{(j)}} \\
\end{eqnarray*}

\vspace{0.2in}

If you have many MCMC iterations, you can use fewer iterations for this forecast by thinning the chain.
}




\end{document}

