\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Bayesian nonparametrics}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE,
               cache=TRUE)
options(width=100)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("plyr")
library("dplyr")
# library("reshape2")
library("ggplot2")
library("grid")
library("gridExtra")

library("edgeR")
library("rjags")
library("DPpackage")
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\begin{document}

\frame{\maketitle}


\section{Bayesian nonparametrics}

\begin{frame}
\frametitle{Bayesian nonparametrics}

There are two main approaches to Bayesian nonparametrics for density estimation
\begin{itemize}[<+->]
\item Dirichlet process and
\item Polya trees
\end{itemize}

\vspace{0.2in} \pause

See Muller and Mitra (2013) for a general overview of all Bayesian nonparametric 
problems, e.g. density estimation, clustering, regression, 
random effects distributions, etc.

\end{frame}


\begin{frame}
\frametitle{Motivation}
<<rnaseq, message=FALSE>>=
library(edgeR)
d = read.table("rna_seq.txt", header=TRUE)

# Keep only total columns
d = d[,c(1, grep("total", names(d)))]


#regmatches(names(d), regexpr('_[B,M]{1,2}\\.', names(d)))


# Rename columns
names(d)[-1] = paste(c("B73","Mo17","B73xMo17","Mo17xB73"), 
                     rep(1:4, each=4), 
                     sep="_")

# Order columns
B_cols = c(2,6,10,14)
d = d[,c(1, B_cols, B_cols+1, B_cols+2, B_cols+3)]

variety = factor(gsub("_[0-9]{1,2}", "", names(d)[-1]), 
                 levels = c("B73","Mo17","B73xMo17","Mo17xB73"))

# phi, alpha, delta parameterization
design = cbind(1,
               ifelse(variety=='Mo17', -1, 1),
               ifelse(variety=='B73',  -1, 1),
               ifelse(variety=='B73xMo17', 1, ifelse(variety=='Mo17xB73',-1,0)))

# GLM fit using edgeR
fit = d[,-1] %>% 
  DGEList() %>%
  calcNormFactors %>%
  estimateCommonDisp %>%
  estimateGLMTagwiseDisp(design) %>%
  glmFit(design)

# Calculate gene-specific estimates for phi, alpha, delta, and psi
hat = data.frame(gene = 1:length(fit$dispersion),
                 phi   = fit$coefficients[,1] + mean(fit$offset[1,]),
                 alpha = fit$coefficients[,2],
                 delta = fit$coefficients[,3],
                 gamma = fit$coefficients[,4],
                 psi   = log(fit$dispersion))

hat$gene = d$GeneID

ggplot(hat, aes(phi)) + 
  geom_histogram(aes(y=..density..)) +
  theme_bw()
@
\end{frame}


\begin{frame}
\frametitle{Goal}
Let $Y_i$ come from an unknown probability measure $\mathcal{G}$, 
i.e. $Y_i\sim \mathcal{G}$. 
\pause 
As a Bayesian, the natural approach is to put a prior on $\mathcal{G}$. 
\pause 
That is, we want to make statements like 
\[ 
P(Y_i \in A) = \mathcal{G}(A)
\]
for any set $A$. 
\end{frame}



\begin{frame}
\frametitle{Dirichlet process}

One approach is to use a Dirichlet process (Ferguson 1973). \pause We write 
\[ 
\mathcal{G} \sim DP(aG_0)
\]
where 
\begin{itemize}[<+->]
\item $a>0$ is concentration (or total mass) parameter and
\item $G_0$ is the base measure, 
i.e. a probability distribution defined on the support of $\mathcal{G}$. 
\end{itemize}

\vspace{0.2in} \pause

For any partition $A_1,\ldots,A_K$ of the sample space $S$, 
\pause 
the probability vector $[\mathcal{G}(A_1),\ldots,\mathcal{G}(A_K)]$ 
\pause 
follows a Dirichlet distribution, i.e.
\[ 
[\mathcal{G}(A_1),\ldots,\mathcal{G}(A_K)] \sim Dir([aG_0(A_1),\ldots,aG_0(a_K)]).
\]
Thus 
\begin{itemize}[<+->]
\item $E[\mathcal{G}(A_1)] = G_0(A_1)$ and 
\item $Var[\mathcal{G}(A_1)] = \frac{G_0(A_1)[1-G_0(A_1)]}{1+a}$.
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Conjugacy of the Dirichlet process}

Assume
\[ 
Y_i \ind \mathcal{G} \qquad \mbox{and} \qquad \mathcal{G}\sim DP(aG_0)
\]
\pause
then for any partition $\{A_1,\ldots,A_K\}$, we have 
\[ \begin{array}{ll}
[\mathcal{G}(A_1),\ldots,\mathcal{G}(A_K)]|y \sim \\
\quad Dir\left(\left[aG_0(A_1) + \sum_{i=1}^n \I(y_i\in A_1),\ldots,aG_0(A_K) + \sum_{i=1}^n \I(y_i\in A_K)\right]\right)
\end{array} \]
\pause
and thus 
\[
\mathcal{G}|y \sim DP\left(aG_0 + \sum_{i=1}^n \delta_{y_i}\right)
\]
which has 
\[ 
E[\mathcal{G}(A)|y] = \left( \frac{a}{a+n} \right)G_0(A) + \left( \frac{n}{a+n} \right)\sum_{i=1}^n \frac{1}{n} \I(y_i\in A)
\]

\end{frame}



\begin{frame}
\frametitle{Stick-breaking representation}

A constructive representation of the Dirichlet process is the stick-breaking 
representation. 
\pause 
Assume $\mathcal{G}\sim DP(aG_0)$, 
then 
\[ 
\mathcal{G}(\cdot) = \sum_{h=1}^\infty \pi_h \delta_{\theta_h}(\cdot)
\]
where $\pi \sim \mbox{stick}(a)$ 
\pause 
and $\theta_h\ind G_0$.
\pause
The stick distribution is the following:
\begin{itemize}
\item $\pi_h = \nu_h\prod_{\ell<h} (1-\nu_\ell)$ and 
\item $\nu_h \ind Be(1,a)$.
\end{itemize}

\pause

<<stick_breaking, fig.height=2>>=
# Stick-breaking realizations
calc_pi = function(v) {
    n = length(v)
    pi = numeric(n)
    cumv = cumprod(1 - v)
    pi[1] = v[1]
    for (i in 2:n) pi[i] = v[i] * cumv[i - 1]
    pi
}
par(mar = rep(0, 4))
plot(0, 0, type = "n", xlim = c(0, 1), ylim = c(-0.31, 0.31), axes = F, xlab = "", 
    ylab = "")
segments(0, 0, 1, 0)
wd = 0.2
segments(c(0, 1), -wd, c(0, 1), wd)
wd = wd/1.1

set.seed(9)
pi = calc_pi(rbeta(5, 1, 10))
cpi = cumsum(pi)
segments(cpi, -wd, cpi, wd)
text(c(0, 1), -0.3, c(0, 1))

midpoint = function(x) {
    n = length(x)
    mp = numeric(n - 1)
    for (i in 2:n) mp[i - 1] = mean(x[c(i - 1, i)])
    mp
}
mp = midpoint(c(0, cpi, 1))
text(mp, -0.3, expression(pi[1], pi[2], pi[3], pi[4], pi[5], ...))
@

\end{frame}



\begin{frame}
\frametitle{Realizations from a DP}
Base measure is a standard normal. Realizations are across the columns and
values for a are down the rows.
<<stick_breaking_realizations, dependson='stick_breaking'>>=
rdp = function(alpha, rP0, H = 10000) {
    theta = rP0(H)
    v = rbeta(H, 1, alpha)
    pi = calc_pi(v)
    return(data.frame(theta = theta, v = v, pi = pi))
}

plot_rdp = function(rdp, ...) {
    plot(rdp$theta, rdp$pi, type = "h", ...)
}

d = ddply(expand.grid(alpha=10^seq(-2,2)), .(alpha), function(x) {
  rdply(4, function() {
    rdp(x$alpha, rnorm)
  })
})

ggplot(d, aes(x=theta, weight=pi)) +
  geom_histogram(aes(y=..density..), binwidth=0.1) + 
  facet_grid(alpha~.n, scales='free_y') + 
  stat_function(fun=dnorm, color='red') +
  theme_bw()
@
\end{frame}



\begin{frame}
\frametitle{DP mixture}

If we have an absolutely continuous distribution we are trying to approximate, 
then a DP is not reasonable. 
\pause 
Thus, we may want to use a \alert{DP mixture}, i.e. 
\[ 
Y_i \ind p(\cdot|\theta_i), 
\quad 
\theta_i \ind \mathcal{G}, 
\quad 
\mathcal{G}\sim DP(aG_0)
\]
for some parametric model $p(\cdot|\theta)$. 

Alternatively, if we use the stick-breaking construction, we have 
\[ 
Y_i \ind p(\cdot|\theta_i), 
\qquad 
\theta_i \ind \sum_{h=1}^\infty \pi_h \delta_{\theta_h^*}
\]
where $\theta_h^* \ind G_0$ and $\pi \sim \mbox{stick}(a)$. 

\end{frame}



\begin{frame}
\frametitle{Finite approximation to the stick-breaking representation}

For some $\epsilon>0$, 
there exists an $H$ such that $\sum_{h=H}^\infty \pi_h < \epsilon$ and 
components $H$ and beyond can reasonably be ignored. 
\pause 
The resulting model is 
\[ 
Y_i \ind p(\cdot|\theta_i), \quad \theta_i \ind \sum_{h=1}^H \pi_h \delta_{\theta_h^*}
\]
where 
\begin{itemize}
\item $\pi_h = V_h\prod_{\ell<h} (1-V_\ell)$ 
\item $V_h \ind Be(1,a)$ for $h<H$, and
\item $V_H=1$. 
\end{itemize}
\pause
\end{frame}



\subsection{Normal example}
\begin{frame}
\frametitle{Normal example}

A DP mixture model for the marginal distribution for $Y_i=\phi_i$ is

\[ 
Y_i \ind N(\mu_i, \sigma_i^2) \quad 
\left(\begin{array}{cc} \mu_i \\ \sigma_i^2 \end{array} \right) \sim \sum_{h=1}^H \pi_h \delta_{(\mu_h,\sigma_h^{2*})}
\]
where $\sum_{h=1}^H \pi_h = 1$. 

\vspace{0.2in} \pause

Alternatively, we can introduce a latent variable $\zeta_i=h$ if observation $i$ came from group $h$. \pause
Then 
\[ \begin{array}{rl}
Y_i|\zeta_i=h &\ind N(\mu_{h},\sigma_{h}^2) \\ 
\zeta_i &\ind Cat(H,\pi)
\end{array} \]
where $\zeta\sim Cat(H,\pi)$ is a categorical random variable with $P(\zeta=h) = \pi_h$ for $h=1,\ldots,H$ and $\pi=(\pi_1,\ldots,\pi_H)$. 

\end{frame}



\subsection{Normal example}
\begin{frame}
\frametitle{Normal example}
Let
\[ 
Y_i \ind N(\mu_i,\sigma_i^2), \qquad (\mu_i,\sigma_i^2) \ind \sum_{h=1}^H \pi_h \delta_{(\mu_h^*,\sigma_h^{2*})}
\]
where the base measure $G_0$ is 
\[
\mu_h^*|\sigma_h^{2*} \ind N(m_h,v_h^2\sigma_h^{2*}) 
\qquad\mbox{and}\qquad 
\sigma_h^{2*} \ind IG(c_h,d_h).
\]
\pause

But since each $(\mu_i,\sigma_i^2)$ must equal $(\mu_h^*,\sigma_h^{2*})$ for some $h$, we can rewrite the model as 
\[ 
Y_i \ind \sum_{h=1}^H \pi_h N(\mu_h^*,\sigma_h^{2*})
\]
with a prior that is equal to the base measure. \pause Thus this model is equivalent to our finite mixture with the exception of the prior for $\pi$. 
\end{frame}



\begin{frame}
\frametitle{MCMC - Blocked Gibbs sampler}
\small

\vspace{-0.05in}

The steps of a Gibbs sampler with stationary distribution 
\[ 
p(\pi,\mu,\sigma^2,\zeta|y) \propto p(y|\zeta,\mu,\sigma^2) p(\zeta|\pi) p(\mu|\sigma^2) p(\sigma^2) p(\pi)
\]
has steps 

\begin{enumerate}
\item For $i=1,\ldots,n$, independently sample $\zeta_i$ from its full conditional
\[ 
P(\zeta_i=h|\ldots) \propto \pi_h N(y_i; \mu_h^*, \sigma_h^{2*}) 
\]
\item Jointly sample $\pi$ and $\mu,\sigma^2$ because they are conditionally
independent.
  \begin{enumerate}
  \item \alert{Sample $V_h \ind Be(1+Z_h,a+Z_h^+)$ for $V=1,\ldots,H-1$ where 
  $Z_h = \sum_{i=1}^n \I(\zeta_i=h)$ and $Z_h^+ = \sum_{h'=h+1}^H Z_{h'}$ and 
  set $V_H=1$. 
  Then calculate $\pi_h = V_h\prod_{\ell<h} (1-V_\ell)$.}
  \item For $h=1,\ldots,H$, sample $\mu_h,\sigma_h^2$ from their full conditional
\[ 
\mu_h^*|\sigma_h^{2*} \stackrel{ind}{\sim} N(m_h', v_h'^2) \quad \sigma_h^{2*} \stackrel{ind}{\sim} IG(c'_h, d'_h)
\]
where $m_h', v_h'^2, c_h'$, and $d_h'$ are exactly the same as in the normal finite mixture MCMC.
\end{enumerate}
\end{enumerate}

\end{frame}



\subsection{JAGS}
\begin{frame}[fragile]
<<dp_normal_blocked_model, echo=TRUE>>=
library("rjags")
dp_normal_blocked = "
model {
  for (i in 1:n) {
    y[i] ~ dnorm(mu[zeta[i]], tau[zeta[i]])
    zeta[i] ~ dcat(pi[])
  }

  for (h in 1:H) {
    mu[h] ~ dnorm(2,1/3)
    tau[h] ~ dgamma(.1,.1)
    sigma[h] <- 1/sqrt(tau[h])
  }

  # Stick breaking
  for (h in 1:(H-1)) { V[h] ~ dbeta(1,a) } 
  V[H] <- 1
  pi[1] <- V[1]
  for (h in 2:H) {
    pi[h] <- V[h] * (1-V[h-1]) * pi[h-1] / V[h-1]
  }
}"
@

<<jags_data, dependson='rnaseq', echo=TRUE>>=
tmp = hat[sample(nrow(hat), 1000),]
dat = list(n=nrow(tmp), H=25, y=tmp$phi, a=1)
@

<<jags_run, dependson=c('dp_normal_blocked_model','jags_data'), echo=TRUE, results='hide'>>=
jm = jags.model(textConnection(dp_normal_blocked), data = dat, n.chains = 3)
r = jags.samples(jm, c('mu','sigma','pi','zeta'), 1e3)
@
\end{frame}



\begin{frame}
\frametitle{Monitor convergence of density}

As previously discussed, the model as constructed as identifiability problems 
among the $\pi_h$, $\mu_h^*$, and $\sigma_h^{2*}$ due to label switching.
\pause
What is identified in the model is the value of the density at any particular value. 

\vspace{0.2in} \pause

So rather than directly monitoring the parameters, we will monitor the estimated density, \pause i.e. at iteration $m$ of the MCMC, the estimated density at location $x$ is 
\[ 
\sum_{h=1}^H \pi_h^{(m)} N(x; \mu_h^{*(m)}, \sigma_h^{2*(m)}).
\]
\pause
Monitoring this quantity at a variety of locations $x$ will provide appropriate convergence assessment. 
\end{frame}




\begin{frame}
\frametitle{Monitor convergence of density}
<<jags_conversion,dependson='jags_run'>>=
xx = seq(min(dat$y)-1, max(dat$y)+1, length=101)
grid = expand.grid(iteration=1:dim(r$mu)[2],
                  chain = 1:dim(r$mu)[3],
                  x = xx)
d = 
ddply(grid, .(iteration,chain,x), function(d) {
  data.frame(density = sum(r$pi[,d$iteration, d$chain] * dnorm(d$x, 
                                                               r$mu[,d$iteration,d$chain], 
                                                               r$sigma[,d$iteration,d$chain])))
})
@

<<example_traceplots, dependson='jags_conversion'>>=
d$chain = as.factor(d$chain)
ggplot(subset(d, x == sample(xx, 4)), 
       aes(iteration, density, color=chain, group=chain)) + 
  geom_line() + 
  facet_wrap(~x, scales='free') +
  theme_bw()
@
\end{frame}



\begin{frame}
\frametitle{Monitoring the number of utilized components}

Since we are using a finite approximation to the DP, 
we should monitor the index of the maximum occupied component 
(or the number of occupied clusters).
\pause 
If the finite approximation is reasonable, 
then this number will be smaller than $H$. 
\pause 
If not, then $H$ should be increased. 

\vspace{0.2in} \pause

Specifically, at iteration $m$, we monitor
\[ 
\max \{ \zeta_1^{(m)}, \ldots,\zeta_n^{(m)}, \}
\]
the index of the maximum occupied cluster, \pause
or 
\[
\sum_{h=1}^H \I(Z_h>0),
\]
the number of occupied clusters.
\end{frame}


\begin{frame}
\frametitle{Monitoring the number of utilized components}

<<max_occupied_index, dependson='jags_run'>>=
index = adply(r$zeta, 2:3, function(d) data.frame(max_index = max(d)))
names(index)[1:2] = c('iteration','chain')
index$chain = as.factor(index$chain)
ggplot(index, aes(iteration, max_index, color=chain, group=chain)) + 
  geom_line() +
  theme_bw()
@

\end{frame}




\begin{frame}
\frametitle{Posterior density estimation}

<<posterior_density_estimation, dependson=c('jags_data','jags_conversion')>>=
sm = ddply(d, .(x), summarize,
           lb = quantile(density, .025),
           ub = quantile(density, .975))
ggplot(sm, aes(x)) + 
  geom_histogram(data=tmp, aes(x=phi, y=..density..), binwidth=0.2, alpha=0.5) + 
  geom_ribbon(aes(ymin=lb, ymax=ub), fill='blue', alpha=0.5) +
  theme_bw()
@


\end{frame}


\begin{frame}
\frametitle{Chinese restaurant process}

\small

Rather than utilizing the finite approximation to the DP, we can use the DP directly, by marginalizing out $\mathcal{G}$. \pause
This results in a prior directly on $\theta_1,\ldots,\theta_n$ via 
\[ 
\theta_i|\theta_1,\ldots,\theta_{i-1} \sim \left( \frac{a}{a+i-1} \right)G_0(\theta_i) + \sum_{j=1}^{i-1} \left(\frac{1}{a+i-1} \right) \delta_{\theta_j}
\]

\pause

The conditional prior for $\theta_i$ is 
\[ 
\theta_i| \theta_{-i} \sim \left( \frac{a}{a+n-1} \right)G_0(\theta_i) + \sum_{j\ne i} \left(\frac{1}{a+n-1} \right) \delta_{\theta_j}
\]
or, equivalently, 
\[ 
\theta_i| \theta_{-i} \sim \left( \frac{a}{a+n-1} \right)G_0(\theta_i) + \sum_{h=1}^{H^{(-i)}} \left(\frac{n_h^{(-i)}}{a+n-1} \right) \delta_{\theta_h^*}
\]
where $H^{(-i)}$ is the number of components without $i$ and $n_h^{(-i)}$ is the number of observations in each component without $i$. 
\end{frame}



\begin{frame}
\frametitle{Marginalized Gibbs sampler}

Using this Chinese restaurant process, we have the following $n+1$-step MCMC

\begin{enumerate}
\item For $i=1,\ldots,n$, sample $\zeta_i$ from its full conditional
\[ 
P(\zeta_i=h|\zeta_{-i},\ldots) \propto \left\{ 
\begin{array}{ll} 
n_h^{(-i)} p(y_i|\theta_h^*) & h=1,\ldots,H^{(-i)} \\
a \int p(y_i;\theta) dG_0(\theta) & h=H^{(-i)}+1
\end{array} \right.
\]
\pause
If $\zeta_i = H^{(-i)}+1$, 
then sample $\theta^*_{\zeta_i}$ from its posterior using $y_i$ as the only 
observation.

\pause

\item For $h=1,\ldots,H$, sample $\theta_h^*$ from their full conditional
\[ 
\theta_h^*|\ldots \propto G_0(\theta_h^*)\prod_{i:\zeta_i=h} p(y_i|\theta_h^*)
\]
\pause
i.e. sample the parameters from their posteriors using only the data in that group.
\end{enumerate}

\end{frame}


\begin{frame}
\frametitle{Marginalized Gibbs sampler - Normal example}

For the normal example, we have this $n+1$-step sampler

\begin{enumerate}
\item For $i=1,\ldots,n$, sample $\zeta_i$ from its full conditional
\[ 
P(\zeta_i=h|\zeta_{-i},\ldots) \propto \left\{ 
\begin{array}{ll} 
n_h^{(-i)} N(y_i;\mu_h^*,\sigma_h^{2*}) & h=1,\ldots,H^{(-i)} \\
a\, t_{2c}(y_i;m,v^2[d/c]) & h=H^{(-i)}+1
\end{array} \right.
\]
\pause
If $\zeta_i = H^{(-i)}+1$, then sample $\mu^*_{\zeta_i},\sigma_{\zeta_i}^{2*}$ from its normal-inverse-gamma posterior using $y_i$ as a the only observation. \pause 

\item For $h=1,\ldots,H$, sample $\mu_h,\sigma_h^2$ from their full conditional
\[ 
\mu_h^*|\sigma_h^{2*} \stackrel{ind}{\sim} N(m_h', v_h'^2) \quad \sigma_h^{2*} \stackrel{ind}{\sim} IG(c'_h, d'_h)
\]
where $m_h', v_h'^2, c_h'$, and $d_h'$ are exactly the same as in the normal finite mixture MCMC.
\end{enumerate}

\end{frame}





\begin{frame}
\frametitle{Putting a prior on the concentration parameter}

If $\mathcal{G}\sim DP(a G_0)$, 
then the concentration parameter ($a$) controls the prior on the number of 
clusters. 
\pause 
For example, 
if $a=1$, 
then in the prior two randomly selected observations have a 0.5 probability of 
belonging to the same cluster. 
\pause 
As $a$ increases, 
then you have more clusters and more concentration around $G_0$. 
\pause
As $a$ decreases, 
then you have fewer clusters and the data are more informative. 

\vspace{0.2in} \pause

Rather than setting the concentration parameter, we can learn it. \pause
Let $\mathcal{G}\sim DP(\alpha G_0)$ and 
\[ 
\alpha \sim Ga(a,b)
\]
\pause
then the full conditional for $\alpha$ is 
\[ 
\alpha|\ldots \sim Ga\left(a+H-1, b - \sum_{h=1}^{H-1} \log(1-V_h)\right).
\]
\end{frame}


\subsection{Hierarchical dependence}
%\subsection{Dependent Dirichlet process}
\begin{frame}
\frametitle{Multiple groups}

Suppose we have $Y_{ij}$ for $i=1,\ldots,n_j$ and $j=1,\ldots,J$, \pause i.e. we have $J$ groups with $n_j$ observations per group. \pause
We may consider a DP for each group individually, i.e. 
\[ 
Y_{ij} \ind \mathcal{G}_j, \quad \mathcal{G}_j \ind DP(\alpha_j G_{0j})
\]
where we must now specify $\alpha_j$ and $G_{0j}$ for $j=1,\ldots,J$. \pause
More importantly, this model does not allow us to borrow any information across the groups since the observations across groups given $\alpha_j$ and $G_{0j}$. 

\vspace{0.2in} \pause

Some possible models to allow borrowing of information are the 
\begin{itemize}
\item Dependent Dirichlet process (DDP)
\item Hierarchical Dirichlet process (HDP)
\item Nested Dirichlet process (NDP)
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{Dependent Dirichlet process}

Suppose we are interested in estimating a collection of random probability 
measures $\mathcal{G}_1,\ldots,\mathcal{G}_J$. \pause
We would like for the measures to be DPs marginally, i.e. 
\[ 
\mathcal{G}_j \ind DP(\alpha_j G_{0j})
\]
\pause
but we may want to incorporate dependency between the measures and thus borrow information across the measures. \pause
One approach is a ``fixed-$\pi$ DDP'' which is defined via the stick-breaking process such that each measure has the same weights $\pi$ but the locations vary, \pause i.e.
\[
\mathcal{G}_j \stackrel{d}{=} \sum_{h=1}^\infty \pi_h \delta_{\theta_{jh}^*}, \quad 
\pi \sim \mbox{stick}(\alpha), \quad 
\theta_{jh}^* \sim G_0
\]

\end{frame}


%\subsection{Hierarchical Dirichlet process}
\begin{frame}
\frametitle{Hierarchical Dirichlet process}

An alternative is to build a hierarchical model, i.e. 
\[
\mathcal{G}_j \ind DP(\alpha \mathcal{G}_0) 
\qquad 
\mathcal{G}_0 \sim DP(\beta G_{00})
\]
\pause
The stick-breaking process related to this model is 
\[ 
\mathcal{G}_j \stackrel{d}{=} \sum_{h=1}^\infty \pi_{jh}\delta_{\theta_h^*}, \quad 
\mathcal{G}_0 \stackrel{d}{=} \sum_{h=1}^\infty \lambda_h \delta_{\theta_h^*}, \quad 
\theta_h^* \sim G_{00}
\]
\pause
where 
\[
\pi_j = (\pi_{j1},\pi_{j2},\ldots) \sim \mbox{stick}(\alpha) 
\quad \mbox{and} \quad 
\lambda = (\lambda_1,\lambda_2,\ldots) \sim \mbox{stick}(\beta).
\]
\pause
Like the DDP, the HDP allows individuals in different groups to be clustered together, i.e. have the same $\theta_h^*$.

\end{frame}

\begin{frame}
\frametitle{Nested Dirichlet process}

Rather than clusting individuals across groups, we may be interested in clustering groups themselves\pause, i.e. groups that have the same distribution should be treated as the same group. \pause
Here we can use the nested Dirichlet process: \pause
\[ 
\mathcal{G}_j \ind \mathcal{G}, 
\quad 
\mathcal{G}\sim DP(\alpha \mathcal{G}_0),
\quad 
\mathcal{G}_0\equiv DP(\beta G_{00}).
\]
\pause
The stick-breaking process related to this model is 
\[
\mathcal{G}_j \ind \mathcal{G} 
\stackrel{d}{=} \sum_{h=1}^\infty \pi_h \delta_{\mathcal{G}_h^*}, \quad 
\pi \sim \mbox{stick}(\alpha), \quad 
\mathcal{G}_h^* \ind DP(\beta G_{00}).
\]
\pause
A natural combination of the HDP and NDP is to place a DP on $G_{00}$ which results in common set of global atoms, i.e. $\theta_h^*$, but with varying weights for each cluster.
\end{frame}



\subsection{Applications}
\begin{frame}
\frametitle{Applications of DP}

Primarily we have been discussing the use of the DP prior as a tool for Bayesian nonparametric density estimation. \pause
Here we discuss additional uses in the context of 
\begin{itemize}
\item Random effects 
\item Error distributions
\item Functional data analysis
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Random effects model}
\small

Let $y_{ij}$ be the observation for individual $i$ in group $j$ \pause and assume 
\[
y_{ij} = \mu_j + \epsilon_{ij}, \quad
\mu_j \ind F, \quad
\epsilon_{ij} \ind G
\]
\pause
A typical parametric model would assume $F\stackrel{d}{=} N(\eta,\tau^2)$ and $G \stackrel{d}{=} N(0,\sigma^2)$. 
\pause
Suppose we would like to be less informative about these distributional assumptions. \pause
One possibility is to assume 
\[ 
F\ind DP(\alpha F_0).
\]
\pause
Now we will estimate the density for the random effects $\mu_j$. \pause
To estimate $F$, we will need many groups, i.e. $J$ should be large. \pause
Alternatively (or additionally), we could assume 
\[
\epsilon_{ij} \ind N(0,\sigma_i^2), \quad
\sigma_i^2 \ind G, \quad
G \sim DP(\beta G_0).
\]
\pause
Here we use the Dirichlet Process mixture to assure that the distribution for the observations are continuous. \pause
To estimate $G$, we need many observations per group. \pause
\end{frame}



\begin{frame}
\frametitle{Functional data analysis}

\small

Let $y_{ij}$ be the observation and $x_{ij}$ be an explanatory variable for individual $i$ in group $j$ \pause and assume 
\[
y_{ij} = f_j(x_{ij}) + \epsilon_{ij}, \quad
f_j(x) = \sum_{h=1}^H \theta_{jh} b_h(x)
\]
where $b_h(x)$ for $h=1,\ldots,H$ be a collection of basis functions. \pause
Now assume 
\[
\theta_{j} = (\theta_{j1},\ldots,\theta_{jH}) \ind G,\quad
G \sim DP(\alpha G_0)
\]
To provide parismony, i.e. dropping basis functions, we can utilize a base measure that a point-mass mixtures, i.e. 
\[
G_{0h} \stackrel{d}{=} \pi_{0h}\delta_0 + (1-\pi_{0h}) N(0,\tau_h^2)
\]
\pause
If we want $t$ alternatives, let $\tau_h^* \sim IG(\cdot,\cdot)$. \pause
A conditionally conjugate prior on the $\pi$ is \pause $\pi_{0h}\ind Be(a,b)$. \pause
If exact zeros are not necessary, then let 
\[ 
\theta_{ch}^* \ind N(0,\tau_{ch}^2),\quad
\tau_{ch}^2 \ind IG(\cdot, \cdot)
\]
and thus have $t$ distribution for the $\theta_{ch}^*$, but now the MCMC is more efficient.

\end{frame}



\subsection{Bayesian nonparametrics in R}
\begin{frame}
\frametitle{Bayesian nonparametrics in R}

From \href{https://cran.r-project.org/web/views/Bayesian.html}{CRAN Task View: Bayesian Inference}, the packages that contain Dirichlet process related Bayesian nonparametrics are 
\begin{itemize}
\item bayesm
\item DPpackage
\item growcurves
\item PReMiuM
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Density estimation in the DPpackage}
<<DPpackage_density_estimation, dependson='rnaseq', echo=TRUE, eval=FALSE>>=
library("DPpackage")
prior = list(alpha=1,
             m1 = 2,
             k0 = 1/3,
             nu1 = 0.2,
             psiinv1=diag(0.2,1))

mcmc = list(nburn=1000, nsave=10000, nskip=10, ndisplay=100)

state = NULL # initial state

dp = DPdensity(y = hat$phi,
               prior = prior,
               mcmc = mcmc,
               state=state, 
               status=TRUE)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Density estimation in the DPpackage}
<<DPpackage_other_functions, echo=TRUE, eval=FALSE>>=
?DPlmm
?DPglmm
?DPMlmm
?PMglmm
?DPolmm
?HDPMdensity

?PTdensity
@
\end{frame}



\begin{frame}
\frametitle{References}

\begin{itemize}
\item Neal, R. M. (2000). Markov Chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9: 249-265. 
\end{itemize}
\end{frame}


\end{document}
