\documentclass{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Shrinkage priors}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}
\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results='hide', echo=FALSE>>=
# These are only needed for the slides
# No need to run if you are just running the R code
opts_chunk$set(fig.width=7, 
               fig.height=5, 
               out.width='.8\\linewidth', 
               fig.align='center', 
               size='tiny',
               echo=FALSE)
options(width=100)
@

<<libraries, echo=FALSE, message=FALSE, warning=FALSE>>=
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\frame{\maketitle}

\section{Normal data model}
\subsection{Normal prior}
\begin{frame}
\frametitle{Normal model with normal prior}

Consider the model 
\[ Y \ind N(\theta,V) \]
with prior 
\[ \theta \sim N(m,C) \]

\vspace{0.2in} \pause

Then the posterior is 
\[ \theta|y \sim N(m',C') \]
\pause where 
\[ \begin{array}{rl}
C' &\pause = 1/(1/C+1/V) \\
m' &\pause = C[m/C + y/V]
\end{array} \]

\end{frame}



\begin{frame}
\frametitle{Normal model with normal prior (cont.)}
For simplicity, let $V=C=1$ and $m=0$, then $\theta|y \sim N(y/2, 1/2)$. \pause 
Suppose $y=1$, then we have

<<normal_model_normal_prior, fig.width=7>>=
y = 1
d = data.frame(x=seq(-2,3,by=.01)) %>%
  mutate(prior = dnorm(x),
         likelihood = dnorm(x,y),
         posterior = dnorm(x,y/2,sqrt(1/2))) %>%
  melt(id.var='x', variable.name='distribution', value.name='density') 

ggplot(d, aes(x,density, color=distribution, linetype=distribution)) + 
  geom_line() +
  theme_bw()
@
\end{frame}



\begin{frame}
\frametitle{Normal model with normal prior (cont.)}
Now suppose $y=10$, then we have

<<normal_model_normal_prior2, fig.width=7>>=
y = 10
d = data.frame(theta=seq(-2,12,by=.01)) %>%
  mutate(prior = dnorm(theta),
         likelihood = dnorm(theta,y),
         posterior = dnorm(theta,y/2,sqrt(1/2))) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

ggplot(d, aes(theta,density, color=distribution, linetype=distribution)) + 
  geom_line() +
  theme_bw()
@
\end{frame}


\begin{frame}
\frametitle{Summary - normal model with normal prior}

\begin{itemize}
\item If the prior and the likelihood agree, then posterior seems reasonable.
\item If the prior and the likelihood disagree, then the posterior is ridiculous.
\item The posterior precision is always the sum of the prior and data precisions and therefore the posterior variance always decreases relative to the prior.
\end{itemize}

\vspace{0.2in} \pause

\alert{Can we construct a prior that allows the posterior to be reasonable always?}

\end{frame}


\subsection{$t$ prior}
\begin{frame}
\frametitle{Normal model with $t$ prior}

Now suppose 
\[ Y \sim N(\theta,V) \]
with 
\[ \theta \sim t_v(m,C), \]
where $E[\theta] = m$ for $v>1$ and $V[\theta] = C \frac{v}{v-2}$ for $v>2$.

\vspace{0.2in} \pause

Now the posterior is 
\[ p(\theta|y) \propto e^{-(y-\theta)^2/2V} \left( 1+\frac{1}{v}\frac{(\theta-m)^2}{C}\right)^{-(v+1)/2}   \]
which is not a known distribution, \pause but we can normalize via 
\[ p(\theta|y) = \frac{e^{-(y-\theta)^2/2V} \left( 1+\frac{1}{v}\frac{(\theta-m)^2}{C}\right)^{-(v+1)/2} }{\int e^{-(y-\theta)^2/2V} \left( 1+\frac{1}{v}\frac{(\theta-m)^2}{C}\right)^{-(v+1)/2} d\theta} \]
\end{frame}



\begin{frame}
\frametitle{Normal model with $t$ prior (cont.)}
Alternatively, we can calculate the \alert{marginal likelihood}
\[ \begin{array}{rl}
p(y) 
&= \int p(y|\theta)p(\theta) d\theta \\
&= \int N(y;\theta,V) t_v(\theta;m,C) d\theta
\end{array} \]
\pause
where 
\begin{itemize}
\item $N(y;\theta,V)$ is the normal density with mean $\theta$ and variance $V$ evaluated at $y$ \pause and
\item $t_v(\theta;m,C)$ is the $t$ distribution with degrees of freedom $v$, location $m$, and scale $C$ evaluated at $\theta$.
\end{itemize}

\vspace{0.2in} \pause
and then find the posterior 
\[ p(\theta|y) = N(y;\theta, V) t_v(\theta;m,C) / p(y). \]
\end{frame}


\begin{frame}[fragile]
\frametitle{Normal model with $t$ prior (cont.)}

Since this is a one dimensional integration, we can easily handle it via the {\tt integrate} function in R: \pause

<<normal_model_t_prior, echo=TRUE>>=
# A non-standard t distribution
my_dt = Vectorize(function(x, v=1, m=0, C=1, log=FALSE) {
  logf = dt((x-m)/sqrt(C), v, log=TRUE) - log(sqrt(C))
  if (log) return(logf)
  return(exp(logf))
})

# This is a function to calculate p(y|\theta)p(\theta).
f = Vectorize(function(theta, y=1, V=1, v=1, m=0, C=1, log=FALSE) {
  logf = dnorm(y, theta, sqrt(V), log=TRUE) + my_dt(theta, v, m, C, log=TRUE)
  if (log) return(logf)
  return(exp(logf))
})

# Now we can integrate it
(py = integrate(f, -Inf, Inf))
@
\end{frame}


\begin{frame}
\frametitle{Normal model with $t$ prior (cont.)}
Let $v=1$, $m=0$, $V=C=1$ and $y=1$. \pause then 

<<normal_model_t_prior2>>=
y = 1
d = data.frame(theta=seq(-2,3,by=.01)) %>%
  mutate(prior = my_dt(theta, 1),
         likelihood = dnorm(y,theta),
         posterior = f(theta,y=y)/py$value) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

ggplot(d, aes(theta,density, color=distribution, linetype=distribution)) + 
  geom_line() +
  theme_bw()
@

\end{frame}



\begin{frame}
\frametitle{Normal model with $t$ prior (cont.)}
Let $v=1$, $m=0$, $V=C=1$, and $y=10$. \pause then 

<<normal_model_t_prior3>>=
y = 10
py = integrate(function(x) f(x,y), -Inf, Inf)
d = data.frame(theta=seq(-2,12,by=.1)) %>%
  mutate(prior = my_dt(theta, 1),
         likelihood = dnorm(y,theta),
         posterior = f(theta,y=y)/py$value) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

ggplot(d, aes(theta,density, color=distribution, linetype=distribution)) + 
  geom_line() +
  theme_bw()
@
\end{frame}



\begin{frame}
\frametitle{Shrinkage of MAP as a function of signal}

Let's take a look at the \emph{maximum a posteriori} (MAP) estimates as a function of the signal ($y$) for the normal and $t$ priors. 

\pause

<<normal_model_map>>=
mx = 5
map = adply(seq(-mx,mx,length=101), 1, function(d){
  o = optimize(function(x) f(x,d), c(-mx,mx), maximum=TRUE)
  data.frame(y = d, map_t = o$maximum, mle=d, map_normal=d/2)
})[,-1] %>%
  melt(id.vars='y', variable.name='model', value.name='theta')
ggplot(map, aes(y, theta, color=model)) + 
  geom_line() + 
  theme_bw()
@

\end{frame}



\begin{frame}
\frametitle{Summary - normal model with $t$ prior}

\begin{itemize}[<+->]
\item A $t$ prior for a normal mean provides a reasonable posterior even if the data and prior disagree. 
\item A $t$ prior provides similar shrinkage to a normal prior when the data and prior agree, but provides little shrinkage when the data and prior disagree.
\item The posterior variance decreases the most when the data and prior agree and decreases less as the data and prior disagree.
\end{itemize}

\vspace{0.2in} \pause

There are many times that we might believe the possibility of $\theta=0$ or, at least, $\theta\approx 0$. \pause In these scenarios, we would like our prior to be able to tell us this. 

\vspace{0.2in} \pause

\alert{Can we construct a prior that allows us to learn about null effects?}

\end{frame}


\subsection{Laplace prior}
\begin{frame}
\frametitle{Laplace distribution}

Let $La(m,b)$ denote a Laplace (or double exponential) distribution with mean $m$, variance $2b^2$, and probability density function
\[ La(x;m,b) = \frac{1}{2b} \exp\left(-\frac{|x-m|}{b}\right). \]

\pause

<<laplace, fig.width=10>>=
dlaplace = function(x,m,b) exp(-abs(x-m)/b)/(2*b)
curve(dlaplace(x, 0, 1), -3, 3, lwd=2, xlab='x', ylab='density')
@

\end{frame}



\begin{frame}
\frametitle{Laplace prior}

Let 
\[ Y \sim N(\theta,V) \]
and 
\[ \theta \sim La(m,b) \]

\vspace{0.2in} \pause

Now the posterior is 
\[ p(\theta|y) = \frac{N(y;\theta,V) La(\theta;m,b)}{p(y)} \propto e^{-(y-\theta)^2/2V} e^{-|\theta-m|/b} \]
where
\[ p(y) = \int N(y;\theta,V) La(\theta;m,b) d\theta. \]


\end{frame}



\begin{frame}
\frametitle{Laplace prior (cont.)}

For simplicity, let $b=V=1$, $m=0$ and suppose we observe $y=1$.

\vspace{0.2in} \pause

<<normal_model_laplace_prior>>=
f = Vectorize(function(theta,y,V,m,b,log=FALSE) {
  logf = dnorm(y,theta,sqrt(V), log=TRUE) - abs(theta-m)/b
  if (log) return(logf)
  return(exp(logf))
})

y = b = V = 1; m = 0
py = integrate(function(x) f(x, y, V, m, b), -Inf, Inf)

d = data.frame(theta=seq(-2,3,by=.01)) %>%
  mutate(prior = dlaplace(theta, m, b),
         likelihood = dnorm(y, theta),
         posterior = f(theta, y=y, V, m, b)/py$value) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

ggplot(d, aes(theta,density, color=distribution, linetype=distribution)) + 
  geom_line() +
  theme_bw()
@
\end{frame}



\begin{frame}
\frametitle{Laplace prior (cont.)}

For simplicity, let $b=V=1$, $m=0$ and suppose we observe $y=10$.

\vspace{0.2in} \pause

<<normal_model_laplace_prior2>>=
y = 10
py = integrate(function(x) f(x, y, V, m, b), -Inf, Inf)

d = data.frame(theta=seq(-2,y+2,length=1001)) %>%
  mutate(prior = dlaplace(theta, m, b),
         likelihood = dnorm(y, theta),
         posterior = f(theta, y=y, V, m, b)/py$value) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

ggplot(d, aes(theta,density, color=distribution, linetype=distribution)) + 
  geom_line() +
  theme_bw()
@
\end{frame}




\begin{frame}
\frametitle{Laplace prior - MAP as a function of signal}

<<normal_model_laplace_prior_map>>=
d = adply(unique(map$y), 1, function(d){
  o = optimize(function(x) f(x,d,V,m,b,log=TRUE), c(-mx,mx), maximum=TRUE)
  data.frame(y = d, map_laplace = o$maximum)
})[,-1] %>%
  melt(id.vars='y', variable.name='model', value.name='theta')
ggplot(rbind(map, d), aes(y, theta, color=model)) + 
  geom_line() + 
  theme_bw()
@

\end{frame}



\begin{frame}
\frametitle{Summary - Laplace prior}

\begin{itemize}[<+->]
\item For small signals, the MAP is zero (or $m$).
\item For large signals, there is less shrinkage toward zero (or $m$) but more shrinkage than a $t$ distribution.
\end{itemize}

\vspace{0.2in} \pause

It's fine that the MAP is zero, but since the posterior is continuous, we have $P(\theta=0|y) = 0$.

\vspace{0.2in} \pause

\alert{Can we construct a prior such that the posterior has mass at zero?}


\end{frame}



\subsection{Point-mass prior}
\begin{frame}
\frametitle{Dirac $\delta$ function }

Let $\delta_c(x)$ be the Dirac $\delta$ function, i.e. heuristically

\[ 
\delta_c(x) = \left\{ \begin{array}{ll}
\infty & x=c \\
0 & x\ne c
\end{array} \right.
\]
and 
\[ \int_{-\infty}^\infty \delta_c(x) dx = 1. \]

\vspace{0.2in} \pause

Thus $\theta\sim \delta_c(\theta) \stackrel{d}{=} \delta_c$ indicate that the random variable $\theta$ is a degenerate random variable with $P(\theta=c)=1$. 
\end{frame}



\begin{frame}
\frametitle{Point-mass distribution}

Let 
\[ \theta \sim p\delta_0 + (1-p) N(m,C) \]
be a distribution such that the random variable $\theta$
\begin{itemize}
\item is $0$ with probability $p$ and 
\item a normal random variable with mean $m$ and variance $C$ with probability $(1-p)$. 
\end{itemize}

\pause

If $p=0.5$, $m=0$, and $C=1$, it's cumulative distribution function is 

\vspace{-0.2in} 

<<fig.width=10>>=
cdf = Vectorize(function(x) {
  if (x<0) return(pnorm(x)/2)
  if (x==0) return(.75)
  return(.75+pnorm(x)/2)
})
curve(cdf(x), -2, 2, 1001, xlab='theta', ylab='CDF', lwd=2)
@


\end{frame}



\begin{frame}
\frametitle{Point-mass prior}

Suppose 
\[ Y\sim N(\theta,V) \]
and 
\[ \theta \sim p\delta_0 + (1-p) N(m,C). \]

\vspace{0.2in} \pause

Then 
\[ 
\theta|y \pause \sim p'\delta_0 + (1-p') N(m',C')
\]
where \pause
\[ \begin{array}{rl}
p' &\pause= \frac{p N(y;0,V)}{p N(y;0,V) + (1-p) N(y;m,C+V)} \pause = \left(1 + \frac{(1-p)}{p} \frac{N(y;m,C+V)}{N(y;0,V)} \right)^{-1}\\
C' &\pause= 1/(1/V+1/C) \\
m' &\pause= C'(y/V+m/C)
\end{array} \]

\end{frame}




\begin{frame}
\frametitle{Point-mass prior (cont.)}

For simplicity, let $V=C=1$, $p=0.5$, $m=0$ and $y=1$. \pause Then 

<<normal_model_point_mass_prior>>=
y = 1
p = 0.5
V = C = 1
m = 0

pp = 1/(1+dnorm(y,m,sqrt(C+V))/dnorm(y,0,sqrt(V)))
d = data.frame(theta=seq(-2,3,by=.01)) %>%
  mutate(prior = dnorm(theta)/2,
         likelihood = dnorm(theta,y),
         posterior = dnorm(theta,y/2,sqrt(1/2))*(1-pp)) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

e = .05
d2 = data.frame(distribution = c('prior','posterior'),
                x = c(-e,e),
                xend = c(-e,e),
                y = c(0,0),
                yend = c(p,pp))

ggplot(d, aes(theta, density, color=distribution, linetype=distribution)) + 
  geom_segment(data = d2, aes(x=x,xend=xend,y=y,yend=yend)) +
  geom_line() +
  theme_bw()
@

\end{frame}



\begin{frame}
\frametitle{Point-mass prior (cont.)}

For simplicity, let $V=C=1$, $p=0.5$, and $m=0$. \pause Suppose we observe $y=1$. 

<<normal_model_point_mass_prior2>>=
y = 10
p = 0.5
V = C = 1
m = 0

pp = 1/(1+dnorm(y,m,sqrt(C+V))/dnorm(y,0,sqrt(V)))
d = data.frame(theta=seq(-2,y+2,by=.01)) %>%
  mutate(prior = dnorm(theta)*p,
         likelihood = dnorm(theta,y),
         posterior = dnorm(theta,y/2,sqrt(1/2))*(1-pp)) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

e = .05
d2 = data.frame(distribution = c('prior','posterior'),
                x = c(-e,e),
                xend = c(-e,e),
                y = c(0,0),
                yend = c(p,pp))

ggplot(d, aes(theta,density, color=distribution, linetype=distribution)) + 
  geom_segment(data = d2, aes(x=x,xend=xend,y=y,yend=yend)) +
  geom_line() +
  theme_bw()
@
\end{frame}


\begin{frame}
\frametitle{Summary - point-mass prior}

\begin{itemize}[<+->]
\item For small signals, the posterior puts most of its mass at zero (or $m$). 
\item For large signals, the posterior puts most of its mass away from zero (or $m$) and therefore has the same problems that a normal prior has.
\end{itemize}

\vspace{0.2in} \pause

\alert{Can we create a prior that 1) puts most of the posterior mass at zero for small signals and 2) leaves large signals unshrunk?}

\end{frame}


\begin{frame}
\frametitle{Point-mass prior with $t$ distribution}

Suppose 
\[ Y\sim N(\theta,V) \]
and 
\[ \theta \sim p\delta_0 + (1-p) t_v(m,C). \]

\vspace{0.2in} \pause

Then 
\[ 
\theta|y \pause \sim p'\delta_0 + (1-p') \, ?
\]
where
\[ 
p' \pause = \left( 1 + \frac{(1-p) \int N(y;\theta,V) t_v(\theta;m,C) d\theta}{p N(y;0,V)} \right)^{-1}
\]
\pause
and 
\[ 
? \propto N(y;\theta,V) t_v(\theta;m,C).
\]
\pause
But we already calculated this posterior earlier in the lecture, i.e. normal model with $t$ prior. 

\end{frame}



\begin{frame}[fragile]
\frametitle{Point-mass prior with $t$ distribution (cont.)}

Suppose $v=V=C=1$, $p=0.5$, $m=0$, and $y=1$.

\vspace{0.2in} \pause

Then, we can calculate the following integral (marginal likelihood) numerically

\[ 
\int N(y;\theta,V) t_v(\theta;m,C) d\theta
\]


<<normal_model_point_mass_t, echo=TRUE>>=
v = C = V = 1; p = 0.5; m = 0; y=1
(int = integrate(function(x) dnorm(y,x,sqrt(V))*my_dt(x), -Inf, Inf))
(int0 = dnorm(y,0,sqrt(V)))
(pp = 1/(1+(1-p)*int$value/(p*int0)))
@

\end{frame}






\begin{frame}
\frametitle{Point-mass prior with $t$ distribution (cont.)}

Suppose $v=V=C=1$, $p=0.5$, and $m=0$. \pause And we observe $y=1$. 

<<normal_model_point_mass_t2>>=
y = 1

# This is a function to calculate p(y|\theta)p(\theta).
f = Vectorize(function(theta, y=1, V=1, v=1, m=0, C=1, log=FALSE) {
  logf = dnorm(y, theta, sqrt(V), log=TRUE) + my_dt(theta, v, m, C, log=TRUE)
  return(ifelse(log, logf, exp(logf)))
})

# Now we can integrate it
py = integrate(f, -Inf, Inf)

# Find posterior
d = data.frame(theta=seq(-2,y+2,by=.01)) %>%
  mutate(prior = (1-p)*my_dt(theta, 1),
         likelihood = dnorm(y,theta),
         posterior = (1-pp)*f(theta,y=y)/py$value) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

e = .05
d2 = data.frame(distribution = c('prior','posterior'),
                x = c(-e,e),
                xend = c(-e,e),
                y = c(0,0),
                yend = c(p,pp))

ggplot(d, aes(theta,density, color=distribution, linetype=distribution)) + 
  geom_segment(data = d2, aes(x=x,xend=xend,y=y,yend=yend)) +
  geom_line() +
  theme_bw() 
@

\end{frame}




\begin{frame}
\frametitle{Point-mass prior with $t$ distribution (cont.)}

Suppose $v=V=C=1$, $p=0.5$, and $m=0$. \pause And we observe $y=10$. 

<<normal_model_point_mass_t3>>=
y = 10
int = integrate(function(x) dnorm(y,x,sqrt(V))*my_dt(x), -Inf, Inf)
int0 = dnorm(y,0,sqrt(V))
pp = 1/(1+(1-p)*int$value/(p*int0))

# Now we can integrate it
py = integrate(function(x) f(x,10), -Inf, Inf)

# Find posterior
d = data.frame(theta=seq(-2,y+2,by=.01)) %>%
  mutate(prior = (1-p)*my_dt(theta, 1),
         likelihood = dnorm(y,theta),
         posterior = (1-pp)*f(theta,y=y)/py$value) %>%
  melt(id.var='theta', variable.name='distribution', value.name='density') 

e = .05
d2 = data.frame(distribution = c('prior','posterior'),
                x = c(-e,e),
                xend = c(-e,e),
                y = c(0,0),
                yend = c(p,pp))

ggplot(d, aes(theta,density, color=distribution, linetype=distribution)) + 
  geom_segment(data = d2, aes(x=x,xend=xend,y=y,yend=yend)) +
  geom_line() +
  theme_bw() 
@

\end{frame}



\subsection{Summary}
\begin{frame}
\frametitle{Summary}

\begin{itemize}[<+->]
\item Heavy tails allow the likelihood to easily overwhelm the prior. 
\item A peak allows ``complete'' shrinkage.
\end{itemize}

\end{frame}



\subsection{Discussion}
\begin{frame}
\frametitle{Discussion questions}

\begin{itemize}
\item Can we take this summary to the logical extreme and use a mixture of a point-mass and an improper uniform prior?
\item Why do we typically talk about mixed effect models with a normal distribution for the random effects?
\end{itemize}

\end{frame}

\end{document}
