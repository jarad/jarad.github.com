\documentclass[aspectratio=169]{beamer}


\input{../../frontmatter}
\input{../../commands}

% \setbeamertemplate{background}
% {\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{video_overlay}}

\title{P1 - Probability}

\begin{document}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=4.4, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='left', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library(dplyr)
library(ggplot2)
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}

\section{Probability}

\subsection{Interpretation}
\begin{frame}
\frametitle{Probability - Interpretation}

What do we mean when we say the word \alert{probability/chance/likelihood/odds}? \pause
For example,
\begin{itemize}
\item The \alert{probability} that Kamala Harris will becomes president in 2025 is 55\%. \pause
\item The \alert{chance} I will win a game of solitaire is 20\%. \pause
\item The \alert{odds} of there being a time capsule behind this wall is 100:1. \pause
\end{itemize}

\vspace{0.1in} 

Interpretations:
\begin{itemize}
\item \alert{Relative frequency}: Probability is the proportion of\\times the 
event occurs as the number of times the\\event is attempted tends to infinity. \pause
\item \alert{Personal belief}: Probability is a statement about\\your personal
belief in the event occuring. 
\end{itemize}
\end{frame}


\subsection{Set operations}
\begin{frame}
\frametitle{Probability - Example}

Let $C$ be a successful connection to the internet from a laptop event. 
\pause
\begin{quote}
From our experience with the wireless network and our internet service provider, 
we believe the probability we successfully connect is 90 \%.
\end{quote}
\pause

We write $P(C) = 0.9$.
\begin{quote}
To be able to work with probabilities, in particular, 
to be able to compute \alert{probabilities of events},
a mathematical foundation is necessary.
\end{quote}

\vspace{2in}

\end{frame}



\begin{frame}
\frametitle{Sets - definition}

A \alert{set} is a collection of things.
\pause
We use the following notation
\begin{itemize}[<+->]
\item $\omega\in A$ means $\omega$ is an element of the set $A$,
\item $\omega\notin A$ means $\omega$ is not an element of the set $A$,
\item $A \subseteq B$ (or $B \supseteq A$) means the set $A$ is a subset of $B$ (with the sets possibly being equal), and
\item $A \subset B$ (or $B \supset A$) means the set $A$ is a 
\alert{proper subset} of $B$, 
i.e. there is at least one element in $B$ that is 
not in $A$.
\end{itemize}
\pause
The \alert{sample space}, $\Omega$, is the set of all outcomes of an\\experiment.

\vspace{2in}

\end{frame}


\begin{frame}
\frametitle{Set - examples}
The set of all possible sums of two 6-sided dice rolls is
$\Omega \pause = \{2,3,4,5,6,7,8,9,10,11,12\}$ 
\pause
and 
\begin{itemize}[<+->]
\item $2 \in    \Omega$
\item $1 \notin \Omega$
\item $\{2,3,4\} \subset \Omega$
\end{itemize}

\vspace{2in}

\end{frame}



\begin{frame}
\frametitle{Set comparison, operations, terminology}
For the following $A,B \subseteq \Omega$ where $\Omega$ is the implied universe of all elements under study,
\begin{enumerate}[<+->]
\item \alert{Union} ($\union$): 
A union of events is an event consisting of all the outcomes in these events.
\[ A\union B = \{\omega\mid \omega \in A \text{ or } \omega \in B\} \]

\item \alert{Intersection} ($\intersection$): 
An intersection of events is an event consisting of the common outcomes in these events.
\[ A\intersection B = \{\omega\mid \omega \in A \text{ and } \omega \in B\} \]

\item \alert{Complement} ($A^C$): 
A complement of an event $A$ is an event that occurs when event $A$ does not happen.
\[ A^C = \{\omega \mid \omega \notin A \text{ and } \omega \in \Omega \} \]


\item \alert{Set difference} ($A\setminus B$):
All elements in $A$ that are not in $B$, i.e. 
\[ 
A\setminus B = \{\omega| \omega\in A \mbox{ and } \omega\notin B\}
\]

\end{enumerate}
\end{frame}





\begin{frame}
\frametitle{Venn diagrams}

<<venn_setup, out.width='0.6\\textwidth'>>=
l = 101
circle = data.frame(
  loc = c(rep("top", l),rep("bot",l)),
  x   = c(seq(-1, 1, length=l), seq(1, -1, length=l))) %>%
  mutate(y   = sqrt(1-x^2)*ifelse(loc=="bot",-1,1))
  
indent = circle %>%
  mutate(x = x-0.5,
         x = ifelse(x>0,-x,x))

yint = sqrt(1-.5^2)

Aonly = indent %>% mutate(set="Aonly")
Bonly = indent %>% mutate(x=-x, set="Bonly")

AandB = data.frame(loc=c(rep("left",l),rep("right",l)),
                   y = c(seq(-yint,yint,length=l), seq(yint,-yint,length=l))) %>%
  mutate(set="AandB",
         x = (sqrt(1-y^2)-.5)*ifelse(loc=="left",-1,1))

# ggplot(Bonly, aes(x,y,fill=set)) + 
#   geom_polygon(color="black") + 
#   coord_fixed() +
#   theme_minimal()
  

all = rbind(Aonly,Bonly,AandB) %>%
  mutate(union = TRUE,
         intersection = set == "AandB",
         difference = set == "Aonly",
         complement = set == "Bonly") %>%
  tidyr::gather(operation,value,union,intersection,complement,difference) %>%
  mutate(operation = factor(operation, 
                            levels=c("union","intersection","complement","difference")))


ggplot(all, aes(x,y, group=set, fill=value)) + 
  geom_rect(data = all %>% filter(operation == "complement"), fill="red", 
            xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, alpha = 0.3) + 
  geom_polygon(color="black") + 
#  geom_polygon(data=all %>% filter(set=="Aonly"), color="black") + 
  facet_wrap(~operation) + 
  coord_fixed() +
#  theme_minimal() +
  scale_fill_manual(values=c("gray","red")) + 
  annotate("text", label="A", x=-1, y=.5,size=8) + 
  annotate("text", label="B", x= 1, y=.5,size=8) +
  theme(axis.line=element_blank(),
      axis.text.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks=element_blank(),
      axis.title.x=element_blank(),
      axis.title.y=element_blank(),
      legend.position="none",
      panel.background=element_blank(),
      panel.border=element_blank(),
      panel.grid.major=element_blank(),
      panel.grid.minor=element_blank(),
      plot.background=element_blank())
@ 
\end{frame}



\begin{frame}
\frametitle{Example}

Consider the set $\Omega$ equal to all possible sum of two 6-sided die rolls
\pause
i.e. $\Omega = \{2,3,4,5,6,7,8,9,10,11,12\}$
and two subsets 
\begin{itemize}
\item all odd rolls: \pause $A = \{3,5,7,9,11\}$ \pause
\item all rolls below 6: \pause $B = \{2,3,4,5\}$
\end{itemize}

\pause
Then we have 
\begin{itemize}
\item $A \union B = \pause \{2,3,4,5,7,9,11\}$ \pause
\item $A \intersection B = \pause \{3,5\}$ \pause
\item $A^C = \pause \{2,4,6,8,10,12\}$ \pause
\item $B^C = \pause \{6,7,8,9,10,11,12\}$ \pause
\item $A\setminus B = \pause \{7,9,11\}$ \pause
\item $B\setminus A = \pause \{2,4\}$
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Set comparison, operations, terminology (cont.)}
\begin{enumerate}[<+->]
\setcounter{enumi}{4}
\item \alert{Empty Set} $\emptyset$ is a set having no elements, i.e. $\{\}$.
The empty set is a subset of every set:
\[ \emptyset \subseteq A \]

\item \alert{Disjoint sets}: Sets $A, B$ are disjoint if their intersection is empty:
\[ A\intersection B = \emptyset \]

\item \alert{Pairwise disjoint sets}: Sets $A_1, A_2, \ldots$ are
  pairwise disjoint if all pairs of these events are disjoint:
\[ A_i\intersection A_j  = \emptyset \text{ for any } i\neq j \]

\item \alert{De Morgan's Laws}:
\[
(A\union B)^C = A^C\intersection B^C
\qquad \mbox{and} \qquad
(A\intersection B)^C = A^C\union B^C
\]
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Examples}

Let $A=\{2,3,4\}, B = \{5,6,7\}, C = \{8,9,10\}, D = \{11,12\}$. 
\pause
Then 
\begin{itemize}
\item $A \intersection B \pause = \emptyset$ \pause
\item $A,B,C,D$ are \pause pairwise disjoint \pause
\item De Morgan's:
\[ \begin{array}{rl}
(A \union B) &\pause = \{2,3,4,5,6,7\} \pause \\
(A \union B)^C &\pause = \{8,9,10,11,12\} \pause \\ \\
A^C &\pause = \{5,6,7,8,9,10,11,12\} \pause \\
B^C &\pause = \{2,3,4,8,9,10,11,12\} \pause \\
A^C \intersection B^C &\pause = \{8,9,10,11,12\}
\end{array} \hspace{2in} \]
\pause 
so, by example,
\[ (A \union B)^C =  A^C \intersection B^C. \hspace{2in} \]
\end{itemize}

\end{frame}



\subsection{Kolmogorov's Axioms}
\begin{frame}
\frametitle{Kolmogorov's Axioms}

A system of probabilities (\alert{a probability model}) is an assignment of 
numbers $P(A)$ to events $A \subseteq \Omega$ \pause such that

\begin{itemize}[<+->]
    \item[(i)]  $0 \le P(A) \le 1$ for all $A$ 
    \item[(ii)] $P(\Omega) = 1$. 
    \item[(iii)]  if $A_{1},A_{2}, \ldots$ are
      pairwise disjoint events 
    (i.e. $A_{i} \intersection A_{j} = \emptyset$ for all $i\neq j$) then 
    \[ \begin{array}{rl}
    P(A_{1} \union A_{2} \union \ldots ) &= P(A_{1}) + P(A_{2}) + \ldots \\
    &= 
    \sum_{i}P(A_{i}).
    \end{array}
    \hspace{3in}
    \]
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Kolmogorov's Axioms (cont.)}
These are the basic rules of operation of a probability model
\begin{itemize}
\item every valid model must obey these,
\item any system that does, is a valid model.
\end{itemize}
\pause
Whether or not a particular model is realistic is different question.

\vspace{0.1in} \pause


Example: Draw a single card from a standard deck of playing cards:
$\Omega = \{ \textcolor{red}{red}, black \}$
Two different, equally valid probability models are:

\vspace{0.2cm}

{\small
\begin{tabular}{l@{\extracolsep{.5in}}l}
    \underline{Model 1} & \underline{Model 2} \\
%    $P(0) = 0$ & $P(0) = 0$ \\
    $P(\Omega) = 1$ & $P(\Omega) = 1$ \\
    $P(\textcolor{red}{red}) = 0.5$ &  $P(\textcolor{red}{red}) = 0.3$ \\
    $P(black) = 0.5$ &  $P(black) = 0.7$
\end{tabular}}

\vspace{0.2cm}
\pause

Mathematically, both schemes are equally valid.\\
But, of course, our real world experience would prefer\\
model 1 over model 2.


\end{frame}


\begin{frame}
\frametitle{Useful Consequences of Kolmogorov's Axioms}
Let $A,\,B\subseteq \Omega$. 

\vspace{0.2in} \pause

\begin{itemize}
\item Probability of the Complementary Event: 
$ P\left(A^C\right) = 1-P(A)$ \pause

{\textcolor{red}{Corollary:}} $P(\emptyset)=0$ \pause

\item Addition Rule of Probability 

\[ P(A \union B) = P(A) + P(B) - P(A \intersection B) \hspace{2in} \]

\item If $A\subseteq B$, then $P(A)\leq P(B)$.
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Example: Using Kolmogorov's Axioms}
We attempt to access the internet from a laptop at home. \pause
We connect successfully if and only if the wireless (WiFi) network works \emph{ and } the internet service provider (ISP) network works. \pause
Assume 
\begin{eqnarray*}
P(\text{ WiFi up } ) &=& .9   \\ 
P(\text{ ISP up } ) &=& .6, \mbox{ and } \\ 
P(\text{ WiFi up and ISP up } ) &=& .55.
\end{eqnarray*}

\vspace{.2in} \pause

\begin{enumerate}[<+->]
\item What is the probability that the WiFi is up or the\\ISP is up?
\item What is the probability that both the WiFi and the\\ISP are down? 
\item What is the probability that we fail to connect?
%\item What is the probability that only the WiFi is up?
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Solution}
Let  $A\equiv \text{WiFi up};\ B\equiv \text{ISP up}$ \pause
\begin{enumerate}[<+->]
\item What is the probability that the WiFi is up or the ISP is up?
\[ P( \text{ WiFi up or ISP up}) =P(A\union B) = 0.9 + 0.6 - 0.55 = 0.95 \hspace{2in} \]
\item  What is the probability that both the WiFi and the ISP are down?
\[ \begin{array}{rl}
P( \text{ WiFi down and ISP down}) &=P\left(A^C\intersection B^C\right) = P\left([A\union B]^C\right) \\
&=1-.95=.05
\end{array} \hspace{2in} \]
\item What is the probability that we fail to connect?
\[ \begin{array}{rl}
\multicolumn{2}{l}{P( \text{ WiFi down or ISP down})} \\
 &=P\left(A^C\union B^C\right) =
 P\left(A^C\right) + P\left(B^C\right) -P\left(A^C\intersection B^C\right) \\
&=P\left(A^C\union B^C\right) =(1-.9)+(1-.6)-.05=.1+.4-.05=.45
\end{array} \hspace{2in} \]
\end{enumerate}

\end{frame}




\subsection{Conditional probability}
\begin{frame}
\frametitle{Conditional probability - Definition}

The \alert{conditional probability} of an event $A$ given an event $B$ is 
\[ 
P(A|B) = \frac{P(A\intersection B)}{P(B)} \hspace{2in}
\]
if $P(B) > 0$. 

\vspace{0.2in} \pause

Intuitively, the fraction of outcomes in $B$ that are also in $A$. 

\vspace{0.2in} \pause

Corrollary:
\[ 
P(A\intersection B) = P(A|B)P(B) = P(B|A)P(A). \hspace{2in}
\]

\vspace{0.4in} 
\end{frame}



\begin{frame}
\frametitle{Random CPUs}
{\scriptsize
A box has 500 CPUs with a speed of 1.8 GHz and 500 with a speed of 2.0 GHz. 
The numbers of good (G) and defective (D) CPUs at the two different speeds are 
as shown below.
\begin{center}
\begin{tabular}{c|c|c|c}
& 1.8 GHz & 2.0 GHz & Total\\
\hline 
G & 480 & 490 & \phantom{1}970  \\
D & \phantom{4}20 & \phantom{4}10 & \phantom{19}30  \\
\hline
Total & 500 & 500 & 1000    
\end{tabular}
\end{center}

\pause

We select a CPU at random and observe its speed. What is the probability that 
the CPU is defective given that its speed is 1.8 GHz?

\pause

Let 
\begin{itemize}
\item $D$ be the event the CPU is defective and
\item $S$ be the event the CPU speed is 1.8 GHz.
\end{itemize}

\pause

Then 
\begin{itemize}
\item $P(S) = 500/1000 = 0.5$
\item $P(S\intersection D) = 20/1000 = 0.02$.
\item $P(D|S) = P(S\intersection D)/P(S) = 0.02/0.5 = 0.04$. 
\end{itemize}
}
\end{frame}


\subsection{Independence}

\begin{frame}
\frametitle{Statistical independence - Definition}

Events $A$ and $B$ are statistically \alert{independent} if 
\[ 
P(A\intersection B) = P(A) \times P(B) \hspace{2in}
\]
\pause
or, equivalently,
\[ 
P(A|B) = P(A). \hspace{2in}
\]

\vspace{0.2in} \pause

Intuition: the occurrence of one event does not affect\\
the probability of the other. 

\vspace{0.2in} \pause

Example:
In two tosses of a coin, the result of the first\\
toss does not affect the probability of the second\\toss being heads.
\end{frame}




\begin{frame}
\frametitle{WiFi example}

In trying to connect my laptop to the internet, I need 
\begin{itemize}
\item my WiFi network to be up (event $A$) and
\item the ISP network to be up (event $B$). 
\end{itemize}

\vspace{0.2in} \pause

Assume the probability the WiFi network is up is 0.9 and the ISP network is up is 0.6. \pause
If the two events are independent, 
what is the probability we can connect to the internet?

\vspace{0.4in} \pause

Since we have independence, we know
\[ 
P(A \intersection B) = P(A)\times P(B) = 0.9 \times 0.6 = 0.54. \hspace{2in}
\]

\end{frame}



\begin{frame}
\frametitle{Independence and disjoint}
\small

\alert{Warning:} Independence and disjointedness are two very different concepts!

\vspace{0.1in} \pause

\begin{columns}[t]
\begin{column}{0.5\textwidth}
{\textcolor{cyan}{Disjoint:}}   
    \centerline{\includegraphics[width=1.5in]{disjoint}}

    If $A$ and $B$ are disjoint, their intersection is empty and therefore has 
    probability 0:
    \[
    P(A \intersection B) = P(\emptyset) = 0.
    \]
\end{column}

\pause

\begin{column}{0.5\textwidth}
{\textcolor{cyan}{Independence: }}
    \centerline{\includegraphics[width=1.5in]{intersection}}    
    If $A$ and $B$ are independent events, the probability of their 
    intersection can be computed as the product of their individual 
    probabilities: 
    \[ P(A \intersection B) = P(A) \cdot P(B) \]
\end{column}
\end{columns}
\end{frame}


\subsection{Reliability}
\begin{frame}
\frametitle{Parallel system - Definition}

A \alert{parallel} system consists of $K$ components $c_{1}, \ldots, c_{K}$ 
arranged in such a way that the system works if {\bf at least one} of the $K$ 
components functions properly.

\includegraphics{parallel-system}
\end{frame}

\begin{frame}
\frametitle{Serial system - Definition}
\setkeys{Gin}{width=\textwidth}
A \alert{serial} system consists of $K$ components $c_{1}, \ldots, c_{K}$ 
arranged in such a way that the system works if and only if {\bf all} of the 
components function properly.

\vspace{0.2in} \pause

\includegraphics{series-system}

\vspace{2in}

\end{frame}



\begin{frame}
\frametitle{Reliability - Definition}

The \alert{reliability} of a system is the probability the system works. 

\vspace{0.2in} \pause

Example: 
The reliability of the WiFi-ISP network (assuming independence) is 0.54.

\vspace{2in}

\end{frame}


\begin{frame}
\frametitle{Reliability of parallel systems with independent components}

Let $c_{1}, \ldots, c_{K}$ denote the $K$ components in a \alert{parallel} system. \pause
\alert{Assume} the $K$ components operate \alert{independently} and 
$P(c_{k} \text{ works})=p_{k}$. \pause
What is the reliability of the system?

\pause

\[ \begin{array}{rl}
P(\text{ system works }) &= P(\text{ at least one component works }) \pause \\
&= 1-P(\text{ all components fail }) \pause \\
&= 1-P(c_{1} \text{ fails and } c_{2} \text{ fails } \ldots \text{ and } c_{k} \text{ fails }) \pause \\
&=1- \prod_{k=1}^K P(c_k \text{ fails}) \pause \\
&= 1- \prod_{k=1}^K(1-p_{k}). \hspace{2in}
\end{array} \]

\vspace{2in}

\end{frame}


\begin{frame}
\frametitle{Reliability of serial systems with independent components}

Let $c_{1}, \ldots, c_{K}$ denote the $K$ components in a \alert{serial} system. \pause
\alert{Assume} the $K$ components operate \alert{independently} 
and $P(c_{k} \text{ works })=p_{k}$. \pause
What is the reliability of the system?

\vspace{0.2in} \pause

\[ \begin{array}{rl}
P(\text{ system works }) &= P(\text{ all components work}) \\
&= \prod_{k=1}^K P(c_k \text{ works}) \\
&= \prod_{k=1}^K p_{k}. \hspace{2in}
\end{array} \]

\vspace{2in}

\end{frame}


\begin{frame}
\frametitle{Reliability example}
\setkeys{Gin}{width=0.6\textwidth}
\small
Each component in the system shown below is opearable with probability 0.92 
independently of other components. Calculate the reliability.

\vspace{0.2in}

\includegraphics{circuit1}
\end{frame}


\begin{frame}
\frametitle{Reliability example}
\small
Each component in the system shown below is opearable with probability 0.92 
independently of other components. Calculate the reliability.

\includegraphics[width = 0.5\textwidth]{circuit1}

\pause

\begin{enumerate} \scriptsize
\item Serial components A and B can be replaced by a\\
component F that operates with probability\\
$P(A\intersection B)=(0.92)^2=0.8464$.
\item Parallel components D and E can be replaced by\\
component G that operates with probability\\$P(D\union E)=1-(1-0.92)^2=0.9936$.
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Reliability example (cont.)}
\setkeys{Gin}{width=0.6\textwidth}
\small
Updated circuit:

\includegraphics{circuit2}

\begin{enumerate}
\setcounter{enumi}{2}
\item Serial components C and G can be replaced\\
by a component H that operates with probability\\
$P(C\intersection G)=(0.92)(0.9936)=0.9141$.
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Reliability example (cont.)}

Updated circuit:

\includegraphics{circuit3}

\begin{enumerate}
\setcounter{enumi}{3}
\item  Parallel componenents F and H are in parallel,\\
so the reliability of the system is\\
$P(F\union H)=1-(1-0.8464)(1-0.9141)\approx 0.99$.
\end{enumerate}
\end{frame}



\subsection{Law of Total Probability}
\begin{frame}
\frametitle{Partition}
\setkeys{Gin}{width=0.4\textwidth}
\begin{definition}
A collection of events $B_{1}, \ldots B_{K}$ is called a \alert{partition} (or \alert{cover}) of $\Omega$ if 
\begin{itemize}
\item the events are pairwise disjoint (i.e., $B_{i}\intersection B_{j} = \emptyset$ for $i\neq j$)\pause, and
\item the union of the events is $\Omega$ (i.e., $\bigcup_{k=1}^{K}B_{k} = \Omega$).
\end{itemize}
\end{definition}

\vspace{0.2in}

\includegraphics{cover}

\end{frame}


\begin{frame}
\frametitle{Example}

Consider the sum of two 6-sided die, i.e. 
\[ \Omega=\{2,3,4,5,6,7,8,9,10,11,12\}. \]

\vspace{0.1in} \pause

Here are some covers:
\begin{itemize}[<+->]
\item $\{2,3,4\},\{5,6,7,8,9,10,11,12\}$
\item $\{2,3,4\}, \{5,6,7\},\{8,9,10\},\{11,12\}$
\item $A_2,A_3,\ldots,A_{12}$ where $A_i = \{i\}$
\item any $A$ and $A^C$ where $A \subseteq \Omega$
\end{itemize}

\end{frame}






\begin{frame}
\frametitle{Law of Total Probability}

\alert{Law of Total Probability}:

If  the collection of events $B_{1}, \ldots, B_{K}$ is a partition of $\Omega$, and $A$ is an event, then 
\[ P(A) = \sum_{k=1}^{K}P(A|B_{k})P(B_{k}). \]


\vspace{0.2in} \pause

\small
Proof:
\[ \begin{array}{rll}
P(A) &= P\left( \bigcup_{k=1}^K A\intersection B_k \right) & \mbox{partition} \pause \\
&= \sum_{k=1}^K P(A\intersection B_k) & \mbox{pairwise disjoint} \pause \\
&= \sum_{k=1}^K P(A|B_k)P(B_k) & \mbox{conditional probability}
\end{array} \hspace{2.5in} \]


\end{frame}


\begin{frame}
\frametitle{Law of Total Probability - Graphically}
\setkeys{Gin}{width=0.6\textwidth}

\includegraphics{total-prob}

\end{frame}



\begin{frame}
\frametitle{Law of Total Probability - Example}

In the come out roll of craps, you win if the roll is a 7 or 11. 
\pause
By the law of total probability, the probability you win is 
\[ 
P(\mbox{Win}) = \sum_{i=2}^{12} P(\mbox{Win}|i)P(i) = P(7) + P(11)
\]
since $P(\mbox{Win}|i) = 1$ if $i=7, 11$ and 0 otherwise.

\vspace{2in}

\end{frame}


\subsection{Bayes' Rule}
\begin{frame}
\frametitle{Bayes' Rule}

Bayes' Rule:

If $B_{1}, \ldots, B_{K}$ is a partition of $\Omega$, and $A$ is an event in $\Omega$, 
\pause then 
\[
P(B_k|A) = \frac{P(A|B_k)P(B_k)}{\sum_{k=1}^{K}P(A|B_k)P(B_k)}.
\]

\vspace{0.2in} \pause

Proof:
\small
\[ \begin{array}{rll}
P(B_k|A) &= \frac{P(A\intersection B_k)}{P(A)} & \mbox{conditional probability} \pause \\
&= \frac{P(A|B_k)P(B_k)}{P(A)} & \mbox{conditional probability} \pause \\
&= \frac{P(A|B_k)P(B_k)}{\sum_{k=1}^K P(A|B_k)P(B_k)} & \mbox{Law of Total Probability} 
\end{array} \hspace{3in} \]

\end{frame}



\begin{frame}
\frametitle{Bayes' Rule: Craps example}

If you win on a come-out roll in craps, 
what is the probability you rolled a 7?

\vspace{0.1in} \pause

\[ \begin{array}{rl}
P(7|\text{Win}) 
&= \frac{P(\text{Win}|7)P(7)}{\sum_{i=2}^{12} P(\text{Win}|i)P(i)}  \\ \\
&= \frac{P(7)}{P(7)+P(11)}.
\end{array} \hspace{2in} \]

\end{frame}



\begin{frame}
\frametitle{Bayes' Rule: CPU testing example}
A given lot of CPUs contains 2\%  defective CPUs. \pause
Each CPU is tested before delivery. \pause
However, the tester is not wholly reliable: \pause
\[ \begin{array}{rl}
    P(\text{ tester says CPU is good } | \text{  CPU is good }) &= 0.95 \pause \\
    P(\text{ tester says CPU is defective } | \text{ CPU is defective }) &=  0.94
\end{array} \]
\pause
If the test device says the CPU is defective, what is the probability that the 
CPU is actually defective?

\vspace{2in}

\end{frame}


\begin{frame}
\frametitle{CPU testing (cont.)}

{\small

Let
\begin{itemize} \small
\item $C_g$ ($C_d$) be the event the CPU is good (defective) \pause
\item $T_g$ ($T_d$) be the event the tester says the CPU is good (defective)
\end{itemize}

We know \pause

\begin{itemize} \small
\item $0.02 = P(C_d) \phantom{|C_g} = 1-P(C_g)$
\item $0.95 = P(T_g|C_g) = 1-P(T_d|C_g)$
\item $0.94 = P(T_d|C_d) = 1-P(T_g|C_d)$
\end{itemize}

\pause
Using Bayes' Rule, we have 

\[ \begin{array}{rl}
P(C_d|T_d) &= \frac{P(T_d|C_d)P(C_d)}{P(T_d|C_d)P(C_d)+P(T_d|C_g)P(C_g)} \pause \\ 
&= \frac{P(T_d|C_d)P(C_d)}{P(T_d|C_d)P(C_d)+[1-P(T_g|C_g)][1-P(C_d)]} \pause \\
&= \frac{0.94\times 0.02}{0.94\times 0.02 + [1-0.95]\times[1-0.02]} \pause \\ 
&= 0.28
\end{array} \hspace{2in} \]
}
\end{frame}


\begin{frame}
\frametitle{Probability Summary}

\begin{itemize}
\item Probability Interpretation
\item Sets and set operations
\item Kolmogorov's Axioms
\item Conditional Probability
\item Independence
\item Reliability
\item Law of Total Probability
\item Bayes' Rule
\end{itemize}

\vfill

\end{frame}

\end{document}
