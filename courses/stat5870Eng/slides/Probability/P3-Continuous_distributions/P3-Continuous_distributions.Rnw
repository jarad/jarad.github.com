\documentclass[aspectratio=169,handout]{beamer}

\input{../../frontmatter}
\input{../../commands}

% \setbeamertemplate{background}
% {\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{video_overlay}}

\title{P3 - Continuous random variables}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=4.4, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("dplyr")
library("ggplot2")
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}


\section{Continuous random variables}
\begin{frame}[t]
\frametitle{Continuous vs discrete random variables}

Discrete random variables \pause have 
\begin{itemize}
\item finite or countable support \pause and 
\item pmf: $P(X=x)$.
\end{itemize}

\vspace{0.1in} \pause

Continuous random variables \pause have 
\begin{itemize}
\item uncountable support \pause and
\item $P(X=x) = 0$ for all $x$.
\end{itemize}

\end{frame}



\subsection{Cumulative distribution function}
\begin{frame}[t]
\frametitle{Cumulative distribution function}

The \alert{cumulative distribution function} for a continuous random variable is 
\[ 
F_X(x) = P(X\le x) \pause = P(X < x)
\]
since $P(X=x)=0$ for any $x$.

\vspace{0.1in} \pause

The cdf still has the properties 

\begin{itemize}
\item $0 \le F_{X}(x) \le 1$ for all $x \in \mathbb{R}$, \pause
\item $F_{X}$ is monotone increasing,\\i.e. if $x_{1} \le x_{2}$ then $F_{X}(x_{1}) \le F_{X}(x_{2})$, \pause and
\item $\lim_{x \to -\infty}F_{X}(x) = 0$ and $\lim_{x \to \infty}F_{X}(x) = 1$.
\end{itemize}
\pause
\end{frame}




\subsection{Probability density functions}
\begin{frame}[t]
\frametitle{Probability density function}

The \alert{probability density function} (pdf) for a continuous random variable is 
\[ 
f_X(x) = \frac{d}{dx} F_X(x) \hspace{2in}
\]
\pause
and 
\[ 
F_X(x) = \int_{-\infty}^x f_X(t) dt.  \hspace{2in}
\]

\vspace{0.1in} \pause

Thus, the pdf has the following properties
\begin{itemize}
\item $f_X(x) \ge 0$ for all $x$ \pause and
\item $\int_{-\infty}^\infty f(x) dx = 1$.
\end{itemize}
\end{frame}


\subsection{Example}
\begin{frame}[t]
\frametitle{Example}

Let $X$ be a random variable with probability density function 
\[ 
f_X(x) = \left\{ \begin{array}{ll}
3x^2 & \mbox{if }0<x<1 \\
0& \mbox{otherwise.}
\end{array} \right.  \hspace{2in}
\]

\pause

$f_X(x)$ defines a valid pdf because $f_X(x) \ge 0$ for all $x$ \pause and
\[ 
\int_{-\infty}^\infty f_X(x) dx = \int_0^1 3x^2 dx = x^3 |_0^1 = 1.  \hspace{2in}
\]

\pause
The cdf is 
\[ 
F_X(x) = \left\{ \begin{array}{ll}
0 & x\le 0 \\
x^3 & 0< x < 1 \\
1 & x\ge 1
\end{array}. \right.  \hspace{2in}
\]

\end{frame}


\subsection{Expectation}
\begin{frame}[t]
\frametitle{Expected value}

Let $X$ be a continuous random variable and $h$ be some function. 
The \alert{expected value} of a function of a continuous random variable is 
\[ 
E[h(X)] = \int_{-\infty}^\infty h(x) \cdot f_X(x) dx.  \hspace{2in}
\]
\pause
If $h(x)=x$, then 
\[ 
E[X] = \int_{-\infty}^\infty x \cdot f_X(x) dx.  \hspace{2in}
\]
\pause
and we call this the \alert{expectation} of $X$. \pause
We commonly\\use the symbol $\mu$ for this expectation.
\end{frame}



\begin{frame}[t]
\frametitle{Example (cont.)}

Let $X$ be a random variable with probability density function 
\[ 
f_X(x) = \left\{ \begin{array}{ll}
3x^2 & \mbox{if }0<x<1 \\
0& \mbox{otherwise.}
\end{array} \right.  \hspace{2in}
\]

\pause

The expected value is 
\[ \begin{array}{rl}
E[X] &= \int_{-\infty}^\infty x \cdot f_X(x) dx \\
&= \int_0^1 3x^3 dx \\
&= 3\frac{x^4}{4} |_0^1 = \frac{3}{4}. 
\end{array}  \hspace{2in} \]
\end{frame}


\begin{frame}[t]
\frametitle{Example - Center of mass}

\bc
<<center_of_mass, fig.height=4, out.width = "\\textwidth">>=
d <- data.frame(x = seq(0,1,by=0.05)) %>%
  mutate(y = 3*x^2)


ggplot(d, aes(x, ymin = 0, ymax = y)) + 
  geom_ribbon() +
  geom_point(data = data.frame(x = .75, y = -0.1), aes(x,y), 
             shape = 24, color = 'red', fill = 'red') + 
  labs(y = "probability density function") +
  theme_bw()
@
\nc\ec

\end{frame}


\subsection{Variance}
\begin{frame}[t]
\frametitle{Variance}

The \alert{variance} of a random variable is defined as the expected squared deviation from the mean. \pause 
For continuous random variables, variance is
\[
Var[X] = E[(X-\mu)^2] = \int_{-\infty}^\infty (x-\mu)^2 f_X(x) dx
\]
where $\mu = E[X]$. \pause
The symbol $\sigma^2$ is commonly used for the variance.

\vspace{0.1in} \pause

\bc
The \alert{standard deviation} is the positive square root of the variance
\[
SD[X] = \sqrt{Var[X]}.
\]
The symbol $\sigma$ is commonly used for the standard deviation.
\nc\ec
\end{frame}




\begin{frame}[t]
\frametitle{Example (cont.)}

Let $X$ be a random variable with probability density function 
\[ 
f_X(x) = \left\{ \begin{array}{ll}
3x^2 & \mbox{if }0<x<1 \\
0& \mbox{otherwise.}
\end{array} \right.
\]

\pause

\bc
The variance is 
\[ \begin{array}{rl}
Var[X] &= \int_{-\infty}^\infty \left(x-\mu\right)^2 f_X(x) dx \\
&= \int_0^1 \left(x-\frac{3}{4}\right)^2 3x^2 dx \\
&= \int_0^1 \left[x^2-\frac{3}{2}x + \frac{9}{16} \right] 3x^2 dx \\
&= \int_0^1 3x^4-\frac{9}{2}x^3 + \frac{27}{16}x^2 dx \\
&= \left[\frac{3}{5}x^5-\frac{9}{8}x^4 + \frac{9}{16}x^3\right]|_0^1 dx \\
&= \frac{3}{5}-\frac{9}{8}+\frac{9}{16} \\
&= \frac{3}{80}.
\end{array} \]
\nc\ec
\end{frame}




\subsection{Comparison of discrete and continuous random variables}
\begin{frame}[t]
\frametitle{Comparison of discrete and continuous random variables}
\small
For simplicity here and later, we drop the subscript $X$. \pause

{\scriptsize
\begin{center}
\begin{tabular}{lll}
& discrete & continuous \pause \\
\hline
support ($\mathcal{X}$) & finite or countable & uncountable \pause \\ \\
pmf &  $p(x) = P(X=x)$ & \pause \\ \\
pdf && $p(x) = f(x) = F'(x)$ \pause \\ \\
cdf & 
$\begin{array}{rl}F(x) &= P(X\le x) \\
                        &= \sum_{t\le x} p(t)\end{array} $ & 
$\begin{array}{rl}F(x) &= P(X\le x) = P(X < x) \\
                        &= \int_{-\infty}^x p(t) \, dt\end{array} $
\pause \\ \\
expected value & $E[h(X)] = \sum_{x\in \mathcal{X}} h(x) p(x)$ & $E[h(X)] = \int_{\mathcal{X}} h(x) p(x) \, dx$ \pause \\ \\
expectation & $\mu = E[X] = \sum_{x\in \mathcal{X}} x \, p(x)$ & $\mu = E[X] = \int_{\mathcal{X}} x \, p(x) \, dx$ \pause \\ \\
variance & 
$ \begin{array}{rl} \sigma^2 = Var[X] &= E[(X-\mu)^2] \\
                            &= \sum_{x\in \mathcal{X}} (x-\mu)^2 \, p(x)
\end{array} $ & 
$ \begin{array}{rl}
\sigma^2 = Var[X] &= E[(X-\mu)^2] \\
       &= \int_{\mathcal{X}} (x-\mu)^2 \, p(x) \, dx
\end{array} $ \\ \\
\hline
\end{tabular}
\end{center}
}

\vspace{0.05in}\pause

Note: we replace summations with integrals when using continuous as opposed to discrete random variables
\end{frame}




\section{Uniform}
\begin{frame}[t]
\frametitle{Uniform}

\vspace{-0.1in} \pause

\small

A \alert{uniform} random variable on the interval $(a,b)$ has equal probability
for any value in that interval and we denote this $X\sim Unif(a,b)$. 
\pause
The pdf for a uniform random variable is 
\[ 
f(x) = \frac{1}{b-a}\I(a<x<b)
\]
\pause
where $\I(A)$ is in indicator function that is 1 if $A$ is true and 0 otherwise,
i.e. 
\[ 
\I(A) = \left\{ \begin{array}{ll} 1 & A\text{ is true} \\ 
0 & \text{otherwise}.
\end{array} \right.
\]
\pause 
\bc
The expectation is 
\[ 
E[X] = \int_a^b x\, \frac{1}{b-a}\, dx = \frac{a+b}{2}
\]
\pause
and the variance is 
\[ 
Var[X] = \int_a^b \frac{1}{b-a} \left(x-\frac{a+b}{2}\right)^2 dx = 
\frac{1}{12}(b-a)^2.
\]
\nc\ec
\end{frame}


\subsection{Standard uniform}
\begin{frame}[t]
\frametitle{Standard uniform}

A \alert{standard uniform} random variable is $X \sim Unif(0,1)$. \pause
This random variable has 
\[ E[X] = \frac{1}{2} \qquad \mbox{and} \qquad Var[X] = \frac{1}{12}. \]

\vspace{-0.2in}\pause

\bc
<<out.width = "\\textwidth", fig.height = 3.5>>=
d = data.frame(x = seq(-0.5, 1.5, by=0.01)) %>%
  dplyr::mutate(pdf = 1 * (x>0) * (x < 1),
                cdf = x,
                cdf = ifelse(x<0, 0, x),
                cdf = ifelse(x>1, 1, x))

ggplot(d, aes(x, pdf)) +
  geom_line() +
  labs(title ="Standard uniform pdf", x = "x", y = "Probability density function") + 
  theme_bw()
@
\nc\ec

\end{frame}



\subsection{Inverse CDF}
\begin{frame}[t,fragile]
\frametitle{Example (cont.)}
\small
Pseudo-random number generators generate pseudo uniform values on (0,1). 
\pause
These values can be used in conjunction with the inverse of the cumulative
distribution function to generate pseudo-random numbers from any 
distribution.

\vspace{0.1in} \pause

The inverse of the cdf $F_X(x) =x^3$ is 
\[ 
F^{-1}_X(u) = u^{1/3}.
\]
\pause
A uniform random number on the interval (0,1) generated using the inverse cdf produces a random draw of $X$. \pause

\bc
<<echo=TRUE>>=
inverse_cdf = function(u) u^(1/3)
x = inverse_cdf(runif(1e6))
mean(x)
var(x); 3/80
@
\nc\ec
\end{frame}


\begin{frame}[t]
\frametitle{}
\bc
<<out.width="\\textwidth">>=
hist(x, 100, prob = TRUE)
curve(3*x^2, col='red', add = TRUE, lwd=2)
@
\nc\ec
\end{frame}





\section{Normal}
\begin{frame}[t]
\frametitle{Normal random variable}

\pause
\small

The \alert{normal (or Gaussian) density} is a ``bell-shaped'' curve. \pause
The density has two parameters: \alert{mean} $\mu$ and \alert{variance} $\sigma^{2}$ and is
\[
f(x) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} 
e^{-(x-\mu)^{2}/2 \sigma^{2}} \qquad \text{ for } -\infty<x<\infty
\]

\vspace{-0.1in} \pause

\bc
If $X\sim N(\mu, \sigma^2)$, \pause then 
\[ \begin{array}{rll}
E[X] &= \int_{-\infty}^{\infty} x\, f(x) dx               = \ldots &= \mu \\
Var[X] &= \int_{-\infty}^{\infty} (x - \mu)^{2}\, f(x) dx = \ldots &= \sigma^{2}. \\
\end{array} \]
\pause
Thus, the parameters $\mu$ and $\sigma^{2}$ are actually the mean and the variance of the $N(\mu,\sigma^2)$ distribution. 

\vspace{0.1in} \pause

There is no closed form cumulative distribution function for a normal random variable. 
\nc\ec
\end{frame}


\subsection{Example pdfs}
\begin{frame}[t]
\frametitle{Example normal probability density functions}

\vspace{-0.5in}

\bc
<<out.width="\\textwidth">>=
d = data.frame(mu = c(0,0,1,1), sigma = c(1,2,1,2))
d$legend_name = paste("mu=", d$mu, ", sigma=", d$sigma)
plot(0,0, type = "n", xlim = c(-4,4), ylim = c(0,0.5), xlab = "x", 
     ylab = "Probability density function")
for (i in 1:nrow(d)) {
	mu = d$mu[i]; sigma = d$sigma[i]
	curve(dnorm(x, mu, sigma), add = TRUE, lty = mu+1, col = sigma)
}
legend("topleft", legend = d$legend_name,
			 lty = d$mu+1, col = d$sigma)
@
\nc\ec
\end{frame}


\subsection{Properties}
\begin{frame}[t]
\frametitle{Properties of normal random variables}

\small

Let $Z\sim N(0,1)$, i.e. a \alert{standard normal} random variable.
\pause
% (If you see $Z$ without explanation, it is a standard normal random variable.)
% \pause
Then for constants $\mu$ and $\sigma$ 
\[ 
X = \mu+\sigma Z \pause \sim N(\mu,\sigma^2)
\]
\pause and
\[ 
Z = \frac{X-\mu}{\sigma} \pause \sim N(0,1)
\]
\pause
which is called \alert{standardizing}.

\vspace{0.1in} \pause

\bc

Let $X_i \ind N(\mu_i,\sigma_i^2)$. \pause Then 
\[
Z_i = \frac{X_i-\mu_i}{\sigma_i} \iid N(0,1) \quad\mbox{for all } i
\]
\pause 
and 
\[ 
Y = \sum_{i=1}^n X_i \sim N\left(\sum_{i=1}^n \mu_i, \sum_{i=1}^n \sigma_i^2 \right).
\]
\nc\ec
\end{frame}


\subsection{Standard normal}
\begin{frame}[t,fragile]
\frametitle{Calculating the standard normal cdf}

If $Z\sim N(0,1)$, what is $P(Z\le 1.5)$? \pause 
Although the cdf does not have a closed form, very good approximations exist and are available as tables or in software, \pause e.g.
<<echo=TRUE>>=
pnorm(1.5) # default is mean=0, sd=1
@

\pause 

\bc
If $Z\sim N(0,1)$, \pause then 
\begin{itemize}[<+->]
\item $P(Z\le z) = \Phi(z)$ 
\item $\Phi(z) = 1-\Phi(-z)$ 
since a normal pdf is\\\alert{symmetric} around its mean.
\end{itemize}
\nc\ec
\end{frame}


\begin{frame}[t,fragile]
\frametitle{Calculating any normal cumulative distribution function}
If $X\sim N(15,4)$ what is $P(X>18)$? \pause
\[ \begin{array}{rl}
P(X>18) &\pause = 1-P(X\le 18) \pause \\
&= 1-P\left(\frac{X-15}{2} \le \frac{18-15}{2} \right) \pause \\
&= 1-P(Z\le 1.5) \pause \\
&\approx 1-0.933 = 0.067
\end{array} \]
\pause
\bc
<<echo=TRUE>>=
1-pnorm((18-15)/2)         # by standardizing
1-pnorm(18, mean = 15, sd = 2) # using the mean and sd arguments
@
\nc\ec
\end{frame}


\subsection{Manufacturing example}
\begin{frame}[t,fragile]
\frametitle{Manufacturing}

Suppose you are producing nails that must be within 5 and 6 centimeters in 
length. 
\pause

If the average length of nails the process produces is 5.3 cm and the standard
deviation is 0.1 cm. 
\pause
What is the probability the next nail produced is outside of the specification?

\vspace{0.1in} \pause

\bc
Let $X\sim N(5.3, 0.1^2)$ be the length (cm) of the next nail produced. 
\pause
We need to calculate 
\[ 
P(X<5 \mbox{ or } X>6) \pause = 1-P(5<X<6). 
\]

\pause

<<echo = TRUE>>=
mu = 5.3
sigma = 0.1

1-diff(pnorm(c(5,6), mean = mu, sd = sigma))
@
\nc\ec
\end{frame}


\subsection{Summary}
\begin{frame}[t]
\frametitle{Summary}
\begin{itemize}
\item Continuous random variables
  \begin{itemize}
  \item Probability density function
  \item Cumulative distribution function
  \item Expectation
  \item Variance \pause
  \end{itemize}
\item Specific distributions
  \begin{itemize}
  \item Uniform
  \item Normal (or Gaussian)
  \end{itemize}
\end{itemize}
\end{frame}

\end{document}
