\documentclass[aspectratio=169,handout]{beamer}

\input{../../frontmatter}
\input{../../commands}

% \setbeamertemplate{background}
% {\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{video_overlay}}

\title{P2 - Discrete Random Variables}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA,
               fig.width=6, fig.height=4.4,
               size='tiny',
               out.width='0.8\\textwidth',
               fig.align='center',
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library(tidyverse)
library(gridExtra)
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}

\section{Random variables}
\begin{frame}[t]
\frametitle{Random variables}
\setkeys{Gin}{width=0.15\textwidth}

If $\Omega$ is the sample space of an experiment,
a \alert{random variable} $X$ is a function $X(\omega):\Omega \mapsto \mathbb{R}$.

\vspace{0.1in} \pause

{\bf Idea}:
If the value of a numerical variable depends on the outcome
of an experiment, we call the variable a {\it random variable}.

\vspace{0.1in} \pause

Examples of random variables from rolling two 6-sided dice: \pause
\begin{itemize}
\item Sum of the two dice \pause
\item Indicator of the sum being greater than 5
\end{itemize}

\vspace{0.1in} \pause

We will use an upper case Roman letter \\
(late in the alphabet) to indicate a random variable\pause\\
and a lower case Roman letter to indicate a realized\\
value of the random variable.

\end{frame}








\begin{frame}[t]
\frametitle{8 bit example}

Suppose, 8 bits are sent through a communication channel.
Each bit has a certain probability to be received incorrectly.
We are interested in the number of bits that are received incorrectly.

\pause

\begin{itemize}
\item Let $X$ be the number of incorrect bits received. \pause
\item The possible values for $X$ are $\{0,1,2,3,4,5,6,7,8\}$. \pause
\item Example events:
	\begin{itemize}
	\item No incorrect bits received: \pause $\{X=0\}$. \pause
	\item At least one incorrect bit received: \pause $\{X\ge 1\}$. \pause
	\item Exactly two incorrect bits received: \pause $\{X=2\}$. \pause
	\item Between two and seven (inclusive) incorrect bits\\received: \pause $\{2\le X \le 7\}$.
	\end{itemize}
\end{itemize}
\end{frame}





\subsection{Range}
\begin{frame}[t]
\frametitle{Range/image of random variables}

\footnotesize

The \alert{range} (or \alert{image}) of a random variable $X$ is defined as
\[
Range(X) := \{x: x=X(\omega) \text{ for some }\omega \in \Omega \}
\]
\pause
If the range is finite or countably infinite, we have a \alert{discrete} random variable. \pause
If the range is uncountably infinite, we have a \alert{continuous} random variable.

\pause

Examples:
\begin{itemize}
\item Put a hard drive into service,
measure $Y$ = ``time until the first major failure''
\pause
and thus $Range(Y) = (0, \infty)$.
\pause
Range of $Y$ is an interval (uncountable range),
so $Y$ is a \alert{continuous} random variable. \pause
\item Communication channel: $X$ = ``\# of incorrectly received bits\\
out of 8 bits sent''
\pause
with $Range(X) = \{ 0,1,2,3,4,5,6,7,8 \}$.\\
\pause
Range of $X$ is a finite set, so $X$ is	a \alert{discrete} random variable. \pause
\item Communication channel: $Z$ = ``\# of incorrectly received bits\\
in 10 minutes''
\pause
with $Range(Z) = \{0,1,\ldots\}$. \\
\pause
Range of $Z$ is a countably infinite set, so $Z$ is a \alert{discrete}\\
random variable.
\end{itemize}
\end{frame}



\section{Discrete random variables}
\subsection{Distribution}
\begin{frame}[t]
\frametitle{Distribution}

The collection of all the probabilities related to $X$ is the
\alert{distribution} of $X$.

\vspace{0.1in} \pause

For a discrete random variable, the function
\[
p_X(x) = P(X=x) \hspace{2in}
\]
is the \alert{probability mass function} (pmf)
\pause
and the \alert{cumulative distribution function} (cdf) is
\[
F_X(x) = P(X\le x) =\sum_{y\le x} p_X(y). \hspace{2in}
\]
\pause
The set of non-zero probability values of $X$ is called\\
the \alert{support} of the distribution $f$. \\
This is the same as the \alert{range} of $X$.

\end{frame}




\begin{frame}[t]
\frametitle{Examples}
A probability mass function is valid if it defines a valid set of probabilities,
i.e. they obey Kolmogorov's axioms.

\vspace{0.1in} \pause

Which of the following functions are a valid probability mass functions?
\begin{itemize}[<+->]
\item
    	\begin{tabular}{c|ccccc}
		$x$ & -3 & -1 & 0 & 5 & 7  \\
		\hline
		$p_{X}(x)$ & 0.1 & 0.45 & 0.15 & 0.25 & 0.05  \\
	\end{tabular}
    \item
    	\begin{tabular}{c|ccccc}
		$y$ & -1 & 0 & 1.5 & 3 & 4.5 \\
		\hline
		$p_{Y}(y)$ & 0.1 & 0.45 & 0.25 & -0.05 & 0.25  \\
	\end{tabular}
    \item
    	\begin{tabular}{c|ccccc}
		$z$ & 0 & 1 & 3 & 5 & 7  \\
		\hline
		$p_{Z}(z)$ & 0.22 & 0.18 & 0.24 & 0.17 & 0.18  \\
	\end{tabular}
\end{itemize}
\end{frame}


\subsection{Die rolling}
\begin{frame}[t]
\frametitle{Rolling a fair 6-sided die}

Let $Y$ be the number of pips on the upturned face of a die. \pause
The support of $Y$ is  \pause $\{1,2,3,4,5,6\}$.	\pause
If we believe the die has equal probability for each face, \pause
then image, pmf, and cdf for $Y$ are

\[ \arraycolsep=2pt\def\arraystretch{2.2}
\begin{array}{c|cccccc}
\hline
y & 1 & 2 & 3 & 4 & 5 & 6 \pause \\
\hline
p_Y(y) = P(Y=y) & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6}  & \frac{1}{6} \pause \\
F_Y(y) = P(Y\le y) & \frac{1}{6} & \frac{2}{6} & \frac{3}{6} & \frac{4}{6} & \frac{5}{6}  & \frac{6}{6} \\
\hline
\end{array}
\hspace{2in} \]

\end{frame}


\subsection{Dragonwood}
\begin{frame}[t,fragile]
\frametitle{Dragonwood}

Dragonwood has 6-sided dice with the following \# on the 6 sides: $\{1,2,2,3,3,4\}$.

\vspace{0.1in} \pause

What is the support, pmf, and cdf for the sum of the upturned numbers when
rolling 3 Dragonwood dice?

\pause

{\Huge
\centering
<<dragonwood, echo=TRUE, tidy=FALSE>>=
# Three dice
die   = c(1,2,2,3,3,4)
rolls = expand.grid(die1 = die, die2 = die, die3 = die)
sum   = rowSums(rolls); tsum = table(sum)
dragonwood3 = data.frame(x   = as.numeric(names(tsum)),
                         pmf = as.numeric(table(sum)/length(sum))) %>%
	mutate(cdf = cumsum(pmf))
@

<<dragonwood_probability_table, dependson='dragonwood', echo = FALSE>>=
knitr::kable(dragonwood3, col.names = c("$x$","$P(X=x)$","$P(X\\le x)$"),
             escape = FALSE,
             digits = c(0,3,3), "latex")
@
}
\end{frame}


\begin{frame}[t]
\frametitle{Dragonwood - pmf and cdf}

<<dragonwood_plots, dependson='dragonwood', fig.width=9>>=
g_pmf = ggplot(dragonwood3, aes(x=x, y=pmf)) +
	geom_point() +
	scale_x_continuous(breaks=3:12) +
	ylim(0,1) +
  theme_bw()

# set  up cdf plot
cdf = bind_rows(data.frame(x=2,pmf=0,cdf=0), dragonwood3) %>%
	mutate(xend = c(x[-1],13))


g_cdf = ggplot(cdf, aes(x=x, y=cdf, xend=xend, yend=cdf)) +
	#      geom_vline(aes(xintercept=x), linetype=2, color="grey") +
	geom_point() +  # Solid points to left
	geom_point(aes(x=xend, y=cdf), shape=1) +  # Open points to right
	geom_segment() +
	scale_x_continuous(breaks=3:12) +
	coord_cartesian(xlim=c(3,12)) +
  theme_bw()

grid.arrange(g_pmf,g_cdf, nrow=1)
@

\end{frame}



\begin{frame}[t]
\frametitle{Properties of pmf and cdf}

\vspace{-0.1in}

Properties of probability mass function $p_X(x) = P(X=x)$:
\begin{itemize}
\item $0 \le p_X(x) \le 1$ for all $x \in \mathbb{R}$.
\item $\sum_{x\in S} p_X(x) = 1$ where $S$ is the support.
\end{itemize}

\vspace{0.1in} \pause

Properties of cumulative distribution function $F_X(x)$:
\begin{itemize}
\item $0 \le F_{X}(x) \le 1$ for all $x \in \mathbb{R}$
\item $F_{X}$ is nondecreasing, (i.e. if $x_{1} \le x_{2}$ then $F_{X}(x_{1}) \le F_{X}(x_{2})$.)
\item $\lim_{x \rightarrow -\infty}F_{X}(x) = 0$ and $\lim_{x \rightarrow \infty}F_{X}(x) = 1$.
\item %$F_{X}(t)$ has a positive jump equal to $p_{X}(x_{i})$ at $\{
    %x_{1}, x_{2}, x_{3}, \ldots \}$; $F_{X}$ is constant in the interval $[x_{i},
    %x_{i+1})$, i.e.
    $F_X(x)$ is right continuous with respect to $x$
\end{itemize}


\end{frame}

% \foilhead[-.8in]{\textcolor{blue}{Properties of $F_X$}}\vspace{.2cm}
% %\no  {$F_X(x)=P(X\leq x)$ is cumulative distribution function, cdf }\\[.2in]
% \no  \no {\textcolor{red}{Properties of $F_{X}$:} }
% The following properties hold for the cdf
% $F_{X}$ of a  random variable $X$.
%  \begin{itemize}
%     \item $ 0 \le F_{X}(t) \le 1$ for all $t \in \mathbb{R}$
%     \item $F_{X}$ is nondecreasing, (i.e. if $x_{1} \le x_{2}$
%     then $F_{X}(x_{1}) \le F_{X}(x_{2})$.)
%     \item $\lim_{t \rightarrow -\infty}F_{X}(t) = 0$ and $\lim_{t \rightarrow \infty}F_{X}(t) = 1$.
%     \item %$F_{X}(t)$ has a positive jump equal to $p_{X}(x_{i})$ at $\{
%     %x_{1}, x_{2}, x_{3}, \ldots \}$; $F_{X}$ is constant in the interval $[x_{i},
%     %x_{i+1})$, i.e.
%     $F_X(t)$ is right continuous with respect to $t$
% \end{itemize}
% %\no If $x\leq y$, what how will $F_X(x)$ and $F_Y(y)$ be related?


\begin{frame}[t]
\frametitle{Dragonwood (cont.)}
\small
In Dragonwood, you capture monsters by rolling a sum equal to or greater than its defense. \pause
Suppose you can roll 3 dice and the following monsters are available to be captured:
\begin{itemize} \small
\item Spooky Spiders worth 1 victory point with a defense of 3.
\item Hungry Bear worth 3 victory points with a defense of 7.
\item Grumpy Troll worth 4 victory points with a defense of 9.
\end{itemize}
\pause
Which monster should you attack?

\end{frame}


\begin{frame}[t]
\frametitle{Dragonwood (cont.)}
\small
Calculate the probability by computing one minus the cdf evaluated at
\\``defense minus 1''. \pause
Let $X$ be the sum of the number on 3 Dragonwood dice. \pause
Then
\begin{itemize} \small
\item $P(X\ge 3) \pause = 1-P(X\le 2) \pause = 1$ \pause
\item $P(X\ge 7) = 1-P(X\le 6) = 0.722$.
\item $P(X\ge 9) = 1-P(X\le 8) = 0.278$.
\end{itemize}

\vspace{0.2in} \pause

If we multiply the probability by the \\
number of victory points,\pause \\
then we have the ``expected points'': \pause \\
\begin{itemize} \small
\item $1\times P(X\ge 3) = 1$ \pause
\item $3\times P(X\ge 7) = 2.17$.
\item $4\times P(X\ge 9) = 1.11$.
\end{itemize}
\end{frame}




\subsection{Expectation}
\begin{frame}[t]
\frametitle{Expectation}

Let $X$ be a random variable and $h$ be some function.
The \alert{expected value} of a function of a (discrete) random variable is
\[
E[h(X)] = \sum_i h(x_i) \cdot p_X(x_i). \hspace{2in}
\]

{\bf Intuition:}
Expected values are \emph{weighted averages}\\of the possible values weighted by their probability.

\vspace{0.1in} \pause

If $h(x)=x$, then
\[
E[X] = \sum_i x_i \cdot p_X(x_i) \hspace{2in}
\]
\pause
and we call this the \alert{expectation} of $X$ \\
and commonly use the symbol $\mu$ for the expectation.
\end{frame}




\begin{frame}[t,fragile]
\frametitle{Dragonwood (cont.)}

What is the expectation of the sum of 3 Dragonwood dice?
<<dragonwood_expectation, dependson='dragonwood', echo=TRUE>>=
expectation = with(dragonwood3, sum(x*pmf))
expectation
@

\pause

The expectation can be thought of as the \alert{center of mass}
if we place mass $p_X(x)$ at corresponding points $x$.

\vspace{0.1in} \pause

<<dragonwood_center_of_gravity, dependson='dragonwood_expectation', fig.height=2>>=
ggplot(dragonwood3 %>% mutate(y=0), aes(x=x, xend=x, y=y, yend=pmf)) +
	geom_segment(linewidth = 2) +
  geom_point(data = data.frame(x=7.5,y=-0.01,pmf=0), aes(x,y), shape=24, color='red', fill='red') +
	scale_x_continuous(breaks=3:12) +
  theme_bw()
@
\end{frame}



\begin{frame}[t]
\frametitle{Biased coin}

Suppose we have a biased coin represented by the following pmf:
\[ \begin{array}{ccc}
\hline
y & 0 & 1 \\
p_Y(y) & 1-p & p \\
\hline
\end{array} \]
\pause
What is the expected value? % E[Y] = p

\vspace{0.1in} \pause

If $p=0.9$,
<<biased_coin, fig.height=3, fig.width = 8, out.width = "0.6\\textwidth", fig.align="left">>=
d = data.frame(x=c(0,1), pmf=c(0.1,0.9))

ggplot(d %>% mutate(y=0), aes(x=x, xend=x, y=y, yend=pmf)) +
	geom_segment(linewidth = 2) +
  geom_point(data = data.frame(x=0.9,y=-0.04,pmf=0), aes(x,y), shape=24, color='red', fill='red') +
	scale_x_continuous(breaks=0:1) +
	labs(x='y', y='pmf') +
  theme_bw()
@
\end{frame}


\subsection{Properties of expectations}
\begin{frame}[t]
\frametitle{Properties of expectations}

Let $X$ and $Y$ be random variables and $a$, $b$, and $c$ be constants. \pause
Then
\[ \begin{array}{rl}
E[aX + bY+c] &= aE[X] + bE[Y] + c.
\end{array} \]

\pause

In particular
\begin{itemize}[<+->]
\item $E[X+Y] = E[X]+E[Y]$,
\item $E[aX] = a E[X]$, and
\item $E[c] = c$.
\end{itemize}

\end{frame}



\begin{frame}[t]
\frametitle{Dragonwood (cont.)}
\small
Enhancement cards in Dragonwood allow you to improve your rolls. \pause Here are two enhancement cards:
\begin{itemize} \small
\item \emph{Cloak of Darkness} adds 2 points to all capture attempts \pause and
\item \emph{Friendly Bunny} allows you (once) to roll an extra die.
\end{itemize}

\pause

What is the expected attack roll total if you had 3 Dragonwood dice, the Cloak of Darkness, and are using the Friendly Bunny?

\vspace{0.1in} \pause

Let
\begin{itemize} \footnotesize
\item $X$ be the sum of 3 Dragonwood dice (we know $E[X]=7.5$),
\item $Y$ be the sum of 1 Dragonwood die \pause which has $E[Y] = 2.5$.
\end{itemize}

\pause

Then the attack roll total is $X+Y+2$ and the\\\emph{expected} attack roll total is
\[
E[X+Y+2] = E[X]+E[Y]+2 = 7.5+2.5+2 = 12. \hspace{2.5in}
\]

\end{frame}



\subsection{Variance}
\begin{frame}[t]
\frametitle{Variance }

The \alert{variance} of a random variable is defined as the
expected squared deviation from the mean. \pause
For discrete random variables, variance is
\[
Var[X] = E[(X-\mu)^2] = \sum_i (x_i-\mu)^2 \cdot p_X(x_i) \hspace{2in}
\]
where $\mu = E[X]$. \pause
The symbol $\sigma^2$ is commonly used for the variance.


The variance is analogous to \alert{moment of intertia} in classical mechanics.

\vspace{0.2in} \pause

The \alert{standard deviation} (sd) is the positive square root\\of the
variance:
\[
SD[X] = \sqrt{Var[X]}. \hspace{2in}
\]
\pause
The symbol $\sigma$ is commonly used for sd.
\end{frame}


\begin{frame}[t]
\frametitle{Properties of variance}

Two discrete random variables $X$ and $Y$ are \alert{independent} if
\[
p_{X,Y}(x,y) = p_X(x)p_Y(y).
\]

\pause

\alert{If $X$ and $Y$ are independent},
and $a$, $b$, and $c$ are constants, \pause
then
\[
Var[aX+bY+c] = a^2 Var[X] + b^2Var[Y].
\]

\vspace{0.1in} \pause

Special cases:
\begin{itemize}
\item $Var[c] = 0$
\item $Var[aX] = a^2 Var[X]$
\item $Var[X+Y] = Var[X] + Var[Y]$\\(if $X$ and $Y$ are independent)
\end{itemize}

\end{frame}



\begin{frame}[t,fragile]
\frametitle{Dragonwood (cont.)}

What is the variance for the sum of the 3 Dragonwood dice?

<<dragonwood_variance, dependson='dragonwood_expectation', echo=TRUE>>=
variance = with(dragonwood3, sum((x-expectation)^2*pmf))
variance
@

\pause

What is the standard deviation for the sum of the pips on 3 Dragonwood dice?
<<dragonwood_standard_deviation, dependson='dragonwood_variance', echo=TRUE>>=
sqrt(variance)
@
\end{frame}



\begin{frame}[t]
\frametitle{Biased coin}

Suppose we have a biased coin represented by the following pmf:
\[ \begin{array}{ccc}
\hline
y & 0 & 1 \\
p_Y(y) & 1-p & p \\
\hline
\end{array} \]
\pause
What is the variance?

\pause

\begin{enumerate}
\item $E[Y] = p$
\item $Var[Y] = (0-p)^2(1-p) + (1-p)^2\times p = p-p^2 = p(1-p)$
\end{enumerate}

\pause
When is this variance maximized?

\pause
<<biased_coin_variance, fig.height=3, fig.width = 8, out.width="0.6\\textwidth", fig.align="left">>=
opar = par(mar=c(4,4,1,0)+.1)
variance = function(p) p*(1-p)
curve(variance, xlab='y', ylab='variance')
par(opar)
@
\end{frame}




\section{Discrete distributions}
\begin{frame}[t]
\frametitle{Special discrete distributions}

\begin{itemize}
\item Bernoulli
\item binomial
\item Poisson
\end{itemize}

\vspace{0.1in} \pause

Note: The range is always finite or countable.

\end{frame}


\subsection{Bernoulli}
\begin{frame}[t]
\frametitle{Bernoulli random variables}

\pause

A Bernoulli experiment has only two outcomes: success/failure.

\vspace{0.1in} \pause

Let
\begin{itemize}
\item $X=1$ represent success and
\item $X=0$ represent failure.
\end{itemize}

\vspace{0.1in} \pause

The probability mass function $p_X(x)$ is
\[
p_X(0) = 1-p \quad p_X(1) = p. \hspace{2in}
\]

\vspace{0.1in} \pause

We use the notation $X\sim Ber(p)$ to denote a random\\
variable $X$ that follows a Bernoulli distribution\\
with success probability $p$, i.e. $P(X=1)=p$.
\end{frame}


\begin{frame}[t]
\frametitle{Bernoulli experiment examples}

\begin{itemize}
\item Toss a coin: $\Omega = \{Heads,Tails\}$ \pause
% \item Win/Loss outcome of a HootOwlHoot! game
\item Throw a fair die and ask if the face value is a six:
$\Omega = \{{\text{face value is a six},\text{face value is not a six}}\}$ \pause
\item Send a message through a network and record whether or not it is received:
$\Omega = \{ \text{successful transmission}, \text{ unsuccessful transmission}\}$ \pause
\item Draw a part from an assembly line and record whether or not it is
defective: $\Omega = \{\text{defective}, \text{ good}\}$ \pause
\item Response to the question\\
``Are you in favor of an increased in property tax\\
xto pay for a new high school?'':\\
$\Omega = \{\text{yes}, \text{no}\}$
\end{itemize}
\end{frame}



\begin{frame}[t]
\frametitle{Bernoulli random variable (cont.)}

The cdf of the Bernoulli random variable is
\[
F_X(x) =P(X\leq x)= \left \{
\begin{array}{cl}
    0 & x < 0 \\
    1-p & 0 \le x < 1 \\
    1 & 1 \le x
\end{array}
\right .
\]

\vspace{0.1in} \pause

The expected value is
\[
E[X]=\sum\limits_{x} p_X(x)=0\cdot (1-p)+1\cdot p=p. \hspace{2in}
\]

\vspace{0.1in} \pause

The variance is
\[ \begin{array}{rl}
Var[X]&=\sum\limits_{x}(x-E[X])^2p_X(x)\\
&= (0-p)^2\cdot (1-p)+(1-p)^2 \cdot p \\
&= p (1-p).
\end{array}
\hspace{2in} \]

\end{frame}




\begin{frame}[t]
\frametitle{Sequence of Bernoulli experiments}

An experiment consisting of $n$ \alert{independent and identically distributed} Bernoulli experiments.

\vspace{0.1in} \pause
Examples:
\begin{itemize}
\item Toss a coin $n$ times and record the nubmer of heads. \pause
\item Send 23 identical messages through the network independently and record the number successfully received. \pause
\item Draw 5 cards from a standard deck with replacement (and reshuffling) and record whether or not the card is a king.
\end{itemize}

\end{frame}



\begin{frame}[t]
\frametitle{Independent and identically distributed}

\small

\vspace{-0.1in}

Let $X_i$ represent the $i^{th}$ Bernoulli experiment.

\vspace{0.1in} \pause

\alert{Independence} means
\[
p_{X_1,\ldots,X_n}(x_1,\ldots,x_n) = \prod_{i=1}^n p_{X_i}(x_i), \hspace{2in}
\]
i.e. the joint probability is the product of the individual probabilities.

\vspace{0.1in} \pause

\alert{Identically distributed} (for Bernoulli random variables) means
\[
P(X_i=1) = p \quad \forall\, i, \hspace{2in}
\]
\pause
and more generally, the distribution is the same for all\\
the random variables.

\vspace{0.1in} \pause

\begin{itemize}
\item $iid$: independent and identically distributed
\item $ind$: independent
\end{itemize}
\end{frame}



\begin{frame}[t]
\frametitle{Sequences of Bernoulli experiments}

Let $X_i$ denote the outcome of the $i^{th}$ Bernoulli experiment. \pause
We use the notation
\[
X_i \iid Ber(p), \quad \mbox{for }i=1,\ldots,n
\]
to indicate a sequence of $n$ independent and identically distributed Bernoulli experiments.

\vspace{0.1in}\pause

We could write this equivalently as
\[
X_i \ind Ber(p), \quad \mbox{for }i=1,\ldots,n \hspace{2in}
\]
\pause
but this is different than
\[
X_i \ind Ber(p_i), \quad \mbox{for }i=1,\ldots,n \hspace{2in}
\]
as the latter has a different success probability for each\\
experiment.

\end{frame}


\subsection{Binomial}
\begin{frame}[t]
\frametitle{Binomial random variable}
\pause
Suppose we perform a sequence of $n$ $iid$ Bernoulli experiments and only record the number of successes, i.e.
\[
Y = \sum_{i=1}^n X_i.
\]

\vspace{0.1in} \pause

Then we use the notation $Y\sim Bin(n,p)$ to indicate a binomial random variable with
\begin{itemize}
\item $n$ attempts and
\item probability of success $p$.
\end{itemize}
\end{frame}


\begin{frame}[t]
\frametitle{Binomial probability mass function}

We need to obtain
\[
p_Y(y) = P(Y=y) \quad \forall \, y\in \Omega \pause = \{0,1,2,\ldots,n\}.
\]

\pause
The probability of obtaining a particular sequence of $y$ success and $n-y$ failures is
\[
p^y(1-p)^{n-y} \hspace{2in}
\]
since the experiments are $iid$ with success probability $p$. \pause But there are
\[
{n\choose y} = \frac{n!}{y!(n-y)!} \hspace{2in}
\]
ways of obtaining a sequence of $y$ success and $n-y$\\
failures.
\pause
Thus, the binomial pmf is
\[
p_Y(y) = P(Y=y) = {n\choose y} p^y(1-p)^{n-y}. \hspace{2in}
\]

\end{frame}


\begin{frame}[t]
\frametitle{Properties of binomial random variables}

\vspace{0.1in}

The expected value is
\[
E[Y]
\pause
= E\left[ \sum_{i=1}^n X_i \right]
\pause
= \sum_{i=1}^n E[X_i] \pause= \sum_{i=1}^n p = np.  \hspace{2in}
\]

\vspace{0.1in} \pause

The variance is
\[
Var[Y]
\pause
% = Var\left[ \sum_{i=1}^n X_i \right]
= \sum_{i=1}^n Var[X_i]
% = \sum_{i=1}^n p(1-p)
= np(1-p) \hspace{2in}
\]
since the $X_i$ are independent.

\vspace{0.1in} \pause

The cumulative distribution function is
\[
F_Y(y) = P(Y\le y) = \sum_{x=0}^{\lfloor y\rfloor} {n\choose x} p^x(1-p)^{n-x}.
\hspace{2in}
\]

\end{frame}



\begin{frame}[t]
\frametitle{Component failure rate}

Suppose a box contains 15 components that each have a failure rate of 5\%.

\vspace{0.1in} \pause

What is the probability that
\begin{enumerate}
	\item exactly two out of the fifteen components are defective? \pause
	\item at most two components are defective? \pause
	\item more than three components are defective? \pause
	\item more than 1 but less than 4 are defective?
\end{enumerate}

\end{frame}


\begin{frame}[t,fragile]
\frametitle{Binomial pmf}

Let $Y$ be the number of defective components
\pause
and assume $Y\sim Bin(15,0.05)$.

\vspace{0.4in} \pause

\begin{columns}
\begin{column}{0.6\textwidth}
<<binomial_pmf>>=
n = 15
p = 0.05
d <- data.frame(x = 0:n) %>%
  dplyr::mutate(pmf = dbinom(x, n, p))

ggplot(d, aes(x, pmf, yend = 0, xend = x)) +
  geom_point() +
  geom_segment() +
  labs(title = paste("Binomial pmf with", n, "attempts and probability ", p),
        x = "Value", y="Probability mass function") +
  theme_bw()
@
\end{column}
\begin{column}{0.4\textwidth}
\end{column}
\end{columns}
\end{frame}


\begin{frame}[t]
\frametitle{Component failure rate - solutions}
Let $Y$ be the number of defective components
and assume $Y\sim Bin(15,0.05)$.
\pause

\[ \begin{array}{lll}
1. & P(Y=2)&= {15\choose 2} (0.05)^2(1-0.05)^{15-2} \pause \\
2. & P(Y\le 2) &= \sum_{x=0}^2 {15\choose x} (0.05)^x(1-0.05)^{15-x} \pause \\
3. & P(Y>3) &= 1-P(Y\le 3) = 1-\sum_{x=0}^3 {15\choose x} (0.05)^x(1-0.05)^{15-x} \pause \\
4. & P(1<Y<4) &= \sum_{x=2}^3 {15\choose x} (0.05)^x(1-0.05)^{15-x} \\
\end{array}
\hspace{2in} \]

\end{frame}


\begin{frame}[t,fragile]
\frametitle{Component failure rate - solutions in R}

\begin{columns}
\begin{column}{0.6\textwidth}
<<component_failure_rate,echo=TRUE>>=
n <- 15
p <- 0.05
choose(15,2)
dbinom(2,n,p)           # P(Y=2)
pbinom(2,n,p)           # P(Y<=2)
1-pbinom(3,n,p)         # P(Y>3)
sum(dbinom(c(2,3),n,p)) # P(1<Y<4) = P(Y=2)+P(Y=3)
@
\end{column}
\begin{column}{0.4\textwidth}
\end{column}
\end{columns}
\end{frame}




\subsection{Poisson}
\begin{frame}[t]
\frametitle{Poisson experiments}

Many experiments can be thought of as
``how many \emph{rare} events will occur in a certain amount of time or space?''
\pause
For example,

\begin{itemize}
\item \# of alpha particles emitted from a polonium bar in an 8 minute period \pause
\item \# of flaws on a standard size piece of manufactured product, e.g., 100m coaxial cable, 100 sq.meter plastic sheeting \pause
\item \# of hits on a web page in a 24h period
\end{itemize}

\end{frame}




\begin{frame}[t]
\frametitle{Poisson random variable}
A Poisson random variable has pmf
\[
p(x) = \frac{e^{-\lambda}\lambda^{x}}{x!} \quad \text{ for } x = 0,1,2,3,\ldots
\hspace{2in}
\]
where $\lambda$ is called the \alert{rate parameter}.

\vspace{0.1in} \pause

We write $X\sim Po(\lambda)$ to represent this random variable. \pause
We can show that
\[ E[X] = Var[X] = \lambda. \hspace{2in}
\]

\end{frame}





\begin{frame}[t,fragile]
\frametitle{Poisson probability mass function}

Customers of an internet service provider initiate new accounts at the average
rate of 10 accounts per day. \pause
What is the probability that more than 8 new accounts will be initiated today? \pause

\begin{columns}
\begin{column}{0.6\textwidth}
<<poisson_pmf>>=
lambda <- 10
n = 30
d <- data.frame(x = 0:n) %>%
  dplyr::mutate(pmf = dpois(x, lambda))

ggplot(d, aes(x, pmf, yend = 0, xend = x)) +
  geom_point() +
  geom_segment() +
  labs(title = paste("Poisson pmf with mean of", lambda),
        x = "Value", y="Probability mass function") +
  theme_bw()
@
\end{column}
\begin{column}{0.4\textwidth}
\end{column}
\end{columns}
\end{frame}



\begin{frame}[t,fragile]
\frametitle{Poisson probability}

\small

Customers of an internet service provider initiate new accounts at the average
rate of 10 accounts per day. \pause
What is the probability that more than 8 new accounts will be initiated today?

\vspace{0.1in}

Let $X$ be the number of accounts initiated today. \pause
Assume $X\sim Po(10)$. \pause
\[ P(X>8) = 1-P(X\le 8) = 1- \sum_{x=0}^8 \frac{\lambda^x e^{-\lambda}}{x!} \approx 1-0.333 = 0.667 \]
\pause
\begin{columns}
\begin{column}{0.6\textwidth}
In R,
<<echo=TRUE>>=
# Using pmf
1-sum(dpois(0:8, lambda=10))
# Using cdf
1-ppois(8, lambda=10)
@
\end{column}
\begin{column}{0.4\textwidth}
\end{column}
\end{columns}
\end{frame}



\begin{frame}[t]
\frametitle{Sum of Poisson random variables}

Let $X_i\ind Po(\lambda_i)$ for $i=1,\ldots,n$. \pause Then
\[
Y = \sum_{i=1}^n X_i \sim Po\left( \sum_{i=1}^n \lambda_i \right). \hspace{2in}
\]

\pause

Let $X_i\iid Po(\lambda)$ for $i=1,\ldots,n$. \pause Then
\[
Y = \sum_{i=1}^n X_i \sim Po\left( n\lambda \right). \hspace{2in}
\]

\end{frame}


\begin{frame}[t,fragile]
\frametitle{Poisson random variable - example}

\small

Customers of an internet service provider initiate new accounts at the average
rate of 10 accounts per day. \pause
What is the probability that more than 16 new accounts will be initiated in the
next two days?

\vspace{0.1in} \pause

Since the rate is 10/day, then for two days we expect, on average, to have 20.
\pause
Let $Y$ be he number initiated in a two-day period and assume $Y\sim Po(20)$.
\pause
Then
\[ \begin{array}{rl}
P(Y>16) &= 1-P(Y\le 16) \\
&= 1-\sum_{x=0}^{16} \frac{\lambda^x e^{-\lambda}}{x!} \\
&= 1-0.221 = 0.779.
\end{array}
\hspace{2in}
\]
\begin{columns}
\begin{column}{0.6\textwidth}
In R,
<<echo=TRUE>>=
# Using pmf
1-sum(dpois(0:16, lambda=20))
# Using cdf
1-ppois(16, lambda=20)
@
\end{column}
\begin{column}{0.4\textwidth}
\end{column}
\end{columns}
\end{frame}



\subsection{Poisson approximation to a binomial}
\begin{frame}[t]
\frametitle{Manufacturing example}

A manufacturer produces 100 chips per day and, on average, 1\% of these chips are defective. \pause
What is the probability that no defectives are found in a particular day?

\vspace{0.1in} \pause

Let $X$ represent the number of defectives and assume $X\sim Bin(100,0.01)$. \pause
Then
\[
P(X=0) = {100\choose 0}(0.01)^0(1-0.01)^{100} \approx 0.366.
\]
\pause
Alternatively,
let $Y$ represent the number of defectives\\and assume $Y\sim Po(100\times 0.01)$. \pause
Then
\[
P(Y=0) = \frac{1^0 e^{-1}}{0!} \approx 0.368. \hspace{2in}
\]
\end{frame}



\begin{frame}[t,fragile]
\frametitle{Poisson approximation to the binomial}

Suppose we have $X \sim Bin(n,p)$ with $n$ large (say $\ge 20$) and $p$ small
(say $\le 0.05$). \pause
We can approximate $X$ by $Y\sim Po(np)$ \pause
because for large $n$ and small $p$
\[
{n \choose k} p^{k}(1-p)^{n-k} \approx e^{-np}\frac{(np)^{k}}{k!}.
% \hspace{2in}
\]

\begin{columns}
\begin{column}{0.6\textwidth}
<<poisson_vs_binomial, fig.width=8>>=
n = 5
p = 0.01
e = 0.01
d = data.frame(x = 0:n) %>%
  dplyr::mutate(binomial = dbinom(x, n, p),
                Poisson = dpois(x, n*p)) %>%
  tidyr::gather("Distribution","pmf", binomial, Poisson) %>%
  dplyr::mutate(x = x-e + 2*e*(Distribution == "Poisson"))

ggplot(d, aes(x=x, y = pmf, yend = 0, xend = x,
              color = Distribution, linetype = Distribution)) +
  geom_point() +
  geom_segment() +
  labs(title = "Poisson vs binomial", x = "Value",
       y = "Probability mass function") +
  theme_bw()
@
\end{column}
\begin{column}{0.4\textwidth}
\end{column}
\end{columns}


\end{frame}



\begin{frame}[t,fragile]
\frametitle{Example}
Imagine you are supposed to proofread a paper.
Let us assume that there are on average 2 typos on a page and a page has 1000 words. \pause
This gives a probability of 0.002 for each word to contain a typo. \pause
What is the probability the page has no typos?

\vspace{0.1in} \pause

Let $X$ represent the number of typos on the page and assume $X\sim Bin(1000,0.002)$. \pause
$P(X=0)$ using R is
\begin{columns}
\begin{column}{0.6\textwidth}
<<typo_binomial, echo=TRUE>>=
n = 1000; p = 0.002
dbinom(0, size=n, prob=p)
@
Alternatively, let $Y$ represent the number of defectives and assume $Y\sim Po(1000\times 0.002)$. \pause
$P(Y=0)$ using R is
<<typo_poisson, echo=TRUE, dependson='typo_binomial'>>=
dpois(0, lambda = n*p)
@
\end{column}
\begin{column}{0.4\textwidth}
\end{column}
\end{columns}
\end{frame}



\begin{frame}[t]
\frametitle{Summary}
\begin{itemize}
\item General discrete random variables
  \begin{itemize}
  \item Probability mass function (pmf)
  \item Cumulative distribution function (cdf)
  \item Expected value
  \item Variance
  \item Standard deviation
  \end{itemize}
\item Specific discrete random variables
  \begin{itemize}
  \item Bernoulli
  \item Binomial
  \item Poisson
  \end{itemize}
\end{itemize}
\end{frame}


% \begin{frame}[t]
% \frametitle{Multiple random variables}
%
% \begin{definition}
% The \alert{joint probability mass function} of two random variables $X$ and $Y$ is
% \[
% p_{X,Y}(x,y) = P(X=x,Y=y).
% \]
% \pause
% The \alert{joint probability mass function} of a random vector $X=(X_1,\ldots,X_p)$ is
% \[
% p_X(x) = P(X=x).
% \]
% \end{definition}
%
% \end{frame}
%
%
% \subsection{Covariance}
% \begin{frame}[t]
% \frametitle{Covariance}
%
% \begin{definition}
% The \alert{covariance} between two random variables $X$ and $Y$ is
% \[ \begin{array}{rl}
% Cov[X,Y]
% &= E\left[ (X-E[X])(Y-E[Y]) \right] \\
% &= E[XY] - E[X]E[Y]
% \end{array} \]
% \end{definition}
%
% \pause
%
% \begin{definition}
% The \alert{correlation} coefficient between two random variables $X$ and $Y$ is
% \[ \begin{array}{rl}
% \rho = \frac{Cov[X,Y]}{\sqrt{Var[X] Var[Y]}} = \frac{Cov[X,Y]}{SD[X] SD[Y]}.
% \end{array} \]
% \pause
% If $rho=0$, we say that $X$ and $Y$ are uncorrelated.
% \end{definition}
%
% \end{frame}
%
%
%
%
%
% \subsection{Properties of variances}
% \begin{frame}[t]
% \frametitle{Properties of variance}
%
% \begin{theorem}
% Let $X$ and $Y$ be random variables and $a$, $b$, and $c$ be constants. \pause
% Then
% \[ \begin{array}{rl}
% Var[aX + bY+c] &= a^2Var[X] + b^2Var[Y] + 2ab Cov(X,Y).
% \end{array} \]
% \end{theorem}
%
% \pause
%
% % needs to be fixed
% % In particular
% % \begin{corollary}
% % \begin{itemize}[<+->]
% % \item $Var[X+Y] = Var[X]+Var[Y]$,
% % \item $Var[aX] = a Var[X]$, and
% % \item $Var[c] = c$.
% % \end{itemize}
% % \end{corollary}
%
% \end{frame}


\end{document}
