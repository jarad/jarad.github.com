\documentclass[t,aspectratio=169,handout]{beamer}

\usepackage{verbatim,multicol,amsmath}

\input{../../frontmatter}
\input{../../commands}

\title{R06 - ANOVA and F-tests}

% \setbeamertemplate{background}
% {\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{video_overlay}}

<<options, echo=FALSE, warning=FALSE, message=FALSE>>=
options(width=120)
opts_chunk$set(comment=NA, 
               fig.width=6, 
               fig.height=2.5, 
               size='tiny', 
               out.width='\\textwidth', 
               fig.align='center', 
               echo=FALSE,
               message=FALSE)
@

<<libraries, message=FALSE, warning=FALSE, echo=FALSE>>=
library("tidyverse"); theme_set(theme_bw())
library("Sleuth3")
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@


\begin{document}

\begin{frame}
\maketitle
\end{frame}

\section{Multi-group data}
\subsection{Assumptions}
\begin{frame}
\frametitle{One-way ANOVA model/assumptions}

The one-way ANOVA (ANalysis Of VAriance) model is 
\[ Y_{ig} \stackrel{ind}{\sim} N\left(\mu_g, \sigma^2\right)
\quad \mbox{or} \quad
Y_{ig} = \mu_g + \epsilon_{ig}, \, \epsilon_{ig} \iid N(0,\sigma^2)
\]
for $g=1,\ldots,G$ and $i=1,\ldots,n_g$. 

\vspace{0.2in} \pause

Assumptions:
\begin{itemize}
\item Errors 
  \begin{itemize} 
  \item are normally distributed.
  \item have a common variance.
  \item are independent. \pause
  \end{itemize}
\item Each group has its own mean.
\end{itemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{ANOVA assumptions graphically}

<<echo=FALSE>>=
set.seed(20190410)
d <- expand.grid(x = seq(-5, 5, length=101),
                 mean = round(sort(rnorm(6)),2)) %>%
  mutate(density = dnorm(x, mean),
         mean = paste0("mean = ", mean))

ggplot(d, aes(x, density, color=mean, linetype=mean)) + 
  geom_line() 
@

\end{frame}

% \begin{frame}
% \frametitle{What if you want to compare two groups?}
% 
% We may still be interested in comparing two groups. 
% 
% \vspace{0.2in} \pause
% 
% Statistical hypothesis: Is there a difference in mean lifetimes between the mice in two groups, e.g. NP and N/N85?
% 
% \vspace{0.2in} \pause
% 
% Statistical question: What is the difference in mean lifetimes between the mice in two groups, e.g. NP and N/N85?
% 
% \end{frame}
% 
% 
% 
% 
% \begin{frame}
% \frametitle{Two-group analysis}
%   Begin with the two group (equal variance) model:
% 
% 	\[ Y_{ig} \stackrel{ind}{\sim} N\left(\mu_g, \sigma^2\right) \]
% 	
% 	\vspace{0.2in} \pause
% 	
%   with	$g=1,2$ and $i=1,\ldots,n_g$ \pause
%   
%   To perform a hypothesis test or a CI for the difference in means, \pause the relevant quantities are:
%   \begin{itemize}[<+->]
%   \item $\overline{Y}_2-\overline{Y}_1$
%   \item $SE(\overline{Y}_2-\overline{Y}_1) = s_p \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}$
%   \item $t$ distribution with $n_1+n_2-2$ degrees of freedom
%   \end{itemize}
%   \pause where 
%   \[ s_p^2 = \frac{ (n_1-1)s_1^2 + (n_2-1)s_2^2 }{n1+n_2-2} \]
%   \pause What if you have more than two groups?
% \end{frame}
% 
% 
% 
% \begin{frame}
% \frametitle{Multi-group analysis}
%   The multi-group (equal variance) model:
% 
%   \[ Y_{ig} \stackrel{ind}{\sim} N\left(\mu_g, \sigma^2\right) \]
% 	
% 	\vspace{0.2in} \pause
% 	
%   but now	$g=1,\alert{\ldots,G}$ and $i=1,\ldots,n_g$ \pause
% 	
% 	{\color{gray}($n_g$ means there can be different \# of observations in each group)} \pause
%   
%   To perform a hypothesis test or a CI for the difference in means, \pause the relevant quantities are:
%   \begin{itemize}[<+->]
%   \item $\overline{Y}_2-\overline{Y}_1$
%   \item $SE(\overline{Y}_2-\overline{Y}_1) = s_p \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}$
%   \item $t$ distribution with $n_1+n_2\alert{+\cdots+n_G-G}$ degrees of freedom
%   \end{itemize}
%   \pause where 
%   \[ s_p^2 = \frac{ (n_1-1)s_1^2 + (n_2-1)s_2^2 \alert{+ \cdots + (n_G-1)s_G^2} }{n1+n_2\alert{+\cdots+n_G-G}} \]
% \end{frame}
% 
% 
% 
% 
% 
% \begin{frame}
% \frametitle{Hypothesis test for comparison of two means (in multi-group data)}
%   If $Y_{ig} \stackrel{ind}{\sim} N(\mu_g,\sigma^2)$ for $g=1,\ldots,G$ and we want to test the hypothesis
% 	\begin{itemize}
% 	\item $H_0: \mu_1=\mu_2$
% 	\item $H_1: \mu_1\ne \mu_2$
% 	\end{itemize}
% 	\pause then we compute:
% 	\[ t = \frac{\overline{Y}_1-\overline{Y}_2}{SE(\overline{Y}_1-\overline{Y}_2)} \]
% 	\pause where 
% 	\[ SE(\overline{Y}_1-\overline{Y}_2) = s_p \sqrt{\frac{1}{n_1}+\frac{1}{n_2}} \]
% 	\pause and
% 	\[ s_p^2 = \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2+\cdots+(n_G-1)s_G^2}{n_1+n_2+\cdots+n_G - G }. \]
%   \pause 
%   Then we compare $t$ to a $t$ distribution with $n_1+n_2+\cdots+n_G-G$ degrees of freedom. 
% \end{frame}
% 
% 
% 
% \subsection{Example}
% \begin{frame}[fragile]
% \frametitle{Diet effect on mice lifetime}
% <<echo=FALSE>>=
% sm <- Sleuth3::case0501 %>%
%   group_by(Diet) %>%
%   summarize(n = n(),
%             mean = mean(Lifetime),
%             sd = sd(Lifetime))
% sm
% @
% 
% \pause
% 
% Test for difference in mean lifetime between NP and N/N85, i.e. 
% \[ H_0: \mu_4=\mu_1 \mbox{ vs } H_1: \mu_4\ne \mu_1. \]
% \end{frame}
% 
% 
% \begin{frame}
% \frametitle{Showing work}
% 
% {\small
% \[ \begin{array}{rl}
% \overline{Y}_1-\overline{Y}_4 &= 32.7-27.4 = 5.3 \\
% df &= 57+60+71+49+56+56-6 = 343 \\
% s_p^2 &= \frac{(57-1)5.1^2 + (60-1)6.7^2+(71-1)7.8^2+(49-1)6.1^2+(56-1)6.7^2+(56-1)7.0^2}{57+60+71+49+56+56-6} \\
% &= \frac{15314}{343} = 44.6 \\
% s_p &= \sqrt{s_p^2} = \sqrt{44.6} = 6.7 \\
% SE(\overline{Y}_1-\overline{Y}_4) &= s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_4}} = 6.7\sqrt{\frac{1}{57}+\frac{1}{49}} = 1.3 \\
% t &= \frac{\overline{Y}_1-\overline{Y}_4}{SE(\overline{Y}_1-\overline{Y}_4) } =\frac{5.3}{1.2} = 4.1 \\
% p &= 2P(t_{343}<-|t|) = 2P(t_{343}<-4.1) = 0.000052 
% \end{array} \]
% }
% So we reject the null hypothesis that there is no difference between mean lifetime of mice on the NP and N/N85 diets.
% 
% \end{frame}



% \begin{frame}[fragile]
% <<echo=FALSE>>=
% sm = ddply(case0501, .(Diet), summarize,
%           n = length(Lifetime),
%           mean = mean(Lifetime),
%           sd = sd(Lifetime))
% sm
% @
% 
% \pause 
% 
% <<>>=
% g1 = which(sm$Diet == "NP")
% g2 = which(sm$Diet == "N/N85")
% (sp = with(sm, sqrt(sum((n-1)*sd^2)/sum(n-1))))
% (se = sp*sqrt(1/sm$n[g1]+1/sm$n[g2]))
% (t = (sm$mean[g1]-sm$mean[g2])/se)
% (p = 2*pt(-abs(t), sum(sm$n-1)))
% @
% 
% confidence interval
% 
% <<>>=
% (t_critical_value = qt(.975,sum(sm$n-1)))
% (sm$mean[g1]-sm$mean[g2]) + c(-1,1)*t_critical_value*se
% @
% 
% \end{frame}



% \begin{frame}[fragile]
% \frametitle{Confidence interval for the difference of two means (in multi-group data)}
% 
%   If $Y_{ig} \stackrel{ind}{\sim} N(\mu_g,\sigma^2)$ for $g=1,\ldots,G$, a $100(1-\alpha)$\% confidence interval for $\mu_1-\mu_2$ is 
% 	\pause 
% 	\[ \overline{Y}_1-\overline{Y}_2 \pm t_{n_1+n_2+\cdots+n_G-G}(1-\alpha/2) SE(\overline{Y}_1-\overline{Y}_2) \]
% 	\pause where the $t$ critical value, $t_{n_1+n_2+\cdots+n_G-G}(1-\alpha/2)$, needs to be calculated using a statistical software.
% 
%   \vspace{0.2in} \pause
%   
%   A 95\% confidence interval for the difference in mean lifetime for N/N85 minus NP ($\mu_1-\mu_4$) is 
%   \[ 5.3 \pm 1.96\times 1.3 = (2.8,7.8). \]
%   The statistical conclusion would be 
%   \begin{quote}
%   In this study, mice on the N/N85 diet lived an average of 5.3 months longer than mice on the NP diet (95\% CI (2.8,7.8)). 
%   \end{quote}
% 
% \end{frame}







\subsection{One-way ANOVA F-test}
\begin{frame}[fragile]
\frametitle{Consider the mice data set}

<<echo=FALSE>>=
ggplot(Sleuth3::case0501, aes(x = Diet, y = Lifetime)) + 
  geom_jitter() 
  # geom_hline(data=sm2, aes(yintercept=mean[7]), 
  #            col='red', linewidth = 2) + 
  # geom_errorbar(data=sm, aes(y=mean, ymin=mean, ymax=mean), 
  #               col='blue', linewidth = 2) 
@

\end{frame}


\begin{frame}[fragile]
\frametitle{One-way ANOVA F-test}

  Are any of the means different?

  \vspace{0.2in} \pause
  
  Hypotheses in English:
  \begin{itemize}
  \item[$H_0$:] all the means are the same
  \item[$H_1$:] at least one of the means is different
  \end{itemize}

  \vspace{0.1in} \pause

  Statistical hypotheses:
	\[ \begin{array}{ll@{\qquad}l}
	H_0: & \mu_g=\mu \mbox{ for all } g & \uncover<3->{Y_{ig} \stackrel{iid}{\sim} N(\mu, \sigma^2)} \\
	H_1: & \mu_g\ne\mu_{g'} \mbox{ for some $g$ and $g'$} & \uncover<3->{Y_{ig} \stackrel{ind}{\sim} N\left(\mu_g, \sigma^2\right)}
	\end{array}	\]
  
  \pause
  
  An ANOVA table organizes the relevant quantities for this test and computes the pvalue. 

\end{frame}



\subsection{ANOVA table}
\frame{\frametitle{ANOVA table}
\footnotesize
	A start of an ANOVA table:
	\[ \begin{array}{llcc}
	\mbox{Source of variation} & 
	\mbox{Sum of squares} &
	\mbox{d.f.} & \mbox{Mean square}  \\
	\hline
	\mbox{Factor A (Between groups)} &  
	SSA=\sum_{g=1}^{G} n_g \left(\overline{Y}_g-\overline{Y}\right)^2 & 
	G-1 & 
	\frac{SSA}{G-1} \phantom{\left(=\hat\sigma^2\right)} \\
	\mbox{Error (Within groups)} & 
	SSE=\sum_{g=1}^{G} \sum_{i=1}^{n_g} \left(Y_{ig}-\overline{Y}_g\right)^2 & 
	n-G &  
	\frac{SSE}{n-G} \left(=\hat\sigma^2\right) \\
	\hline
	\mbox{Total} & 
	SST=\sum_{g=1}^{G} \sum_{i=1}^{n_g} \left(Y_{ig}-\overline{Y}\right)^2 &
	n-1 \\
	\end{array} \]
	where 
	
	\pause
	
	\begin{itemize}
	\item $G$ is the number of groups, \pause 
	\item $n_g$ is the number of observations in group $g$, \pause
	\item $n=\sum_{g=1}^G n_g$ (total observations), \pause
	\item $\overline{Y}_g = \frac{1}{n_g}\sum_{i=1}^{n_g} Y_{ig}$ (average in group $g$), \pause
	\item and $\overline{Y} = \frac{1}{n}\sum_{g=1}^G \sum_{i=1}^{n_g} Y_{ig}$ (overall average).
	\end{itemize}
}

\frame{\frametitle{ANOVA table}
  {\footnotesize
	An easier to remember ANOVA table:
	
	\vspace{0.1in}
	
	\begin{tabular}{llcccc}
	Source of variation & Sum of squares & df & Mean square & F-statistic & p-value \\
	\hline
	Factor A (between groups) & SSA & $G-1$ & MSA = SSA/$G-1$ & MSA/MSE & \mbox{(see below)} \\
	Error (within groups) & SSE & $n-G$ & MSE = SSE/$n-G$ \\
	\hline
	Total & SST = SSA+SSE & $n-1$ 
	\end{tabular}
	}
	
	\vspace{0.3in}
	 
	 \pause
	 

	Under $H_0$ ($\mu_g = \mu$), 
	\begin{itemize}
	\item the quantity MSA/MSE has an F-distribution with $G-1$ numerator and $n-G$ denominator degrees of freedom, \pause
	\item larger values of MSA/MSE indicate evidence against $H_0$, \pause and
	\item the p-value is determined by $P(F_{G-1,n-G}>MSA/MSE)$.
	\end{itemize}

}


\begin{frame}[fragile]
\frametitle{F-distribution}

$F$-distribution has two parameters: 
\begin{itemize}
\item numerator degrees of freedom (ndf)
\item denominator degrees of freedom (ddf)
\end{itemize}

<<F-distribution, echo=FALSE, fig.height = 2>>=
dfs = c(5,300)
ggplot(data.frame(x = c(0,4)), aes(x=x)) + 
  stat_function(fun = df, 
                args = list(df1 = dfs[1], 
                            df2 = dfs[2]), 
                xlim = c(2,4), 
                geom = "area", 
                fill = "magenta") +
  stat_function(fun = df, 
                args = list(df1 = dfs[1], 
                            df2 = dfs[2])) +
  labs(title = paste0("F(", dfs[1], ", ", dfs[2], ")"),
       x = "F", y = "density") 
@

\end{frame}



\begin{frame}[fragile]
\frametitle{One-way ANOVA F-test (by hand)}
{\tiny 
<<echo=FALSE, warning=FALSE>>=
sm <- Sleuth3::case0501 %>%
  group_by(Diet) %>%
  summarize(n    = n(),
            mean = mean(Lifetime),
            sd   = sd(Lifetime))
total <- Sleuth3::case0501 %>% 
  summarize(n    = n(), 
            mean = mean(Lifetime),
            sd   = sd(Lifetime)) %>%
  mutate(Diet = "Total")
(sm2 <- bind_rows(sm,total))
@

\pause 


So 
\[ \begin{array}{rl}
SSA =& 57\times (32.7 - 38.8)^2 + 60\times (45.1 - 38.8)^2 + 71\times (42.3 - 38.8)^2 + 49\times (27.4-38.8)^2 \\
&+ 56\times (42.9 - 38.8)^2 + 56\times(39.7 - 38.8)^2 = 12734  \\
% SST =& (35.5-38.8)^2 + (35.4-38.8)^2 + (34.9-38.8)^2 + \cdots + (19.6-38.8)^2 + (47.6-38.8)^2 = 28031 \pause \\
SST = & (349 - 1)\times 8.97^2 = 28000  \\
SSE =& SST - SSA = 28000 - 12734 = 15266 \\
G-1 =& 5 \\
n-G =& 349 - 6 = 343 \\
n-1 =& 348 \\
MSA =& SSA / G-1 = 12734 / 5 = 2547 \\
MSE =& SSE / n-G = 15266 / 343 = 44.5 = \hat\sigma^2 \\
F =& MSA / MSE = 2547 / 44.5 = 57.2 \\
p =& P(F_{5,343} > 57.2) < 0.0001
\end{array} \]
\pause
F statistic is off by 0.1 relative to the table later, 
because of rounding of 8.97. 
The real SST is 28031 which would be the F statistic of 57.1. 

}
\end{frame}




\begin{frame}[fragile]
\frametitle{Graphical comparison}

<<echo=FALSE>>=
ggplot(Sleuth3::case0501, aes(x = Diet)) + 
  geom_jitter(aes(y = Lifetime), 
              size = 3) + 
  geom_hline(data = sm2, 
             aes(yintercept = mean[7]), 
             col = 'red', 
             linewidth = 2) + 
  geom_errorbar(data = sm, 
                aes(x = Diet, ymin = mean, ymax = mean), 
                col = 'blue', 
                linewidth = 2)
@

\end{frame}





\begin{frame}[fragile]
\frametitle{R code and output for one-way ANOVA}
<<echo=TRUE>>=
m <- lm(Lifetime ~ Diet, case0501)
anova(m)
@

\pause

There is evidence against the null model $Y_{ig} \ind N(\mu,\sigma^2)$,
i.e. our data seem incompatible with this model.
\end{frame}




\section{General F-tests}
\begin{frame}
\frametitle{General F-tests}

The one-way ANOVA F-test is an example of a general hypothesis testing framework that uses F-tests. 
\pause 
This framework can be used to test 
\begin{itemize}
\item composite alternative hypotheses \pause or, equivalently,
\item a full vs a reduced model. \pause 
\end{itemize}

The general idea is to balance the amount of variability remaining when moving from the reduced model to the full model measured using the sums of squared errors (SSEs) relative to the amount of complexity, i.e. parameters, added to the model. 

\end{frame}


\subsection{Full vs Reduced Models}
% \frame{\frametitle{Simple vs Composite Hypotheses}
%   Suppose \[ Y_{ig} \stackrel{ind}{\sim} N(\mu_g,\sigma^2) \]
% 	for $g=1,\ldots,3$ \pause then a \alert{simple hypothesis} is 
% 	\begin{itemize}
% 	\item $H_0: \mu_1=\mu_2$
% 	\item $H_1: \mu_1\ne \mu_2$
% 	\end{itemize}
% 	\pause and a \alert{composite hypothesis} is 
% 	\begin{itemize}
% 	\item $H_0: \mu_1=\mu_2=\mu_3$
% 	\item $H_1: \mu_g\ne \mu_{g'}$ for some $g$ and $g'$
% 	\end{itemize}
% 	\pause since there are four possibilities under $H_1$
% 	\begin{itemize}[<+->]
% 	\item $\mu_1=\mu_2\ne\mu_3$
% 	\item $\mu_2=\mu_3\ne\mu_1$
% 	\item $\mu_3=\mu_1\ne\mu_2$
% 	\item none of $\mu_1,\mu_2,\mu_3$ are equal
% 	\end{itemize}
% }




\frame{\frametitle{Testing full vs reduced models}
	If $Y_{ig} \stackrel{ind}{\sim} N(\mu_g,\sigma^2)$ for $g=1,\ldots,G$ and we want to test the hypotheses 
	\begin{itemize}
	\item $H_0: \mu_g=\mu$ for all $g$
	\item $H_1: \mu_g\ne \mu_{g'}$ for some $g$ and $g'$
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	 think about this as two models: \pause
	\begin{itemize}
	\item $H_0: Y_{ig} \stackrel{ind}{\sim} N(\mu,\sigma^2)$ \uncover<6->{\alert{(reduced)}}\pause
	\item $H_1: Y_{ig} \stackrel{ind}{\sim} N(\mu_g,\sigma^2)$ \uncover<5->{\alert{(full)}}
	\end{itemize}
	
	\pause
	
	
	\uncover<7->{We can use an F-test to calculate a p-value for tests of this type.}	
	
}


\begin{frame}
\frametitle{Nested models: full vs reduced}

Two models are \alert{nested} if the \alert{reduced} model is a special case of 
the \alert{full} model.

\vspace{0.2in} \pause


For example, consider the full model 
\[ Y_{ig} \stackrel{ind}{\sim} N(\mu_g,\sigma^2). \]
\pause One special case of this model occurs when $\mu_g=\mu$ \pause and thus 
\[ Y_{ig} \stackrel{ind}{\sim} N(\mu,  \sigma^2). \]
is a reduced model and these two models are nested. 

\end{frame}


\frame{
\frametitle{Calculating the sum of squared residuals (errors)}
\footnotesize

\vspace{-0.2in}


  \[ \begin{array}{|l|c|c|}
	\hline && \\
	\mbox{Model}& \mbox{Full} & \mbox{Reduced} \\ &&\\ \hline && \\
	\mbox{Assumption} & 
	\uncover<2->{H_1:  Y_{ig} \stackrel{ind}{\sim} N\left(\mu_g, \sigma^2\right)} & \uncover<6->{
	H_0:  Y_{ig} \stackrel{iid}{\sim} N(\mu, \sigma^2)} \\ && \\ \hline && \\
	\mbox{Mean} & 
	\uncover<3->{\hat{\mu}_g = \overline{Y}_g = \frac{1}{n_g} \sum_{i=1}^{n_g} Y_{ig}} & 
	\uncover<7->{\hat{\mu} = \overline{Y} = \frac{1}{n} \sum_{g=1}^{G} \sum_{i=1}^{n_g} Y_{ig}} \\ && \\ \hline && \\
	\mbox{Residual}  & 
	\uncover<4->{r_{ig} = Y_{ig}-\hat{\mu}_g = Y_{ig}-\overline{Y}_g} & 
	\uncover<8->{r_{ig} = Y_{ig}-\hat{\mu} = Y_{ig}-\overline{Y}} \\ &&\\ \hline && \\
	\mbox{SSE} & 
	\uncover<5->{\sum_{g=1}^{G} \sum_{i=1}^{n_g} r_{ig}^2}  & 
	\uncover<9->{\sum_{g=1}^{G} \sum_{i=1}^{n_g} r_{ig}^2}  \\ &&\\\hline
	\end{array} \]

}



\frame{\frametitle{General F-tests}
\footnotesize

	Do the following
	\begin{enumerate}[1.]
	\item Calculate 
	\[ \begin{array}{r} 
	\multicolumn{1}{l}{\mbox{Extra sum of squares =}}\\
	 \mbox{ Residual sum of squares (reduced) - Residual sum of squares (full)} 
	\end{array} \] \vspace{-0.2in} \pause 
	\item Calculate 
	\[ \begin{array}{r}
	\multicolumn{1}{l}{\mbox{Extra degrees of freedom =}} \\ 
	\mbox{ \# of mean parameters (full) - \# of mean parameters (reduced)} \end{array} \]  \vspace{-0.2in} \pause
	\item Calculate F-statistics
	\[ \mbox{F} = 
	\frac{\mbox{Extra sum of squares / Extra degrees of freedom}}{\mbox{Estimated residual variance in full model }(\hat\sigma^2)} \]   \vspace{-0.2in} \pause 
	\item A pvalue is $P(F_{\mbox{ndf},\mbox{ddf}} > \mbox{F})$
		\begin{itemize}
		\item numerator degrees of freedom (ndf) = Extra degrees of freedom
		\item denominator degrees of freedom (ddf): df associated with $\hat{\sigma}^2$
		\end{itemize}
	\end{enumerate}
	
}

\subsection{Example}
\frame{\frametitle{Mice lifetimes}
Consider the hypothesis that mice on all diets have a common mean lifetime 
except NP.
	
\vspace{0.1in} \pause
	
	Let
	\[ Y_{ig} \stackrel{ind}{\sim} N(\mu_g,\sigma^2) \]
	with $g=1$ being the NP group \pause then the hypotheses are
	\begin{itemize}
	\item $H_0: \mu_g=\mu$ for $g\ne 1$
	\item $H_1: \mu_g\ne\mu_{g'}$ for some $g,g'=2,\ldots,6$
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	As models:
	\begin{itemize}
	\item $H_0: Y_{i1}\iid N(\mu_1,\sigma^2)$ and $Y_{ig}\iid N(\mu,\sigma^2)$ for $g\ne 1$
	\item $H_1: Y_{ig}\ind N(\mu_g,\sigma^2)$
	\end{itemize}
}


\begin{frame}[fragile]
\frametitle{As a picture}

<<echo=FALSE>>=
sm3 = sm 
sm3$mean[-which(sm3$Diet == "NP")] = mean(case0501$Lifetime[case0501$Diet != "NP"])
ggplot(case0501, aes(x = Diet)) + 
  geom_jitter(aes(y = Lifetime), size = 3) + 
  geom_errorbar(data = sm, 
                aes(ymin = mean, ymax = mean), 
                col = 'blue', linewidth = 2) + 
  geom_errorbar(data = sm3, 
                aes(ymin = mean, ymax = mean), 
                col = 'red',
                linewidth = 2)
@

\end{frame}





\begin{frame}[fragile]
\frametitle{Making R do the calculations}

<<echo = TRUE>>=
case0501$NP = factor(case0501$Diet == "NP")

modR = lm(Lifetime ~ NP,   case0501) # (R)educed model
modF = lm(Lifetime ~ Diet, case0501) # (F)ull    model
anova(modR,modF)
@

\end{frame}


% \subsection{Another example}
% \begin{frame}
% \frametitle{Are there differences in means amongst low calorie diets?}
% 
% Let $Y_{ig}$ be the lifetime in months for mouse $i$ in group $g$ where the groups are N/N85 (g=1), N/R40 (g=2), N/R50 (g=3), NP (g=4), R/R50 (g=5), and lopro (g=6). \pause Assume 
% \[ Y_{ig} \stackrel{ind}{\sim} N(\mu_g, \sigma^2) \]
% and test the hypotheses
% \begin{itemize}
% \item[$H_0$:] $\mu_2=\mu_3=\mu_5=\mu_6$
% \item[$H_1$:] at least one of $\mu_2,\mu_3,\mu_5,\mu_6$ is different from the rest
% \end{itemize}
% 
% \vspace{0.2in} \pause
% 
% Implicitly, we are allowing $\mu_1$ and $\mu_4$ to be different from the others. 
% 
% \end{frame}
% 
% 
% 
% 
% \begin{frame}[fragile]
% \frametitle{Making R do the calculations}
% 
% <<>>=
% case0501$local = ifelse(case0501$Diet=='N/N85', 1, 2) # NP is 2 here 
% case0501$local[case0501$Diet=='NP'] = 0               # now NP is 1
% case0501$local = factor(case0501$local)
% mod1 = lm(Lifetime~1,     case0501)
% modR = lm(Lifetime~local, case0501)
% modF = lm(Lifetime~Diet,  case0501)
% anova(mod1, modR, modF)
% 
% anova(modF) # To get the pooled estimate of the variance for the full model
% @
% 
% \end{frame}




\subsection{Lack-of-fit F-test for linearity}

\frame{\frametitle{Lack-of-fit F-test for linearity}
  Let $Y_{ig}$ be the $i^{th}$ observation from the $g^{th}$ group where the group is defined by those observations having the same explanatory variable value ($X_g$). 
  
  \vspace{0.1in} \pause

	Two models:
	
	\begin{tabular}{lll}
	ANOVA: & $Y_{ig} \stackrel{ind}{\sim} N(\mu_g,\sigma^2)$ & \pause \uncover<3->{\alert{(full)}} \\
	Regression: & $Y_{ig} \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_g, \sigma^2)$ & \uncover<3->{\alert{(reduced)}}
	\end{tabular}
	
	\vspace{0.2in} \pause
	
	
	\begin{itemize}
	\item Regression model is reduced: \pause
	\begin{itemize}
	\item ANOVA has $G$ parameters for the mean \pause
	\item Regression has 2 parameters for the mean \pause
	\item Set $\mu_{g} = \beta_0+\beta_1 X_g$. \pause
	\end{itemize}
	\item Small pvalues indicate a lack-of-fit, i.e. the 
	regression (reduced) model is not adequate. \pause
	\item Lack-of-fit F-test requires multiple observations at a few $X_g$ values!
	\end{itemize}
	
}

\begin{frame}[fragile]
\frametitle{pH vs Time - ANOVA}

<<echo=FALSE>>=
ggplot(Sleuth3::ex0816, aes(factor(Time), pH))+
  geom_boxplot(color = "gray")+
  geom_point()+
  labs(x = "Time", 
       y = "pH", 
       title = "pH vs Time in Steer Carcasses") 
@

\end{frame}

\begin{frame}[fragile]
\frametitle{pH vs Time - Regression}

<<echo=FALSE>>=
ggplot(Sleuth3::ex0816, aes(Time, pH))+
  geom_point() +
  stat_smooth(method = "lm") +
  labs(x = "Time", y = "pH", 
       title = "pH vs Time in Steer Carcasses")
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Lack-of-fit F-test in R}
<<echo = TRUE>>=
# Use as.factor to turn a continuous variable into a categorical variable
m_anova = lm(pH ~ as.factor(Time), Sleuth3::ex0816) 
m_reg   = lm(pH ~           Time , Sleuth3::ex0816)
anova(m_reg, m_anova)
@

\pause


There is evidence the data are incompatible with the null hypothesis
that states the means of each group fall along a line. 

\end{frame}



\subsection{Summary}
\frame{\frametitle{Summary}
	\begin{itemize}[<+->]
	\item Use F-tests for comparison of full vs reduced model
    \begin{itemize}
    \item One-way ANOVA F-test
    \item General F-tests
    \item Lack-of-fit F-tests
    \end{itemize}
	\end{itemize}
  
  \vspace{0.2in} \pause 
  
  Think about F-tests as comparing models.
}



\end{document}



