\documentclass[t,aspectratio=169,handout]{beamer}

\usepackage{verbatim,multicol,amsmath}

\input{../../frontmatter}
\input{../../commands}

\title{R02 - Regression diagnostics}

% \setbeamertemplate{background}
% {\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{video_overlay}}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=2.5, 
               size='tiny', 
               out.width='\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=FALSE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, echo=FALSE>>=
library("tidyverse")
library("ggResidpanel")
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\begin{document}


\begin{frame}
\maketitle
\end{frame}


\begin{frame}
\frametitle{All models are wrong!}

George Box (Empirical Model-Building and Response Surfaces, 1987):
\begin{quote}
All models are wrong, but some are useful. 
\end{quote}

\vspace{0.2in} \pause

{\tiny \url{http://stats.stackexchange.com/questions/57407/what-is-the-meaning-of-all-models-are-wrong-but-some-are-useful}}


{\scriptsize
\begin{quotation}
``All models are wrong" that is, 
every model is wrong because it is a simplification of reality. 
Some models, especially in the "hard" sciences, 
are only a little wrong. 
They ignore things like friction or the gravitational 
effect of tiny bodies. 
Other models are a lot wrong - they ignore bigger things. 
\pause \\

``But some are useful" - simplifications of reality can be quite useful. 
They can help us explain, 
predict and understand the universe and all its various components.
\pause \\

This isn't just true in statistics! 
Maps are a type of model; 
they are wrong. 
But good maps are very useful. 
\end{quotation}
}

\end{frame}



\frame{\frametitle{Simple Linear Regression}
  The simple linear regression model is 
  \[ Y_i \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_i, \sigma^2) \]
	\pause this can be rewritten as 
	\[ Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \quad \epsilon_i \stackrel{iid}{\sim} N(0,\sigma^2). \]
	
	\vspace{0.1in} \pause
	
	
	Key assumptions are:
	\begin{itemize}[<+->]
  \item The errors are 
    \begin{itemize}
    \item normally distributed,
    \item have constant variance, and
    \item are independent of each other.
    \end{itemize}
  \item There is a linear relationship between the expected response and the
  explanatory variables.
	\end{itemize}
	
}



\frame{\frametitle{Multiple Regression}
  The \alert{multiple regression} model is 
	\[ Y_i = \beta_0 + \beta_1 X_{i,1} + \cdots + \beta_p X_{i,p} + \epsilon_i \quad \epsilon_i \stackrel{iid}{\sim} N(0,\sigma^2). \]
	
	\vspace{0.1in} \pause
	
	
	Key assumptions are:
	\begin{itemize}
  \item The errors are 
    \begin{itemize}
    \item normally distributed,
    \item have constant variance, and
    \item are independent of each other. \pause
    \end{itemize}
  \item There is a specific relationship between the expected response and the
  explanatory variables.
	\end{itemize}
	
}



\begin{frame}[fragile]
\frametitle{Telomere data}

\vspace{-0.2in} 


<<data, echo=FALSE, warning=FALSE>>=
# From `abd` package
Telomeres <- structure(list(years = c(1L, 1L, 1L, 2L, 2L, 2L, 2L, 3L, 2L, 
4L, 4L, 5L, 5L, 3L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 7L, 
6L, 8L, 8L, 8L, 7L, 7L, 8L, 8L, 8L, 10L, 10L, 12L, 12L, 9L), 
    telomere.length = c(1.63, 1.24, 1.33, 1.5, 1.42, 1.36, 1.32, 
    1.47, 1.24, 1.51, 1.31, 1.36, 1.34, 0.99, 1.03, 0.84, 0.94, 
    1.03, 1.14, 1.17, 1.23, 1.25, 1.31, 1.34, 1.36, 1.22, 1.32, 
    1.28, 1.26, 1.18, 1.03, 1.1, 1.13, 0.98, 0.85, 1.05, 1.15, 
    1.14, 1.24)), .Names = c("years", "telomere.length"), class = "data.frame", row.names = c(NA, 
-39L))

ggplot(Telomeres, aes(years, telomere.length)) + 
  geom_point() + 
  stat_smooth(method = "lm") + 
  labs(title = "Telomere length vs years post diagnosis",
       x = "Years since diagnosis", y = "Telomere length") + 
  theme_bw()
@

\end{frame}


\section{Case statistics}
\begin{frame}
\frametitle{Case statistics}

To evaluate these assumptions, 
we will calculate a variety of \alert{case statistics}:

\vspace{0.1in} \pause

\begin{itemize}
\item Leverage
\item Fitted values
\item Residuals
  \begin{itemize}
  \item Standardized residuals
  \item Studentized residuals
  \end{itemize}
\item Cook's distance
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Default diagnostic plots in R}

\vspace{-0.3in}


<<default_diagnostics, dependson = "data", fig.height=3.2>>=
m <- lm(telomere.length ~ years, Telomeres)
opar = par(mfrow=c(2,3)); plot(m, 1:6, ask = FALSE); par(opar)
@

\end{frame}




\subsection{Leverage}
\begin{frame}
\frametitle{Leverage}

The \alert{leverage} ($0\le h_i\le 1$) of an observation $i$ is a measure of
how far away that observation's explanatory variable value is from the other
observations. \pause
Larger leverage indicates a larger \alert{potential} influence of a single 
observation on the regression model.

\vspace{0.1in}\pause


In simple linear regression, 
\[
h_i = \frac{1}{n} + \frac{(\overline{x}-x_i)^2}{(n-1)s_X^2}
\]
which is involved in the standard error for the line for a location $x_i$. 

\vspace{0.1in} \pause

The variability in the residuals is a function of the leverage, i.e. 
\[
Var[r_i] = \sigma^2(1-h_i)
\]

\end{frame}



\begin{frame}[fragile]
\frametitle{Telomere data}

<<leverage, dependson="data", echo=TRUE>>=
m <- lm(telomere.length~years, Telomeres)

cbind(Telomeres, leverage = hatvalues(m)) %>%
  select(years, leverage) %>% 
  unique() %>% 
  arrange(-years)
@

\end{frame}


\subsection{Residuals}
\begin{frame}
\frametitle{Residuals and Fitted values}

A regression model can be expressed as 
\[ 
Y_i 
= \mu_i + \epsilon_i \pause
= \beta_0 + \beta_1 X_i + \epsilon_i, \pause
\quad \epsilon_i \iid N(0,\sigma^2)
\]

\pause


A \alert{fitted value} $\hat{Y}_i$ for an observation $i$ is 
\[ 
\hat{Y}_i = \hat\mu_i = \hat{\beta}_0+\hat{\beta}_1 X_i
\]
\pause
and the \alert{residual} is 
\[ \begin{array}{rl}
r_i &= Y_i - \hat{Y}_i = \hat{e}_i 
% &= Y_i - \hat\mu_i \pause \\
% &= Y_i - (\hat{\beta}_0+\hat{\beta}_1 X_i) \pause \\
% &= Y_i - \hat{\beta}_0-\hat{\beta}_1 X_i \\
\end{array} \]

\end{frame}


\subsection{Standardized residuals}
\begin{frame}
\frametitle{Standardized residuals}

Often we will \alert{standardize} residuals, \pause i.e.

\[
\frac{r_i}{\sqrt{\widehat{Var[r_i]}}} = \frac{r_i}{\hat\sigma\sqrt{1-h_i}}
\]

\vspace{0.1in} \pause


If $|r_i|$ is large, 
it will have a large impact on $\hat\sigma^2 = \sum_{i=1}^n r_i^2/(n-2)$.
\pause
Thus, we can calculate an \alert{externally studentized residual}

\[
\frac{r_i}{\hat\sigma_{(i)}\sqrt{1-h_i}}
\]

where $\hat\sigma_{(i)}^2 = \sum_{j\ne i} r_j^2/(n-3)$.

\vspace{0.1in} \pause

Both of these residuals can be compared to a standard normal distribution.

\end{frame}


\begin{frame}[fragile]
\frametitle{Telomere data: residuals}

\vspace{-0.2in}


<<residuals, dependson="data">>=
m <- lm(telomere.length ~ years, Telomeres)

Telomeres %>%
  mutate(leverage     = hatvalues(m), 
         residual     = residuals(m), 
         standardized = rstandard(m),
         studentized  = rstudent(m)) 
@

\end{frame}


\subsection{Cook's distance}
\begin{frame}
\frametitle{Cook's distance}

The \alert{Cook's distance} for an observation $i$ ($d_i>0$) is a measure of 
how much the regression parameter estimates change when that observation
is included versus when it is excluded.

\vspace{0.1in} \pause

Operationally, we might be concerned when $d_i$ is 
\begin{itemize}
\item larger than 1 or
\item larger then 4/n.
\end{itemize}
\end{frame}



\section{Default regression diagnostics in R}
\subsection{Residuals vs fitted values}
\begin{frame}
\frametitle{Residuals vs fitted values}

\vspace{-0.6in}


\tiny
<<residual_vs_fitted, dependson="residuals", fig.height=3>>=
plot(m, 1)
@
\pause
\begin{tabular}{ll}
Assumption & Violation \\
\hline
Linearity & Curvature \\
Constant variance & Funnel shape \\
\hline
\end{tabular}

\end{frame}


\subsection{QQ-plot}
\begin{frame}
\frametitle{QQ-plot}

\vspace{-0.6in}



\tiny
<<qqplot, dependson="residuals", fig.height=3>>=
plot(m, 2)
@
\pause
\begin{tabular}{ll}
Assumption & Violation \\
\hline
Normality & Points don't generally fall along the line \\
\hline
\end{tabular}

\end{frame}


\subsection{Absolute standardized residuals vs fitted values}
\begin{frame}
\frametitle{Absolute standardized residuals vs fitted values}

\vspace{-0.6in}


\tiny
<<absolute_residuals, dependson="residuals", fig.height=3, echo=FALSE>>=
plot(m, 3)
@
\pause
\begin{tabular}{ll}
Assumption & Violation \\
\hline
Constant variance & Increasing (or decreasing) trend \\
\hline
\end{tabular}

\end{frame}



\subsection{Cook's distance}
\begin{frame}
\frametitle{Cook's distance}

\vspace{-0.6in}


\tiny
<<cooks_distance, dependson="residuals", fig.height=3>>=
plot(m, 4)
@
\pause
\begin{tabular}{ll}
Outlier & Violation \\
\hline
Influential observation & Cook's distance larger than (1 or 4/n) \\
\hline
\end{tabular}

\end{frame}



\subsection{Residuals vs leverage}
\begin{frame}
\frametitle{Residuals vs leverage}

\vspace{-0.6in}


\tiny
<<residuals_vs_leverage, dependson="residuals", fig.height=3>>=
plot(m, 5)
@
\pause
\begin{tabular}{ll}
Outlier & Violation \\
\hline
Influential observation & Points outside red dashed lines \\
\hline
\end{tabular}

\end{frame}


\subsection{Cook's distance vs leverage}
\begin{frame}
\frametitle{Cooks' distance vs leverage}

\vspace{-0.6in}


\tiny
<<cooks_distance_vs_leverage, dependson="residuals", fig.height=3>>=
plot(m, 6)
@
\pause
This plot is pretty confusing.

\end{frame}






\subsection{Additional plots}
\begin{frame}
\frametitle{Additional plots}
Default plots do not assess all model assumptions.

\vspace{0.1in} \pause

Two additional suggested plots:
\begin{itemize} \footnotesize
\item Residuals vs row number
\item Residuals vs (each) explanatory variable
\end{itemize}
\end{frame}


\subsection{Plot residuals vs row number (index)}
\begin{frame}[fragile]
\frametitle{Plot residuals vs row number (index)}

\vspace{-0.8in}

\tiny
<<residuals_vs_index, dependson="residuals">>=
plot(residuals(m))
@

<<echo=TRUE, eval=FALSE>>=
plot(residuals(m))
@

\begin{tabular}{ll}
Assumption & Violation \\
\hline
Independence & A pattern suggests temporal correlation (or an artefact) \\
\hline
\end{tabular}

\end{frame}



\subsection{Residual vs explanatory variable}
\begin{frame}[fragile]
\frametitle{Residual vs explanatory variable}

\vspace{-0.8in}

\tiny
<<residual_vs_explanatory, dependson="residuals", echo=FALSE>>=
plot(Telomeres$years, residuals(m))
@

<<eval=FALSE, echo=TRUE>>=
plot(Telomeres$years, residuals(m))
@

\begin{tabular}{ll}
Assumption & Violation \\
\hline
Linearity & A pattern suggests non-linearity \\
\hline
\end{tabular}

\end{frame}



\subsection{ggResidpanel: R}
\begin{frame}[fragile]
\frametitle{ggResidpanel: R default}

\vspace{-0.2in}

<<resid-panel-r, echo = TRUE>>=
resid_panel(m, plots = "R")
@

\end{frame}

\subsection{ggResidpanel: R all plots}
\begin{frame}[fragile]
\frametitle{ggResidpanel: R all plots}

\vspace{-0.2in}

<<resid-panel-r-all, echo = TRUE>>=
resid_panel(m, plots = c("qq", "hist", "resid", "index", "yvp", "cookd"),
            bins = 30, smoother = TRUE, qqbands = TRUE,
            type = "standardized") # what I was calling studentized
@

\end{frame}


\begin{frame}[fragile]
\frametitle{ggResidpanel: R explanatory}

<<resid-xpanel, echo = TRUE>>=
resid_xpanel(m)
@

\end{frame}


\subsection{ggResidpanel: SAS}
\begin{frame}[fragile]
\frametitle{ggResidpanel: SAS}
\pause


<<resid_panel-sas, echo = TRUE>>=
resid_panel(m, plots = "SAS") 
@

\end{frame}



\subsection{Summary}
\begin{frame}
\frametitle{Summary}


Case statistics:
\begin{itemize} 
\item Fitted values
\item Leverage
\item Residuals
  \begin{itemize} 
  \item Standardized residuals
  \item Studentized residuals
  \end{itemize}
\item Cook's distance
\end{itemize}

\vspace{0.1in} \pause

Model assumptions:
\begin{itemize} 
\item Normality
\item Constant variance
\item Independence
\item Linearity
\end{itemize}

\end{frame}


\end{document}



