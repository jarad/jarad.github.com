\documentclass[aspectratio=169,handout]{beamer}

\input{../../frontmatter}
\input{../../commands}

\title{$\I$02 - Likelihood}

% \setbeamertemplate{background}
% {\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{video_overlay}}

\begin{document}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=4.4, 
               size='scriptsize',
               out.width='\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("dplyr")
library("tidyr")
library("ggplot2")
@

<<set_seed>>=
set.seed(2)
@


\begin{frame}[t]
\maketitle
\end{frame}


\section{Modeling}
\begin{frame}[t]
\frametitle{Statistical modeling}

A \alert{statistical model} is a pair $(\mathcal{S}, \mathcal{P})$ where 
$\mathcal{S}$ is the set of possible observations, i.e. the sample space, and 
$\mathcal{P}$ is a set of probability distributions on $\mathcal{S}$.

\vspace{0.1in} \pause

Typically, assume a \alert{parametric model}
\[ p(y|\theta) \]
\pause
where 
\begin{itemize}
\item $y$ is our data \pause and 
\item $\theta$ is unknown parameter vector.
\end{itemize}
\pause
The 
\begin{itemize}
\item allowable values for $\theta$ determine $\mathcal{P}$ \pause and
\item the support of $p(y|\theta)$ is the set $\mathcal{S}$. 
\end{itemize}
\end{frame}


\subsection{Binomial}
\begin{frame}[t]
\frametitle{Binomial model}
\small

Suppose we will collect data were we have 
\begin{itemize} \small
\item the number of success $y$ \pause
\item out of some number of attempts $n$ \pause
\item where each attempt is independent \pause 
\item with a common probability of success $\theta$.
\end{itemize}
\pause

\bc
Then a reasonable statistical model is 
\[ 
Y \sim Bin(n,\theta).
\]

\vspace{0.1in} \pause

Formally, 

\begin{itemize}
\item $\mathcal{S} = \{0,1,2,\ldots,n\}$  \pause and
\item $\mathcal{P} = \{ Bin(n,\theta): 0<\theta<1 \}$.
\end{itemize}
\nc\ec
\end{frame}



\subsection{Normal}
\begin{frame}[t]
\frametitle{Normal model}
\small

Suppose we have one datum
\begin{itemize}\small
\item \alert{real} number,  \pause
\item has a \alert{mean $\mu$} and \alert{variance $\sigma^2$}, \pause and
\item uncertainty is represented by a \alert{bell-shaped curve}.
\end{itemize}
\pause
\bc
Then a reasonable statistical model is 
\[
Y \sim N(\mu,\sigma^2).
\]

\vspace{0.1in} \pause

Marginally, 

\begin{itemize}
\item $\mathcal{S} = \{y:y \in\mathbb{R}\}$ 
\item $\mathcal{P} = \{ N(\mu,\sigma^2): -\infty<\mu<\infty, 0<\sigma^2<\infty \}$ 
where $\theta = (\mu,\sigma^2)$.
\end{itemize}
\nc\ec
\end{frame}


\begin{frame}[t]
\frametitle{Normal model}
\small

Suppose our data are
\begin{itemize}\small
\item \alert{$n$} real numbers,  \pause
\item \alert{each} has a mean $\mu$ and variance is $\sigma^2$, \pause
\item a \alert{histogram} is reasonably approximated by a bell-shaped curve, \pause and
\item each observation is \alert{independent} of the others.
\end{itemize}
\pause
\bc
Then a reasonable statistical model is 
\[
Y_i \stackrel{ind}{\sim} N(\mu,\sigma^2).
\]

\vspace{0.1in} \pause

Marginally, 

\begin{itemize}
\item $\mathcal{S} = \{(y_1,\ldots,y_n): y_i\in\mathbb{R}, i\in\{1,2,\ldots,n\}\}$ 
\item $\mathcal{P} = \{ N_n(\mu,\sigma^2\mathrm{I}): -\infty<\mu<\infty, 0<\sigma^2<\infty \}$ 
where $\theta = (\mu,\sigma^2)$.
\end{itemize}
\nc\ec
\end{frame}




\section{Likelihood}
\begin{frame}[t]
\frametitle{Likelihood}

\small

The \alert{likelihood function}, or simply \alert{likelihood}, is the joint 
probability mass/density function
for fixed data when viewed as a function of the parameter (vector) $\theta$.
\pause
Generically, let $p(y|\theta)$ be the joint probability mass/density function of 
the data \pause and thus the likelihood is 
\[ 
L(\theta) = p(y|\theta)
\]
\pause
but where $y$ is fixed and known, i.e. it is your data.

\vspace{0.1in} \pause

\bc

The \alert{log-likelihood} is the (natural) logarithm of the likelihood, i.e. 
\[ 
\ell(\theta) = \log L(\theta).
\]

\vspace{0.1in} \pause

\emph{Intuition:} 
The likelihood describes the relative support in the data for different values 
for your parameter, i.e. the larger the likelihood is the more consistent 
that parameter value is with the data.
\nc\ec
\end{frame}


\subsection{Binomial}
\begin{frame}[t]
\frametitle{Binomial likelihood}

Suppose $Y\sim Bin(n,\theta)$, then 
\[ 
p(y|\theta) = {n\choose y} \theta^y (1-\theta)^{n-y}.
\]
\pause
where $\theta$ is considered fixed (but often unknown) and the argument to this
function is $y$.

\vspace{0.1in} \pause

\bc
Thus the likelihood is 
\[ 
L(\theta) = {n\choose y} \theta^y (1-\theta)^{n-y}
\]
\pause
where $y$ is considered fixed and known and the argument to this function is
$\theta$.

\vspace{0.1in} \pause

\emph{Note}: I write $L(\theta)$ without any conditioning, e.g. on $y$, 
so that you don't confuse this with a probability mass (or density) function.
\nc\ec
\end{frame}



\begin{frame}[t,fragile]
\frametitle{Binomial likelihood}
\bc
<<binomial_likelihood>>=
d2 <- data.frame(theta = seq(0,1,by=0.01)) %>%
  mutate(`y=3` = dbinom(3,10,prob=theta),
         `y=6` = dbinom(6,10,prob=theta)) %>%
  gather(data,likelihood,-theta) 

ggplot(d2, aes(theta, likelihood, color=data, linetype=data)) +
  geom_line() +
  labs(x = expression(theta),
       y = expression(L(theta)),
       title = "Binomial likelihoods (n=10)") + 
  theme_bw()
@
\nc\ec
\end{frame}


\subsection{Independent observations}
\begin{frame}[t]
\frametitle{Likelihood for independent observations}

Suppose $Y_i$ are independent with marginal probability mass/density function
$p(y_i|\theta)$. 

\vspace{0.1in} \pause

The joint distribution for $y=(y_1,\ldots,y_n)$ is 
\[ 
p(y|\theta) = \prod_{i=1}^n p(y_i|\theta).
\]

\vspace{0.1in} \pause

\bc
The likelihood for $\theta$ is 
\[ 
L(\theta) = p(y|\theta) = \prod_{i=1}^n p(y_i|\theta)
\]
where we are thinking about this as a function  of $\theta$ for fixed $y$.
\nc\ec
\end{frame}


\subsection{Normal}
\begin{frame}[t]
\frametitle{Normal model}

Suppose $Y_i\ind N(\mu,\sigma^2)$, \pause then 
\[
p(y_i|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(y_i-\mu)^2}
\]
\pause 
and 
\bc
\[ \begin{array}{rl}
p(y|\mu,\sigma^2) 
&= \prod_{i=1}^n p(y_i|\mu,\sigma^2) \pause \\ 
&= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(y_i-\mu)^2} \pause \\
&= \frac{1}{(2\pi\sigma^2)^{n/2}} e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2} 
\end{array} \]
\pause 
where $\mu$ and $\sigma^2$ are fixed (but often unknown) and the argument to 
this function is $y=(y_1,\ldots,y_n)$.
\nc\ec
\end{frame}


\begin{frame}[t]
\frametitle{Normal likelihood}

If $Y_i\ind N(\mu,\sigma^2)$, then
\[
p(y|\mu,\sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}} e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2}
\]
\pause
\bc
The likelihood is
\[
L(\mu,\sigma) = p(y|\mu,\sigma^2)
= \frac{1}{(2\pi\sigma^2)^{n/2}} e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2}
\]
\pause
where $y$ is fixed and known and $\mu$ and $\sigma^2$ are the arguments to this
function.
\nc\ec
\end{frame}



\begin{frame}[t]
\frametitle{Normal likelihood - example contour plot}

\bc
<<normal_likelihood>>=
x <- rnorm(3)

d <- expand.grid(mu = seq(-2,2,length=100),
                 sigma = seq(0,2,length=100)) %>%
  dplyr::mutate(density = dnorm(x[1], mu, sigma)*dnorm(x[2], mu, sigma)*dnorm(x[3], mu, sigma))

ggplot(d, aes(x = mu, y = sigma, z = density)) +
  geom_contour_filled(show.legend = FALSE) +
  labs(x = expression(mu), 
       y = expression(sigma),
       title = "Example normal likelihood") +
  # theme(legend.title = element_blank()) +
  theme_bw()
@
\nc\ec
\end{frame}


\section{Maximum likelihood estimator}
\begin{frame}[t]
\frametitle{Maximum likelihood estimator (MLE)}

\begin{definition}
The \alert{maximum likelihood estimator (MLE)}, $\hat{\theta}_{MLE}$ 
is the parameter value $\theta$ that maximizes the likelihood function\pause,
i.e. 
\[ 
\hat{\theta}_{MLE} = \mbox{argmax}_\theta \, L(\theta).
\]
\end{definition}


\vspace{0.1in} \pause

When the data are discrete, the MLE maximizes the probability of the observed data.
\end{frame}


\section{Binomial MLE}
\subsection{Derivation}
\begin{frame}[t]
\frametitle{Binomial MLE - derivation}

If $Y\sim Bin(n,\theta)$, then 
\[ 
L(\theta) = {n\choose y} \theta^y (1-\theta)^{n-y}.
\]

\pause

\bc
To find the MLE, 
\begin{enumerate}
\item Take the derivative of $\ell(\theta)$ with respect to $\theta$.
\item Set it equal to zero and solve for $\theta$.
\end{enumerate}
\pause
\[ \begin{array}{rl}
\ell(\theta) &= \log {n\choose y} + y\log(\theta) + (n-y)\log(1-\theta) \pause \\
\frac{d}{d\theta} \ell(\theta) &= \frac{y}{\theta} - \frac{n-y}{1-\theta} \pause \stackrel{set}{=} 0 \pause \implies \\
\hat{\theta}_{MLE} &= y/n \pause 
\end{array} \]

Take the second derivative of $\ell(\theta)$ with respect to $\theta$ and 
check to make sure it is negative.
\nc\ec
\end{frame}


\subsection{Graph}
\begin{frame}[t,fragile]
\frametitle{Binomial MLE - graphically}
\bc
<<binomial_mle>>=
y <- 3
n <- 10
d3 <- data.frame(theta = seq(0,1,by=0.01)) %>%
  mutate(likelihood = dbinom(y,n,prob=theta)) 

ggplot(d3, aes(theta, likelihood)) +
  geom_line() +
  geom_vline(xintercept = y/n, col='red') +
  labs(x = expression(theta)) +
  theme_bw()
@
\nc\ec
\end{frame}


\subsection{Numerical maximization}
\begin{frame}[t,fragile]
\frametitle{Binomial MLE - Numerical maximization}
\vspace{-0.4in}
\bc
<<binomial_mle_numerical, echo=TRUE>>=
log_likelihood <- function(theta) {
  dbinom(3, size = 10, prob = theta, log = TRUE)
}

o <- optim(0.5, log_likelihood, 
           method='L-BFGS-B',            # this method to use bounds
           lower = 0.001, upper = .999,  # cannot use 0 and 1 exactly
           control = list(fnscale = -1)) # maximize

o$convergence # 0 means convergence was achieved
o$par         # MLE
o$value       # value of the likelihood at the MLE
@
\nc\ec
\end{frame}


\section{Normal MLE}
\subsection{Derivation}
\begin{frame}[t]
\frametitle{Normal MLE - derivation}

\small

If $Y_i\ind N(\mu,\sigma^2)$, then 

{\tiny

\[ \begin{array}{rl}
L(\mu,\sigma^2) 
&= \frac{1}{(2\pi\sigma^2)^{n/2}} e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2}  \\
&= \frac{1}{(2\pi\sigma^2)^{n/2}} e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\overline{y}+\overline{y}-\mu)^2}  \\
&= (2\pi \sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n  \left[(y_i-\overline{y})^2 + 2(y_i-\overline{y})(\overline{y}-\mu) + (\overline{y}-\mu)^2 \right]\right)  \\
&= (2\pi \sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i-\overline{y})^2 + -\frac{n}{2\sigma^2}(\overline{y}-\mu)^2 \right) \quad\mbox{since } \sum_{i=1}^n (y_i-\overline{y}) = 0 \\ \\

\ell(\mu,\sigma^2) &= -\frac{n}{2} \log(2\pi\sigma^2) -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i-\overline{y})^2 -\frac{1}{2\sigma^2} n (\overline{y}-\mu)^2  \\
\\
\frac{\partial}{\partial \mu} \ell(\mu,\sigma^2) &= \frac{n}{\sigma^2}(\overline{y}-\mu)  \stackrel{set}{=} 0 \implies \hat{\mu}_{MLE} = \overline{y}  \\ \\
\frac{\partial}{\partial \sigma^2} \ell(\mu,\sigma^2) &= 
-\frac{n}{2\sigma^2}+\frac{1}{2(\sigma^2)^2} \sum_{i=1}^n (y_i-\overline{y})^2 \stackrel{set}{=} 0  \\
& \implies 
\hat{\sigma}_{MLE}^2 = \frac{1}{n}\sum_{i=1}^n (y_i-\overline{y})^2  
= \frac{n-1}{n}S^2
\end{array} \]
}

\pause
\bc
Thus, the MLE for a normal model is 
\[
\hat{\mu}_{MLE} = \overline{y}, \quad \hat{\sigma}_{MLE}^2 = \frac{1}{n}\sum_{i=1}^n (y_i-\overline{y})^2
\]
\nc\ec
\end{frame}


\subsection{Numerical maximization}
\begin{frame}[t,fragile]
\frametitle{Normal MLE - numerical maximization}
\vspace{-0.4in}
\bc
<<normal_numerical_maximization, echo=TRUE, dependson="normal_likelihood">>=
x
log_likelihood <- function(theta) {
  sum(dnorm(x, mean = theta[1], sd = exp(theta[2]), log = TRUE))
}

o <- optim(c(0,0), log_likelihood,
            control = list(fnscale = -1))
c(o$par[1], exp(o$par[2])^2)               # numerical MLE
n <- length(x); c(mean(x), (n-1)/n*var(x)) # true MLE
@
\nc\ec
\end{frame}

\subsection{Graph}
\begin{frame}[t]
\frametitle{Normal likelihood - graph}
\bc
<<normal_mle, dependson=c("normal_likelihood","normal_numerical_maximization")>>=
ggplot(d, aes(mu,sigma,z=density)) +
  geom_contour_filled(show.legend = FALSE) +
  geom_point(aes(x = mean(x), y = sqrt((n-1)/n*var(x))), shape=4, color='red') + 
  labs(x = expression(mu), y = expression(sigma)) +
  theme_bw()
@
\nc\ec
\end{frame}


\section{Summary}
\begin{frame}[t]
\frametitle{Summary}

\begin{itemize}[<+->]
\item For independent observations, the \alert{joint probability mass (density) function}
is the product of the marginal probability mass (density) functions.
\item The \alert{likelihood} is the joint probability mass (density) function
when the argument of the function is the parameter (vector).
\item The \alert{maximum likelihood estimator (MLE)} is the value of the
parameter (vector) that maximizes the likelihood.
\end{itemize}
\end{frame}



\end{document}



