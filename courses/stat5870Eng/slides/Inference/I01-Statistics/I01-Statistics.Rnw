\documentclass[aspectratio=169,handout]{beamer}


\input{../../frontmatter}
\input{../../commands}

\title{$\I$01 - Statistics}

% \setbeamertemplate{background}
% {\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{video_overlay}}


\begin{document}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=4.4, 
               size='tiny', 
               out.width='\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("dplyr")
library("ggplot2")
# library("gridExtra")
@

<<set_seed>>=
set.seed(2)
@


\begin{frame}[t]
\maketitle
\end{frame}


\section{Descriptive statistics}
\begin{frame}[t]
\frametitle{Statistics}
The \alert{field of statistics} is the study of the collection, analysis, interpretation, 
presentation, and organization of data.

{\tiny \url{https://en.wikipedia.org/wiki/Statistics}}


\vspace{0.1in} \pause

There are two different phases of statistics: \pause
\begin{itemize}
\item descriptive statistics \pause 
	\begin{itemize}
	\item statistics
	\item graphical statistics \pause
	\end{itemize}
\item inferential statistics \pause
  \begin{itemize}
  \item uses a sample to make statements about a population.
  \end{itemize}
\end{itemize}

\end{frame}




\subsection{Population and sample}
\begin{frame}[t]
\frametitle{Convenience sample}
The \alert{population} consists of all units of interest. \pause 
Any numerical characteristic of a population is a \alert{parameter}. \pause
The \alert{sample} consists of observed units collected from the population. 
\pause
Any function of a sample is called a \alert{statistic}.

\vspace{0.3in} \pause


\alert{Population}: in-use routers by graduate students at Iowa State University. 

\vspace{0.1in} \pause

\alert{Parameter}: proportion of those routers that have Gigabit speed. 

\vspace{0.1in} \pause

\alert{Sample}: routers of students in STAT 5870-1/A

\vspace{0.1in} \pause

\alert{Statistics}: proportion of routers that have gigabit speed 

\end{frame}



\subsection{Random sample}
\begin{frame}[t]
\frametitle{Simple random sampling}
A \alert{simple random sample} is a sample from the population where all subsets
of the same size are equally likely to be sampled. \pause
Random samples ensure that statistical conclusions will be valid.

\vspace{0.3in} \pause 


\alert{Population}: in-use routers by graduate students at Iowa State University. 

\vspace{0.1in}

\alert{Parameter}: proportion of those routers that have Gigabit speed. 

\vspace{0.1in} \pause

\alert{Sample}: a pseudo-random number generator gives each graduate student a 
Unif(0,1) number and the lowest 100 are contacted

\vspace{0.1in} \pause

\alert{Statistics}: proportion of routers that have gigabit speed 

\end{frame}


\begin{frame}[t]
\frametitle{Sampling and non-sampling errors}

\alert{Sampling errors} are caused by the mere fact that only a sample, a 
portion of a population, is observed.
\pause 
Fortunately,
\[\mbox{error} \downarrow \quad \mbox{as sample size ($n$)} \uparrow \]

\vspace{0.1in} \pause

\alert{Non-sampling errors} are caused by inappropriate sampling schemes and
wrong statistical techniques. \pause 
Often, no statistical technique can rescue a poorly collected sample of data. 

\vspace{0.2in} \pause


\alert{Sample}: students in STAT 5870-1/A


\end{frame}


\subsection{Statistics}
\begin{frame}[t]
\frametitle{Statistics and estimators}
\small

A \alert{statistic} is any function of the data. 

\vspace{0.2in} \pause

Descriptive statistics:
\begin{itemize}
\item Sample mean, median, mode
\item Sample quantiles
\item Sample variance, standard deviation
\end{itemize}

\vspace{0.2in} \pause


When a statistic is meant to estimate a corresponding population parameter, 
\pause 
we call that statistic an \alert{estimator}. 

\end{frame}



\subsection{Sample mean}
\begin{frame}[t]
\frametitle{Sample mean}

Let $X_1,\ldots,X_n$ be a random sample from a distribution with 
\[ E[X_i] = \mu \qquad \mbox{ and } \qquad Var[X_i] = \sigma^2 \] 
where we assume independence between the $X_i$. 

\vspace{0.1in} \pause

The sample mean is 
\[ 
\uncover<4->{\hat\mu = } \overline{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]
\pause
and estimates the population mean $\mu$. \pause

\end{frame}




\subsection{Sample variance}
\begin{frame}[t]
\frametitle{Sample variance}

Let $X_1,\ldots,X_n$ be a random sample from a distribution with 
\[ E[X_i] = \mu \qquad \mbox{ and } \qquad Var[X_i] = \sigma^2 \] 
where we assume independence between the $X_i$. 

\vspace{0.1in} \pause


The sample variance is 
\[ 
\uncover<5->{\hat\sigma^2 = } S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\overline{X})^2 \pause
= \frac{\sum_{i=1}^n X_i^2 - n\overline{X}^2}{n-1} \pause
\]
and estimates the population variance $\sigma^2$.

\vspace{0.2in} \pause \pause

The sample standard deviation is $\hat\sigma = \sqrt{\hat\sigma^2}$ and estimates the 
population standard deviation.

\end{frame}


\subsection{Quantiles}
\begin{frame}[t]
\frametitle{Quantiles}
\small
A \alert{$p$-quantile} of a population is a number $x$ that solves
\[ 
P(X<x) \le p \quad \mbox{and} \quad P(X>x) \le 1-p.
\]
\pause
A \alert{sample $p$-quantile} is any number that exceeds at most $100p$\% of the
sample, and is exceeded by at most $100(1-p)$\% of the sample.  \pause
A \alert{$100p$-percentile} is a $p$-quantile. 
\pause
First, second, and third \alert{quartiles} are the 25th, 50th, and 75th 
percentiles. 
They split a population or a sample into four equal parts.
\pause
A \alert{median} is a 0.5-quantile, 50th percentile, and 2nd quartile.

\vspace{0.1in} \pause


The \alert{interquartile range} is the third quartile minus the first quartile,
i.e.
\[ 
IQR = Q_3 - Q_1
\]
and the \alert{sample interquartile range} is the third sample quartile minus
the first sample quartile, i.e.
\[ 
\widehat{IQR} = \hat{Q}_3 - \hat{Q}_1
\]

\end{frame}



\begin{frame}[t,fragile]
\frametitle{Standard normal quartiles}

<<quantiles, fig.height=4>>=
d <- data.frame(x = seq(-3, 3, by = 0.01)) %>%
  dplyr::mutate(y = dnorm(x))

quartiles = (1:3)/4
ggplot(d, aes(x=x,y=y)) + 
  geom_line() + 
  geom_vline(xintercept = qnorm(p = quartiles), 
             color = "slategray", linetype = "dashed") + 
  labs(y = "Probability density function, p(x)",
       title = "Standard normal") +
  theme_bw()

curve(expr = dnorm, from = -3, to = 3, ylab = "f(x)")
@

\end{frame}




\begin{frame}[t,fragile]
\frametitle{Sample quartiles from a standard normal}

<<sample_quantiles, dependson='quantiles', fig.height=4>>=
n = 1000
sample = data.frame(x=rnorm(n))
ggplot(sample, aes(x=x)) + 
  geom_histogram(aes(y=after_stat(density)), fill = "gray") + 
  geom_vline(xintercept = quantile(sample$x, prob = quartiles),
             color = "red") + 
  geom_vline(xintercept = qnorm(p = quartiles), 
             color = "slategray", linetype = "dashed") + 
  labs(title = "Standard normal samples") +
  theme_bw()
@

\end{frame}


\subsection{Properties of statistics and estimators}
\begin{frame}[t]
\frametitle{Properties of statistics and estimators}
Statistics can have properties, e.g.
\begin{itemize}
\item standard error
\end{itemize}

\vspace{0.1in} \pause

Estimators can have properties, e.g.
\begin{itemize}
\item unbiased
\item consistent
\end{itemize}
\end{frame}


\subsection{Standard error}
\begin{frame}[t]
\frametitle{Standard error}
The \alert{standard error} of a statistic $\hat{\theta}$ is the standard
deviation of that statistic (when the data are considered random).

\vspace{0.1in} \pause

If $X_i$ are independent and have $Var[X_i]=\sigma^2$,
\pause
then

\[ \begin{array}{rl}
Var\left[\,\overline{X}\,\right] &= \pause
Var\left[ \frac{1}{n} \sum_{i=1}^n X_i \right] \pause \\ \\
&= \frac{1}{n^2} \sum_{i=1}^n Var[X_i] = \pause
\frac{1}{n^2} \sum_{i=1}^n \sigma^2 = \pause
\frac{\sigma^2}{n}
\end{array} \]
\pause
and thus
\[
SD\left[\,\overline{X}\,\right] = \sqrt{Var\left[\,\overline{X}\,\right]} = \sigma/\sqrt{n}.
\]
\pause 
Thus the standard error of the sample mean is $\sigma/\sqrt{n}$.

\end{frame}




\subsection{Unbiased}
\begin{frame}[t]
\frametitle{Unbiased}

An estimator $\hat\theta$ is \alert{unbiased} for a parameter $\theta$ if its expectation 
(when the data are considered random) equals the parameter,
\pause
i.e. 
\[ E[\hat\theta] = \theta. \]

\vspace{0.2in} \pause

The sample mean is unbiased for the population mean $\mu$ since

\[ 
E\left[\,\overline{X}\,\right] 
= \pause E\left[\frac{1}{n} \sum_{i=1}^n X_i \right]
= \pause \frac{1}{n} \sum_{i=1}^n E[X_i] = 
\pause \mu.
\]
\pause
and the sample variance is unbiased for the population variance $\sigma^2$.

\end{frame}



\subsection{Consistent}
\begin{frame}[t]
\frametitle{Consistent}

An estimator $\hat{\theta}$, or $\hat{\theta}_n(x)$, \pause
is \alert{consistent} for a parameter $\theta$ if 
the probability of its sampling error of any magnitude converges to 0 
as the sample size $n$ increases to infinity, \pause i.e.
\[ 
P\left( \left|\hat{\theta}_n(X)-\theta\right|>\epsilon \right) \to 0 \mbox{ as } n\to\infty
\]
for any $\epsilon>0$.

\vspace{0.1in} \pause 


The sample mean is consistent for $\mu$ since $Var\left[\,\overline{X}\,\right] = \sigma^2/n$
\pause
and 
\[ 
P\left(\left|\overline{X} - \mu\right| > \epsilon \right) \le 
\frac{Var\left[\,\overline{X}\,\right]}{\epsilon^2} = \frac{\sigma^2/n}{\epsilon^2} \to 0
\]
where the inequality is from \href{https://en.wikipedia.org/wiki/Chebyshev's_inequality}{Chebyshev's inequality.}

\end{frame}










\subsection{Binomial example}
\begin{frame}[t]
\frametitle{Binomial example}

Suppose $Y\sim Bin(n,\theta)$ where $\theta$ is the probability of success.
\pause
The statistic $\hat{\theta} = Y/n$ is an estimator of $\theta$. 

\vspace{0.1in} \pause

Since 
\[ 
E\left[\hat{\theta}\right] = \pause 
E\left[\frac{Y}{n}\right] = \pause 
\frac{1}{n} E[Y] = \pause
\frac{1}{n} n\theta = \pause
\theta
\]
\pause
the estimator is \alert{unbiased}.
\end{frame}


\begin{frame}[t]
\frametitle{Binomial example}

Suppose $Y\sim Bin(n,\theta)$ where $\theta$ is the probability of success.
The statistic $\hat{\theta} = Y/n$ is an estimator of $\theta$. 

\vspace{0.1in} \pause

The variance of the estimator is 
\[ 
Var\left[\hat{\theta}\right] = \pause
Var\left[ \frac{Y}{n} \right] = \pause
\frac{1}{n^2} Var[Y] = \pause
\frac{1}{n^2} n\theta(1-\theta) = \pause
% \ldots = 
\frac{\theta(1-\theta)}{n}.
\]
\pause


Thus the \alert{standard error} is 
\[ 
SE(\hat{\theta}) = \sqrt{Var[\hat{\theta}]} = \sqrt{\frac{\theta(1-\theta)}{n}}.
\]
\pause
By Chebychev's inequality, this estimator is \alert{consistent} for $\theta$.

\end{frame}




\subsection{Summary}
\begin{frame}[t]
\frametitle{Summary}
\begin{itemize}
\item Statistics are functions of data.
\item Statistics have some properties:
	\begin{itemize}
	\item Standard error
	\end{itemize}
\item Estimators are statistics that estimate population parameters.
\item Estimators may have properties:
  \begin{itemize}
  \item Unbiased
  \item Consistent 
  \end{itemize}
\end{itemize}

\end{frame}





\section{Graphical statistics}
\begin{frame}[t]
\frametitle{Look at it!}
{\Huge
\alert{Before you do anything with a data set, \pause LOOK AT IT!}
}
\end{frame}



\begin{frame}[t]
\frametitle{Why should you look at your data?}

\pause

\begin{enumerate}
\item Find errors \pause
	\begin{itemize}
	\item Do variables have the correct range, e.g. positive?
	\item How are Not Available encoded?
	\item Are there outliers? \pause
	\end{itemize}
\item Do known or suspected relationships exist? \pause
	\begin{itemize}
	\item Is X linearly associated with Y?
	\item Is X quadratically associated with Y? \pause
	\end{itemize}
\item Are there new relationships? \pause
	\begin{itemize}
	\item What is associated with Y and how? \pause
	\end{itemize}
\item Do variables adhere to distributional assumptions? \pause
	\begin{itemize}
	\item Does Y have an approximately normal distribution?
	\item Right/left skew
	\item Heavy tails
	\end{itemize}
\end{enumerate}
\end{frame}







\begin{frame}[t]
\frametitle{Principles of professional statistical graphics}

{\tiny \url{https://moz.com/blog/data-visualization-principles-lessons-from-tufte}}

\begin{itemize}
\item Show the data
	\begin{itemize}
	\item Avoid distorting the data, e.g. pie charts, 3d pie charts, exploding wedge 3d pie charts, bar charts that do not start at zero \pause 
	\end{itemize}
\item Plots should be self-explanatory
	\begin{itemize}
	\item Use informative caption, legend
	\item Use normative colors, shapes, etc \pause
	\end{itemize}
\item Have a high information to ink ratio
	\begin{itemize}
	\item Avoid bar charts \pause
	\end{itemize}
\item Encourage eyes to compare
	\begin{itemize}
	\item Use size, shape, and color to highlight differences
	\end{itemize}
\end{itemize}

\end{frame}


\begin{frame}[t]
\frametitle{Stock market return}

\vspace{-0.2in}

\setkeys{Gin}{width=\textwidth}
{\tiny \url{http://www.nytimes.com/interactive/2011/01/02/business/20110102-metrics-graphic.html?_r=0}}

\vspace{-0.2in}


\begin{center}
\includegraphics{market_return}
\end{center}

\end{frame}

\end{document}



