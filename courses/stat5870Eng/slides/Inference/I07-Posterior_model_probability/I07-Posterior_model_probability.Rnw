\documentclass[t,aspectratio=169,handout]{beamer}

\usepackage{verbatim,multicol,amsmath}

\input{../../frontmatter}
\input{../../commands}

\title{I07 - Posterior model probability}

% \setbeamertemplate{background}
% {\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{video_overlay}}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=4.4, 
               size='tiny', 
               out.width='\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=FALSE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
# library("plyr")
library("dplyr")
library("tidyr")
library("ggplot2")
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\begin{document}

\begin{frame}
\maketitle
\end{frame}

\section{Posterior model probabilities}
\subsection{One-sided alternative hypotheses}
\begin{frame}
\frametitle{One-sided alternative hypotheses}

For ``one-sided alternative hypotheses'' just calculate 
posterior probabilities.

\vspace{0.1in} \pause

\bc
For example, with hypotheses
\[ H_0: \theta \le \theta_0
\qquad \mbox{versus} \qquad
H_A: \theta > \theta_0 \]

\pause

Calculate 
\[ 
p(H_0|y) \pause = P(\theta\le \theta_0|y)
\]
\pause
and
\[
p(H_A|y) = P(\theta > \theta_0|y).
\]
\nc\ec
\end{frame}



\begin{frame}[fragile]
\frametitle{Posterior probabilities}

\small

Let $Y \sim Bin(n,\theta)$ with hypotheses
\[ H_0: \theta \le 0.5
\qquad \mbox{and} \qquad
H_A: \theta > 0.5. \]
\pause
\bc
Assume $\theta \sim Unif(0,1)$ and obtain the posterior
\pause
i.e. 
\[ 
\theta|y \sim Be(1+y, 1+n-y).
\]
\pause
Then calculate 
\[ p(H_0|y) \pause = P(\theta \le 0.5|y) \pause = 1-p(H_A|y). \]

\vspace{-0.2in} \pause 

<<echo=TRUE>>=
n = 10
y = 3
probH0 = pbeta(0.5, 1+y, 1+n-y)
probH0   # p(H_0|y)
1-probH0 # p(H_A|y)
@
\nc\ec
\end{frame}



\subsection{Posterior model probabilities}
\begin{frame}
\frametitle{Posterior model probabilities}

\small

Calculate the \alert{posterior model probabilities} over some set of $J$
models
\pause
i.e,
\bc
\[ 
p(M_j|y) \pause
= \frac{p(y|M_j)p(M_j)}{p(y)} \pause
= \frac{p(y|M_j)p(M_j)}{\sum_{k=1}^J p(y|M_k)p(M_k)}.
\]

\vspace{0.1in} \pause

In order to accomplish this, we need to determine 
\begin{itemize}
\item \alert{prior model probabilities}: 
\[ \alert{p(M_j)}  \quad \mbox{for all} \quad j=1,\ldots,J \]
\pause and
\item \alert{priors over parameters in each model}: 
\[ 
p(y|M_j) = \int p(y|\theta)\alert{p(\theta|M_j)} d\theta.
\]
\end{itemize}
\nc\ec
\end{frame}



\subsection{Prior predictive distribution}
\begin{frame}
\frametitle{Prior predictive distribution}

The \alert{prior predictive distribution} for model $M_j$ is 
\[ 
p(y|M_j) = \int p(y|\theta)p(\theta|M_j) d\theta.
\]

\vspace{0.1in} \pause

\bc
For example, let 
\[ y|\mu,M_j \sim N(\mu, 1) \] 
\pause 
and 
\[ \mu|M_j \sim N(0,C), \]
\pause
then 
\[ 
y|M_j \sim N(0,1+C).
\]
\nc\ec
\end{frame}



\subsection{Bayes Factor}
\begin{frame}
\frametitle{Bayes Factor}

In the context of a null hypothesis ($H_0$) and an alternative hypothesis ($H_A$)
\pause
we have 
\pause
\bc
\[ \begin{array}{rl}
p(H_0|y) 
&= \frac{p(y|H_0)p(H_0)}{p(y|H_0)p(H_0)+p(y|H_A)p(H_A)} \pause \\ \\
% &= \frac{1}{1+\frac{p(y|H_A)p(H_A)}{p(y|H_0)p(H_0)}} \pause \\ \\
&= \left[1+\frac{p(y|H_A)}{p(y|H_0)}\frac{p(H_A)}{p(H_0)}\right]^{-1} \pause \\ \\
&= \left[1+BF(H_A:H_0)\frac{p(H_A)}{p(H_0)}\right]^{-1}
\end{array} \]
\pause
where 
\[ BF(H_A:H_0) = \frac{p(y|H_A)}{p(y|H_0)} \]
is the \alert{Bayes Factor} for $H_A$ over $H_0$.
\nc\ec
\end{frame}



\subsection{Normal model}
\begin{frame}[fragile]
\frametitle{Normal model}

Let $Y\sim N(\mu,1)$ and $H_0: \mu=0$ vs $H_A: \mu\ne 0$.

\vspace{0.1in} \pause

Assume $p(H_0) = p(H_A)$ \pause and $\mu|H_A \sim N(0,1)$,
\pause
\bc
then 
\[ \begin{array}{rl}
y|H_0 &\sim N(0,1) \pause \\
y|H_A &\sim N(0,2).
\end{array} \]
\pause
<<echo=TRUE, size="scriptsize">>=
y = 0.3
probH0 = 1/(1+dnorm(y, 0, sqrt(2))/dnorm(y, 0, 1))
probH0   # p(H_0|y)
1-probH0 # p(H_A|y)
@
\nc\ec
\end{frame}


\begin{frame}
\frametitle{Ratio of predictive densities}
\bc
<<>>=
densities = data.frame(y = seq(-5, 5, length = 101)) %>%
  dplyr::mutate(H0 = dnorm(y, 0, 1),
                HA = dnorm(y, 0, sqrt(2))) %>%
  tidyr::gather("Hypothesis","density", H0, HA) 

ggplot(densities, aes(x = y, y = density, 
                      color = Hypothesis, linetype = Hypothesis)) +
  geom_line(size = 2) + 
  geom_vline(xintercept = y, color = "gray",linetype = "dotted", size = 2 ) + 
  geom_point(data = data.frame(y = y, 
                                density = dnorm(y, 0, c(1,sqrt(2))), 
                                Hypothesis = c("H0","HA")),
              aes(x=y, y=density, color = Hypothesis),
             size = 4) + 
  labs(title = "Ratio of predictive densities",
       y = "Prior predictive density") +
  theme_bw()
@
\nc\ec
\end{frame}



\begin{frame}
\frametitle{Normal model}
\bc
<<>>=
d = data.frame(y = seq(-5, 5, length = 101)) %>%
  dplyr::mutate(H0 = 1/(1+dnorm(y, 0, sqrt(2))/dnorm(y, 0, 1)),
                HA = 1-H0) %>%
  tidyr::gather("Hypothesis", "Probability", H0, HA)

ggplot(d, aes(x=y, y=Probability, color = Hypothesis, linetype = Hypothesis)) + 
  geom_line(size = 2) +
  labs(title = "Posterior probabilities as a function of the data",
    y = "Posterior model probability") +
  theme_bw()
@
\nc\ec
\end{frame}





% \subsection{Bayesian hypothesis tests}
% \begin{frame}
% \frametitle{Bayesian hypothesis tests}
% 
% To conduct a Bayesian hypothesis test, 
% \pause
% you need to specify 
% \begin{itemize}
% \item $p(M_j)$ \pause and 
% \item $p(\theta|M_j)$ 
% \end{itemize}
% for every hypothesis $j=1,\ldots,J$.
% \pause
% Then, you can calculate 
% \[ 
% p(M_j|y) = \frac{p(y|M_j)p(M_j)}{\sum_{k=1}^J p(y|M_k)p(M_k)} \pause
% = \left[ 1+ \sum_{k\ne j} \frac{p(y|M_k)}{p(y|M_j)}\frac{p(M_k)}{p(M_j)} \right]^{-1}
% \]
% \pause
% where 
% \[
% BF(M_k:M_j) = \frac{p(y|M_k)}{p(y|M_j)} 
% \]
% \pause 
% are the \alert{Bayes factor} for hypothesis $M_k$ compared to hypothesis $M_j$ \pause 
% and
% \[
% p(y|M_j) = \int p(y|\theta)p(\theta|M_j) d\theta
% \]
% for all $j$.
% 
% \end{frame}



\subsection{Prior impact}
\begin{frame}
\frametitle{Prior impact}

Let $Y \sim N(\mu,1)$ and $H_0: \mu=0$ vs $H_A: \mu\ne 0$.

\vspace{0.1in} \pause

Assume $p(H_0) = p(H_A)$ \pause and $\mu|H_A \sim N(0,C)$,
\pause
\bc
then 
\[ \begin{array}{rl}
y|H_0 &\sim N(0,1) \pause \\
y|H_A &\sim N(0,1+C)
\end{array} \]
\pause
and
\[ 
p(H_0|y) = \left[ 1+\frac{p(y|H_A)}{p(y|H_0)}\right]^{-1}.
\]

% \vspace{0.1in} \pause 
% 
% \bc
% Thus 
% \[
% p(H_0|y) = \left[1+ \frac{p(y|H_A)}{p(y|H_0)}  \right]^{-1} \pause 
% = \left[ 1 + \frac{N(y;0,1+C)}{N(y;0,1)}  \right]^{-1}
% \]
% \pause
% where $N(y;\mu,\sigma^2)$ is evaluating the probability density function for 
% a normal distribution with mean $\mu$ and variance $\sigma^2$ at the value $y$.
\nc\ec
\end{frame}



\begin{frame}[fragile]
\frametitle{Prior impact}
\bc
<<normal_bayes_factor>>=
d = expand.grid(y=seq(0,5,by=1), C=10^seq(0,4,by=0.1)) %>%
   dplyr::mutate(post_prob_H0 = 1/(1+1/exp(dnorm(y,0,1,log=TRUE)-dnorm(y,0,1+C,log=TRUE))))
  
ggplot(d, aes(sqrt(C), post_prob_H0, color=factor(y))) + 
  geom_line() + 
  labs(x = expression(sqrt(C)), y = expression(paste("p(",H[0],"|y)"))) + 
  scale_color_discrete(name="y") +
  theme_bw()
@
\nc\ec
\end{frame}




\begin{frame}
\frametitle{Interpretation}

Since posterior model probabilities depend on the prior predictive distribution
\[ 
p(y|M_j) = \int p(y|\theta)p(\theta|M_j) d\theta
\]
\pause
posterior model probabilities tell you which model does a better job of 
\alert{prediction}
\pause
and priors, $p(\theta|M_j)$, must be informative. 

\end{frame}



\begin{frame}
\frametitle{Do pvalues and posterior probabilities agree?}
Suppose $Y\sim Bin(n,\theta)$ and we have the hypotheses 
$H_0:\theta=0.5$ and $H_A:\theta\ne 0.5$ \pause
We observe $n=10,000$ and $y=4,900$ 
\pause 
and find the \pvalue is 
\[ \mbox{\pvalue{}} \approx 2P(Y\le 4900) = 0.0466 \]
\pause so we would reject $H_0$ at the 0.05 level. 

\vspace{0.1in} \pause

If we assume $p(H_0) = p(H_A) = 0.5$ \pause and $\theta|H_A \sim Unif(0,1)$, 
\pause
then the posterior probability of $H_0$, \pause is 
\bc
\[ p(H_0|y) \approx \frac{1}{1+1/10.8} = 0.96, \]
\pause so the probability of $H_0$ being true is 96\%. 

\vspace{0.1in} \pause 

It appears the posterior probability of $H_0$ and \pvalue{} completely disagree!
\nc\ec
\end{frame}

% \begin{frame}[fragile]
% \frametitle{Binomial $\overline{y}=0.49$ with $n\to\infty$} 
% <<paradox,fig.width=8>>=
% paradox = expand.grid(n=10^(seq(0,5,by=0.1)), ybar=0.49)
% paradox = ddply(paradox, .(n,ybar), summarize, pvalue=pvalue(lrt(n,ybar)), post_prob=post_prob(bf(n,ybar)))
% m = melt(paradox, id=c("n","ybar"))
% p = ggplot(m, aes(log10(n),value,col=variable)) + 
%   geom_line() +
%   theme_bw()
% print(p)
% @
% \end{frame}

\subsection{Jeffreys-Lindley Paradox}
\frame{\frametitle{Jeffreys-Lindley Paradox}
  The \alert{Jeffreys-Lindley Paradox} concerns a situation when comparing two hypotheses $H_0$ and $H_1$ given data $y$ \pause and find
  \begin{itemize}\small
  \item a frequentist test result is significant leading to rejection of $H_0$, \pause but
  \item the posterior probability of $H_0$ is high. 
  \end{itemize}
  
  \vspace{0.2in} \pause
  
  This can happen when 
  \begin{itemize}[<+->]\small
  \item the effect size is small, 
  \item $n$ is large, 
  \item $H_0$ is relatively precise, 
  \item $H_1$ is relative diffuse, and
  \item the prior model odds is $\approx 1$. 
  \end{itemize}
}


\begin{frame}
\frametitle{No real paradox}

\small

\pause

\pvalue{}s:
\begin{itemize}[<+->]
\item a \pvalue{} measure how incompatible your data are with the null hypothesis\pause, but
% \item The smaller the \pvalue{}, the more incompatible.
\item it says nothing about how incompatible your data are with the alternative hypothesis.
\end{itemize}

\vspace{0.1in} \pause

\bc

Posterior model probabilities are
\begin{itemize}
\item a measure of the (prior) predictive ability of a model \pause relative to the other models\pause, but
% \item The larger the posterior probability, the more predictive that hypothesis 
% was compared to the other hypotheses.
\item this requires you to have at least two (or more) well-thought out models \pause with
informative priors.
\end{itemize}

\vspace{0.1in} \pause

Thus, these two statistics provide completely different measures of model 
adequecy.
\nc\ec
\end{frame}



\subsection{Summary}
\begin{frame}
\frametitle{Summary}

\begin{itemize}
\item Use posterior probabilities for one-sided alternative hypotheses. \pause
\item Posterior model probabilities evaluate relative predictive ability.
\end{itemize}

\end{frame}



\end{document}



