\documentclass[t,aspectratio=169,handout]{beamer}

\usepackage{verbatim,multicol,amsmath}

\input{../../frontmatter}
\input{../../commands}

\title{$\mathrm{I}$09 - Comparing means}

% \setbeamertemplate{background}
% {\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{video_overlay}}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA,
               fig.width=6, fig.height=2.5,
               size='tiny',
               out.width='\\textwidth',
               fig.align='center',
               message=FALSE,
               echo = FALSE,
               cache=FALSE)
options(width=120)
@

<<libraries, warning=FALSE>>=
library("tidyverse")
@

<<set_seed>>=
set.seed(2)
@

\begin{document}

\begin{frame}
\maketitle
\end{frame}

\section{One mean summary}
\subsection{One mean}
\begin{frame}
\frametitle{One mean}

Consider the model $Y_i \ind N(\mu,\sigma^2)$.
\pause
We have discussed a number of statistical procedures to draw inferences about
$\mu$: \pause
\begin{itemize}
\item Frequentist: based on distribution of $\frac{\overline{Y}-\mu}{S/\sqrt{n}} \sim t_{n-1}$ \pause
  \begin{itemize}
  \item \pvalue{} for a hypothesis test, e.g. $H_0: \mu=m$, \pause
  \item confidence interval for $\mu$, \pause
  \end{itemize}
\item Bayesian: $\frac{\mu - \overline{y}}{s/\sqrt{n}} \sim t_{n-1}$
  \begin{itemize}
  \item credible interval for $\mu$, \pause
  \item posterior model probability, e.g. $p(H_0|y)$, \pause and
  \item posterior probabilities, e.g. $P(\mu < m|y)$.
  \end{itemize}
\end{itemize}
\pause

Now, we will consider what happens when you have multiple groups.

\end{frame}


\section{Comparing two means}
\subsection{Normal model}
\begin{frame}
\frametitle{Two means}

\footnotesize

Consider the model
\[ Y_{g,i} \ind N(\mu_g,\sigma^2)\]
for $g=1,2$ and $i=1,\ldots,n_g$.
\pause
and you are interested in the relationship between $\mu_1$ and $\mu_2$.

\pause

\begin{itemize}
\item Frequentist: based on distribution of
\[\frac{\overline{Y}_1-\overline{Y}_2 - (\mu_1-\mu_2)}{S_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t_{n_1+n_2-2}
\quad\mbox{where}\quad\pause
S_p^2 = \frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2 - 2}
\]

\pause

  \begin{itemize}
  \item \pvalue{} for a hypothesis test, e.g. $H_0: \mu_1-\mu_2=d$, \pause
  \item confidence interval for $\mu_1-\mu_2$, \pause
  \end{itemize}
\item Bayesian
\[
\frac{\mu_1-\mu_2 - (\overline{y}_1 + \overline{y}_2)}{s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}\sim t_{n_1+n_2-2}
\]
  \begin{itemize}
  \item credible interval for $\mu_1-\mu_2$, \pause
  % \item posterior model probability, e.g. $p(H_0|y)$, \pause and
  \item probability statements, e.g. $P(\mu_1 - \mu_2 < d|y)$. \pause
  \end{itemize}
\end{itemize}
where $y=(y_{1,1},\ldots,y_{1,n_1},y_{2,1},\ldots,y_{2,n_2})$.

\vspace{0.1in} \pause

Approaches are slightly different if you assume $\sigma_g = \sigma$ for all
groups.

\end{frame}


% \subsection{Simulating data}
% \begin{frame}[fragile]
% \frametitle{Simulating data}
%
% We will simulate the following data
% \[ Y_{g,i} \ind N(\mu_g,\sigma_g^2)\]
% for $g=1,2,3$ and $i=1,\ldots,n_g$. \pause
% For the moment we will only use the first two groups,
% but later we will use all 3 groups.
%
% \vspace{0.1in} \pause
%
%
<<data>>=
set.seed(20170301)

# Using the unknown population means and standard deviations
d <- bind_rows(
  tibble(process = "P1", sensitivity = rnorm(22,  7.8, 2.3)),
  tibble(process = "P2", sensitivity = rnorm(34,  9.3, 2.3)),
  tibble(process = "P3", sensitivity = rnorm( 7, 10.0, 2.3)))

# readr::write_csv(d, path="sensitivity.csv")
@
%
% \end{frame}


\begin{frame}[fragile]
\frametitle{Data example}

Suppose you have two manufacturing processes to produce sensors and
you are interested in the average sensitivity of the sensors.

\vspace{0.1in} \pause

So you run the two processes and record the sensitivity of each sensor in units
of mV/V/mm Hg (\url{http://www.ni.com/white-paper/14860/en/})
\pause
and you observe the following summary statistics:
<<d2, dependson="data">>=
# d <- readr::read_csv("sensitivity.csv")
d2 <- d %>% filter(process %in% c("P1","P2"))
@

<<sm, dependson="d2">>=
sm <- d2 %>%
  group_by(process) %>%
  summarize(
    n    = n(),
    mean = mean(sensitivity),
    sd   = sd(sensitivity)
  )
sm
@

\pause

Let $Y_{g,i}$ be the sensitivity of the $i$th sensor in the $g$th group. \pause
Assume
\[
Y_{g,i} \ind N(\mu_g, \sigma^2).
\]

\end{frame}


% \subsection{Frequentist analysis}
% \begin{frame}
% \frametitle{Frequentist analysis}
%
% Analyses are based on the distribution of
% \[
% \frac{\overline{Y}_1-\overline{Y}_2 - (\mu_1-\mu_2)}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}}
% \uncover<3->{\sim t_{df}(0,1).}
% \]
% \pause
% Unfortunately, this distribution is unknown.
% \pause
%
% The standard (although somewhat controversial) approach is to assume this
% has a $t$ distribution but with an unknown degrees of freedom.
% \pause
% The degrees of freedom is often computed using the Satterthwaite approximation:
% \[
% df \approx \frac{\left( \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2}{\frac{s_1^4}{n_1^2(n_1-1)} + \frac{s_2^4}{n_2^2(n_2-1)}}.
% \]
%
% \end{frame}


\begin{frame}
\frametitle{Frequentist analysis formulas}
Consider the hypothesis $H_0: \mu_1 = \mu_2$ or, equivalently, 
$H_0: \mu_1-\mu_2 = 0$. 
\pause
We calculate the \pvalue{} using
\[ 
2P\left(T_{n_1+n_2-1} < -|t|\right)
\quad\mbox{where}\quad \pause
t = \frac{\overline{y}_1-\overline{y}_2 - 0}{s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}.
\]
\pause
We calculate a $100(1-a)$\% confidence interval using
\[
\overline{y}_1-\overline{y}_2 \pm t_{n_1+n_2-2,1-a/2} s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}.
\]

\end{frame}


\begin{frame}[fragile]
\frametitle{Frequentist analysis by hand}
<<two-means-frequentist-by-hand, dependson="d2", echo=TRUE>>=
v    <- sm$n[1] + sm$n[2] - 2
diff <- sm$mean[1] - sm$mean[2]

# Calculate standard error
sp2  <- ( (sm$n[1]-1)*sm$sd[1]^2 + (sm$n[2]-1)*sm$sd[2]^2 ) / v # Pooled variance
sp   <- sqrt(sp2)
se   <- sp * sqrt(1/sm$n[1] + 1/sm$n[2])

# Two-sided p-value
2 * pt(-abs(diff / se), df = v)

# Equal-tail confidence interval
diff + c(-1,1) * qt(.975, df = v) * se
@
\end{frame}

\begin{frame}
\frametitle{Bayesian analysis formulas}
We calculate a $100(1-a)$\% credible interval using
\[
\overline{y}_1-\overline{y}_2 \pm t_{n_1+n_2-2,1-a/2} s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}.
\]
\pause
We calculate a posterior probability using
\[ 
P(\mu_1-\mu_2 < 0|y) = P\left( T_{n_1+n_2-2} < \frac{0 - (\overline{y}_1-\overline{y}_2)}{s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \right)
\]
\pause
Thus, half the \pvalue{} corresponds to either 
$P(\mu_1-\mu_2 < 0|y)$ or $P(\mu_1-\mu_2 > 0|y)$.
\end{frame}


\begin{frame}[fragile]
\frametitle{Analyses using t.test}

<<two-means-frequentist-t-test, dependson="d2",echo=TRUE>>=
(tt <- t.test(sensitivity ~ process, data = d2, var.equal = TRUE))

# Since estimate of the difference is negative
# the following is P(mu_1 - mu_2 > 0)
tt$p.value / 2
@

\end{frame}


\subsection{Unequal variances}
\begin{frame}
\frametitle{Unequal variances}
Consider the model
\[ Y_{g,i} \ind N(\mu_g,\alert{\sigma_g}^2)\]
for $g=1,2$ and $i=1,\ldots,n_g$.
\pause
and you are interested in the relationship between $\mu_1$ and $\mu_2$.

\pause

Frequentist: 
\[
\frac{\overline{Y}_1-\overline{Y}_2 - (\mu_1-\mu_2)}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}} \stackrel{\cdot}{\sim} t_{v}
\]
using Satterthwaite approximation for the degrees of freedom $v$. 

\vspace{0.1in} \pause

Bayesian:
\[
\left. \frac{\mu_g-\overline{y}_g}{s_g/\sqrt{n_g}}\right|y \ind t_{n_g-1}
\]
\pause
Simulate means separately and take the difference.
\end{frame}


\begin{frame}[fragile]
\frametitle{Analyses using t.test}

<<two-means-frequentist-t-test-unequal-variance, dependson="d2",echo=TRUE>>=
t.test(sensitivity ~ process, 
       data = d2, 
       var.equal = FALSE)     # this was the default
@

\end{frame}


\subsection{Bayesian analysis}
\begin{frame}[fragile]
\frametitle{Posterior for $\mu_1,\mu_2$}

Assume
\[ Y_{g,i} \ind N(\mu_g,\sigma_g^2)
\quad \text{and} \quad
p(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2) \propto
\frac{1}{\sigma_1^2} \frac{1}{\sigma_2^2}.
\]
Then
\[
\mu_g|y \ind t_{n_g-1}(\overline{y}_g, s_g^2/n_g)
\]
\pause

and a draw for $\mu_g$ can be obtained by taking
\[
\overline{y}_g + T_{n_g-1} s_g/\sqrt{n_g}, \quad T_{n_g-1} \ind t_{n_g-1}(0,1).
\]

\end{frame}

\begin{frame}[fragile]
\frametitle{Bayesian analysis in R}
<<mu_draws, dependson="sm", echo=TRUE>>=
nr = 1e5
sims <- bind_rows(
  tibble(           # tibble is just a special data.frame
    rep     = 1:nr,
    process = "P1",
    mu      = sm$mean[1] + rt(nr, df = sm$n[1]-1) * sm$sd[1] / sqrt(sm$n[1])),
  tibble(
    rep     = 1:nr,
    process = "P2",
    mu      = sm$mean[2] + rt(nr, df = sm$n[2]-1) * sm$sd[2] / sqrt(sm$n[2]))
)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{We can use these draws to compare the posteriors}


<<mu_posteriors, dependson="mu_draws">>=
ggplot(sims, aes(x=mu, y=after_stat(density), fill=process, group=process)) +
  geom_histogram(position = "identity", alpha=0.5, binwidth=0.1) +
  theme_bw()
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Credible interval for the difference}

To obtain statistical inference on the difference,
we use the samples and take the difference


<<mu_difference, dependson="mu_draws", echo=TRUE>>=
d3 <- sims %>%
  spread(process, mu) %>%
  mutate(diff = P1-P2)

# Bayes estimate for the difference
mean(d3$diff)

# Estimated 95% equal-tail credible interval
quantile(d3$diff, c(.025,.975))

# Estimate of the probability that mu1 is smaller than mu2
mean(d3$diff < 0)
@

\end{frame}



\subsection{Three or more means}
\begin{frame}
\frametitle{Three or more means}

Now, let's consider the more general problem of
\[ Y_{g,i} \ind N(\mu_g,\sigma_g^2)\]
for $g=1,2,\ldots,G$ and $i=1,\ldots,n_g$
\pause
and you are interested in the relationship amongst the $\mu_g$.

\vspace{0.1in} \pause

We can perform the following statistical procedures:
\begin{itemize}
\item Frequentist:
  \begin{itemize}
  \item \pvalue{} for test of $H_0: \mu_g=\mu$ for all $g$, \pause
  \item confidence interval for $\mu_g-\mu_{g'}$, \pause
  \end{itemize}
\item Bayesian: based on posterior for $\mu_1,\ldots,\mu_G$
  \begin{itemize}
  \item credible interval for $\mu_g-\mu_{g'}$ , \pause
  % \item posterior model probability, e.g. $p(H_0|y)$, \pause and
  \item probability statements, e.g. $P(\mu_g<\mu_{g'}|y)$
  \end{itemize}
where $g$ and $g'$ are two different groups.
\end{itemize}
\end{frame}



\begin{frame}[fragile]
\frametitle{Data example}

Suppose you have three manufacturing processes to produce sensors and
you are interested in the average sensitivity of the sensors.

\vspace{0.1in} \pause

So you run the three processes and record the sensitivity of each sensor in units
of mV/V/mm Hg (\url{http://www.ni.com/white-paper/14860/en/}).
\pause
And you have the following summary statistics:


<<summary2, dependson="data">>=
sm <- d %>%
  group_by(process) %>%
  summarize(
    n    = n(),
    mean = mean(sensitivity),
    sd   = sd(sensitivity)
  )
sm
@

\end{frame}



\begin{frame}[fragile]
\frametitle{\pvalue{}s}

\small

When there are lots of means, the first null hypothesis is typically
\[
H_0: \mu_g = \mu \, \forall \, g
\]

\pause

<<ftest, dependson="data", echo=TRUE>>=
oneway.test(sensitivity ~ process, data = d)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Pairwise differences}

Then we typically look at pairwise differences:
<<pairwise, dependson="ftest", echo=TRUE>>=
pairwise.t.test(d$sensitivity,
                d$process,
                pool.sd = FALSE,
                p.adjust.method = "none")
@

\end{frame}



% \begin{frame}[fragile]
% \frametitle{Confidence intervals}
%
% We can consider confidence intervals for the following quantities
% \begin{itemize}
% \item $\mu_g$ for all $g$
% \item $\mu_g - \mu_j$ for any $i$ and $j$
% \end{itemize}
%
% \pause
%
% <<>>=
% t.test(d$sensitivity[d$process == 1])$conf.int %>% as.numeric
% t.test(d$sensitivity[d$process == 2])$conf.int %>% as.numeric
% t.test(d$sensitivity[d$process == 3])$conf.int %>% as.numeric
% @
%
% \pause
%
% <<>>=
% TukeyHSD()
% @
%
% \end{frame}




\begin{frame}[fragile]
\frametitle{Posteriors for $\mu$}

When
\[ Y_{g,i} \ind N(\mu_g,\sigma_g^2),\]
\pause
we have
\[
\mu_g|y \ind t_{n_g-1}(\overline{y}_g, s_g^2/n_g)
\]
\pause
and that a draw for $\mu_g$ can be obtained by taking

\[
\overline{y}_g + T_{n_g-1} s_g/\sqrt{n_g}, \quad T_{n_g-1} \ind t_{n_g-1}(0,1).
\]


<<mu3_draws, dependson="summary2">>=
sims <- bind_rows(
  sims, # groups 1 and 2
  tibble(
    rep = 1:nr,
    process = "P3",
    mu = sm$mean[3] + rt(nr, df = sm$n[3]-1) * sm$sd[3] / sqrt(sm$n[3]))
)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Compare posteriors}

<<mu3_posteriors, dependson="mu3_draws">>=
ggplot(sims, aes(x=mu, y=after_stat(density), fill=process, group=process)) +
  geom_histogram(position = "identity", alpha=0.5, binwidth=0.1) +
  theme_bw()
@

\end{frame}







\begin{frame}[fragile]
\frametitle{Credible intervals for differences}

Use the simulations to calculate posterior probabilities and
credible intervals for differences.

<<mu3_comparison, dependson="mu3_draws", echo=TRUE>>=
# Estimate of the probability that one mean is larger than another
sims %>%
  spread(process, mu) %>%
  mutate(`mu1-mu2` = P1-P2,
            `mu1-mu3` = P1-P3,
            `mu2-mu3` = P2-P3) %>%
  select(`mu1-mu2`,`mu1-mu3`,`mu2-mu3`) %>%
  gather(comparison, diff) %>%
  group_by(comparison) %>%
  summarize(probability = mean(diff>0) %>% round(4),
            lower = quantile(diff, .025) %>% round(2),
            upper = quantile(diff, .975) %>% round(2)) %>%
  mutate(credible_interval = paste("(",lower,",",upper,")", sep="")) %>%
  select(comparison, probability, credible_interval)
@

\end{frame}




\subsection{Common variance}
\begin{frame}[fragile]
\frametitle{Common variance model}

\footnotesize

In the model
\[
Y_{g,i} \ind N(\mu_g,\sigma_g^2)
\]
we can calculate a \pvalue{} for the following null hypothesis:
\[
H_0:\sigma_g = \sigma \quad \mbox{for all} \quad g
\]

\vspace{-0.1in} \pause


<<bartlett, dependson="data", echo=TRUE>>=
bartlett.test(sensitivity ~ process, data = d)
@
\pause
This may give us reason to proceed as if the variances is the same in all
groups, i.e.
\[
Y_{g,i} \ind N(\mu_g,\sigma^2).
\]
\pause
This assumption is common when the number of observations in the groups is
small.

\end{frame}



\begin{frame}[fragile]
\frametitle{Comparing means when the variances are equal}

Assuming $Y_{g,i} \ind N(\mu_g,\sigma^2)$, we can test
\[
H_0: \mu_g = \mu \, \forall \, g
\]

\pause

<<ftest2, dependson="data", echo=TRUE>>=
oneway.test(sensitivity ~ process, data = d, var.equal = TRUE)
@

\pause

Then we typically look at pairwise differences,

i.e. $H_0: \mu_g = \mu_{g'}$.
<<pairwise2, dependson="ftest", echo=TRUE>>=
pairwise.t.test(d$sensitivity, d$process, p.adjust.method = "none")
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Posteriors for $\mu$}

If $Y_{g,i} \ind N(\mu_g,\sigma^2)$ and we use the prior
$p(\mu_1,\ldots,\mu_G,\sigma^2) \propto 1/\sigma^2$,
\pause then
\[
\mu_g|y,\sigma^2 \ind N(\overline{y}_g, \sigma^2/n_g)
\quad
\sigma^2|y \sim IG\left( \frac{n-G}{2}, \frac{1}{2}\sum_{g=1}^G \sum_{i=1}^{n_g} (y_{g,i}-\overline{y}_g)^2\right)
\]
where $n=\sum_{g=1}^G n_g$.
\pause
and thus, we obtain joint samples for $\mu$ by performing the following
\begin{enumerate}
\item $\sigma^{2(m)} \sim p(\sigma^2|y)$ \\
\item For $g=1,\ldots,G$, $\mu_g \sim p(\mu_g|y,\sigma^{2(m)})$.
\end{enumerate}


<<mu2_draws, dependson="summary2">>=
nr = 1e5
sims <- data.frame(rep = 1:nr,
  sigma = 1/sqrt( rgamma(nr,
                         shape = sum(sm$n-1)/2,
                         rate = sum((sm$n-1)*sm$sd^2)/2
                         )
                  )
  ) %>%
  mutate(
    mu1 = rnorm(nr, mean = sm$mean[1], sd = sigma / sqrt(sm$n[1])),
    mu2 = rnorm(nr, mean = sm$mean[2], sd = sigma / sqrt(sm$n[2])),
    mu3 = rnorm(nr, mean = sm$mean[3], sd = sigma / sqrt(sm$n[3])))
@

\end{frame}



\begin{frame}
\frametitle{Compare posteriors}

<<mu2_posteriors, dependson="mu2_draws">>=
d_plot <- sims %>% gather(parameter, draw, -rep, -sigma)
ggplot(d_plot, aes(draw, fill=parameter, group=parameter)) +
  geom_histogram(position = "identity", alpha = 0.5, binwidth=0.1) +
  theme_bw()
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Credible interval for the differences}

To compare the means,
we compare the samples drawn from the posterior.


<<mu2_comparison, dependson="mu2_draws", echo=TRUE>>=
sims %>%
  mutate(`mu1-mu2` = mu1-mu2,
         `mu1-mu3` = mu1-mu3,
         `mu2-mu3` = mu2-mu3) %>%
  select(`mu1-mu2`,`mu1-mu3`,`mu2-mu3`) %>%
  gather(comparison, diff) %>%
  group_by(comparison) %>%
  summarize(probability = mean(diff>0) %>% round(4),
            lower = quantile(diff, .025) %>% round(2),
            upper = quantile(diff, .975) %>% round(2)) %>%
  mutate(credible_interval = paste("(",lower,",",upper,")", sep="")) %>%
  select(comparison, probability, credible_interval)
@

\end{frame}



\subsection{Summary}
\begin{frame}
\frametitle{Summary}

Multiple (independent) normal means \pause
\begin{itemize}
\item \pvalue{}s
\item confidence intervals
\item posterior densities
\item credible intervals
\item posterior probabilities
\end{itemize}

\end{frame}

\end{document}



