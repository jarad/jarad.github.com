\documentclass[aspectratio=169,handout]{beamer}

\usepackage{verbatim,multicol,amsmath}

\input{../../frontmatter}
\input{../../commands}

\title{$\I$03 - Bayesian parameter estimation}

% \setbeamertemplate{background}
% {\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{video_overlay}}


<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=2.5, 
               size='scriptsize', 
               out.width='\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("dplyr")
library("tidyr")
library("ggplot2")
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\begin{document}

\begin{frame}[t]
\maketitle
\end{frame}


\begin{frame}[t]
\frametitle{Outline}
\begin{itemize}
\item Bayesian parameter estimation
  \begin{itemize}
  \item Condition on what is known
  \item Describe {\bf belief} using probability
  \item Terminology
    \begin{itemize}
    \item Prior $\to$ posterior
    \item Posterior expectation
    \item Credible intervals
    \end{itemize}
  \item Binomial example
    \begin{itemize}
    \item Beta distribution
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}



\section{Bayesian statistics}
\begin{frame}[t]
\frametitle{A Bayesian statistician}

Let 
\begin{itemize}
\item $y$ be the data we will collect from an experiment, \pause
\item $K$ be everything we know for certain about the world (aside from $y$), \pause and
\item $\theta$ be anything we don't know for certain.
\end{itemize}

\vspace{0.2in} \pause


My definition of a Bayesian statistician is an individual who makes decisions based on the probability distribution of those things we don't know conditional on what we know, \pause i.e. 
\[ p(\theta|y, K). \]
\pause
Typically, the $K$ is dropped from the notation.

\end{frame}


\begin{frame}[t]
\frametitle{Bayes' Rule}

Bayes' Rule applied to a partition $P=\{A_1,A_2,\ldots\}$, 
\[ P(A_i|B) = \frac{P(B|A_i)P(A_i)}{P(B)} = \frac{P(B|A_i)P(A_i)}{\sum_{i=1}^\infty P(B|A_i)P(A_i)} \]

\vspace{0.2in} \pause


Bayes' Rule also applies to probability density (or mass) functions, e.g. 
\[ p(\theta|y) =\frac{p(y|\theta)p(\theta)}{p(y)} \pause
= \frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta) d\theta}  \]
\pause
where the integral plays the role of the sum in the previous statement.

\end{frame}



\subsection{Parameter estimation}
\begin{frame}[t]
\frametitle{Parameter estimation}
Let $y$ be data from some model with unknown parameter (vector) $\theta$. \pause Then
\[ p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)}= \frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta) d\theta} \]
\pause and we use the following terminology 

\begin{tabular}{ll}
Terminology & Notation \\
\hline 
Posterior & $p(\theta|y)$ \pause \\
Prior & $p(\theta)$ \pause \\
Model (likelihood) & $p(y|\theta)$ \pause \\
Prior predictive & $p(y)$ \\
(marginal likelihood) & \pause \\
\hline
\end{tabular}

\vspace{0.1in} \pause

Bayesian parameter estimation involves updating your prior \alert{belief} about
$\theta$, $p(\theta)$, into a posterior \alert{belief} about 
$\theta$, $p(\theta|y)$, based on the data observed.

\end{frame}


\begin{frame}[t]
\frametitle{Bayesian notation}

We now have two distributions for our parameter $\theta$: prior and posterior.
To distinguish these two, we will have no conditioning in the prior 
and we will condition on $y$ in the posterior.
\pause

For example,

\begin{tabular}{lll}
& Prior & Posterior \\
\hline
Density & $p(\theta)$ & $p(\theta|y)$ \\
Expectation & $E[\theta]$ & $E[\theta|y]$ \\
Variance & $Var[\theta]$ & $Var[\theta|y]$ \\
Probabilities & $P(\theta<c)$ & $P(\theta<c|y)$ \\
\hline
\end{tabular}


\end{frame}


\begin{frame}[t]
\frametitle{Binomial model}

Suppose $Y\sim Bin(n,\theta)$, then 
\[ 
p(y|\theta) = {n\choose y} \theta^y (1-\theta)^{n-y}.
\]
A reasonable default prior is the uniform distribution on the interval $(0,1)$
\pause 
\[ 
p(\theta) = \I(0<\theta<1).
\]
\pause

Using Bayes Rule, you can find 
\[
\theta|y \sim Be(1+y, 1+n-y).
\]

\end{frame}


\begin{frame}[t]
\frametitle{Beta distribution}

\small

The \alert{beta distribution} defines a distribution for a probability, 
i.e. a number on the interval (0,1). 
The probability density function is 
\[ 
p(\theta) = \frac{\theta^{a-1}(1-\theta)^{b-1}}{Beta(a,b)}\I(0<\theta<1)
\]
where $a,b>0$ and $Beta$ is the beta function, \pause 
i.e. 

\[ 
Beta(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} 
\quad\mbox{and} \quad
\Gamma(a) = \int_0^\infty x^{a-1}e^{-x} dx.
\]
\pause
The beta distribution has the following properties:
\begin{itemize} \small
\item $E[\theta] = \frac{a}{a+b}$, \pause
\item $Var[\theta] = \frac{ab}{(a+b)^2(a+b+1)}$, \pause and
\item $Be(1,1) \stackrel{d}{=} Unif(0,1)$.
\end{itemize}

\end{frame}


\begin{frame}[t]
\frametitle{Beta densities}

<<>>=
my_dbeta <- function(x) {
  data.frame(theta = seq(0, 1, length=101)) %>%
    mutate(density = dbeta(theta, x$a, x$b))
}

d <- expand.grid(a = c(.5,1,2, 10),
            b = c(.5,1,2, 10)) %>%
  group_by(a,b) %>%
  do(my_dbeta(.)) %>%
  ungroup() %>%
  mutate(a = factor(paste0("a = ", a), levels=paste0("a = ", c(.5,1,2,10))),
         b = factor(paste0("b = ", b), levels=paste0("b = ", c(.5,1,2,10))))

ggplot(d, aes(x=theta, y=density)) +
  geom_line() +
  facet_grid(a~b) +
  theme_bw() +
  labs(x = expression(theta)) +
  ylim(0,5)
@

\end{frame}


\begin{frame}[t]
\frametitle{Beta posterior}

Suppose we have made 100 sensors according to a particular protocol and 2 have 
a sensitivity below a pre-determined threshold. 
\pause
Let $Y$ be the number below the threshold.
Assume $Y\sim Bin(n,\theta)$ with $n=100$ \pause and 
$\theta \sim Be(1,1)$, 
\pause 
then 
\[ 
\theta|y \sim Be(1+y,1+n-y) \stackrel{d}{=} Be(3,99).
\]
\end{frame}


\begin{frame}[t,fragile]
\frametitle{Posterior density}

<<>>=
n <- 100
y <- 2

ggplot(data.frame(x=c(0,.1)), aes(x)) +
  stat_function(fun = dbeta, args = list(shape1 = 1+y, shape2 = 1+n-y)) +
  labs(x = expression(theta),
       y = expression(paste("p(",theta,"|y)")),
       title = "Posterior density") +
  theme_bw()
@

\end{frame}



\begin{frame}[t,fragile]
\frametitle{Posterior expectation}

Often times it is inconvenient to provide a full posterior and so we often 
summarize using a point estimate from the posterior. 
\pause
For a point estimate, we can use the posterior expectation:
\pause
\[
\hat{\theta}_{Bayes} = E[\theta|y] \pause 
= \frac{1+y}{(1+y)+(1+n-y)} \pause
= \frac{1+y}{2+n}
\]

<<echo=TRUE>>=
(1 + y) / (2 + n)
@
\pause
Note that this is close, but not exactly equal to $\hat{\theta}_{MLE} = y/n$. 
\pause
Since the MLE is unbiased, this posterior expectation will generally be biased
but it is still consistent since $\hat{\theta}_{Bayes} \to \hat{\theta}_{MLE}$. 

\end{frame}




\begin{frame}[t,fragile]
\frametitle{Credible intervals}

A \alert{$100(1-a)$\% credible interval} is any interval $(L,U)$ such that

\[ 
1-a = \int_L^U p(\theta|y)\, d\theta. \hspace{2in}
\]
\pause

An \alert{equal-tail $100(1-a)$\% credible interval} is the interval 
$(L,U)$ such that 
\[ 
a/2 = \int_{-\infty}^L p(\theta|y)\, d\theta = \int_U^\infty p(\theta|y)\, d\theta.
\]
\pause
<<echo=TRUE>>=
# 95% credible interval is 
ci <- qbeta(c(.025, .975), 1 + y, 1 + n - y) 
round(ci, 3)
@

\end{frame}


\begin{frame}[t,fragile]
\frametitle{Equal-tail 95\% credible interval}

<<echo=FALSE>>=
ggplot(data.frame(x=c(0,.1)), aes(x)) +
  stat_function(fun = dbeta, args = list(shape1 = 1+y, shape2 = 1+n-y)) +
  stat_function(fun = dbeta, args = list(shape1 = 1+y, shape2 = 1+n-y), 
                xlim = ci,
                geom = "area", fill = "red") +
  labs(x = expression(theta),
       y = expression(paste("p(",theta,"|y)")),
       title = "Posterior density with 95% area shaded") +
  theme_bw()
@

\end{frame}


\begin{frame}[t]
\frametitle{Summary}


Bayesian parameter estimation involves
\begin{enumerate}[<+->]
\item Specifying a model $p(y|\theta)$ for your data.
\item Specifying a prior $p(\theta)$ for the parameter. 
\item Deriving the posterior 
\[
p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)} \propto p(y|\theta)p(\theta).
\]
This equation updates your prior \alert{belief}, $p(\theta)$, about the 
unknown parameter $\theta$ into your posterior \alert{belief}, $p(\theta|y)$,
about $\theta$.
\item Calculating quantities of interest, e.g.
  \begin{itemize}
  \item Posterior expectation, $E[\theta|y]$
  \item Credible interval
  \end{itemize}
\end{enumerate}

\end{frame}


\begin{frame}[t,fragile]
\frametitle{Bayesian analysis for binomial model summary}

Let $Y\sim Bin(n,\theta)$ and assume $\theta\sim Be(a,b)$. 
\pause
Then 
\[
\theta|y \sim Be(a+y,b+n-y).
\]
\pause
A default prior is $\theta\sim Be(1,1) \stackrel{d}{=} Unif(0,1)$.

\vspace{0.2in} \pause

R code for binomial analysis:
<<eval = FALSE, echo = TRUE, size = 'tiny'>>=
a <- 1; b <- 1                         # default uniform prior
y <- 3; n <- 10                        # data

curve(dbeta(x, ay, b + n - y))         # posterior (pdf)
(a + y)/(a + b + n)                    # posterior mean
qbeta(.5, a + y, b + n - y)            # posterior median
qbeta(c(.025, .975), a + y, b + n - y) # 95% equal tail credible interval

# Probabilities
pbeta(0.5, a + y, b + n - y)           # P(theta < 0.5|y) 

# Special cases
qbeta(c(0, .95), a + y, b + n - y)     # if y=0, use a lower one-sided CI
qbeta(c(.05, 1), a + y, b + n - y)     # if y=n, use a upper one-sided CI
@

\end{frame}


\end{document}
