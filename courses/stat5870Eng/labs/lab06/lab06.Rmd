---
title: "Lab06 - Built-in analyses in R"
author: "Jarad Niemi"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
---


[![](../rcode_button.png){fig-alt="R Code Button" fig-align="left" width=20%}](lab06.R)

```{r metadata, include=FALSE}
# Author: Jarad Niemi
# Date:   2024-10-10
# Purpose: Introduce built-in analyses in R including one population 
#          binomial and normal data.
#-------------------------------------------------------------------------------

```

```{r packages}
library("tidyverse")
```



## Normal model

```{r normal-section, include=FALSE}
#-------------------------------------------------------------------------------
# Normal model

```

### Background

Suppose $Y_i\stackrel{ind}{\sim} N(\mu,\sigma^2)$. 
Recall that a Bayesian analysis with the default prior 
$p(\mu,\sigma^2) \propto 1/\sigma^2$ provides the same analysis for $\mu$ as a
non-Bayesian analysis. 
That is 

- MLE and the posterior mean/median for $\mu$ are the same
- confidence and credible intervals for $\mu$ are exactly the same

This is because the posterior for $\mu$ is 
$$
\mu|y \sim t_{n-1}(\overline{y}, s^2/n)
$$
which means that 
$$
\left.\frac{\mu-\overline{y}}{s/\sqrt{n}} \right|y \sim t_{n-1}(0,1).
$$
while the sampling distribution for $\overline{y}$ is such that 
$$
T=\frac{\overline{Y}-\mu}{S/\sqrt{n}} \sim t_{n-1}(0,1).
$$

Please note the difference in these two statements is what is considered 
random.
In the first two $\mu$ is considered random while the data which are used to
calculate $\overline{y}$ and $s$ are fixed.
This does not mean that a Bayesian considered the actual true value of $\mu$ to
be random. Instead it means that we are expressing our uncertainty in $\mu$ 
through probability.
In the last statement, the data are considered random 
which is why $\overline{Y}$ and $S$ are capitalized while $\mu$ is considered 
fixed.

### Manual analysis

Suppose you observe the following data
```{r normal-data-simulation}
# Normal data
set.seed(20180219)
(y <- rnorm(10, mean = 3.2, sd = 1.1)) # 3.2 and 1.1 are the population parameters
```
Then you can manually construct an MLE and posterior mean using `mean()`.

```{r normal-mean}
# Normal mean MLE and posterior expectation
(ybar <- mean(y))
```

and a 95% credible/confidence interval using

```{r normal-ci}
# Manual confidence/credible interval
n <- length(y)
s <- sd(y)
a <- 0.05

ybar + c(-1,1) * qt(1 - a/2, df = n-1) * s / sqrt(n)
```

### Built-in analysis

You can use the `t.test()` function to perform this for you.

```{r normal-t-test}
# T-test (one-sample normal model)
t.test(y)
```

You can extract the estimates and confidence/credible intervals,
but first you need to know how to access the appropriate objects within the
t-test object.

```{r normal-t-test-objects}
# T-test objects
tt <- t.test(y)
names(tt)
str(tt)
```

It isn't always obvious what object we want and thus we often need to just look
at all of them until we figure out which ones we need. 
In this case, `conf.int` seems like a good guess for the confidence/credible
intervals. 
It turns out that `estimate` will gives us the MLE and posterior mean/median.

```{r normal-estimate-and-ci}
# Normal estimators for the mean (mu)
tt$estimate # MLE and Posterior expectation
tt$conf.int # Credible and confidence interval
```

#### Modifying arguments

We can also change the default argument values to

- change the type of hypothesis test
- change the null value
- change the confidence level

Suppose we wanted to test $H_0:\mu\ge 1$ vs $H_a:\mu < 1$ at a signficance
level of 0.9 and/or we wanted to construct a 90\% one-sided lower
confidence interval.

```{r normal-t-test-arguments}
# Normal arguments 
t.test(y, alternative = "less",    mu = 1,  conf.level = 0.9)
```

Suppose we wanted to test $H_0:\mu\le -1$ vs $H_a:\mu > -1$ at a signficance
level of 0.99 and/or we wanted to construct a 99\% one-sided upper
confidence interval.

```{r normal-t-test-arguments2}
t.test(y, alternative = "greater", mu = -1, conf.level = 0.99)
```


#### Activity

Using the following data, compare the point estimate and confidence/credible
intervals obtained using the `t.test()` function to estimates and 
intervals you create yourself.

If you have more time, try to play around with the function arguments to create
one-sided confidence intervals, change the null hypothesis value, 
and change the confidence/significance level.

```{r normal-activity-data}
# Normal activity data
set.seed(1)
y <- rnorm(1001, mean = 256, sd = 34.6)
```

<details><summary>Click for solution</summary> 
```{r normal-activity-solution, purl=FALSE}
# Normal activity solution
t.test(y)
mean(y)
mean(y) + c(-1, 1) * qt(.975, df = length(y) - 1) * sd(y) / sqrt(length(y))
```
</details>




## Binomial model

```{r binomial-section, include=FALSE}
#-------------------------------------------------------------------------------
# Binomial model

```

### Bayesian analysis

Let $Y\sim Bin(n,\theta)$.
Recall that our default prior for $\theta$ is $\theta \sim Be(1,1)$, which is
equivalent to a $Unif(0,1)$ prior.
The posterior under this prior is $\theta|y \sim Be(1+y,1+n-y)$. 
In order to perform this analysis, 
we simply use the beta distribution in R. 

Suppose you observe 9 successes out of 13 attempts. 

```{r binomial-data}
# Data
n <- 13
y <- 9

# Prior, Be(a,b)
a <- b <- 1
```

The posterior is 

```{r binomial-posterior}
# Posterior
ggplot(data.frame(x=c(0,1)), aes(x=x)) + 
  stat_function(fun = dbeta, 
                args = list(shape1 = a+y,
                            shape2 = b+n-y)) + 
  labs(x = expression(theta),
       y = paste(expression("p(",theta,"|y)")),
       title = "Posterior distribution for probability of success") +
  theme_bw()
```

The posterior expectation is 

```{r binomial-posterior-expectation}
# Posterior expectation
(a + y) / (a + b + n)
```

A 95\% equal-tail credible interval is 

```{r binomial-credible-interval}
# 95% equal-tail credible interval
qbeta(c(.025, .975), a + y, b + n - y)
```

The probability that $\theta$ is greater than 0.5, i.e. $P(\theta>0.5|y)$ is 

```{r binomial-probabilities}
# Binomial probabilities
1 - pbeta(0.5, a + y, b + n - y) # P(theta > 0.5 | y)
```

#### Jeffreys prior activity

An alternative prior is called Jeffreys prior and it corresponds to a 
Be(0.5, 0.5) prior. 
Suppose you observed 17 successes out of 20 attempts and you are willing to assume 
independence and a common probability of success.
Use Jeffreys prior on this probability of success to do the following 

- Plot the posterior for the true probability of success
- Calculate the posterior median for the true probability of success
- Calculate a one-sided upper 95% credible interval, i.e. (L, 1), for the true probability of success $\theta$
- Calculate the probability that the true probability of success $\theta$ is greater than 0.9.


<details><summary>Click for solution</summary>  
```{r, purl=FALSE}
# Binomial activity solutions
a <- b <- 0.5 # Jeffreys prior, Be(a,b)
n <- 20 # attempts
y <- 17 # successes

curve(dbeta(x, a + y, b + n - y))   # Posterior density
qbeta(0.5, a + y, b + n - y)        # Posterior median
c(qbeta(0.05, a + y, b + n - y), 1) # One-sided credible interval
1 - pbeta(0.9, a + y, b + n - y)    # P(theta > 0.9 | y)
```
</details>




### Non-Bayesian analyses when n is small 

To perform non-Bayesian analyses when n is small,
use the `binom.test` function.
This will calculate pvalues and confidence intervals 
(based on inverting hypothesis tests).

Suppose you observe 9 successes out of 13 attempts. 

```{r binomial-frequentist-n-small}
# Small n data
n <- 13
y <- 9

(bt <- binom.test(y, n))
```

The p-value here is  is equivalent to calculating the probability of observing y 
equal to 0, 1, 2, 3, 4, 9, 10, 11, 12, 13 if $\theta$ is 0.5.
This range of integers from 0 to 4 and 9 to 13 is the as or more extreme 
regions.

```{r binomial-manual-p-value}
# Binomial manual p value calculation
sum(dbinom(c(0:4, 9:13), size = n, prob = 0.5))
```

Recall that there is a one-to-one correspondence between pvalues and confidence
intervals.
This confidence interval is constructed by finding those values for $\theta$
such that the pvalue is half of one minus the confidence level
(since it is a two-sided interval).

```{r binomial-confidence-interval}
# Binomial confidence interval
(ci <- bt$conf.int)

# Using the end points as the hypothesized probability should result in 
# a p-value that is half of 1 - confidence level
binom.test(y, n, p = ci[2])$p.value # This one matches exactly
binom.test(y, n, p = ci[1])$p.value # This one is close

# Find the "correct" endpoint
# Create a function that should be zero when we have the correct probability
f <- function(p) {
  binom.test(y, n, p = p)$p.value - 0.025 # 0.025 is what the p-value should be
}

# Use unitroot to find value for p such that its p-value is 0.025
(u <- uniroot(f, lower = 0, upper = y/n))

# Check the p-value for this probability
binom.test(y, n, p = u$root)$p.value      
```



#### binom.test Activity

Suppose you observe 11 successes out of 12 attempts. 
Calculate a pvalue for the two-sided test that $\theta=0.5$ and construct
a 95\% confidence interval.


<details><summary>Click for solution</summary> 

```{r, purl=FALSE}
# Binomial binom.test activity solution
(bt <- binom.test(11, 12))

# Extract just the relevant results
bt$p.value
bt$conf.int
```
</details>




### Non-Bayesian analyses when n is large (and p is not close to 0 or 1) 

If you observe 78 successes out of 100 attempts, 
then you can use the `prop.test()` function to generate a number of statistics
automatically based on the CLT.

```{r binomial-n-large}
# Data with large n (and p not too close to 0 or 1)
n <- 100
y <- 78
```

When n is large, you can use the `prop.test` function. 

```{r binomial-prop-test}
# prop.test for large n (and p not too close to 0 or 1)
(pt <- prop.test(y, n))
```

The estimate is 

```{r binomial-large-n-mle}
# Binomial MLE
pt$estimate
```

An approximate 95\% confidence interval is 

```{r binomial-large-n-ci}
# Binomial CI
pt$conf.int
```

We can construct this yourself using the following formula

```{r binomial-clt}
# Manual CI
p <- y/n
p + c(-1, 1) * qnorm(.975) * sqrt(p * (1 - p) / n)
```

The results don't quite match because `prop.test` uses the continuity correction
(which is the appropriate thing to do).

```{r binomial-no-continuity-correction}
# You should always use the continuity correct
# this is just for illustrative purposes
prop.test(y, n, correct = FALSE)
```

Unfortunately, the results still don't quite match.
It turns out there is a 
[large literature on how to do this](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval) 
and the suggestions 
are to either 1) use Wilson's interval (which is what `prop.test` actually does)
or to  
2) perform a Bayesian analysis using Jeffreys prior, i.e. a Beta(1/2, 1/2).


#### Activity

Suppose you observed 694 success out of 934 attempts. 
Compute an approximate 95\% equal-tail confidence interval using `prop.test` 
and compare this to a 95\% equal-tail confidence interval you construct using
the Central Limit Theorem.

<details><summary>Click for solution</summary> 
```{r binomial-activity2-solution, purl=FALSE}
# Binomial activity data
y <- 694
n <- 934

# Use prop.test since n is large (and p is not too close to 0 or 1)
prop.test(y, n)$conf.int

# MLE
p <- y/n

# CI from CLT
p + c(-1,1)*qnorm(.975)*sqrt(p*(1-p)/n)

# If you turn off the continuity correction, you will get closer to the 
# pure CLT interval
prop.test(y, n, correct = FALSE)$conf.int
```
If you really want to figure out what the function is doing, 
you can look at the function by just typing `prop.test` and hitting enter.
</details>



#### Activity

Suppose you observe 0 success out of 77 attempts. 
Compare 95% confidence intervals given by `prop.test` and `binom.test` to 
an interval you construct based on the CLT and to 95% credible intervals.

<details><summary>Click for solution</summary> 

So this is a bit of a trick question since there were 0 successes. 
When you run `prop.test` and `binom.test` you are given one-sided confidence 
intervals.
The CLT interval doesn't exist since the standard error is zero.
The appropriate credible interval to use is a one-sided interval.

```{r, purl=FALSE}
# Binomial activity 3 data
y <- 0
n <- 77

# Large n (but p is close to 0)
prop.test(y, n)$conf.int

# Small n (but y/n is 0)
binom.test(y, n)$conf.int

# Bayesian one-sided interval with uniform prior
qbeta(c(0, .95), 1 + y, 1 + n - y)
```
</details>



## Reading data from files

### Set working directory

Set your working directory using 
Session > Set Working Directory > Choose Directory in RStudio or using the 
`setwd()` function.
You can also save [creativity.csv](creativity.csv) into your working directory
if you want.

### Write/read data from file

First, let's write some data to a file. 

```{r}
set.seed(20180220)
# Generate some simulated data
n <- 100
d <- data.frame(rep = 1:n,
                response = sample(c("Yes","No"), n, replace=TRUE, prob = c(.2,.8)),
                measurement = rnorm(n, mean = 55, sd = 12))

# Write it to a file
# make sure you have set your working directory to someplace where you want this
# file to be written
write.csv(d, 
          file = "data.csv",
          row.names = FALSE)
```

Alternatively, you could use the `write_csv()` function in the 
[readr](https://cran.r-project.org/web/packages/readr/index.html) package. 

```{r, eval=FALSE}
install.packages("readr")
library("readr") 
write_csv(d, path = "data.csv")
```

Now let's read this data back in. 

```{r}
my_data <- read.csv("data.csv")
```

If you want to delete the file, you can run the following

```{r, purl = FALSE}
if (file.exists("data.csv")) file.remove("data.csv")
```

Take a look at the data to make sure it looks correct:

```{r}
head(my_data)
str(my_data)
```

### Binomial data

To use `prop.test()` and `binom.test()`, 
you need to calculate the number of successes and the number of attempts.

```{r}
y <- sum(my_data$response == "Yes")
n <- length(my_data$response)
prop.test(y, n)
binom.test(y, n)
```

### Normal data

To analyze the normal data, you can just use `t.test()` directly.

```{r}
t.test(my_data$measurement)
```



#### Online activity

Read in the data at [creativity.csv](creativity.csv) 
and then construct confidence/credible intervals for mean creativity score
for both the Intrinsic and Extrinsic groups. 

<details><summary>Click for solution</summary> 

There are a variety of ways to do this. 
I will construct two new data frames to contain the Intrinsic and Extrinsic 
data and then construct the intervals.

```{r, purl=FALSE}
creativity <- read.csv("http://www.jarad.me/courses/stat5870Eng/labs/lab06/creativity.csv")

intrinsic_score <- creativity$Score[creativity$Treatment == "Intrinsic"]
extrinsic_score <- creativity$Score[creativity$Treatment == "Extrinsic"]
t.test(intrinsic_score)
t.test(extrinsic_score)

# Using the `subset` command which subsets the data.frame
intrinsic <- subset(creativity, Treatment == "Intrinsic")
extrinsic <- subset(creativity, Treatment == "Extrinsic")

t.test(intrinsic$Score)
t.test(extrinsic$Score)


# Another way to subset uses the dplyr package
# This will be prettier when you have a long sequence of commands
library(dplyr)
intrinsic <- creativity %>% filter(Treatment == "Intrinsic")
extrinsic <- creativity %>% filter(Treatment == "Extrinsic")
```



If you want to find out more about these data, 
take a look at the help file for `case0101` in the 
[Sleuth3](https://cran.r-project.org/web/packages/Sleuth3/index.html) package.

```{r, eval=FALSE}
install.packages("Sleuth3")
library("Sleuth3")
?case0101
```

</details>
