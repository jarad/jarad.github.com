---
title: "Lab06 - Built-in analyses in R"
author: "Jarad Niemi"
date: "`r Sys.Date()`"
output: html_document
---


[![](../rcode_button.png){fig-alt="R Code Button" fig-align="left" width=20%}](lab06.R)

```{r}
library("tidyverse")
```

Set your working directory using 
Session > Set Working Directory > Choose Directory in RStudio or using the 
`setwd()` function.
You can also save [creativity.csv](creativity.csv) into your working directory
if you want.



## Normal data

### Background

Suppose $Y_i\stackrel{ind}{\sim} N(\mu,\sigma^2)$. 
Recall that a Bayesian analysis with the default prior 
$p(\mu,\sigma^2) \propto 1/\sigma^2$ provides the same analysis for $\mu$ as a
non-Bayesian analysis. 
That is 

- MLE and the posterior mean/median for $\mu$ are the same
- confidence and credible intervals for $\mu$ are exactly the same

This is because the posterior for $\mu$ is 
$$
\mu|y \sim t_{n-1}(\overline{y}, s^2/n)
$$
which means that 
$$
\left.\frac{\mu-\overline{y}}{s/\sqrt{n}} \right|y \sim t_{n-1}(0,1).
$$
while the sampling distribution for $\overline{y}$ is such that 
$$
T=\frac{\overline{Y}-\mu}{S/\sqrt{n}} \sim t_{n-1}(0,1).
$$

Please note the difference in these two statements is what is considered 
random.
In the first two $\mu$ is considered random while the data which are used to
calculate $\overline{y}$ and $s$ are fixed.
This does not mean that a Bayesian considered the actual true value of $\mu$ to
be random. Instead it means that we are expressing our uncertainty in $\mu$ 
through probability.
In the last statement, the data are considered random 
which is why $\overline{Y}$ and $S$ are capitalized while $\mu$ is considered 
fixed.

### Manual analysis

Suppose you observe the following data
```{r}
set.seed(20180219)
(y <- rnorm(10, mean = 3.2, sd = 1.1)) # 3.2 and 1.1 are the population parameters
```
Then you can manually construct an MLE and posterior mean using `mean()`.

```{r}
(ybar <- mean(y))
```

and a 95% credible/confidence interval using

```{r}
n <- length(y)
s <- sd(y)

ybar + c(-1,1)*qt(.975, df = n-1)*s/sqrt(n)
```

### Built-in analysis

You can use the `t.test()` function to perform this for you.

```{r}
t.test(y)
```

You can extract the estimates and confidence/credible intervals,
but first you need to know how to access the appropriate objects within the
t-test object.

```{r}
tt <- t.test(y)
names(tt)
str(tt)
```

It isn't always obvious what object we want and thus we often need to just look
at all of them until we figure out which ones we need. 
In this case, `conf.int` seems like a good guess for the confidence/credible
intervals. 
It turns out that `estimate` will gives us the MLE and posterior mean/median.

```{r}
tt$estimate
tt$conf.int
```

#### Modifying arguments

We can also change the default argument values to

- change the type of hypothesis test
- change the null value
- change the confidence level

Suppose we wanted to test $H_0:\mu\ge 1$ vs $H_a:\mu < 1$ at a signficance
level of 0.9 and/or we wanted to construct a 90\% one-sided lower
confidence interval.

```{r}
t.test(y, alternative = "less",    mu = 1,  conf.level = 0.9)
```

Suppose we wanted to test $H_0:\mu\le -1$ vs $H_a:\mu > -1$ at a signficance
level of 0.99 and/or we wanted to construct a 99\% one-sided upper
confidence interval.

```{r}
t.test(y, alternative = "greater", mu = -1, conf.level = 0.99)
```


#### Activity

Using the following data, compare the point estimate and confidence/credible
intervals obtained using the `t.test()` function to estimates and 
intervals you create yourself.

If you have more time, try to play around with the function arguments to create
one-sided confidence intervals, change the null hypothesis value, 
and change the confidence/significance level.

```{r}
set.seed(1)
y <- rnorm(1001, mean = 256, sd = 34.6)
```

<details><summary>Click for solution</summary> 
```{r, purl=FALSE}
t.test(y)
mean(y)
mean(y) + c(-1,1)*qt(.975, df = length(y)-1)*sd(y)/sqrt(length(y))
```
</details>




## Binomial data

### Bayesian analysis

Let $Y\sim Bin(n,\theta)$.
Recall that our default prior for $\theta$ is $\theta \sim Be(1,1)$, which is
equivalent to a $Unif(0,1)$ prior.
The posterior under this prior is $\theta|y \sim Be(1+y,1+n-y)$. 
In order to perform this analysis, 
we simply use the beta distribution in R. 

Suppose you observe 9 successes out of 13 attempts. 

```{r}
a <- b <- 1
n <- 13
y <- 9
```

The posterior is 

```{r}
ggplot(data.frame(x=c(0,1)), aes(x=x)) + 
  stat_function(fun = dbeta, 
                args = list(shape1 = a+y,
                            shape2 = b+n-y)) + 
  labs(x = expression(theta),
       y = paste(expression("p(",theta,"|y)")),
       title = "Posterior distribution for probability of success") +
  theme_bw()
```

The posterior expectation is 

```{r}
(a+y)/(a+b+n)
```

A 95\% equal-tail credible interval is 

```{r}
qbeta(c(.025,.975), a+y, b+n-y)
```

The probability that $\theta$ is greater than 0.5, i.e. $P(\theta>0.5|y)$ is 

```{r}
1-pbeta(0.5, a+y, b+n-y)
```

#### Jeffreys prior activity

An alternative prior is called Jeffreys prior and it corresponds to a 
Be(0.5,0.5) prior. 
Suppose you observed 17 successes out of 20 attempts and you are willing to assume 
independence and a common probability of success.
Use Jeffreys prior on this probability of success to do the following 

- Plot the posterior for the true probability of success
- Calculate the posterior median for the true probability of success
- Calculate a one-sided upper 95% credible interval for the true probability of success
- Calculate the probability that the true probability of success is greater than 0.9.


<details><summary>Click for solution</summary>  
```{r, purl=FALSE}
a <- b <- 0.5
n <- 20
y <- 17
curve(dbeta(x, a+y, b+n-y))
qbeta(0.5, a+y, b+n-y)
c(qbeta(0.05, a+y, b+n-y), 1)
1-pbeta(0.9, a+y, b+n-y)
```
</details>




### Non-Bayesian analyses when n is small 

To perform non-Bayesian analyses when n is small,
use the `binom.test` function.
This will calculate pvalues and confidence intervals 
(based on inverting hypothesis tests).

Suppose you observe 9 successes out of 13 attempts. 

```{r}
n <- 13
y <- 9
binom.test(y, n)
```

This is equivalent to calculating the probability of observing y equal to 
0, 1, 2, 3, 4, 9, 10, 11, 12, 13 if $\theta$ is 0.5.

```{r}
sum(dbinom(c(0:4,9:13), size = n, prob = 0.5))
```

Recall that there is a one-to-one correspondence between pvalues and confidence
intervals.
This confidence interval is constructed by finding those values for $\theta$
such that the pvalue is half of the confidence level
(since it is a two-sided interval).

```{r}
ci <- binom.test(y,n)$conf.int
binom.test(y, n, p=ci[2])$p.value # This one matches exactly
binom.test(y, n, p=ci[1])$p.value # This one is close
```



#### binom.test Activity

Suppose you observe 11 successes out of 12 attempts. 
Calculate a pvalue for the two-sided test that $\theta=0.5$ and construct
a 95\% confidence interval.


<details><summary>Click for solution</summary> 

```{r, purl=FALSE}
binom.test(11,12)
```
</details>




### Non-Bayesian analyses when n is large (and p is not close to 0 or 1) 

If you observe 78 successes out of 100 attempts, 
then you can use the `prop.test()` function to generate a number of statistics
automatically based on the CLT.

```{r}
n <- 100
y <- 78
prop.test(y,n)
```

The estimate is 

```{r}
pt <- prop.test(y,n)
pt$estimate
```

An approximate 95\% confidence interval is 

```{r}
pt$conf.int
```

We can construct this ourself using the following formula

```{r}
p <- y/n
p + c(-1,1)*qnorm(.975)*sqrt(p*(1-p)/n)
```

The results don't quite match because `prop.test` uses the continuity correction
(which is the appropriate thing to do).

```{r}
prop.test(y, n, correct = FALSE)
```

Unfortunately, the results still don't quite match.
It turns out there is a large literature on how to do this and the suggestions 
are to either 1) use Wilson's interval (which is what `prop.test` does) or to 
use 2) Jeffreys, i.e. a Beta(1/2,1/2) prior in a Bayesian analysis.


#### Activity

Suppose you observed 694 success out of 934 attempts. 
Compute an approximate 95\% equal-tail confidence interval using `prop.test` 
and compare this to a 95\% equal-tail confidence interval you construct using
the Central Limit Theorem.

<details><summary>Click for solution</summary> 
```{r, purl=FALSE}
y <- 694
n <- 934
prop.test(y, n)$conf.int

p <- y/n
p + c(-1,1)*qnorm(.975)*sqrt(p*(1-p)/n)

# If you turn off the continuity correction, you will get closer
prop.test(y, n, correct = FALSE)$conf.int
```
If you really want to figure out what the function is doing, 
you can look at the function by just typing `prop.test` and hitting enter.
</details>



#### Activity

Suppose you observe 0 success out of 77 attempts. 
Compare 95% confidence intervals given by `prop.test` and `binom.test` to 
an interval you construct based on the CLT and to 95% credible intervals.

<details><summary>Click for solution</summary> 

So this is a bit of a trick question since there were 0 successes. 
When you run `prop.test` and `binom.test` you are given one-sided confidence 
intervals.
The CLT interval doesn't exist since the standard error is zero.
The appropriate credible interval to use is a one-sided interval.

```{r, purl=FALSE}
y <- 0
n <- 77
prop.test(y, n)$conf.int
binom.test(y, n)$conf.int
qbeta(c(0,.95), 1+y, 1+n-y)
```
</details>



## Reading data from files

First, let's write some data to a file. 

```{r}
set.seed(20180220)
# Generate some simulated data
n <- 100
d <- data.frame(rep = 1:n,
                response = sample(c("Yes","No"), n, replace=TRUE, prob = c(.2,.8)),
                measurement = rnorm(n, mean = 55, sd = 12))

# Write it to a file
# make sure you have set your working directory to someplace where you want this
# file to be written
write.csv(d, 
          file = "data.csv",
          row.names = FALSE)
```

Alternatively, you could use the `write_csv()` function in the 
[readr](https://cran.r-project.org/web/packages/readr/index.html) package. 

```{r, eval=FALSE}
install.packages("readr")
library("readr") 
write_csv(d, path = "data.csv")
```

Now let's read this data back in. 

```{r}
my_data <- read.csv("data.csv")
```

If you want to delete the file, you can run the following

```{r, purl = FALSE}
if (file.exists("data.csv")) file.remove("data.csv")
```

Take a look at the data to make sure it looks correct:

```{r}
head(my_data)
str(my_data)
```

### Binomial data

To use `prop.test()` and `binom.test()`, 
you need to calculate the number of successes and the number of attempts.

```{r}
y <- sum(my_data$response == "Yes")
n <- length(my_data$response)
prop.test(y, n)
binom.test(y, n)
```

### Normal data

To analyze the normal data, you can just use `t.test()` directly.

```{r}
t.test(my_data$measurement)
```



#### Online activity

Read in the data at [creativity.csv](creativity.csv) 
and then construct confidence/credible intervals for mean creativity score
for both the Intrinsic and Extrinsic groups. 

<details><summary>Click for solution</summary> 

There are a variety of ways to do this. 
I will construct two new data frames to contain the Intrinsic and Extrinsic 
data and then construct the intervals.

```{r, purl=FALSE}
creativity <- read.csv("http://www.jarad.me/courses/stat5870Eng/labs/lab06/creativity.csv")

intrinsic_score <- creativity$Score[creativity$Treatment == "Intrinsic"]
extrinsic_score <- creativity$Score[creativity$Treatment == "Extrinsic"]
t.test(intrinsic_score)
t.test(extrinsic_score)

# Using the `subset` command which subsets the data.frame
intrinsic <- subset(creativity, Treatment == "Intrinsic")
extrinsic <- subset(creativity, Treatment == "Extrinsic")

t.test(intrinsic$Score)
t.test(extrinsic$Score)


# Another way to subset uses the dplyr package
# This will be prettier when you have a long sequence of commands
library(dplyr)
intrinsic <- creativity %>% filter(Treatment == "Intrinsic")
extrinsic <- creativity %>% filter(Treatment == "Extrinsic")
```



If you want to find out more about these data, 
take a look at the help file for `case0101` in the 
[Sleuth3](https://cran.r-project.org/web/packages/Sleuth3/index.html) package.

```{r, eval=FALSE}
install.packages("Sleuth3")
library("Sleuth3")
?case0101
```

</details>
