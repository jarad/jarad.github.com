\documentclass{beamer}

\usepackage{verbatim,multicol,amsmath}

% Theme settings
\usetheme{AnnArbor}\usecolortheme{beaver}
\setbeamertemplate{navigation symbols}{}


% Document settings
\newcommand{\lecturetitle}{Regression diagnostics}
\title[\lecturetitle]{STAT 401A - Statistical Methods for Research Workers}
\subtitle{\lecturetitle}
\author[Jarad Niemi]{Jarad Niemi (Dr. J)}
\institute[Iowa State]{Iowa State University}
\date[\today]{last updated: \today}


\setkeys{Gin}{width=0.6\textwidth}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

<<options, echo=FALSE, warning=FALSE, message=FALSE>>=
opts_chunk$set(comment=NA, fig.width=6, fig.height=5, size='tiny', out.width='0.6\\textwidth', fig.align='center', message=FALSE)
library(plyr)
library(ggplot2)
library(xtable)
library(Sleuth3)
library(abd)
@


\begin{document}



\begin{frame}
\maketitle
\end{frame}


\begin{frame}
\frametitle{All models are wrong!}

George Box (Empirical Model-Building and Response Surfaces, 1987):
\begin{quote}
All models are wrong, but some are useful. 
\end{quote}

\vspace{0.2in} \pause

{\tiny \url{http://stats.stackexchange.com/questions/57407/what-is-the-meaning-of-all-models-are-wrong-but-some-are-useful}}

{\small
\begin{quotation}
``All models are wrong" that is, every model is wrong because it is a simplification of reality. Some models, especially in the "hard" sciences, are only a little wrong. They ignore things like friction or the gravitational effect of tiny bodies. Other models are a lot wrong - they ignore bigger things. \pause \\

``But some are useful" - simplifications of reality can be quite useful. They can help us explain, predict and understand the universe and all its various components. \pause \\

This isn't just true in statistics! Maps are a type of model; they are wrong. But good maps are very useful. 
\end{quotation}
}


\end{frame}


\section{Regression diagnostics}
\frame{\frametitle{Regression}
  The simpler linear regression model is 
  \[ Y_i \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_i, \sigma^2) \]
	\pause this can be rewritten as 
	\[ Y_i = \beta_0 + \beta_1 X_i + e_i \quad e_i \stackrel{ind}{\sim} N(0,\sigma^2) \]
	\pause where we estimate the errors via the residuals
	\[ r_i = \hat{e}_i = Y_i - (\hat{\beta}_0+\hat{\beta}_1 X_i). \]
	
	\vspace{0.2in} \pause
	
	Key assumptions are:
	\begin{itemize}[<+->]
  \item Normality of the errors
	\item Constant variance of the errors
	\item Independence of the errors
  \item Linearity between mean response and explanatory variable
	\end{itemize}
}



\subsection{Normality}
\begin{frame}[fragile]
\frametitle{Histograms with best fitting bell curves}

<<echo=FALSE, out.width='0.8\\textwidth'>>=
set.seed(20141018)
x = rnorm(100)
hist(x, freq=F, main="Normal data", ylim=c(0,dnorm(0)))
curve(dnorm, add=TRUE, lwd=2)
@

\end{frame}









\begin{frame}
\frametitle{Normal QQ-plot}

\begin{definition}
The quantile-quantile or qq-plot is an exploratory graphical device used to check the validity of a distributional assumption for a data set.
\end{definition}

\vspace{0.2in} \pause

A normal qq-plot graphs the theoretical quantiles from a normal distribution versus the observed quantiles. \pause With a line that indicates perfect normality.

\vspace{0.2in} \pause

\begin{remark}
The bottom line is that, if the distribution assumption is satisfied, the points should fall roughly along the line. \pause Systematic variation from this line indicates skewness, 
\end{remark}

\end{frame}



\begin{frame}[fragile]
\frametitle{Normal}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=

n = 1000
x = rnorm(n)
opar = par(mfrow=c(1,3))
for (i in c(10,100,1000)) { 
  qqnorm(x[1:i], main=paste("n=",i))
  qqline(x[1:i]) 
}
par(opar)
@
	
\end{frame}





\begin{frame}[fragile]
\frametitle{Normal (n=10)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
my_normal_qqplots = function(n) {
  opar = par(mfrow=c(2,5), mar=rep(0,4)+.5)
  for (i in 1:10) {
    qqnorm(rnorm(n), main='', xlab="", ylab="", axes=FALSE, frame=TRUE)
    qqline(x)
  }
  par(opar)
}

my_normal_qqplots(10)
@
	
\end{frame}




\begin{frame}[fragile]
\frametitle{Normal(n=100)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
my_normal_qqplots(100)
@
  
\end{frame}



\begin{frame}[fragile]
\frametitle{Normal (n=1000)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
my_normal_qqplots(100)
@
  
\end{frame}




\begin{frame}
\frametitle{Not normal (n=10)}
<<echo=FALSE>>=
my_qqplots = function(n) {
  x = rnorm(n)
  
  opar = par(mfrow=c(2,3), mar=c(0,0,4,0)+.5)
  qqnorm(x, main="normal", axes=FALSE, frame=TRUE); qqline(x)
  qqnorm(exp(x), main="right-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(exp(x))
  qqnorm(55-exp(x), main="left-skewed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(55-exp(x))
  plot(0,0,type='n', axes=FALSE, xlab='', ylab='')
  qqnorm(y <- runif(n), main="light-tailed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(y)
  qqnorm(y <- rt(n,5), main="heavy-tailed", xlab="", ylab="", axes=FALSE, frame=TRUE); qqline(y)
  par(opar)
}


my_qqplots(10)
@
	
\end{frame}



\begin{frame}
\frametitle{Not normal (n=100)}
<<echo=FALSE>>=
my_qqplots(100)
@
  
\end{frame}



\begin{frame}
\frametitle{Not normal (n=1000)}
<<echo=FALSE>>=
my_qqplots(1000)
@
  
\end{frame}


\begin{frame}
\frametitle{Summary}

For normal qq-plots with (standardized) residuals (y-axis) vs theoretical quantiles (x-axis), the following interpretations apply 

\vspace{0.2in} \pause

\begin{itemize}[<+->]
\item If the residuals fall roughly along the line, then normality is reasonable.
\item If the residuals have a U pattern, then there is right-skewness.
\item If the residuals have an upside down U pattern, then there is left-skewness.
\item If the residuals have an S pattern, then there are light tails (we are not too concerned about this situation because our inferences will be conservative). 
\item If the residuals have a rotated Z pattern, then there are heavy tails. 
\end{itemize}

\vspace{0.2in} \pause

Other patterns are certainly possible, but these are the most common. 

\end{frame}



\subsection{Constant variance}
\begin{frame}
\frametitle{Constant variance}

Recall the model
\[ Y_i = \beta_0+\beta_1 X_i + e_i \quad e_i \stackrel{iid}{\sim} N(0,\sigma^2) \]
so the variance for the $e_i$ is constant. 

\vspace{0.2in} \pause

To assess this assumption, we look at plots of residuals vs anything and look for patterns that show different ``spreads''\pause, e.g.
\begin{itemize}
\item funnels
\item football shapes
\end{itemize}

\vspace{0.2in} \pause

The most common way this assumption is violated is by having increasing variance with increasing mean\pause, thus we often look at a residuals vs predicted (fitted) mean plot.

\end{frame}



\begin{frame}[fragile]
\frametitle{Constant variance}

<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=

par(mfrow=c(1,3), mar=c(5,4,4,2)+.1)
for (n in c(10,100,1000)) { 
  plot(runif(n), rnorm(n), axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
}
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Constant variance}

<<echo=FALSE, out.width='0.8\\textwidth'>>=

par(mfrow=c(1,1), mar=c(5,4,4,2)+.1)
n = 1000
plot(runif(n), rnorm(n), axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
xx = (1:4)/5
yy = 2.5
arrows(xx, -yy, xx, yy, col="red", lwd=2, code = 3)
abline(h=0, col="blue")
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Extreme non-constant variance (funnel)}

<<echo=FALSE, out.width='0.8\\textwidth'>>=

opar = par(mfrow=c(1,1), mar=c(5,4,4,2)+.1)
n = 1000
ub = 1
x = runif(n,0, ub)
e = rnorm(n,0,x)
plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")

# par = opar on next slide
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Extreme non-constant variance (funnel)}

<<echo=FALSE, out.width='0.8\\textwidth'>>=
plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
xx = ub*(1:4)/5
yy = ub*(.5+(1:4))/2
arrows(xx, -yy, xx, yy, col="red", lwd=2, code = 3)
abline(h=0, col="blue")
par(opar)
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Non-constant variance (n=10, $\sigma_2/\sigma_1=4$)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
my_nonconstant_variance_plots = function(n) {
  opar = par(mfrow=c(2,5), mar=rep(0,4)+.5)
  lb = 1
  ub = 4
  for (i in 1:10) {
    x = runif(n, lb, ub)
    e = rnorm(n,0,x)
    plot(x, e, axes=FALSE, frame=TRUE, xlab="", ylab="") 
  }
  par = opar
}

my_nonconstant_variance_plots(10)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Non-constant variance (n=100, $\sigma_2/\sigma_1=4$)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
my_nonconstant_variance_plots(100)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Non-constant variance (n=1000, $\sigma_2/\sigma_1=4$)}
<<echo=FALSE, fig.width=8, out.width='0.8\\textwidth'>>=
my_nonconstant_variance_plots(1000)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Extreme non-constant variance (football)}

<<echo=FALSE, out.width='0.8\\textwidth'>>=

par(mfrow=c(1,1), mar=c(5,4,4,2)+.1)
n = 1000
ub = 1
x = runif(n, -1, 1)
e = rnorm(n,0, 1-abs(x))
plot(x, e, axes=FALSE, frame=TRUE, xlab="Predicted mean", ylab="Residual")
@

\end{frame}





\subsection{Independence}
\frame{\frametitle{Independence}
	Lack of independence includes
	\begin{itemize}
	\item Cluster effect
	\item Serial correlation
	\item Spatial association
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	Make plots of residuals vs relevant explanatory variable(s) and look for patterns, \pause e.g.
	\begin{itemize}[<+->]
	\item Residuals vs groups 
	\item Residuals vs time (or observation number)
	\item Residuals vs spatial variable 
	\end{itemize}
}


\begin{frame}[fragile]
\frametitle{No evidence for lack of independence}

\vspace{-0.1in}

<<echo=FALSE, out.width='0.7\\textwidth'>>=
my_lack_of_independence_plots = function(d) {
  m = lm(y~x, d)
  d$r = residuals(m)
  opar = par(mfrow=c(2,2), mar=c(5,4,0,2)+.1)
  plot(y~x, d, xlab="Explanatory variable", ylab="Response")
  plot(y~i, d, xlab="Observation number", ylab="Response")
  plot(x~i, d, xlab="Observation number", ylab="Explanatory variable")
  plot(r~i, d, xlab="Observation number", ylab="Residual")
  par(opar)
}

n = 100
d = data.frame(i = 1:n, 
               x = rnorm(n))
d$y = with(d, rnorm(n,3*x))
my_lack_of_independence_plots(d)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Evidence for lack of independence}

\vspace{-0.1in}

<<echo=FALSE, out.width='0.7\\textwidth'>>=
d$y = with(d, rnorm(n, 3*x+i/20))
my_lack_of_independence_plots(d)
@

\end{frame}





\subsection{Linearity}
\begin{frame}[fragile]
\frametitle{Linearity}
  Assess using scatterplots of (transformed) response vs (transformed) explanatory variable:
	
<<echo=FALSE>>=
n = 100
set.seed(1) 
x = runif(n,0,10)
e = rnorm(n)
y = x+e

myplot = function(x,y,main="") {
  plot(x,y,axes=FALSE, frame=TRUE, main=main)
}

par(mfrow=c(2,4), mar=c(0,0,4,0)+.5)
myplot(x, y)
myplot(x, exp(y))
myplot(exp(x), y)
myplot(exp(x), exp(y))
myplot(x, y)
myplot(x, y, "log(y)")
myplot(x, y, "log(x)")
myplot(x, y, "log(x), log(y)")
@
\end{frame}



\subsection{Lack-of-fit F-test}
\frame{\frametitle{Testing Composite hypotheses}
  Comparing two models
	\begin{itemize}
	\item $H_0:$ \alert{(reduced)}
	\item $H_1:$ \alert{(full)}
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	\small
	Do the following
	\begin{enumerate}[1.]
	\item Calculate extra sum of squares.
	\item Calculate extra degrees of freedom
	\item Calculate 
	\[ \mbox{F-statistic} = \frac{\mbox{Extra sum of squares / Extra degrees of freedom}}{\hat{\sigma}^2_{full}} \]
	\item Compare this to an F-distribution with 
		\begin{itemize}
		\item numerator degrees of freedom = extra degrees of freedom
		\item denominator degrees of freedom = degrees of freedom in estimating $\hat{\sigma}_{full}^2$
		\end{itemize}
	\end{enumerate}
}


\frame{\frametitle{Lack-of-fit F-test}
  Let $Y_{ij}$ be the $i^{th}$ observation from the $j^{th}$ group where the group is defined by those observations having the same explanatory variable value ($X_j$). 
  
  \vspace{0.1in} \pause

	Two models:
	
	\begin{tabular}{lll}
	ANOVA: & $Y_{ij} \stackrel{ind}{\sim} N(\mu_j,\sigma^2)$ & \pause \uncover<3->{\alert{(full)}} \\
	Regression: & $Y_{ij} \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_j, \sigma^2)$ & \uncover<3->{\alert{(reduced)}}
	\end{tabular}
	
	\vspace{0.2in} \pause \pause
	
	\begin{itemize}[<+->]
	\item Regression model is reduced:
	\begin{itemize}
	\item ANOVA has $J$ parameters for the mean
	\item Regression has 2 parameters for the mean
	\item Set $\mu_{j} = \beta_0+\beta_1 X_j$.
	\end{itemize}
	\item Small pvalues indicate a lack-of-fit, i.e. the reduced model is not adequate.
	\item Lack-of-fit F-test requires multiple observations at a few $X_j$ values!
	\end{itemize}
}

\begin{frame}[fragile]
\frametitle{Telomere length}
<<echo=FALSE>>=
ggplot(Telomeres, aes(factor(years), telomere.length))+
  geom_boxplot()+
  geom_jitter()+
  labs(x="Years", y="Telomere length", 
       title="Telomere length vs years since diagnosis")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Telomere length}
<<echo=FALSE>>=
ggplot(Telomeres, aes(years, telomere.length))+
  geom_jitter()+
  geom_smooth(method=lm, se=FALSE)+
  labs(x="Years", y="Telomere length", 
       title="Telomere length vs years since diagnosis")
@
\end{frame}

\frame[containsverbatim]{\frametitle{SAS code}
\tiny
\begin{verbatim}
DATA t;
  INFILE 'telomeres.csv' DSD FIRSTOBS=2;
  INPUT years length;

PROC REG DATA=t;
  MODEL length = years / CLB LACKFIT;
  RUN;

                                        The REG Procedure
                                          Model: MODEL1
                                   Dependent Variable: length 

                             Number of Observations Read          39
                             Number of Observations Used          39


                                      Analysis of Variance
 
                                             Sum of           Mean
         Source                   DF        Squares         Square    F Value    Pr > F

         Model                     1        0.22777        0.22777       8.42    0.0062
         Error                    37        1.00033        0.02704                     
           Lack of Fit             9        0.18223        0.02025       0.69    0.7093
           Pure Error             28        0.81810        0.02922                     
         Corrected Total          38        1.22810           
\end{verbatim}

\pause

Indicates no evidence for a lack of fit, i.e. regression seems adequate.
}



\begin{frame}[fragile]
<<>>=
# Use as.factor to turn a continuous variable into a categorical variable
m_anova = lm(telomere.length ~ as.factor(years), Telomeres) 
m_reg   = lm(telomere.length ~           years , Telomeres)
anova(m_reg, m_anova)
@

\vspace{0.2in} \pause

No evidence of a lack of fit. 

\end{frame}


\frame{\frametitle{Lack-of-fit F-test summary}
	\begin{itemize}[<+->]
	\item Lack-of-fit F-test tests the assumption of linearity
	\item Needs multiple observations at various explanatory variable values
	\item Small pvalue indicates a lack-of-fit, i.e. means are not linear
		\begin{itemize}
		\item Transform response, e.g. log
		\item Transform explanatory variable
		\item Add other explanatory variable(s)
		\end{itemize}
	\end{itemize}
}


\subsection{Summary of diagnostics}
\frame{\frametitle{Summary of diagnostics}
  \begin{itemize}[<+->]
  \item Normality
    \begin{itemize}
    \item Normal qq-plots should have points falling on the line
    \end{itemize}
  \item Constant variance
    \begin{itemize}
    \item Residuals vs predicted values should have random scatter on y-axis
    \item Residuals vs explanatory variable(s) should have random scatter on y-axis
    \end{itemize}
  \item Independence
    \begin{itemize}
    \item Residuals vs anything should not have a pattern
    \end{itemize}
  \item Linearity
    \begin{itemize}
    \item Response vs explanatory variable should be linear
    \item Lack-of-fit F-test should not be significant
    \end{itemize}
  \end{itemize}
}


\begin{frame}
\frametitle{Default diagnostics in SAS}

\begin{center}
\includegraphics[width=0.6\textwidth]{Ch08_DiagnosticsPanel}
\end{center}

\end{frame}



\begin{frame}
\frametitle{Default diagnostics in R}

<<echo=FALSE,out.width='0.75\\textwidth'>>=
m = lm(sqrt(Time)~Voltage, case0802)
opar = par(mfrow=c(2,3))
plot(m, which=1:6)
par(opar)
@

\end{frame}




\section{Interpretations using logs}
\begin{frame}
\frametitle{Interpretations using logs}

The most common transformation of either the response or explanatory variable(s) is to take logarithms \pause because
\begin{itemize}[<+->]
\item linearity will often then be approximately true,
\item the variance will likely be approximately constant, and
\item there is a (relatively) convenient interpretation.
\end{itemize}

\vspace{0.2in} \pause

We will talk about interpretation of $\beta_0$ and $\beta_1$ when
\begin{itemize}[<+->]
\item only the response is logged, 
\item only the explanatory variable is logged, and
\item when both are logged.
\end{itemize}
\end{frame}


\subsection{Neither response nor explanatory variable are logged}
\begin{frame}
\frametitle{Neither response nor explanatory variable are logged}

If 
\[ E[Y|X] = \beta_0 + \beta_1 X, \]
\pause
then
\begin{itemize}[<+->]
\item $\beta_0$ is the expected response when $X$ is zero and
\item $\beta_1$ is the expected change in the response for a one unit increase in the explanatory variable.
\end{itemize}

\vspace{0.2in} \pause

For the following discussion,
\begin{itemize}[<+->]
\item $Y$ is always going to be the original response and
\item $X$ is always going to be the original explanatory variable.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Example}

Suppose 
\begin{itemize}
\item $Y$ is corn yield per acre
\item $X$ is fertilizer level in lbs/acre
\end{itemize}

\pause 

Then, if 
\[ E[Y|X] = \beta_0 + \beta_1 X \]

\begin{itemize}[<+->]
\item $\beta_0$ is the expected corn yield per acre when fertilizer level is zero and 
\item $\beta_1$ is the expected change in corn yield per acre when fertilizer is increase by 1 lbs/acre. 
\end{itemize}

\end{frame}





\subsection{Response is logged}
\begin{frame}
\frametitle{Response is logged}

If 
\[ E[\log(Y)|X] = \beta_0 + \beta_1 X \quad \mbox{or} \quad Median\{Y|X\} = e^{\beta_0} e^{\beta_1 X}, \]
\pause
then
\begin{itemize}[<+->]
\item $\beta_0$ is the expected $\log(Y)$ when $X$ is zero and
\item $\beta_1$ is the expected change in $\log(Y)$ for a one unit increase in the explanatory variable. 
\end{itemize}

\pause 
Alternatively,

\begin{itemize}[<+->]
\item $e^{\beta_0}$ is the median of $Y$ when $X$ is zero and 
\item $e^{\beta_1}$ is the multiplicative effect on the median of $Y$ for a one unit increase in the explanatory variable.
\end{itemize}

\end{frame}




\begin{frame}
\frametitle{Response is logged}

Suppose 
\begin{itemize}
\item $Y$ is corn yield per acre
\item $X$ is fertilizer level in lbs/acre
\end{itemize}

\pause 

Then, if 
\[ E[\log(Y)|X] = \beta_0 + \beta_1 X \quad \mbox{or} \quad Median\{Y|X\} = e^{\beta_0} e^{\beta_1 X} \]

\begin{itemize}[<+->]
\item $e^{\beta_0}$ is the median corn yield per acre when fertilizer level is 0 and 
\item $e^{\beta_1}$ is the multiplicative effect in median corn yield per acre when fertilizer is increase by 1 lbs/acre. 
\end{itemize}

\end{frame}



\subsection{Explanatory variable is logged}
\begin{frame}
\frametitle{Explanatory variable is logged}

If 
\[ E[Y|X] = \beta_0 + \beta_1 \log(X),  \]
\pause
then
\begin{itemize}[<+->]
\item $\beta_0$ is the expected response when $\log(X)$ is zero and
\item $\beta_1$ is the expected change in the response for a one unit increase in $\log(X)$. 
\end{itemize}

\pause 
Alternatively,

\begin{itemize}[<+->]
\item $\beta_0$ is the expected response when $X$ is 1 and 
\item $\beta_1 \log(d)$ is the expected change in the response when $X$ increase multiplicatively by $d$, e.g.
  \begin{itemize}
  \item $\beta_1 \log(2)$ is the expected change in the response for each doubling of $X$ or
  \item $\beta_1 \log(10)$ is the expected change in the response for each ten-fold increase in $X$.
  \end{itemize}
\end{itemize}

\end{frame}




\begin{frame}
\frametitle{Explanatory variable is logged}

Suppose 
\begin{itemize}
\item $Y$ is corn yield per acre
\item $X$ is fertilizer level in lbs/acre
\end{itemize}

\pause 

Then, if 
\[ E[Y|X] = \beta_0 + \beta_1 \log(X)  \]

\begin{itemize}[<+->]
\item $\beta_0$ is the expected corn yield per acre when fertilizer level is 1 lb/acre and 
\item $\beta_1 \log(2)$ is the expected change in corn yield when fertilizer level is doubled. 
\end{itemize}

\end{frame}



\subsection{Both response and explanatory variable are logged}
\begin{frame}
\frametitle{Both response and explanatory variable are logged}

If 
\[ E[\log(Y)|X] = \beta_0 + \beta_1 \log(X) \quad \mbox{or} \quad Median\{Y|X\} = e^{\beta_0} e^{\beta_1 \log(X)} = e^{\beta_0} X^{\beta_1}, \]
\pause
then
\begin{itemize}[<+->]
\item $\beta_0$ is the expected $\log(Y)$ when $\log(X)$ is zero and
\item $\beta_1$ is the expected change in $\log(Y)$ for a one unit increase in $\log(X)$. 
\end{itemize}

\pause 
Alternatively,

\begin{itemize}[<+->]
\item $e^{\beta_0}$ is the median of $Y$ when $X$ is 1 and 
\item $d^{\beta_1}$ is the multiplicative change in the median of the response when $X$ increase multiplicatively by $d$, e.g.
  \begin{itemize}
  \item $2^{\beta_1}$ is the multiplicative effect on the median of the response for each doubling of $X$ or
  \item $10^{\beta_1}$ is the multiplicative effect on the median of the response for each ten-fold increase in $X$.
  \end{itemize}
\end{itemize}

\end{frame}




\begin{frame}
\frametitle{Both response and explanatory variable are logged}

Suppose 
\begin{itemize}
\item $Y$ is corn yield per acre
\item $X$ is fertilizer level in lbs/acre
\end{itemize}

\pause 

Then, if 
\[ E[\log(Y)|X] = \beta_0 + \beta_1 \log(X) \quad \mbox{or} \quad Median\{Y|X\} = e^{\beta_0} e^{\beta_1 \log(X)} = e^{\beta_0} X^{\beta_1} \]

\begin{itemize}[<+->]
\item $e^{\beta_0}$ is the median corn yield per acre when fertilizer level is 1 lb/acre and 
\item $2^{\beta_1}$ is the multiplicative effect on median corn yield per acre when fertilizer level doubles.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Summary of interpretations when using logarithms}

\begin{itemize}[<+->]
\item When using the log of the response, 
  \begin{itemize}
  \item $\beta_0$ will affect the median response
  \item $\beta_1$ will affect the multiplicative change in the median response
  \end{itemize}
\item When using the log of the explanatory variable ($X$),
  \begin{itemize}
  \item $\beta_0$ will affect the response when $X=1$
  \item $\beta_1$ will affect the change in the response when there is a multiplicative change in $X$
  \end{itemize}
\end{itemize}

\vspace{0.2in} \pause

To construct confidence intervals for $e^\beta$, find a confidence interval for $\beta$ and exponentiate the endpoints, i.e. if $(L,U)$ is a confidence interval for $\beta$, then $(e^L, e^U)$ is a confidence interval for $e^\beta$. 

\end{frame}


\end{document}
