\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{M4S1 - Central Limit Theorem}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='small', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=FALSE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("dplyr")
library("ggplot2")
library("xtable")
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@



\begin{document}


\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Sampling distribution
  \begin{itemize}
  \item Standard error
  \end{itemize}
\item Central Limit Theorem
\item Estimation
  \begin{itemize}
  \item Bias
  \item Variability
  \end{itemize}
\end{itemize}

\end{frame}


\section{Sampling distribution}
\begin{frame}
\frametitle{Sampling distribution}

\begin{definition}
A \alert{summary statistic} is a numerical value calculated from the sample.
\end{definition}

\vspace{0.1in} \pause

But this sample is only one of many possibilities. 
\pause
What could have happened if we had a different sample?

\vspace{0.1in} \pause

\begin{definition}
The \alert{sampling distribution of a statistic} is the distribution of that 
statistic over different samples of a fixed size.
\end{definition}
\end{frame}


\subsection{Binomial distribution}
\begin{frame}
\frametitle{Flipping a coin}
Suppose we repeatedly tossed a fair coin 10 times and recorded the number of heads.
\pause
The sampling distribution is the binomial distribution with 10 attempts and 
probability of success 0.5.
<<fig.width=8>>=
plot(0:10, dbinom(0:10, 10, 0.5), lwd=2, type='h', main='Bin(10,0.5)', xlab='y', ylab='P(Y=y)')
@
\end{frame}


\begin{frame}
\frametitle{Rolling a die}
<<>>=
n <- 24
@
Suppose we repeatedly rolled a fair 6-sided die \Sexpr{n} times and recorded the number of 1s.
\pause
The sampling distribution is the binomial distribution with \Sexpr{n} attempts and probability of success 1/6.
<<fig.width=8>>=
plot(0:n, dbinom(0:n, n, 1/6), lwd=2, type='h', main=paste0('Bin(',n,',1/6)'), xlab='y', ylab='P(Y=y)')
@
\end{frame}



\begin{frame}
\frametitle{Rolling a die}
<<>>=
n <- 120
@
Suppose we repeatedly rolled a fair 6-sided die \Sexpr{n} times and recorded the number of 1s.
\pause
The sampling distribution is the binomial distribution with \Sexpr{n} attempts and probability of success 1/6.
<<fig.width=8>>=
plot(0:n, dbinom(0:n, n, 1/6), lwd=2, type='h', main=paste0('Bin(',n,',1/6)'), xlab='y', ylab='P(Y=y)')
@
\end{frame}


\subsection{Maximum}
\begin{frame}
\frametitle{Rolling a die}
<<>>=
n <- 5
@
Suppose we repeatedly rolled a fair 6-sided die \Sexpr{n} times and recorded the maximum.
\pause
It's hard to analytically determine what happens, 
but we can use a computer to perform the experiment.
<<fig.width=8>>=
x <- replicate(1e3, {
  mm <- sample(6, n, replace=TRUE)
  max(mm)
})
hist(x, (0:6)+0.5, prob=TRUE, main="Histogram of simulated die rolls")
@
\end{frame}


\begin{frame}
\frametitle{Rolling a die}
<<>>=
n <- 50
@
Suppose we repeatedly rolled a fair 6-sided die \Sexpr{n} times and recorded the maximum.
\pause
It's hard to analytically determine what happens, 
but we can use a computer to perform the experiment.
<<fig.width=8>>=
x <- replicate(1e3, {
  mm <- sample(6, n, replace=TRUE)
  max(mm)
})
hist(x, (0:6)+0.5, prob=TRUE, main="Histogram of simulated die rolls")
@
\end{frame}



\subsection{Mean}
\begin{frame}
\frametitle{Sample mean}
<<>>=
n <- 8
@
Suppose we repeatedly rolled a fair 6-sided die \Sexpr{n} times and recorded the mean.
\pause
It's hard to analytically determine what happens, 
but we can use a computer to perform the experiment.
\pause
<<fig.width=8>>=
x <- replicate(1e4, {
  mm <- sample(6, n, replace=TRUE)
  mean(mm)
})
hist(x, seq(1,6,by=1/8)+1/16, prob=TRUE, main="Histogram of mean of simulated die rolls")
@
\end{frame}




\begin{frame}
\frametitle{Sample mean}
<<>>=
n <- 80
@
Suppose we repeatedly rolled a fair 6-sided die \Sexpr{n} times and recorded the mean.
\pause
It's hard to analytically determine what happens, 
but we can use a computer to perform the experiment.
\pause
<<fig.width=8>>=
x <- replicate(1e4, {
  mm <- sample(6, n, replace=TRUE)
  mean(mm)
})
hist(x, seq(1,6,by=1/80)+1/160, prob=TRUE, main="Histogram of mean of simulated die rolls")
@
\end{frame}




\section{Central Limit Theorem}
\small
\begin{frame}
\frametitle{Central Limit Theorem}
\begin{theorem}
Suppose you have a sequence of independent and identically distributed 
random variables $X_1,X_2,\ldots$ with 
mean $E[X_i]=\mu$ and variance $Var[X_i]=\sigma^2$.
\pause
The \alert{Central Limit Theorem} (CLT) says the \alert{sampling distribution of the sample mean} \pause
converges to a normal distribution.
\pause
Specifically
\[ 
\frac{\overline{X}_n-\mu}{\sigma/\sqrt{n}} \to N(0,1) \quad \mbox{as} \quad n\to\infty
\]
\pause
where $\overline{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$. 
\pause
Thus, for large $n$, we can approximate the sample mean by a normal distribution, \pause i.e.
\[
\overline{X}_n \stackrel{\cdot}{\sim} N(\mu,\sigma^2/n)
\]
\pause
where $\stackrel{\cdot}{\sim}$ means ``approximately distributed.''
The standard deviation of the sampling distribution of a statistic is known as 
the \alert{standard error} (SE), i.e. $\sigma/\sqrt{n}$ is the standard error 
from the CLT.
\end{theorem}
\end{frame}



\begin{frame}
\frametitle{Mean of the sample mean}

Recall the following property:
\[
E[aX+bY+c] = aE[X] + bE[Y] + c
\]
\pause 

If we have $E[X_i]=\mu$ for all $i$, 
\pause
then
\[ \begin{array}{rl}
E[\overline{X}_n] &= \pause E\left[ \frac{1}{n} \sum_{i=1}^n X_i \right] \pause \\
&= \frac{1}{n} E\left[ \sum_{i=1}^n X_i \right] \pause \\
&= \frac{1}{n} \sum_{i=1}^n E[X_i] \pause \\
&= \frac{1}{n} \sum_{i=1}^n \mu \pause \\
&= \frac{1}{n} n\cdot \mu \pause \\
&= \mu
\end{array} \]

\end{frame}



\begin{frame}
\frametitle{Variance of the sample mean}

Recall the following property for independent random variables $X$ and $Y$:
\[
Var[aX+bY+c] = a^2Var[X] + b^2Var[Y]
\]
\pause 

If we have $Var[X_i]=\sigma^2$ for all $i$, 
\pause
then
\[ \begin{array}{rl}
Var[\overline{X}_n] &= \pause Var\left[ \frac{1}{n} \sum_{i=1}^n X_i \right] \pause \\
&= \frac{1}{n^2} Var\left[ \sum_{i=1}^n X_i \right] \pause \\
&= \frac{1}{n^2} \sum_{i=1}^n Var[X_i] \pause \\
&= \frac{1}{n^2} \sum_{i=1}^n \sigma^2 \pause \\
&= \frac{1}{n^2} n\cdot \sigma^2 \pause \\
&= \sigma^2/n
\end{array} \]

\end{frame}



\begin{frame}
\frametitle{Sampling distribution of sample mean}

If $X_1,X_2,\ldots$ are a sequence of independent and identically distributed
random variables with $E[X_i]=\mu$ and $Var[X_i]=\sigma^2$, \pause
then 
\[
E[\overline{X}_n] = \mu \qquad Var[\overline{X}_n] = \sigma^2/n
\]
\pause
for any $n$. 
\pause
The CLT says that, as $n$ gets large, the sampling distribution of the 
\alert{sample mean} converges to a \alert{normal distribution}.

\end{frame}



\begin{frame}
\frametitle{Coin flipping}
Sampling distribution for the proportion of heads on an unbiased coin flip.
\pause
<<fig.width=8>>=
opar = par(mfrow=c(1,3))
for (n in c(10,30,50)) {
  plot( 0:n/n, dbinom(0:n, n, 0.5), type='h', main=paste0('Bin(',n,',1/2)'), xlab='y', ylab='P(Y=y)')
  curve(dnorm(x, 0.5, sqrt(0.25/n))/n, col='red', add=TRUE)
}
par(opar)
@
\end{frame}



\begin{frame}
\frametitle{Die rolling}
Sampling distribution for the proportion of 1s on an unbiased 6-sided die roll.
\pause
<<fig.width=8>>=
opar = par(mfrow=c(1,3))
for (n in c(10,30,50)) {
  plot( 0:n/n, dbinom(0:n, n, 1/6), type='h', main=paste0('Bin(',n,',1/6)'), xlab='y', ylab='P(Y=y)')
  curve(dnorm(x, 1/6, sqrt(5/6/6/n))/n, col='red', add=TRUE)
}
par(opar)
@
\end{frame}



\begin{frame}
\frametitle{Die rolling}
Sampling distribution for the sample mean of an unbiased 6-sided die roll.
\pause
<<fig.width=8>>=
opar = par(mfrow=c(1,3))
for (n in c(10,30,50)) {
  x <- replicate(1e4, {
    mm <- sample(6, n, replace=TRUE)
    mean(mm)
  })
  hist(x, seq(1,6,by=0.1), prob=TRUE, main=paste0("n=",n), xlim=c(1,6))
  curve(dnorm(x, 3.5, sqrt(sum((1:6-3.5)^2)/6/n)), col='red', add=TRUE)
}
par(opar)
@
\end{frame}



\begin{frame}
\frametitle{Welfare}

A certain group of welfare recipients receives SNAP benefits of \$110 per week 
with a standard deviation of \$20. 
\pause
A random sample of 30 people is taken and sample mean is calculated. 
\pause 
\begin{itemize}
\item What is the expected value of the sample mean?

\pause
Let $X_i$ be the SNAP benefit for individual $i$. 
We know $E[X_i] = \$110$ and $Var[X_i]=\$20^2$. 
Thus, $E[\overline{X}_{30}] = \$110$.
\pause
\vfill


\item What is the the standard error of the sample mean?

\pause
The standard error is $\sigma/\sqrt{n}=\$20/\sqrt{30}\approx \$3.65.$
\pause
\vfill

\item What is the approximate probability the sample mean will be greater than \$120?

\pause
We know $\overline{X}_{30} \stackrel{\cdot}{\sim} N(\$110, \$3.65^2)$.
\[ \begin{array}{rl}
P(\overline{X}_{30}>\$120) &= P\left( \frac{\overline{X}_{30}-\$110}{\$3.65} > \frac{\$120-\$110}{\$3.65} \right) \pause \\
&\approx P(Z > 2.74) \pause \\
&= 1-P(Z<2.74) \pause \\
&= 1-0.9969 \pause
= 0.0031
\end{array}\]
\vfill

\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Process to use CLT}

Given a scientific question, do the following

\begin{enumerate}[<+->]
\item Identify the random variables $X_1,X_2,\ldots$.
\item Verify these are independent and identically distributed.
\item Determine the expectation/mean and variance (or standard deviation) of the $X_i$.
\item Determine the sample size. Is the sample size large enough for the CLT to apply?
\item If yes, determine the approximate sampling distribution for the sample mean.
\item Write the scientific question in mathematical/probabilistic notation.
\item Calculate your answer.
\end{enumerate}
\end{frame}



\section{Estimation}
\begin{frame}
\frametitle{Estimation}

\begin{definition}
An \alert{estimator} is a summary statistic that is used to estimate 
a population parameter.
\end{definition}

\vspace{0.1in} \pause

\begin{definition}
An estimator is \alert{unbiased} for a population parameter if the 
expectation/mean of the estimator is equal to the population parameter.
\pause
Otherwise the estimator is \alert{biased}.
\end{definition}

\vspace{0.1in} \pause 

The standard error of a statistic describes the \alert{variability} of the 
statistic.

\end{frame}


\subsection{Sample mean}
\begin{frame}
\frametitle{Sample mean}

Let $X_1,X_2,\ldots$ be independent and identically distributed with 
expectation/mean $\mu$ and variance $\sigma^2$. 
\pause
Then the sample mean 
\[
\overline{X} = \frac{1}{n}\sum_{i=1}^n X_i
\]
\pause 
has $E[\overline{X}] = \pause \mu$ 
\pause
and standard error $SE[\overline{X}] \pause = \sigma/\sqrt{n}$. 

\vspace{0.1in} \pause

Thus, the sample mean \pause is 
\begin{itemize}
\item an unbiased estimator of the population mean \pause and
\item its variability (standard error) decreases by the square root of the sample size. 
\end{itemize}

\end{frame}



\subsection{Bayesian estimator}
\begin{frame}
\frametitle{Bayesian estimator}

In a Bayesian analysis, 
you specify your prior belief about $\mu$ before you observe the data.
\pause
Suppose you are willing to specify that your prior belief about $\mu$ is 
normally distributed with mean $m$ and variance $v^2$. 
\pause
Then you plan to collect data $X_1,X_2,\ldots$ that are independent and 
identically distributed with expectation/mean $\mu$ and variance $\sigma^2$. 

\vspace{0.1in} \pause

A Bayesian estimator of the population mean $\mu$ is 
\[ 
\frac{1/v^2}{1/v^2+n/\sigma^2}m + \frac{n/\sigma^2}{1/v^2+n/\sigma^2}\overline{X},
\]
\pause
and it has standard error 
\[
\frac{\sqrt{n/\sigma^2}}{1/v^2+n/\sigma^2}.
\]
\pause
Note that as $v^2\to\infty$ (indicating a very uncertainty prior belief), 
\pause
then this estimator becomes $\overline{X}$ which is unbiased and 
has standard error $\sigma/\sqrt{n}$.

\end{frame}



\begin{frame}
\frametitle{Bayesian estimator (cont.)}

The Bayesian estimator is biased because 
\[ \begin{array}{rl}
E\left[ \frac{1/v^2}{1/v^2+n/\sigma^2}m + \frac{1/v^2}{1/v^2+n/\sigma^2}\overline{X} \right] &= \pause
\frac{1/v^2}{1/v^2+n/\sigma^2}m + \frac{1/v^2}{1/v^2+n/\sigma^2}E[\overline{X}] \\
&= \pause
\frac{1/v^2}{1/v^2+n/\sigma^2}m + \frac{1/v^2}{1/v^2+n/\sigma^2}\mu
\end{array} \]
\pause
but it has less variability because 
\pause
\[ \begin{array}{rl}
\frac{\sqrt{n/\sigma^2}}{1/v^2+n/\sigma^2} &=
\frac{1}{\frac{1/v^2}{n/\sigma^2}+\sqrt{n/\sigma^2}} \pause \\
&< \frac{1}{\sqrt{n/\sigma^2}} \pause \\
&= \frac{1}{\sqrt{n}/\sigma} \pause \\
&= \sigma/\sqrt{n}.
\end{array} \]

\pause 

Thus the Bayesian estimator adds some bias to reduce variability. 
\pause
We call this the \alert{bias-variance} tradeoff. 

\end{frame}



\begin{frame}
\frametitle{Bias and variability}

Suppose you have the ability to take samples from one of two populations that 
both have the same mean. 
\pause
Population 1 has a standard deviation of 10 while population 2 has a standard
deviation of 5. 
\pause
Due to the cost of sampling, you can either 
\begin{enumerate}
\item take 100 samples of population 1 or 
\item take 49 samples of population 2. 
\end{enumerate}
\pause 
If your goal is to estimate the population mean using a sample mean, 
\pause
which of these two samples would you prefer to take?

\vspace{0.1in}\pause

The sample mean will have the same expectation/mean, so they are both unbiased.
\pause
The standard error of population 1 is $10/\sqrt{100} = 10/10=1$ \pause
while the standard error of population 2 is $5/\sqrt{49} = 5/7 < 1$.
\pause
Thus, on average, the sample mean from population 2 will be closer to the 
population mean than the sample mean from population 1. 
\pause
How few sample of population 2 would have the same standard error as the sample
from population 1? 
\pause
25

\end{frame}



% \section{CLT Sample size}
% \begin{frame}
% \frametitle{CONAN}
% 
% 
% 
% \end{frame}

\end{document}
