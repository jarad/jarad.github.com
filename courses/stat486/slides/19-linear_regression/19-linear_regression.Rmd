---
layout: page
title: "Linear regression"
author: "Jarad Niemi"
date: "`r Sys.Date()`"
header-includes:
- \usepackage{blkarray}
- \usepackage{amsmath}
output: 
  html_document:
      toc: true
      toc_float: true
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[R code](19-linear_regression.R)

```{r}
library("tidyverse"); theme_set(theme_bw())
library("Sleuth3")
library("ggResidpanel")
```

# Background

As a general approach, regression allows the response variable mean 
(or expectation) to depend on categorical and continuous explanatory variables 
in complex patterns. 

## Energy expenditure 

Scientific question: 
How does echolocation affect energy expenditure?

### Two-sample normal

```{r}
d <- case1002 %>%
  mutate(echolocating = Type == "echolocating bats")

d %>% 
  summarize(n = )

t.test(Energy ~ echolocating, data = d)
```

### Regression

```{r}
ggplot(case1002, aes(x = Mass, y = Energy, color = Type, shape = Type)) +
  geom_point() + 
  scale_y_log10() + 
  scale_x_log10()
```

## Model

The general structure of a regression model is 
\[ 
Y_i \stackrel{ind}{\sim} N(\beta_0 + \beta_1X_{i,1} + \cdots + \beta_{p-1} X_{i,p-1}, \sigma^2)
\]
for $i=1,\ldots,n$
or, equivalently,
\[ 
Y_i = \beta_0 + \beta_1X_{i,1} + \cdots + \beta_{p-1} X_{i,p-1}, \qquad \epsilon_i \stackrel{ind}{\sim} N(0, \sigma^2).
\]

where, for observation $i$, 

- $Y_i$ is the value of the response variable,
- $X_{i,j}$ is the value of the $j$th explanatory variable, and
- $\epsilon_i$ is the error.

You have a lot of flexibility in your choice for the response and explanatory
variables.



## Matrix notation

An alternative way to represent the model uses matrix notation and the 
multivariate normal distribution. 
\[
Y = X\beta + \epsilon, \qquad \epsilon \sim N_n(0,\sigma^2 \mathrm{I})
\]
where 

- $Y = (Y_1,\ldots,Y_n)^\top$ is an $n\times 1$ response variable vector
- $X$ is an $n\times p$ *design matrix*
  - each row $X_{r,\cdot}$ contains the explanatory variables values for one observation
  - each column $X_{\cdot, c}$ contains the one explanatory variable values for all observations
- $\beta = (\beta_0,\beta_1,\ldots,\beta_{p-1})$ is a $p\times 1$ coefficient parameter vector
- $\epsilon = (\epsilon_1,\ldots,\epsilon_n)$ is an $n\times 1$ error vector
- $N_n(\mu,\Sigma)$ is a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)
with 
  - mean $\mu$ and
  - covariance matrix $\Sigma$ where 
    - $\Sigma_{i,j}$ is the covariance between $\epsilon_i$ and $\epsilon_j$
- $\mathrm{I}$ is the $n\times n$ identity matrix

If you are interested in learning more about the multivariate normal distribution
and its uses, look for a course in multivariate data analyses, 
e.g. STAT 475.


### Parameter estimates

Use of matrix notation and linear algebra eases parameter estimation:

- estimated coefficients $\hat\beta = (X^\top X)^{-1} X^\top y$
- fitted/predicted values $\hat{y} = X\hat\beta$
- residuals $\hat{\epsilon} = y - \hat{y}$
- estimated error variance $\hat\sigma^2 = \frac{1}{n-p}\hat{e}^\top \hat{e}$
- coefficient of determination ($R^2$) $1-\hat{e}^\top \hat{e}/(y-\overline{y})^\top (y-\overline{y})$

```{r}
m <- lm(log(Energy) ~ log(Mass) * Type, data = case1002)
s <- summary(m)

# Coefficient estimates and confidence/credible intervals
coef(m)            # or m$coefficients
confint(m)

# SD estimate
s$sigma

# Coefficint of determination
s$r.squared

# Fitted and residuals
head(fitted(m))    # or head(m$fitted.values)
head(residuals(m)) # or head(m$residuals)

# A reasonable display of most of this
summary(m)
```



## Assumptions

The assumptions in every regression model are 

- errors are independent,
- errors are normally distributed,
- errors have constant variance, 

and 

- the expected response, $E[Y_i]$, depends on the explanatory variables according
to a linear function (of the parameters).

We generally use graphical techniques to assess these assumptions. 
In particular, we utilize 

- residuals v fitted
- qqplot of residuals
- residuals v explanatory
- residuals v index

### base R graphics

```{r}
opar = par(mfrow = c(2,3))
plot(m, 1:6, ask = FALSE)
par(opar)
```

```{r}
d <- case1002 %>%
  mutate(residuals = residuals(m))

plot(d$residuals) # residuals v index
plot(residuals ~ Type, data = d, type = "p")
plot(residuals ~ log(Mass), data = d)
```

### ggResidpanel

```{r}
resid_panel(m, plots = "all")
resid_panel(m, plots = c("resid","index","qq","cookd"))
resid_xpanel(m)
```


## Interpretation

In a linear regression model there are $p$ coefficients $\beta_0,\ldots,\beta_{p-1}$
and one variance $\sigma^2$. 
Generally, we are most interested in understanding the effect of explanatory 
variables by interpreting (a transformation of) the coefficients.

The general interpretation is 

    a one unit increase in the explanatory variable is associated with a $\beta$
    increase in the mean of the response when holding all other variables constant
    
There are several issues with this interpretation

- is the one-unit increase relevant?
- is the increase in mean relevant?
- can you hold the other variables constant?

In this class, we will focus on graphical interpretations. 


## Tests

### T-tests for regression coefficients

If a regression coefficient is exactly zero, 
this means that explanatory variable has no effect on the mean of the 
response. 
Thus constructing a hypothesis test with the null value being zero is reasonable.
If the p-value for this test is small, 
it only tells us there is something wrong with the model that includes this
coefficient being zero. 
But this small p-value could arise due to any (or multiple) model assumptions. 
Thus, the analyst needs to evaluate all model assumptions. 

Recall that each regression coefficient can be tested using the following
t statistic
\[ 
T = \frac{\hat\beta - b_0}{SE(\hat\beta)}
\]
where $b_0$ is the hypothesized value. 
Then this statistic is compared to a t-distribution with $n-p$ degrees of 
freedom. 

```{r}
coef(s) # or s$coefficients
```

This table provides $\hat\beta$, $SE(\hat\beta)$, the t statistic with 
$b_0, and the two-sided p-value. 


### F-tests

Two models are *nested* when setting some parameters in one of the models to 
specific values (usually 0) produces the other model. 
The second model is called the *simpler* model. 

When we would like to compare two nested models we can use an F-test. 
The null model for this comparison is the simpler model. 
A small p-value provides evidence against the simpler model. 

In the standard regression output, R provides an F-test to compare the 
intercept only model to the full model. 

```{r}
summary(m)
```

We can recreate this F-test using the ANOVA function. 

```{r}
m  <- lm(log(Energy) ~ log(Mass) * Type, data = case1002)
m0 <- lm(log(Energy) ~ 1, data = case1002)
anova(m0, m)
```
But, generally this F-test is not very interesting. 
It may help us to consider adding terms to the model sequentially. 

```{r}
anova(m)
```

This is a sequential comparison, i.e. 

1. comparing log(Mass) to intercept-only model
1. comparing Type + log(Mass) to log(Mass)
1. comparing Type + log(Mass) + Type:log(Mass) to Type + log(Mass)

If we just want to look at the highest order terms, 
we can use `drop()`.


```{r}
drop1(m, test = "F")
```




## Model construction

There are various ways to construct regression models in R. 

### Multiple explanatory variables

If we want to include multiple explanatory variables, 
we simply use the `+` symbol.

```{r}
lm(Energy ~ Mass + Type, data = case1002)
```

### Transformations

Transformations of the response and explanatory variables can generally be 
included directly in the call. 

```{r}
lm(log(Energy) ~ sqrt(Mass) + Type, data = case1002)
```

The most common transformation is the `log()`. 

#### I()

If we want to include a polynomial term, e.g. quadratic, we need to 
encapsulate it in `I()`. 

```{r}
lm(log(Energy) ~ Mass + I(Mass^2) + Type, data = case1002)
```
This can also be used to "Shift the intercept" to provide an interpretable
intercept. 

```{r}
meanMass <- mean(case1002$Mass)
lm(Energy ~ I(Mass - meanMass), data = case1002)
```


### Interactions

Interactions between variables can be included by using `:`.

```{r}
lm(log(Energy) ~ log(Mass) + Type + log(Mass):Type, data = case1002)
```

But a convenient shorthand is to use `*` which will include the main effects
as well as the interaction. 

```{r}
lm(log(Energy) ~ log(Mass) * Type, data = case1002)
```

### Include everything

We can include everything in the data.frame using the `.`. 

```{r}
lm(log(Energy) ~ ., data = case1002)
```

Interactions (and main effects) can be included using `.^2`. 

```{r}
lm(log(Energy) ~ .^2, data = case1002)
```

If there are more explanatory variables, this can be used to include higher
order interactions. 


### Remove terms

```{r}
lm(log(Energy) ~ log(Mass) + Type - 1, data = case1002) # remove intercept
```

```{r}
lm(log(Energy) ~ log(Mass) * Type - Type, data = case1002)
```




# Examples

## Simple linear regression

```{r}
ggplot(case0701, aes(x = Velocity, y = Distance)) +
  geom_point() + 
  geom_smooth(method = "lm", formula = y ~ x)
```

An alternative, more general approach, utilizes the `predict()` function to
create data for the lines. 

```{r}
m <- lm(Distance ~ Velocity, data = case0701)

d <- data.frame(Velocity = range(case0701$Velocity)) %>%
  mutate(prediction = predict(m, newdata = .))

ggplot(case0701, aes(x = Velocity)) +
  geom_point(aes(y = Distance)) + 
  geom_line(
    data = d,
    aes(y = prediction), color = "blue")
```

We can obtain the equation for the line using the `summary()` output.

```{r}
summary(m)
```

Take a look at diagnostics

```{r}
resid_panel(m, plots = c("resid","index","qq","cookd"))
resid_xpanel(m)
```


## One categorical variable

```{r}
ggplot(case0501, aes(x = Diet, y = Lifetime)) + 
  geom_jitter(width = 0.1)
```

```{r}
m <- lm(Lifetime ~ Diet, data = case0501)
```

```{r}
resid_panel(m, plots = c("resid","index","qq","cookd"))
resid_xpanel(m)
```

Visual representation

```{r}
d <- data.frame(Diet = unique(case0501$Diet)) 
p <- d %>% mutate(prediction = )
```


## Polynomial

```{r}
ggplot(Sleuth3::case1001, aes(Height, Distance)) +
  geom_point() +
  theme_bw()
```

```{r}
m1 <- lm(Distance ~ Height,                             case1001)
m2 <- lm(Distance ~ Height + I(Height^2),               case1001)
m3 <- lm(Distance ~ Height + I(Height^2) + I(Height^3), case1001)
```

```{r}
g <- ggplot(case1001, aes(x=Height, y=Distance)) + geom_point(size=3)

g 
g + stat_smooth(method="lm", formula = y ~ x)                                   
g + stat_smooth(method="lm", formula = y ~ x + I(x^2))          
g + stat_smooth(method="lm", formula = y ~ x + I(x^2) + I(x^3)) 
```

Recall that error variance will ALWAYS decrease as you add more explanatory
variables. 
So reduced uncertainty in the line does not ensure a better out-of-sample
fit. 

```{r}
m <- lm(Distance ~ Height + I(Height^2) + I(Height^3) + I(Height^4), case1001)
anova(m)
```

This suggests that that model with a quadratic term is insufficient while the 
model with a cubic does not raise any red flags. 
But, you should still check model assumptions. 


## Transformations

```{r}
m <- lm(log(Energy) ~ log(Mass) + Type, data = case1002)

d <- expand.grid(
  Mass = seq(min(case1002$Mass), max(case1002$Mass), length = 1001),
  Type = unique(case1002$Type)
)

p <- d %>%
  mutate(Energy = exp(predict(m, newdata = d))) # note the exp()

g <- ggplot() + 
  geom_point(
    data = case1002, aes(x = Mass, y = Energy, color = Type, shape    = Type)) + 
  geom_line(
    data = p,        aes(x = Mass, y = Energy, color = Type, linetype = Type)) 

g
```

These lines will look straight (and parallel) on log-log axes

```{r}
g + 
  scale_x_log10() + 
  scale_y_log10()
```



## Interactions

```{r}
m <- lm(log(Energy) ~ log(Mass) * Type, data = case1002) # note the *

d <- expand.grid(
  Mass = seq(min(case1002$Mass), max(case1002$Mass), length = 1001),
  Type = unique(case1002$Type)
)

p <- d %>%
  mutate(Energy = exp(predict(m, newdata = d))) # note the exp()

g <- ggplot() + 
  geom_point(
    data = case1002, aes(x = Mass, y = Energy, color = Type, shape    = Type)) + 
  geom_line(
    data = p,        aes(x = Mass, y = Energy, color = Type, linetype = Type)) 

g
```

These lines will look straight (and parallel) on log-log axes

```{r}
g + 
  scale_x_log10() + 
  scale_y_log10()
```