\documentclass[handout]{beamer}

\usepackage{verbatim,multicol,amsmath}

\input{../../frontmatter}
\input{../../commands}

\title{I09 - Comparing means}

% \newenvironment{remark}[1][Remark]{\begin{trivlist}
% \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo = FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, echo=FALSE, cache=FALSE>>=
library("dplyr")
library("tidyr")
library("ggplot2")
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@

\begin{document}

\begin{frame}
\maketitle
\end{frame}

\section{One mean summary}
\subsection{One mean}
\begin{frame}
\frametitle{One mean}

Consider the model $Y_i \ind N(\mu,\sigma^2)$.
\pause
We have discussed a number of statistical procedures to draw inferences about
$\mu$: \pause
\begin{itemize}
\item Frequentist: based on distribution of $\frac{\overline{Y}-\mu}{s/\sqrt{n}}$ \pause
  \begin{itemize}
  \item pvalue for a hypothesis test, e.g. $H_0: \mu=\mu_0$, \pause
  \item confidence interval for $\mu$, \pause
  \end{itemize}
\item Bayesian: based on posterior for $\mu$
  \begin{itemize}
  \item credible interval for $\mu$, \pause
  \item posterior model probability, e.g. $p(H_0|y)$, \pause and
  \item posterior probabilities, e.g. $P(\mu<\mu_0|y)$.
  \end{itemize}
\end{itemize}
\pause
Now, we will consider what happens when you have multiple $\mu$s.

\end{frame}


\section{Comparing two means}
\subsection{Normal model}
\begin{frame}
\frametitle{Two means}

Consider the model
\[ Y_{g,i} \ind N(\mu_g,\sigma_g^2)\]
for $g=1,2$ and $i=1,\ldots,n_g$.
\pause
and you are interested in the relationship between $\mu_1$ and $\mu_2$.
\pause
We can perform the following statistical procedures:
\begin{itemize}
\item Frequentist: based on distribution of
\[\frac{\overline{Y}_1-\overline{Y}_2 - (\mu_1-\mu2)}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}}\]
where $S_g^2$ is the (random) sample variance of group $g$.
\pause
  \begin{itemize}
  \item pvalue for a hypothesis test, e.g. $H_0: \mu_1=\mu_2$, \pause
  \item confidence interval for $\mu_1-\mu_2$, \pause
  \end{itemize}
\item Bayesian: based on posterior for $\mu_1,\mu_2$, i.e. $p(\mu_1,\mu_2|y)$
  \begin{itemize}
  \item credible interval for $\mu_1-\mu_2$, \pause
  \item posterior model probability, e.g. $p(H_0|y)$, \pause and
  \item probability statements, e.g. $P(\mu_1<\mu_2|y)$.
  \end{itemize}
\end{itemize}
where $y=(y_{1,1},\ldots,y_{1,n_1},y_{2,1},\ldots,y_{2,n_2})$.

\end{frame}


\subsection{Simulating data}
\begin{frame}[fragile]
\frametitle{Simulating data}

We will simulate the following data
\[ Y_{g,i} \ind N(\mu_g,\sigma_g^2)\]
for $g=1,2,3$ and $i=1,\ldots,n_g$. \pause
For the moment we will only use the first two groups,
but later we will use all 3 groups.

\vspace{0.1in} \pause

<<data, echo=TRUE>>=
set.seed(20170301)

# Using the unknown population means and standard deviations
d <- bind_rows(
  data_frame(process = "P1", sensitivity = rnorm(22,  7.8, 2.3)),
  data_frame(process = "P2", sensitivity = rnorm(34,  9.3, 2.3)),
  data_frame(process = "P3", sensitivity = rnorm( 7, 10.0, 2.3)))

# readr::write_csv(d, path="sensitivity.csv")
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Data example}

Suppose you have two manufacturing processes to produce sensors and
you are interested in the average sensitivity of the sensors.

\vspace{0.1in} \pause

So you run the two processes and record the sensitivity of each sensor in units
of mV/V/mm Hg (\url{http://www.ni.com/white-paper/14860/en/}).
\pause
And you have the following summary statistics:
<<d2, dependson="data", echo=TRUE>>=
# d <- readr::read_csv("sensitivity.csv")
d2 <- d %>% filter(process %in% c("P1","P2"))
@

<<sm, dependson="d2", echo=TRUE>>=
sm <- d2 %>%
  group_by(process) %>%
  summarize(
    n    = n(),
    mean = mean(sensitivity),
    sd   = sd(sensitivity)
  )
sm
@

\end{frame}


\subsection{Frequentist analysis}
\begin{frame}
\frametitle{Frequentist analysis}

Analyses are based on the distribution of
\[
\frac{\overline{Y}_1-\overline{Y}_2 - (\mu_1-\mu2)}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}}
\uncover<3->{\sim t_{df}(0,1).}
\]
\pause
Unfortunately, this distribution is unknown.
\pause
The standard (although somewhat controversial) approach is to assume this
has a $t$ distribution but with an unknown degrees of freedom.
\pause
The degrees of freedom is often computed using the Satterthwaite approximation:
\[
df \approx \frac{\left( \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2}{\frac{s_1^4}{n_1^2(n_1-1)} + \frac{s_2^4}{n_2^2(n_2-1)}}.
\]

\end{frame}


\begin{frame}[fragile]
\frametitle{Pvalues and confidence intervals}

Because there is no indication that you have any expectation regarding the
sensitivities of process 1 compared to process 2, we will conduct a two-sided
\alert{two-sample t-test} assuming the variances are not equal,
\pause
i.e.
\[ Y_{g,i} \ind N(\mu_g,\sigma_g^2)\]
\pause
and

\[
H_0: \mu_1=\mu_2
\quad\text{and}\quad
H_A: \mu_1\ne \mu_2
\]

\pause

<<welch, dependson="d2",echo=TRUE>>=
t.test(sensitivity ~ process, data = d2)
@

\end{frame}


\subsection{Bayesian analysis}
\begin{frame}[fragile]
\frametitle{Posterior for $\mu_1,\mu_2$}

Assume
\[ Y_{g,i} \ind N(\mu_g,\sigma_g^2) 
\quad \text{and} \quad
p(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2) \propto 
\frac{1}{\sigma_1^2} \frac{1}{\sigma_2^2}.
\]
Then
\[
\mu_g|y \ind t_{n_g-1}(\overline{y}_g, s_g^2/n_g)
\]
\pause
and a draw for $\mu_g$ can be obtained by taking
\[
\overline{y}_g + T_{n_g-1} s_g/\sqrt{n_g}, \quad T_{n_g-1} \ind t_{n_g-1}(0,1).
\]
\pause

Simulations:
<<mu_draws, dependson="sm", echo=TRUE>>=
nr = 1e5
sims <- bind_rows(
  data_frame(
    rep = 1:nr,
    process = "P1",
    mu = sm$mean[1] + rt(nr, df = sm$n[1]-1) * sm$sd[1] / sqrt(sm$n[1])),
  data_frame(
    rep = 1:nr,
    process = "P2",
    mu = sm$mean[2] + rt(nr, df = sm$n[2]-1) * sm$sd[2] / sqrt(sm$n[2]))
)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{We can use these draws to compare the posteriors}

Compare posterior histograms for $\mu_1$ and $\mu_2$.
\pause

<<mu_posteriors, dependson="mu_draws", fig.height=3.5, echo=TRUE>>=
ggplot(sims, aes(x=mu, y=..density.., fill=process, group=process)) +
  geom_histogram(position = "identity", alpha=0.5, binwidth=0.1) +
  theme_bw()
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Credible interval for the difference}

To obtain statistical inference on the difference,
we use the samples and take the difference

<<mu_difference, dependson="mu_draws", echo=TRUE>>=
d3 <- sims %>%
  spread(process, mu) %>%
  mutate(diff = P1-P2)

# Bayes estimate for the difference
mean(d3$diff)

# Estimated 95% equal-tail credible interval
quantile(d3$diff, c(.025,.975))

# Estimate of the probability that mu1 is larger than mu2
mean(d3$diff > 0)
@

\end{frame}



\subsection{Three or more means}
\begin{frame}
\frametitle{Three or more means}

Now, let's consider the more general problem of
\[ Y_{g,i} \ind N(\mu_g,\sigma_g^2)\]
for $g=1,2,\ldots,G$ and $i=1,\ldots,n_g$
\pause
and you are interested in the relationship amongst the $\mu_g$.

\vspace{0.1in} \pause

We can perform the following statistical procedures:
\begin{itemize}
\item Frequentist: 
  \begin{itemize}
  \item pvalue for a hypothesis test, e.g. $H_0: \mu_g=\mu$ for all $g$, \pause
  \item confidence interval for $\mu_k-\mu_{k'}$ for a specified $k$ and $k'$, \pause
  \end{itemize}
\item Bayesian: based on posterior for $\mu_1,\ldots,\mu_G$
  \begin{itemize}
  \item credible interval for $\mu_k-\mu_{k'}$ for a specified $k$ and $k'$, \pause
  \item posterior model probability, e.g. $p(H_0|y)$, \pause and
  \item probability statements, e.g. $P(\mu_k<\mu_{k'}|y)$ for a specified $k$ and $k'$.
  \end{itemize}
\end{itemize}
\end{frame}



\begin{frame}[fragile]
\frametitle{Data example}

Suppose you have three manufacturing processes to produce sensors and
you are interested in the average sensitivity of the sensors.

\vspace{0.1in} \pause

So you run the three processes and record the sensitivity of each sensor in units
of mV/V/mm Hg (\url{http://www.ni.com/white-paper/14860/en/}).
\pause
And you have the following summary statistics:
<<summary2, dependson="data", echo=TRUE>>=
sm <- d %>%
  group_by(process) %>%
  summarize(
    n    = n(),
    mean = mean(sensitivity),
    sd   = sd(sensitivity)
  )
sm
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Pvalues}

\small

When there are lots of means, the first null hypothesis is typically
\[
H_0: \mu_g = \mu \, \forall \, g
\]

\pause

<<ftest, dependson="data", echo=TRUE>>=
oneway.test(sensitivity ~ process, data = d)
@

\pause

Then we typically look at pairwise differences:

<<pairwise, dependson="ftest", echo=TRUE>>=
pairwise.t.test(d$sensitivity,
                d$process,
                pool.sd = FALSE,
                p.adjust.method = "none")
@

\end{frame}


% \begin{frame}[fragile]
% \frametitle{Confidence intervals}
%
% We can consider confidence intervals for the following quantities
% \begin{itemize}
% \item $\mu_g$ for all $g$
% \item $\mu_g - \mu_j$ for any $i$ and $j$
% \end{itemize}
%
% \pause
%
% <<>>=
% t.test(d$sensitivity[d$process == 1])$conf.int %>% as.numeric
% t.test(d$sensitivity[d$process == 2])$conf.int %>% as.numeric
% t.test(d$sensitivity[d$process == 3])$conf.int %>% as.numeric
% @
%
% \pause
%
% <<>>=
% TukeyHSD()
% @
%
% \end{frame}




\begin{frame}[fragile]
\frametitle{Posteriors for $\mu$}

When
\[ Y_{g,i} \ind N(\mu_g,\sigma_g^2),\]
\pause
we have 
\[
\mu_g|y \ind t_{n_g-1}(\overline{y}_g, s_g^2/n_g)
\]
\pause
and that a draw for $\mu_g$ can be obtained by taking
\[
\overline{y}_g + T_{n_g-1} s_g/\sqrt{n_g}, \quad T_{n_g-1} \ind t_{n_g-1}(0,1).
\]
\pause


<<mu3_draws, dependson="summary2", echo=TRUE>>=
sims <- bind_rows(
  sims, # groups 1 and 2
  data_frame(
    rep = 1:nr,
    process = "P3",
    mu = sm$mean[3] + rt(nr, df = sm$n[3]-1) * sm$sd[3] / sqrt(sm$n[3]))
)
@



\end{frame}


\begin{frame}[fragile]
\frametitle{We can use these draws to compare the posteriors}

We can obtain posteriors for $\mu$ and plot histograms (or smoothed histograms)
to compare the posteriors.
\pause

<<mu3_posteriors, dependson="mu3_draws", fig.height=3.5, echo=TRUE>>=
ggplot(sims, aes(x=mu, y=..density.., fill=process, group=process)) +
  geom_histogram(position = "identity", alpha=0.5, binwidth=0.1) +
  theme_bw()
@

\end{frame}







\begin{frame}[fragile]
\frametitle{Credible intervals for differences}

Use the simulations to calculate posterior probabilities and
credible intervals for differences.

<<mu3_comparison, dependson="mu3_draws", echo=TRUE>>=
# Estimate of the probability that one mean is larger than another
sims %>%
  spread(process, mu) %>%
  mutate(`mu1-mu2` = P1-P2,
            `mu1-mu3` = P1-P3,
            `mu2-mu3` = P2-P3) %>%
  select(`mu1-mu2`,`mu1-mu3`,`mu2-mu3`) %>%
  gather(comparison, diff) %>%
  group_by(comparison) %>%
  summarize(probability = mean(diff>0) %>% round(4),
            lower = quantile(diff, .025) %>% round(2),
            upper = quantile(diff, .975) %>% round(2)) %>%
  mutate(credible_interval = paste("(",lower,",",upper,")", sep="")) %>%
  select(comparison, probability, credible_interval)
@

\end{frame}




\subsection{Common variance}
\begin{frame}[fragile]
\frametitle{Common variance model}

In the model 
\[
Y_{g,i} \ind N(\mu_g,\sigma^2)
\]
we can calculate a pvalue for the following null hypothesis:
\[
H_0:\sigma_g = \sigma \, \forall i
\]
\pause
<<bartlett, dependson="data", echo=TRUE>>=
bartlett.test(sensitivity ~ process, data = d)
@
\pause
This may give us reason to proceed as if the variances is the same in all 
groups, i.e. 
\[
Y_{g,i} \ind N(\mu_g,\sigma^2).
\]
\pause
This assumption is common when the number of observations in the groups is 
small.

\end{frame}



\begin{frame}[fragile]
\frametitle{Comparing means when the variances are equal}

Assuming $Y_{g,i} \ind N(\mu_g,\sigma^2)$, we can test
\[
H_0: \mu_g = \mu \, \forall \, g
\]

\pause

<<ftest2, dependson="data", echo=TRUE>>=
oneway.test(sensitivity ~ process, data = d, var.equal = TRUE)
@

\pause

Then we typically look at pairwise differences, i.e. $H_0: \mu_k = \mu_{k'}$.

<<pairwise2, dependson="ftest", echo=TRUE>>=
pairwise.t.test(d$sensitivity, d$process, p.adjust.method = "none")
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Posteriors for $\mu$}

If $Y_{g,i} \ind N(\mu_g,\sigma^2)$ and we use the prior
$p(\mu_1,\ldots,\mu_G,\sigma^2) \propto 1/\sigma^2$,
\pause then
\[
\mu_g|y,\sigma^2 \ind N(\overline{y}_g, \sigma^2/n_g)
\quad
\sigma^2|y \sim IG\left( \frac{n-G}{2}, \frac{1}{2}\sum_{g=1}^G \sum_{i=1}^{n_g} (y_{g,i}-\overline{y}_g)^2\right)
\]
where $n=\sum_{g=1}^G n_g$.
\pause
and thus, we obtain joint samples for $\mu$ by performing the following
\begin{enumerate}
\item $\sigma^{2(m)} \sim p(\sigma^2|y)$ \\
\item For $g=1,\ldots,G$, $\mu_g \sim p(\mu_g|y,\sigma^{2(m)})$.
\end{enumerate}


<<mu2_draws, dependson="summary2", echo=TRUE>>=
nr = 1e5
sims <- data.frame(rep = 1:nr,
  sigma = 1/sqrt( rgamma(nr,
                         shape = sum(sm$n-1)/2,
                         rate = sum((sm$n-1)*sm$sd^2)/2 
                         )
                  )
  ) %>%
  mutate(
    mu1 = rnorm(nr, mean = sm$mean[1], sd = sigma / sqrt(sm$n[1])),
    mu2 = rnorm(nr, mean = sm$mean[2], sd = sigma / sqrt(sm$n[2])),
    mu3 = rnorm(nr, mean = sm$mean[3], sd = sigma / sqrt(sm$n[3])))
@


\end{frame}



\begin{frame}[fragile]
\frametitle{We can use these draws to compare the posteriors}

We can obtain posteriors for $\mu$ and plot histograms (or smoothed histograms)
to compare the posteriors.
\pause

<<mu2_posteriors, dependson="mu2_draws", fig.height=3.5, warning=FALSE>>=
d_plot <- sims %>% gather(parameter, draw, -rep, -sigma)
ggplot(d_plot, aes(draw, fill=parameter, group=parameter)) +
  geom_histogram(position = "identity", alpha = 0.5, binwidth=0.1) +
  theme_bw()
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Credible interval for the differences}

To compare the means,
we compare the samples drawn from the posterior.


<<mu2_comparison, dependson="mu2_draws", echo=TRUE>>=
sims %>%
  mutate(`mu1-mu2` = mu1-mu2,
         `mu1-mu3` = mu1-mu3,
         `mu2-mu3` = mu2-mu3) %>%
  select(`mu1-mu2`,`mu1-mu3`,`mu2-mu3`) %>%
  gather(comparison, diff) %>%
  group_by(comparison) %>%
  summarize(probability = mean(diff>0) %>% round(4),
            lower = quantile(diff, .025) %>% round(2),
            upper = quantile(diff, .975) %>% round(2)) %>%
  mutate(credible_interval = paste("(",lower,",",upper,")", sep="")) %>%
  select(comparison, probability, credible_interval)
@

\end{frame}



\end{document}



