\documentclass[handout]{beamer}

\usepackage{tikz, animate}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title{Gibbs sampling}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\I}{\mathrm{I}}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library(reshape2)
library(plyr)
library(ggplot2)
library(xtable)
library(MASS)
library(mvtnorm)
@

<<set_seed>>=
set.seed(1)
@

\frame{\maketitle}

\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Two-component Gibbs sampler
  \begin{itemize}
  \item Full conditional distribution
  \end{itemize}
\item $K$-component Gibbs sampler
  \begin{itemize}
  \item Blocked Gibbs sampler
  \end{itemize}
\item Metropolis-within-Gibbs
\item Slice sampler
  \begin{itemize}
  \item Latent variable augmentation
  \end{itemize}
\end{itemize}

\end{frame}

% \section{Markov chain Monte Carlo}
% \begin{frame}
% \frametitle{Markov chain construction}
% 
% Suppose we wish to simulate from $p(\theta|y)$, but cannot do so directly. 
% 
% \vspace{0.2in} 
% 
% Construct a Markov chain with stationary distributon $p(\theta|y)$ and run it long enough that the samples are approximately from $p(\theta|y)$. 
% 
% \end{frame}


\section{Two-component Gibbs sampling}
\begin{frame}
\frametitle{Two component Gibbs sampler}
  Suppose our target distribution is $p(\theta|y)$ with $\theta=(\theta_1,\theta_2)$  \pause and we can sample from $p\left(\theta_1|\theta_2,y\right)$ and $p\left(\theta_2|\theta_1, y\right)$\pause. Beginning with an initial value $\left(\theta_1^{(0)}, \theta_2^{(0)}\right)$\pause, an \alert{iteration} of the \alert{Gibbs sampler} involves
	\begin{enumerate}[\quad1.]
	\item Sampling $\theta_1^{(t)}\sim p\left(\theta_1\left|\theta_2^{(t-1)},y\right.\right)$. \pause 
	\item Sampling $\theta_2^{(t)}\sim p\left(\theta_2\left|\theta_1^{(t)},y\right.\right)$.
	\end{enumerate}
	
  \vspace{0.2in} \pause
  
  Thus in order to run a Gibbs sampler, we need to derive the \alert{full conditional} for $\theta_1$ and $\theta_2$, i.e. the distribution for $\theta_1$ and $\theta_2$ conditional on everything else.

\end{frame}





% \vspace{0.1in} \pause
% 	
% 	Notes:
% 	\begin{itemize}[<+->]
%   \item The sequence $\{\theta^{(0)},\theta^{(1)},\ldots,\}$ is a Markov chain with transition distribution 
%   \[ p(\theta^{(t)}|\theta^{(t-1)},y) = p(\theta_2^{(t)}|\theta_1^{(t)},y)p(\theta_1^{(t)}|\theta_2^{(t-1)},y). \]
% 	\item $\theta^{(t)} \stackrel{d}{\rightarrow} \theta$ where $\theta \sim p\left(\theta|y\right)$.  
% %	\item 
% %	\[  \frac{1}{J} \sum_{j=1}^J h\left(\theta^{(t)}\right) \rightarrow E_p[h(\theta)] = \int_{\Theta} h(\theta) f(\theta) d\theta  \]
% 	\end{itemize}



% \begin{frame}
% \frametitle{}
% \begin{theorem}
% If $K(\theta,\theta')= p(\theta_2'|\theta_1',y)p(\theta_1'|\theta_2,y)$ \pause then $p(\theta|y)$ is the stationary distribution, \pause i.e.
% \[ p(\theta'|y) = \int K(\theta,\theta') p(\theta|y) d\theta. \]
% \end{theorem}
% \pause 
% \begin{proof}
% \[ \begin{array}{rl}
% p(\theta'|y) &= p(\theta_2'|\theta_1',y)p(\theta_1'|y) \pause \\
% &= p(\theta_2'|\theta_1',y) \int p(\theta_1'|\theta_2,y) p(\theta_2|y) d\theta_2 \pause \\
% &= p(\theta_2'|\theta_1',y)\int p(\theta_1'|\theta_2,y) \int p(\theta_2,\theta_1|y) d\theta_1 d\theta_2 \pause \\
% &= \int \int p(\theta_2'|\theta_1',y) p(\theta_1'|\theta_2,y) p(\theta_2,\theta_1|y) d\theta_1 d\theta_2 \pause \\
% &= \int K(\theta,\theta') p(\theta|y) d\theta 
% \end{array} \]
% \end{proof}
% 
% \end{frame}


\subsection{Bivariate normal example}
\frame{\frametitle{Bivariate normal example}
\small
	Let our target be 
	\[ \theta \sim N_2(0,\mathrm\Sigma) \pause \qquad \mathrm{\Sigma} = \left[ \begin{array}{cc} 1 & \rho \\ \rho & 1 \end{array} \right].\]
	\pause Then 
	\[ \begin{array}{rl}
	\theta_1|\theta_2 &\sim \pause N\left(\rho\theta_2, [1-\rho^2]\right) \pause \\
	\theta_2|\theta_1 &\sim \pause N\left(\rho\theta_1, [1-\rho^2]\right) 
	\end{array} \]
	are the conditional distributions. 
	
\vspace{0.1in} \pause
	
	Assuming initial value $\left(\theta_1^{0}, \theta_2^{0}\right)$, the Gibbs sampler proceeds as follows:
	\[ \begin{array}{ccc}
	\mbox{Iteration} & \mbox{Sample }\theta_1 & \mbox{Sample }\theta_2 \pause \\
	\hline 
	1 & \theta_1^{(1)}\sim N\left(\rho\theta_2^{0}, [1-\rho^2]\right) &\theta_2^{(1)} \sim N\left(\rho\theta_1^{(1)}, [1-\rho^2]\right) \pause \\
	& \vdots & \\
	t & \theta_1^{(t)}\sim N\left(\rho\theta_2^{(t-1)}, [1-\rho^2]\right) &\theta_2^{(t)} \sim N\left(\rho\theta_1^{(t)}, [1-\rho^2]\right) \pause \\
	& \vdots &
	\end{array} \]	
}

\begin{frame}[fragile]
\frametitle{R code for bivariate normal Gibbs sampler}
<<bivariate_normal_mcmc, echo=TRUE>>=
gibbs_bivariate_normal = function(theta0, n_points, rho) {
  theta = matrix(theta0, nrow=n_points, ncol=2, byrow=TRUE)
  v = sqrt(1-rho^2)
  for (i in 2:n_points) {
    theta[i,1] = rnorm(1, rho*theta[i-1,2], v)
    theta[i,2] = rnorm(1, rho*theta[i  ,1], v)
  }
  return(theta)
}

theta = gibbs_bivariate_normal(c(-3,3), n<-20, rho=rho<-0.9)
@
\end{frame}

<<bivariate_normal, dependson='bivariate_normal_mcmc', echo=FALSE>>=
bivariate_normal_animation = function(x, rho, ask=interactive()) {
  # Create contour plot
  n.out = 101
  xx <- seq(-3, 3, length=n.out)
  grid <- expand.grid(x=xx, y=xx)
  Sigma = diag(rep(.1,2))+rho
  like <- matrix(apply(grid, 1, function(x) mvtnorm::dmvnorm(x,sigma=Sigma)),n.out,n.out)
  
  for (i in 2:nrow(x)) {
    jj = (2:i)[-(i-1)] # vector from 2:(i-1) and NULL if i=2
    for (j in 1:6) {
      plot.new()
      
      # All previous plotting
      contour(xx, xx, like, drawlabels=F, nlevels=10, xlim=c(-3,3), ylim=c(-3,3), 
              xlab=expression(theta[1]), ylab=expression(theta[2]))  
      segments(x[jj-1,1], x[jj-1,2], x[jj,1], x[jj-1,2], col="gray")
      segments(x[jj  ,1], x[jj-1,2], x[jj,1], x[jj  ,2], col="gray")
      points(x[(1:(i-1)),1], x[(1:(i-1)),2], col="red", pch=19)
      
      # New plotting
      if (j>1 & j<4) abline(h=x[i-1,2], lty=2)
      if (j>2) arrows(x[i-1,1], x[i-1,2], x[i,1], x[i-1,2], length=0.1)
      if (j>3 & j<6) abline(v=x[i,1], lty=2)
      if (j>4) arrows(x[i,1], x[i-1,2], x[i,1], x[i,2], length=0.1)
      if (j>5) points(x[i,1], x[i,2], col="red", pch=19)
      
      if (ask) readline("hit <enter>:")
    }
  }
  
  jj=2:nrow(x)
  contour(xx, xx, like, drawlabels=F, nlevels=10, xlim=c(-3,3), ylim=c(-3,3), 
          xlab=expression(theta[1]), ylab=expression(theta[2]))  
  segments(x[jj-1,1], x[jj-1,2], x[jj,1], x[jj-1,2], col="gray")
  segments(x[jj  ,1], x[jj-1,2], x[jj,1], x[jj  ,2], col="gray")
  points(x[,1], x[,2], col="red", pch=19)
}
@

\begin{frame}[fragile]
<<bivariate_normal_animation, dependson=c('bivariate_normal_mcmc','bivariate_normal'), fig.show='animate', echo=FALSE, cache=TRUE>>=
bivariate_normal_animation(theta, rho)
@
\end{frame}



\subsection{Normal model}
\begin{frame}
\frametitle{Normal model}
\footnotesize

Suppose $Y_i\stackrel{ind}{\sim} N(\mu,\sigma^2)$ and we assume the prior
\[ \mu \sim N(m,C) \qquad \mbox{and} \qquad \sigma^2 \sim \mbox{Inv-}\chi^2(v,s^2). \]
\pause
Note: this is NOT the conjugate prior. \pause 

\vspace{0.2in} \pause

The full posterior we are interested in is 
\[ \begin{array}{rl}
p(\mu,\sigma^2|y) \propto& (\sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2}(\sum_{i=1}^n (y_i-\mu)^2 \right) \exp\left( -\frac{1}{2C} (\mu-m)^2 \right) \\
&\times(\sigma^2)^{-(v/2+1)} \exp\left( -\frac{vs^2}{2\sigma^2} \right) 
\end{array} \]

To run the Gibbs sampler, we need to derive
\begin{itemize}
\item $\mu|\sigma^2,y$ and 
\item $\sigma^2|\mu,y$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Derive $\mu|\sigma^2,y$.}
Recall
\[ \begin{array}{rl}
p(\mu,\sigma^2|y) \propto& (\sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\mu)^2 \right) \exp\left( -\frac{1}{2C} (\mu-m)^2 \right) \\
&\times(\sigma^2)^{-(v/2+1)} \exp\left( -\frac{vs^2}{2\sigma^2} \right) 
\end{array} \]

Now find $\mu|\sigma^2,y$:
\[\begin{array}{rl}
p(\mu|\sigma^2,y) &\propto p(\mu,\sigma^2|y) \\
&\propto \exp\left( -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\mu)^2 \right) \exp\left( -\frac{1}{2C} (\mu-m)^2 \right) \\
&\propto \exp\left( -\frac{1}{2} \left[ \left( \frac{1}{\sigma^2/n} + \frac{1}{C} \right)\mu^2 - 2\mu\left( \frac{\overline{y}}{\sigma^2/n} + \frac{m}{C} \right) \right] \right)
\end{array} \]
thus $\mu|\sigma^2,y \sim N(m',C')$ where 
\[ 
\begin{array}{rl} 
m' &= C'\left( \frac{\overline{y}}{\sigma^2/n} + \frac{m}{C} \right) \\
C' &= \left( \frac{1}{\sigma^2/n} + \frac{1}{C} \right)^{-1}
\end{array} \]

\end{frame}


\begin{frame}
\frametitle{Derive $\sigma^2|\mu,y$.}
Recall
\[ \begin{array}{rl}
p(\mu,\sigma^2|y) \propto& (\sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\mu)^2 \right) \exp\left( -\frac{1}{2C} (\mu-m)^2 \right) \\
&\times(\sigma^2)^{-(v/2+1)} \exp\left( -\frac{vs^2}{2\sigma^2} \right) 
\end{array} \]

Now find $\sigma^2|\mu,y$:
\[\begin{array}{rl}
p(\sigma^2|\mu,y) &\propto p(\mu,\sigma^2|y) \\
&\propto (\sigma^2)^{-([v+n]/2+1)}\exp\left( -\frac{1}{2\sigma^2} \left[ vs^2 + \sum_{i=1}^n (y_i-\mu)^2 \right]\right)
\end{array} \]
and thus $\sigma^2|\mu,y \sim \mbox{Inv-}\chi^2(v', (s')^2)$ where
\[ \begin{array}{rl}
v' & = v+n \\
v'(s')^2 &= vs^2 + \sum_{i=1}^n (y_i-\mu)^2 
\end{array} \]

\end{frame}



\begin{frame}[fragile]
\frametitle{R code for Gibbs sampler}

<<data, echo=TRUE>>=
# Data and prior
y = rnorm(10)
m = 0; C = 10
v = 1; s = 1

# Initial values
mu = 0
sigma2 = 1

# Save structures
n_iter = 1000
mu_keep = rep(NA, n_iter)
sigma_keep = rep(NA, n_iter)

# Pre-calculate 
n = length(y)
sum_y = sum(y)
vp = v+n
vs2 = v*s^2
@

\end{frame}



\begin{frame}[fragile]
\frametitle{R code for Gibbs sampler}

<<gibbs, dependson='data', echo=TRUE>>=
# Gibbs sampler
for (i in 1:n_iter) {
  # Sample mu
  Cp = 1/(n/sigma2+1/C)
  mp = Cp*(sum_y/sigma2+m/C)
  mu = rnorm(1, mp, sqrt(Cp))
  
  # Sample sigma
  vpsp2 = vs2 + sum((y-mu)^2)
  sigma2 = 1/rgamma(1, vp/2, vpsp2/2)
  
  # Save iterations
  mu_keep[i] = mu
  sigma_keep[i] = sqrt(sigma2)
}
@

\end{frame}




\begin{frame}[fragile]
\frametitle{Posteriors}

<<gibbs_posteriors, dependson='gibbs', fig.width=15, warning=FALSE>>=
d = data.frame(mu=mu_keep, sigma=sigma_keep, t = 1:length(mu_keep))
m = melt(d, id.vars='t', variable.name='parameter')
ggplot(m, aes(x=t, y=value)) + 
  geom_line() + 
  facet_wrap(~parameter, scales='free') +
  theme_bw()

ggplot(m, aes(x=value, y=..density..)) + 
  geom_histogram(binwidth=0.02) + 
  facet_wrap(~parameter, scales='free') +
  theme_bw()
@

\end{frame}


\section{$K$-component Gibbs sampler}
\frame{\frametitle{$K$-component Gibbs sampler}

\small

	Suppose $\theta = (\theta_1,\ldots,\theta_K)$\pause, then an iteration of a $K$-component Gibbs sampler is  \pause
	\[ \begin{array}{rll}
	\theta_1^{(t)} &\sim p\left(\theta_1\left|\theta_2^{(t-1)},\ldots, \theta_K^{(t-1)},y\right.\right) \pause \\ \\
	\theta_2^{(t)} &\sim p\left(\theta_2\left|\theta_1^{(t)}, \theta_3^{(t-1)},\ldots, \theta_K^{(t-1)},y\right.\right) \pause\\
	&\vdots \\
	\theta_k^{(t)} &\sim p\left(\theta_k\left|\theta_1^{(t)}\ldots,\theta_{k-1}^{(t)}, \theta_{k+1}^{(t-1)},\ldots, \theta_K^{(t-1)},y\right.\right) \\
	&\vdots \pause\\
	\theta_K^{(t)} &\sim p\left(\theta_K\left|\theta_1^{(t)},\ldots, \theta_{K-1}^{(t)},y\right.\right) \pause
	\end{array} \]

\vspace{0.1in} \pause
	
	The distributions above are called the \alert{full conditional distributions}. 
	\pause 
	If some of the $\theta_k$ are vectors, 
	then this is called a \alert{block} Gibbs sampler.
}


\begin{frame}
\frametitle{Hierarchical normal model}

Let 
\[ Y_{ij} \stackrel{ind}{\sim} N(\mu_i,\sigma^2), \qquad \mu_i \stackrel{ind}{\sim} N(\eta,\tau^2)\]
\pause
for $i=1,\ldots,\mathrm{I}$, $j=1,\ldots,n_i$, $n = \sum_{i=1}^\mathrm{I} n_i$ and \pause prior 
\[ p(\eta,\tau^2,\sigma) \propto IG(\tau^2; a_\tau, b_\tau) IG(\sigma^2; a_\sigma, b_\sigma). \]

\vspace{0.2in} \pause

The full conditionals are 
\[ \begin{array}{rl}
p(\mu|\eta,\sigma^2,\tau^2,y) \pause &= \prod_{i=1}^n p(\mu_i|\eta,\sigma^2,\tau^2,y_i) \pause \\
p(\mu_i|\eta,\sigma^2,\tau^2,y_i) \pause &= N\left(\left[ \frac{1}{\sigma^2/n_i} + \frac{1}{\tau^2} \right] \left[ \frac{\overline{y}_i}{\sigma^2/n_i} + \frac{\eta}{\tau^2} \right] , \left[ \frac{1}{\sigma^2/n_i} + \frac{1}{\tau^2} \right]^{-1} \right) \pause \\
p(\eta|\mu,\sigma^2,\tau^2,y) \pause &= N\left( \overline{\mu}, \tau^2/\mathrm{I}\right) \pause \\
p(\sigma^2|\mu,\eta,\tau^2,y) \pause &= IG(a_\sigma+n/2, b_\sigma + \sum_{i=1}^\mathrm{I} \sum_{j=1}^{n_j} (y_{ij}-\mu_i)^2/2) \pause \\
p(\tau^2|\mu,\eta,\sigma^2,y) \pause &= IG(a_\tau+\mathrm{I}/2, b_\tau + \sum_{i=1}^\mathrm{I} (\mu_i-\eta)^2/2) 
\end{array} \]
\pause where $n_i\overline{y}_i = \sum_{j=1}^{n_i} y_{ij}$ and $\mathrm{I}\,\overline{\mu} = \sum_{i=1}^\mathrm{I} \mu_i$. 

\end{frame}



\section{Metropolis-within-Gibbs}
\frame{\frametitle{Metropolis-within-Gibbs}
  We have discussed two Markov chain approaches to sample from a target distribution:
	\begin{itemize}
	\item Metropolis-Hastings algorithm
	\item Gibbs sampling
	\end{itemize}
	
\vspace{0.1in} \pause
	
	Gibbs sampling assumed we can sample from $p(\theta_k|\theta_{-k},y)$ for all $k$\pause, but what if we cannot sample from all of these full conditional distributions? \pause For those $p(\theta_k|\theta_{-k})$ that cannot be sampled directly, a single iteration of the Metropolis-Hastings algorithm can be substituted. 
}




\begin{frame}[fragile]
\frametitle{Bivariate normal with $\rho=0.9$}

Reconsider the bivariate normal example substituting a Metropolis step in place of a Gibbs step:

<<gibbs_and_metropolis_mcmc, echo=TRUE>>=
gibbs_and_metropolis = function(theta0, n_points, rho) {
  theta = matrix(theta0, nrow=n_points, ncol=2, byrow=TRUE)
  v = sqrt(1-rho^2)
  for (i in 2:n_points) {
    theta[i,1] = rnorm(1, rho*theta[i-1,2], v)
    
    # Now do a random-walk Metropolis step
    theta_prop = rnorm(1, theta[i-1,2], 2.4*v) # optimal proposal variance
    logr   = dnorm(theta_prop,   rho*theta[i,1], v, log=TRUE) - 
             dnorm(theta[i-1,2], rho*theta[i,1], v, log=TRUE)
    theta[i,2] = ifelse(log(runif(1))<logr, theta_prop, theta[i-1,2])
  }
  return(theta)
}

theta = gibbs_and_metropolis(c(-3,3), n, rho)
length(unique(theta[,2]))/length(theta[,2]) # acceptance rate
@
\end{frame}

\begin{frame}[fragile]
<<gibbs_and_metropolis_animation, dependson='gibbs_and_metropolis_mcmc', fig.show='animate', echo=FALSE, warning=FALSE>>=
bivariate_normal_animation(theta, rho)
@
\end{frame}


\begin{frame}
\frametitle{Hierarchical normal model}

Let 
\[ Y_{ij} \stackrel{ind}{\sim} N(\mu_i,\sigma^2), \qquad \mu_i \stackrel{ind}{\sim} N(\eta,\tau^2)\]
\pause
for $i=1,\ldots,\mathrm{I}$, $j=1,\ldots,n_i$, $n = \sum_{i=1}^\mathrm{I} n_i$ and \pause prior 
\[ p(\eta,\tau,\sigma) \propto Ca^+(\tau; 0, b_\tau) Ca^+(\sigma; 0, b_\sigma). \]

\vspace{0.2in} \pause

The full conditionals are \pause exactly the same except \pause
\[ \begin{array}{rl}
p(\sigma|\mu,\eta,\tau^2,y) \pause &\propto IG(\sigma^2;n/2, \sum_{i=1}^\mathrm{I} \sum_{j=1}^{n_j} (y_{ij}-\mu_i)^2/2)Ca^+(\sigma;0,b_\sigma) \pause \\
p(\tau^2|\mu,\eta,\sigma^2,y) \pause &\propto IG(\tau^2;\mathrm{I}/2, \sum_{i=1}^\mathrm{I} (\mu_i-\eta)^2/2) Ca^+(\tau;0,b_\tau)
\end{array} \]
\pause 
where $n_i\overline{y}_i = \sum_{j=1}^{n_i} y_{ij}$ and $\mathrm{I}\,\overline{\mu} = \sum_{i=1}^\mathrm{I} \mu_i$. 

\end{frame}



\begin{frame}
\frametitle{Hierarchical normal model}

To sample from $p(\tau|\mu,\sigma, \eta, y) \propto IG(\tau^2;a,b) Ca^+(0,b_\tau)$ (or equivalently $p(\sigma|\mu,\eta, \tau, y)$), we have a variety of possibilities. \pause Here are three:
\begin{enumerate}[<+->]
\item Rejection sampling with $(\tau^*)^2\sim IG(a,b)$ and thus $M^*_{opt} = Ca^+(0;0,b_\tau)$ and the acceptance probability is $Ca^+(\tau^*;0,b_\tau) / M^*_{opt}$.
\item Independence Metropolis-Hastings with $(\tau^*)^2\sim IG(a,b)$ and thus the acceptance probability is $Ca^+(\tau^*;0,b_\tau)/Ca^+(\tau^{(t)};0,b_\tau)$.
\item Random-walk Metroplis-Hastings with $\tau^* \sim g(\cdot|\tau^{(t)})$ and acceptance probability is $q(\tau^*|y)/q(\tau^{(t)}|y)$. 
\end{enumerate}

\end{frame}






\section{Hierarchical binomial model}
\begin{frame}
\frametitle{Hierarchical binomial model}

Let 
\[ \begin{array}{rl}
Y_i&\stackrel{ind}{\sim} Bin(n_i,\theta_i) \pause \\
\theta_i &\stackrel{iid}{\sim} Be(\alpha,\beta) \pause \\
p(\alpha,\beta) &\propto (\alpha+\beta)^{-5/2} \pause
\end{array} \]

If we want to use a Gibbs sampler to sample $\theta_1,\ldots,\theta_n,\alpha,\beta$\pause, we need to derive the following conditional distributions:\pause
\begin{itemize}
\item $\theta_i|\theta_1,\ldots,\theta_{i-1},\theta_{i+1},\ldots,\theta_n,\alpha,\beta,y$ \pause
\item $\alpha|\theta_1,\ldots,\theta_n,\beta,y$ \pause 
\item $\beta|\theta_1,\ldots,\theta_n,\alpha,y$ \pause
\end{itemize}
For shorthand, I often use $\theta_i|\ldots$ where ``$\ldots$'' indicates \emph{everything else}.
\end{frame}



\begin{frame}
\frametitle{Full conditional for $\theta_i$}

\[ 
Y_i\stackrel{ind}{\sim} Bin(n_i,\theta_i), \quad
\theta_i \stackrel{iid}{\sim} Be(\alpha,\beta), \quad
p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2}
\]

\vspace{0.2in} \pause

The full conditional for $\theta_i$ is 
\[ \begin{array}{rl}
p(\theta_i|\ldots)
&\propto p(y|\theta)p(\theta|\alpha,\beta)p(\alpha,\beta) \pause \\
&\propto \left[ \prod_{i=1}^n p(y_i|\theta_i) \right] \left[ \prod_{i=1}^n  p(\theta_i|\alpha,\beta) \right] \pause \\
&\propto \prod_{j=1}^n \theta_j^{y_j}(1-\theta_j)^{n_j-y_j} \theta_j^{\alpha-1}(1-\theta_j)^{\beta-1}  \pause \\
&\propto \theta_i^{\alpha+y_i-1}(1-\theta_i)^{\beta+n_i-y_i-1} \pause
\end{array} \]
Thus $\theta_i|\ldots\sim Be(\alpha+y_i,\beta+n_i-y_i)$.
\end{frame}




\begin{frame}
\frametitle{Full conditional for $\alpha$ and $\beta$}

{\scriptsize
\[ 
Y_i\stackrel{ind}{\sim} Bin(n_i,\theta_i), \quad
\theta_i \stackrel{iid}{\sim} Be(\alpha,\beta), \quad
p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2}
\] }

\pause

The full conditional for $\alpha$ \pause is 
\[ \begin{array}{rl}
p(\alpha|\ldots) &\propto p(y|\theta)p(\theta|\alpha,\beta)p(\alpha,\beta) \pause \\
&\propto \left[ \prod_{i=1}^n  p(\theta_i|\alpha,\beta) \right] p(\alpha,\beta) \pause \\
&\propto \frac{\left(\prod_{i=1}^n \theta_i\right)^{\alpha-1}}{Beta(\alpha,\beta)^n}(\alpha+\beta)^{-5/2}
\end{array} \]
which is not a known density. 

\vspace{0.2in} \pause

The full conditional for $\beta$ is 
\[ \begin{array}{rl}
p(\beta|\ldots) &\propto p(y|\theta)p(\theta|\alpha,\beta)p(\alpha,\beta) \pause \\
&\propto \left[ \prod_{i=1}^n  p(\theta_i|\alpha,\beta) \right] p(\alpha,\beta) \pause \\
&\propto \frac{\left(\prod_{i=1}^n [1-\theta_i]\right)^{\beta-1}}{Beta(\alpha,\beta)^n}(\alpha+\beta)^{-5/2}
\end{array} \]
which is not a known density.

\end{frame}




\begin{frame}[fragile]
\frametitle{Full conditional functions}
{\small 
\[ \begin{array}{rl}
\log p(\alpha|\ldots) &\propto (\alpha-1) \sum_{i=1}^n \log\left(  \phantom{1-}\theta_i \right) +n\log(Beta(\alpha,\beta))-5/2 \log(\alpha+\beta) \\
\log p(\beta|\ldots) &\propto (\beta-1) \sum_{i=1}^n \log\left(  1-\theta_i \right) +n\log(Beta(\alpha,\beta))-5/2 \log(\alpha+\beta) 
\end{array} \]
}


<<full_conditionals, echo=TRUE>>=
log_fc_alpha = function(theta, alpha, beta) {
  if (alpha<0) return(-Inf)
  n = length(theta)
  (alpha-1)*sum(log(theta))-n*lbeta(alpha,beta)-5/2*(alpha+beta)
}

log_fc_beta = function(theta, alpha, beta) {
  if (beta<0) return(-Inf)
  n = length(theta)
  (beta-1)*sum(log(1-theta))-n*lbeta(alpha,beta)-5/2*(alpha+beta)
}
@
\end{frame}


\begin{frame}[fragile]
<<mcmc, dependson='full_conditionals', echo=TRUE>>=
mcmc = function(n_sims, dat, inits, tune) {
  n_groups = nrow(dat)
  alpha = inits$alpha
  beta  = inits$beta
  
  # Recording structure
  theta_keep = matrix(NA, nrow=n_sims, ncol=n_groups)
  alpha_keep = rep(alpha, n_sims)
  beta_keep  = rep(beta , n_sims)
  
  for (i in 1:n_sims) {
    # Sample thetas
    theta = with(dat, rbeta(length(y), alpha+y, beta+n-y))
    
    # Sample alpha
    alpha_prop = rnorm(1, alpha, tune$alpha)
    logr = log_fc_alpha(theta, alpha_prop, beta)-log_fc_alpha(theta, alpha, beta)
    alpha = ifelse(log(runif(1))<logr, alpha_prop, alpha)
    
    # Sample beta
    beta_prop = rnorm(1, beta, tune$beta)
    logr = log_fc_beta(theta, alpha, beta_prop)-log_fc_beta(theta, alpha, beta)
    beta = ifelse(log(runif(1))<logr, beta_prop, beta)
    
    # Record parameter values
    theta_keep[i,] = theta
    alpha_keep[i]  = alpha
    beta_keep[ i]  = beta
  }
  
  return(data.frame(iteration=1:n_sims, 
                    parameter=rep(c("alpha","beta",paste("theta[",1:n_groups,"]",sep="")),each=n_sims),
                    value=c(alpha_keep,beta_keep,as.numeric(theta_keep))))
}
@
\end{frame}
 
 
 
\begin{frame}[fragile]
<<run_mcmc, dependson='mcmc', echo=TRUE>>=
d = read.csv("../Ch05/Ch05a-dawkins.csv")
dat=data.frame(y=d$made, n=d$attempt)
inits = list(alpha=1, beta=1)

# Run the MCMC
r = mcmc(2000, dat=dat, inits=inits, tune=list(alpha=1,beta=1))
@

<<mcmc_summary, dependson='run_mcmc', fig.width=10>>=
r_subset = r[r$parameter %in% c("alpha","beta"),]
ddply(r_subset, .(parameter), summarize, 
      acceptance_rate = length(unique(value))/max(iteration))
ggplot(r_subset, aes(x=iteration, y=value)) +
  geom_line() +
  facet_wrap(~parameter) +
  theme_bw()
@
\end{frame}
 
 
 
 
 
\begin{frame}
\frametitle{Block Gibbs sampler}
\scriptsize
It appears that the Gibbs sampler we have constructed \pause iteratively samples from
\begin{enumerate}[<+->]
\item $\theta_1\sim p(\theta_1|\theta_{-1},\alpha,\beta,y)$
\item $\vdots$
\item $\theta_n\sim p(\theta_n|\theta_{-n},\alpha,\beta,y)$
\item $\alpha\sim p(\alpha|\theta,\beta,y)$
\item $\beta\sim p(\beta|\theta,\alpha,y)$
\end{enumerate}
where $\theta=(\theta_1,\ldots,\theta_n)$ and $\theta_{-i}$ is $\theta$ without element $i$.

\vspace{0.2in} \pause

But notice that 
\[ p(\theta|\alpha,\beta,y) = \prod_{i=1}^n p(\theta_i|\alpha,\beta,y_i) \]
and thus the $\theta_i$ are conditionally independent. \pause Thus, we actually ran the following block Gibbs sampler:
\begin{enumerate}[<+->]
\item $\theta\sim p(\theta|\alpha,\beta,y)$
\item $\alpha\sim p(\alpha|\theta,\beta,y)$
\item $\beta\sim p(\beta|\theta,\alpha,y)$
\end{enumerate}
\end{frame}
 

 
 \section{Slice sampling}
\begin{frame}
\frametitle{Slice sampling}
Suppose the target distribution is $p(\theta|y)$ with scalar $\theta$. \pause Then,
\[ p(\theta|y) = \int_0^{p(\theta|y)} du \]
\pause Thus, $p(\theta|y)$ can be thought of as the marginal distribution of 
\[ (\theta,U) \sim \mbox{Unif}\{(\theta,u):0<u<p(\theta|y)\} \]
where $u$ is an \alert{auxiliary variable}.

\vspace{0.2in} \pause 

\alert{Slice sampling} performs the following Gibbs sampler: \pause 
\begin{enumerate}
\item $u^{(t)}|\theta^{(t-1)},y \sim \mbox{Unif}\{u:0<u<p(\theta^{(t-1)}|y)\}$ \pause and
\item $\theta^{(t)}|u^{(t)},y \sim \mbox{Unif}\{\theta: u^{(t)}<p(\theta|y) \}$.
\end{enumerate}
\end{frame}





\begin{frame}[fragile]
\frametitle{Slice sampler for exponential distribution}
Consider the target $\theta|y \sim Exp(1)$,\pause  then 
\[ \{\theta: u<p(\theta|y) \} = (0,-\log(u)). \]

\pause 

<<uniform_region, fig.width=10, fig.height=4>>=
u = 0.5; x = -log(u)
curve(dexp, 0, 1.5, ylab="u", xlab=expression(theta), main="Target disribution", ylim=c(0,1))
curve(dexp, 0, x, add=TRUE, lwd=3, col="red")
segments(0,u,x,u,col="gray")
segments(x,0,x,u,col="gray")
segments(0,0,x,0,lwd=3, col="red")
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Slice sampling in R}
<<slice_sampler, echo=TRUE>>=
# Slice sampler 
slice = function(n,init_theta,target,A) {
  u = theta = rep(NA,n)
  theta[1] = init_theta
  u[1] = runif(1,0,target(theta[1])) # This never actually gets used

  for (i in 2:n) {
    u[i] = runif(1,0,target(theta[i-1]))
    endpoints = A(u[i],theta[i-1]) # The second argument is used in the second example
    theta[i] = runif(1, endpoints[1],endpoints[2])
  }
  return(list(theta=theta,u=u))
}
# Exponential example
set.seed(6)
A = function(u,theta=NA) c(0,-log(u))
res = slice(10, 0.1, dexp, A)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{}
<<slice_sampling_plot, dependson='slice_sampler', fig.show='animate', warning=FALSE, echo=FALSE>>=
slice_sampling_animation = function(theta, u, ask=interactive()) {
  x = cbind(theta,u)
  for (i in 2:nrow(x)) {
    jj = (2:i)[-(i-1)] # vector from 2:(i-1) and NULL if i=2
    for (j in 1:6) {
      plot.new()
      
      # All previous plotting
      curve(dexp, 0, 3, xlab=expression(theta), ylab="u", main="Slice sampling an Exp(1) distribution")
      segments(x[jj-1,1], x[jj,2], x[jj-1,1], x[jj-1,2], col="gray")
      segments(x[jj-1,1], x[jj,2], x[jj,1], x[jj  ,2], col="gray")
      points(x[(1:(i-1)),1], x[(1:(i-1)),2], col="red", pch=19)
      
      # New plotting
      if (j>1 & j<4) abline(v=x[i-1,1], lty=2)
      if (j>2) arrows(x[i-1,1], x[i-1,2], x[i-1,1], x[i,2], length=0.1)
      if (j>3 & j<6) abline(h=x[i,2], lty=2)
      if (j>4) arrows(x[i-1,1], x[i,2], x[i,1], x[i,2], length=0.1)
      if (j>5) points(x[i,1], x[i,2], col="red", pch=19)
      
      if (ask) readline("hit <enter>:")
    }
  }
  
  # Final plot
  jj=2:nrow(x)
  curve(dexp, 0, 3, xlab=expression(theta), ylab="u", 
        main="Slice sampling an Exp(1) distribution")
  segments(x[jj-1,1], x[jj,2], x[jj-1,1], x[jj-1,2], col="gray")
  segments(x[jj-1,1], x[jj,2], x[jj,1], x[jj  ,2], col="gray")
  points(x[,1], x[,2], col="red", pch=19)
}
with(res, slice_sampling_animation(theta,u))
@
\end{frame}



\begin{frame}[fragile]
<<slice_sampling_histogram, dependson='slice_sampler'>>=
hist(slice(1e4, 0.1, dexp, A)$theta, freq=F, 100, main="Slice sampling approximation to Exp(1) distribution", xlab=expression(theta))
curve(dexp, add=TRUE, col='red')
@
\end{frame}



\section{Summary}
\begin{frame}
\frametitle{Summary}

\begin{itemize}[<+->]
\item Gibbs sampling breaks down a hard problem of sampling from a high dimensional distribution to a set of easier problems, i.e. sampling from low dimensional full conditional distributions.
\item If the low dimensional distributions have an unknown form, then alternative methods can be used, e.g. (adaptive) rejection sampling, Metropolis-Hastings, etc.
\item A Gibbs sampler can always be constructed by introducing an auxiliary variable that horizontally slices the target density. 
\end{itemize}

\end{frame}
 
 
 
\end{document}
