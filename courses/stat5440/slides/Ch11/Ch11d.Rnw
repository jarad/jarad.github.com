\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Markov chain Monte Carlo}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library("reshape2")
library("plyr")
library("ggplot2")
library("xtable")
library("coda")
library("mcmcse")
library("rstan")
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}

\section{Markov chain Monte Carlo}
\begin{frame}
\frametitle{Markov chain construction}

The techniques we have discussed thus far, e.g.
\begin{itemize}
\item Metropolis-Hastings
  \begin{itemize}
  \item independent Metropolis-Hastings
  \item random-walk Metropolis
  \item Hamiltonian Monte Carlo
  \end{itemize}
\item Gibbs sampling
  \begin{itemize}
  \item Slice sampling
  \end{itemize}
\end{itemize}
form a set of techniques referred to as \alert{Markov chain Monte Carlo} (MCMC). 

\vspace{0.2in} \pause

Today we look at some practical questions involving the use of MCMC:
\begin{itemize}
\item What initial values should I use?
\item How long do I need to run my chain?
\item What can I do with the samples I obtain?
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Markov chain Monte Carlo}

An MCMC algorithm with transition kernel $K(\theta^{(t-1)},\theta^{(t)})$ constructed to sample from $p(\theta|y)$ is the following: \pause
\begin{enumerate}[<+->]
\item Sample $\theta^{(0)}\sim \pi^{(0)}$.
\item For $t=1,\ldots,T$, perform the kernel $K(\theta^{(t-1)},\theta^{(t)})$ to obtain a sequence $\theta^{(1)},\ldots,\theta^{(T)}$. 
\end{enumerate}

\vspace{0.2in} \pause

The questions can then be rephrased as
\begin{itemize}
\item What should I use for $\pi^{(0)}$?
\item What should $T$ be?
\item What can I do with $\theta^{(1)},\ldots,\theta^{(T)}$?
\end{itemize}
\end{frame}


\section{Initial values}
\begin{frame}
\frametitle{Initial values}

For ergodic Markov chains with stationary distribution $p(\theta|y)$, theory states that 
\[ \theta^{(t)} \stackrel{d}{\to} \theta \mbox{ where } \theta\sim p(\theta|y) \]
for \alert{almost all $\theta^{(0)}$} (all with Harris recurrence). 

\vspace{0.2in} \pause

If $p(\theta^{(0)}|y)<<p(\theta_{MAP}|y)$, then this can take a long time. \pause For example, let 
\[ \theta^{(t)} = 0.99 \theta^{(t-1)} + \epsilon_t \qquad \epsilon_t \stackrel{iid}{\sim} N(0,1-.99^2) \]
which has stationary distribution \pause $p(\theta|y) \stackrel{d}{=}  N(0,1)$. \pause If 
\begin{itemize}
\item $\theta^{(0)}\sim p(\theta|y)$ then $\theta^{(t)} \approxdist p(\theta|y)$ for all $t$\pause, but if 
\item $\theta^{(0)}$ is very far from $p(\theta|y)$ then $\theta^{(t)} \approxdist p(\theta|y)$ only for $t$ very large. 
\end{itemize}
\end{frame}


\begin{frame}[fragile]
<<random_walk>>=
rwalk = function(n,theta0,rho=0.99) {
  theta = rep(theta0,n)
  v = sqrt(1-rho^2)
  for (i in 2:n) 
    theta[i] = rho*theta[i-1] + rnorm(1,0,v)
  return(theta)
}

n = 1000
good = rwalk(n, rnorm(1   )) # Draw from the stationary distribution
bad  = rwalk(n, rnorm(1,100)) # Draw from terrible initial distribution

par(mfrow=c(1,2))
plot(good, type="l", ylim=c(-3,3)); abline(h=c(-2,2), col="red")
plot(bad,  type="l"); abline(h=c(-2,2), col="red")
polygon(c(-100,-100,400,400),c(-100,200,200,-100), density=2)
text(400,90, "burn-in", pos=2, bg="white")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{How many iterations do I need for burn-in?}
Imagine two different chains
\pause 
<<no_mixing, dependson='random_walk', echo=FALSE, fig.width=10>>=
d = list()
d[[1]] = ddply(data.frame(rep=1:2, mu=c(-3,3)), .(rep), 
          function(x) data.frame(x=1:1000, y=x$mu+rwalk(1000,0,0.7)))
d[[1]]$text = 'Not mixing'

d[[2]] = ddply(data.frame(rep=1:2,beta=c(-1,1)), .(rep), function(x) {
  xx = 1:1000
  data.frame(x=xx, y=xx*x$beta/167+rwalk(1000,0,0.7)-x$beta*3)
})
d[[2]]$text = 'Not stationary'

d[[3]] = ddply(data.frame(rep=1:2,theta0=c(-3,3)), .(rep), function(x) {
  xx = 1:1000
  data.frame(x=xx, y=rwalk(1000,x$theta0,0.7))
})
d[[3]]$text = 'Mixing and stationary'
dd = rbind.fill(d)
dd$rep = factor(dd$rep)
dd$text = factor(dd$text, levels=c('Not mixing','Not stationary','Mixing and stationary'))
ggplot(dd, aes(x=x,y=y,color=rep)) +
  geom_line() + 
  theme(legend.position="none") + 
  facet_wrap(~text) + 
  theme_bw()
@
\end{frame}


\subsection{Potential scale reduction factor}
\begin{frame}
\frametitle{Gelman-Rubin potential scale reduction factor}

\begin{enumerate}[<+->]
\item Start multiple chains with initial values that are \alert<7->{well dispersed values relative to $p(\theta|y)$}.
\item For each scalar estimand $\psi$ of interest,
  \begin{itemize}
  \item Calculate the between $B$ and within $W$ chain variances
  \item Estimate the the marginal posterior variance of the estimand, i.e. $Var(\psi|y)$:
  \[ \widehat{Var}^+(\psi|y) =  \frac{t-1}{t}W + \frac{1}{t}B \]
  where $t$ is the number of iterations. 
  \item Calculate the potential scale reduction factor
  \[ \hat{R}_\psi = \sqrt{\frac{\widehat{Var}^+(\psi|y)}{W}} \]
  \end{itemize}
\item If the $\hat{R}_\psi$ are approximately 1, e.g. $<$1.1, then \alert{there is no evidence of non-convergence}.
\end{enumerate}
\end{frame}


\begin{frame}[fragile]
\frametitle{Example potential scale reduction factors}
<<calculate_psrf, dependson='no_mixing'>>=
d_ply(dd, .(text), function(x) {
  print(as.character(x$text[1]))
  print(gelman.diag(mcmc.list(dlply(x, .(rep), function(x) as.mcmc(x$y)))))
})
@
\end{frame}




\subsection{Methods}
\begin{frame}
\frametitle{Methods for finding good initial values}

From \url{http://users.stat.umn.edu/~geyer/mcmc/burn.html}:
\begin{quote}
Any point you don't mind having in a sample is a good starting point. 
\end{quote}

\vspace{0.2in} \pause

Methods for finding good initial values:

\begin{itemize}
\item burn-in: throw away the first X iterations
\item Start at the MLE, i.e. $\mbox{argmax}_\theta p(y|\theta)$
\item Start at the MAP (maximum aposterior), i.e. $\mbox{argmax}_\theta p(\theta|y)$
\end{itemize}
\end{frame}


\section{How many iterations?}
\begin{frame}[fragile]
\frametitle{How many iterations should I run (post `convergence')?}

Compute the effective sample size, i.e. how many independent samples would we need to get the equivalent precision of our estimates?

<<effective_sample_size, dependson='random_walk', echo=TRUE>>=
d = ddply(data.frame(rho=c(0,.9,.99)), .(rho), function(x) data.frame(x=rwalk(1000,0,x$rho)))
ddply(d, .(rho), summarize, 
      effective_size = round(coda::effectiveSize(x)))
@

BDA3 a total of 100-2000 effective samples. \pause But this really depends on what you want to estimate. \pause If you are interested in estimating probabilities of rare events, i.e. tail probabilities, you may need many more samples.

\end{frame}
 
 
\begin{frame}[fragile]
\frametitle{Autocorrelation function}
<<autocorrelations, dependson='effective_sample_size', fig.width=10>>=
opar = par(mfrow=c(1,3))
d_ply(d, .(rho), function(x) acf(x$x, 100, main=paste("rho=",x$rho[1])))
par(opar)
@
\end{frame}

\section{What can I do with the samples?}
\frame{\frametitle{Monte Carlo integration}
\small
  Consider approximating the integral via it's Markov chain Monte Carlo (MCMC) estimate, i.e.
  \[ E_{\theta|y}[h(\theta)|y] = \int_{\Theta} h(\theta) p(\theta|y) d\theta \quad \mbox{and} \quad
  \hat{h}_T = \frac{1}{T} \sum_{t=1}^{(t)} h\left(\theta^{(t)}\right). \]
	\pause where $\theta^{(t)}$ is the $t^{th}$ iteration from the MCMC.  \pause Under regularity conditions,
	\begin{itemize}
	\item SLLN: $\hat{h}_T$ converges almost surely to $E[h(\theta)|y]$. \pause
	\item CLT: \alert{under stronger regularity conditions}, \pause
	\[ \hat{h}_T \stackrel{d}{\to} N\left(E[h(\theta)|y], \sigma^2/T\right) \pause \]
	where
	\[ \sigma^2 = Var[h(\theta)|y]\left(1+ 2\sum_{k=1}^\infty \rho_k \right) \]
  where $\rho_k$ is the $k^{th}$ autocorrelation of the $h(\theta)$ values.
	\end{itemize}
}


\begin{frame}[fragile]
\frametitle{Sequential estimates}
<<running_average, dependson='effective_sample_size', fig.width=10, echo=TRUE>>=
opar = par(mfrow=c(1,3))
d_ply(d, .(rho), function(x)
  plot(cumsum(x$x)/1:length(x$x), type="l", ylim=c(-1,1),
       ylab="Posterior mean", xlab="Iteration (t)", main=paste("rho=", x$rho[1]))
)
par(opar)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Treat the MCMC samples as samples from the posterior}
Use {\tt mcmcse::mcse} to estimate the MCMC variance
<<summary_statistics, dependson='effective_sample_size', fig.width=10, echo=TRUE>>=
# Mean
ddply(d, .(rho), function(x) round(as.data.frame(mcmcse::mcse(x$x)),2))

# Quantiles
ddply(d, .(rho), function(x) round(as.data.frame(mcmcse::mcse.q(x$x, .025)),2))
ddply(d, .(rho), function(x) round(as.data.frame(mcmcse::mcse.q(x$x, .975)),2))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Treat the MCMC samples as samples from the posterior}
<<summary_plots, fig.width=10>>=
opar= par(mfrow=c(1,3))
d_ply(d, .(rho), function(x) {
  hist(x$x, seq(-4,4,by=0.2)+.1, xlim=c(-3,3), freq=F, main=paste("rho=", x$rho[1]))
  curve(dnorm, col="red", add=TRUE, lwd=2)
})
par(opar)
@
\end{frame}


\section{One really long chain}
\begin{frame}
\frametitle{A wasteful approach}

The Gelman approach in practice is the following
\begin{enumerate}[<+->]
\item Run an initial chain or, in some other way, approximate the posterior.
\item (Randomly) choose initial values for multiple chains well dispersed relative to this approximation to the posterior.
\item Run the chain until all estimands of interest have potential scale reduction factors less than 1.1.
\item Continuing running until you have a total of around 4,000 effective draws.
\item Discard the first half of all the chains.
\end{enumerate}

\vspace{0.2in} \pause

Assuming this approach correctly diagnosis convergence or lack thereof, it seems computationally wasteful since
\begin{itemize}
\item You had to run an initial chain, but then threw it away.
\item You threw away half of your later iterations.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{One really long chain}

From \url{http://users.stat.umn.edu/~geyer/mcmc/one.html}
\begin{quote}
If you can't get a good answer with one long run, then you can't get a good answer with many short runs either.
\end{quote}

\vspace{0.2in} \pause

\begin{enumerate}
\item Start a chain at a reasonable starting value.
\item Run it for many iterations (and keep running it).
\end{enumerate}

\vspace{0.2in} \pause

If you really want a convergence diagnostic, you can try Geweke's which tests for equality of means in the first and last parts of the chain.
\end{frame}


\begin{frame}[fragile]
\frametitle{Geweke diagnostic}
<<geweke, fig.width=10, echo=TRUE>>=
# Z-score for test of equality of means
par(mfrow=c(1,3))
d_ply(d, .(rho), function(x) geweke.plot(mcmc(x$x), auto=F, main=paste("rho=", x$rho[1])))
par(opar)
@
\end{frame}


\section{Thinning}
\begin{frame}
\frametitle{Thinning}

You will hear of people \alert{thinning} their Markov chain by only recording every $n^{th}$ observation.

\vspace{0.2in} \pause

This has the benefit of reducing the autocorrelation in the retained samples.

\vspace{0.2in} \pause

But should only be used if memory or hard drive space is a limiting factor.
\end{frame}


\begin{frame}[fragile]
\frametitle{Thinning}
<<thinning, echo=TRUE>>=
sq = seq(10,1000,by=10)
ddply(d, .(rho), summarize, full=effectiveSize(x), thinned=effectiveSize(x[sq]))

# Calculate standard error
ddply(d, .(rho), function(x) {
  rbind(data.frame(s="full", mcmcse::mcse(x$x)),data.frame(s="thinned", mcmcse::mcse(x$x[sq])))
})
@
\end{frame}


\begin{frame}
\frametitle{Alternative use for burn-in}

For MCMC algorithms that have tuning parameters, use burn-in (warm-up) to tune tuning parameters.

\vspace{0.2in} \pause

Suppose the target distribution is $N(0,1)$ and we are performing a random-walk Metropolis with a normal proposal. \pause The variance of this proposal is a tuning parameter and we can tune it during burn-in:
\begin{itemize}
\item if a proposal is accepted, then likely our variance is too small and therefore we should increase it
\item if a proposal is rejected, then likely our variance is too big and therefore we should decrease it
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Alternative use for burn-in}
<<tune, echo=TRUE>>=
rw = function(n, theta0, tune=1, autotune=TRUE) {
  theta = rep(theta0, n)
  for (i in 2:n) {
    theta_prop = rnorm(1, theta[i-1], tune)
    logr = dnorm(theta_prop, log=TRUE) - dnorm(theta[i-1], log=TRUE)

    # This tuning tunes to an acceptance rate of 50%
    if (log(runif(1))<logr) {
      theta[i] = theta_prop
      if (autotune) tune = tune*1.1
    } else {
      theta[i] = theta[i-1]
      if (autotune) tune = tune/1.1
    }
  }
  return(list(theta=theta,tune=tune))
}

# Tune during burn-in
burnin = rw(1000, 0)
burnin$tune

# Turn off tuning after burn-in for theory to hold
inference = rw(10000, burnin$theta[1000], burnin$tune, autotune=FALSE)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Alternative use for burn-in}
<<tune_plot, fig.width=8, echo=TRUE>>=
hist(inference$theta, 100, prob=T)
curve(dnorm, col="red", add=TRUE, lwd=2)
@
\end{frame}

\section{Summary}
\begin{frame}
\frametitle{Summary}

Since computing time/power is not very limited these days, my suggestion is

\begin{enumerate}
\item Run one long chain and continue running it \pause
\item Run multiple chains according to suggestions in BDA
  \begin{enumerate}
  \item Start multiple chains with initial values relative to the posterior learned by the long chain
  \item Monitor the potential scale reduction factor until $<$ 1.1 for all quantities of interest
  \item Monitor traceplots and cumulative mean plots
  \item Discard burn-in (first half is probably overkill)
  \item Run until effective sample size is around 2000 \pause
  \end{enumerate}
\item Use all samples for posterior inference
\end{enumerate}

If things are not going well,
\begin{enumerate}
\item Check for identifiability of the parameters in your model.
\item Construct a better sampler.
\end{enumerate}

\end{frame}


\section{Stan output}
\begin{frame}[fragile]
\frametitle{A simple model}

Let
\[ Y_i\stackrel{ind}{\sim} N(\mu,\sigma^2) \qquad \mbox{and} \qquad
p(\mu,\sigma) \propto Ca^+(\sigma;0,1)
\]

\vspace{0.2in} \pause

In RStan,
<<echo=TRUE>>=
model = "
data {
  int<lower=1> n;
  real y[n];
}
parameters{
  real mu;
  real<lower=0> sigma;
}
model {
  sigma ~ cauchy(0,1);
  y ~ normal(mu,sigma);
}
"
@
\end{frame}

\begin{frame}[fragile]
\frametitle{RStan}

<<rstan, cache=TRUE, results='hide', echo=TRUE>>=
y = rnorm(10)
m = stan_model(model_code = model)
r = sampling(m, list(n=length(y), y=y), iter = 10000)
@

<<rstan_summary, dependson='rstan', echo=TRUE>>=
r
laply(extract(r, c("mu","sigma")), function(x) length(unique(x))/length(x)) # Acceptance rate
@

\end{frame}


\begin{frame}[fragile]
\frametitle{RStan plot}
<<dependson='rstan'>>=
traceplot(r, inc_warmup = TRUE)
@
\end{frame}





\begin{frame}[fragile]
\frametitle{Hierarchical binomial model}

\small

Recall the game-wise Andre Dawkins 3-point percentage data set with a
hierarchical binomial model:
\[ \begin{array}{rl}
Y_i      &\ind Bin(n_i,\theta_i) \\
\theta_i &\ind Be(\alpha,\beta) \\
p(\alpha,\beta) &\propto (\alpha+\beta)^{-5/2}
\end{array} \]

\pause

In RStan,
<<binomial_model, echo=TRUE>>=
hierarchical_binomial_model = "
data {
  int<lower=1> N; int<lower=0> n[N]; int<lower=0> y[N];
}
parameters {
  real<lower=0,upper=1> theta[N]; real<lower=0> alpha; real<lower=0> beta;
}
transformed parameters {
  real<lower=0,upper=1> mu; real<lower=0> eta;
  eta = alpha+beta; mu = alpha/eta;
}
model {
  target += -5*log(alpha+beta)/2;
  theta ~ beta(alpha,beta);
  y ~ binomial(n,theta);
}
"
@
\end{frame}

<<dawkins_data, echo=FALSE>>=
dawkins = structure(list(date = structure(c(16L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 20L, 17L, 18L, 19L, 21L, 7L, 8L, 1L, 2L, 3L, 4L, 5L,
6L, 22L, 23L, 24L), .Label = c("1/11/14", "1/13/14", "1/18/14",
"1/22/14", "1/25/14", "1/27/14", "1/4/14", "1/7/14", "11/12/13",
"11/15/13", "11/18/13", "11/19/13", "11/24/13", "11/27/13", "11/29/13",
"11/8/13", "12/16/13", "12/19/13", "12/28/13", "12/3/13", "12/31/13",
"2/1/14", "2/4/14", "2/8/14"), class = "factor"), opponent = structure(c(5L,
13L, 9L, 21L, 6L, 22L, 1L, 2L, 15L, 11L, 20L, 7L, 8L, 17L, 12L,
4L, 23L, 16L, 14L, 10L, 18L, 19L, 24L, 3L), .Label = c("alabama",
"arizona", "boston college", "clemson", "davidson", "east carolina",
"eastern michigan", "elon", "florida atlantic", "florida state",
"gardner-webb", "georgia tech", "kansas", "miami", "michigan",
"nc state", "notre dame", "pitt", "syracuse", "ucla", "unc asheville",
"vermont", "virginia", "wake forest"), class = "factor"), made = c(0L,
0L, 5L, 3L, 0L, 3L, 0L, 1L, 2L, 4L, 1L, 6L, 5L, 1L, 1L, 0L, 1L,
3L, 2L, 3L, 6L, 4L, 4L, 0L), attempts = c(0L, 0L, 8L, 6L, 1L,
9L, 2L, 1L, 2L, 8L, 5L, 10L, 7L, 4L, 5L, 4L, 1L, 7L, 6L, 6L,
7L, 9L, 7L, 1L)), .Names = c("date", "opponent", "made", "attempts"
), class = "data.frame", row.names = c(NA, -24L))
@

\begin{frame}[fragile]
\frametitle{Initial run}

<<rstan2, dependson="binomial_model", cache=TRUE, results='hide', echo=TRUE>>=
m = stan_model(model_code = hierarchical_binomial_model)

r = sampling(m,
             data = list(N = nrow(dawkins),
                            n = dawkins$attempts,
                            y = dawkins$made),
             pars = c("alpha","beta","mu","eta"),
             chains = 4,
             iter = 10000)
@
\end{frame}
% 
% 
% 
\begin{frame}[fragile]
\frametitle{RStan}

<<rstan2_summary, dependson='rstan2', echo=TRUE>>=
r
laply(rstan::extract(r, c("mu","eta")), function(x) length(unique(x))/length(x)) # Acceptance rate
@

\end{frame}


\begin{frame}[fragile]
\frametitle{RStan plot}
<<dependson='rstan2'>>=
traceplot(r, inc_warmup = TRUE)
@
\end{frame}


\end{document}
