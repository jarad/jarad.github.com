\documentclass[handout,aspectratio=169]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Bayesian hypothesis testing (cont.)}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=5, fig.height=3, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library("tidyverse"); theme_set(theme_bw())
library(reshape2)
library(plyr)
library(xtable)
@

<<set_seed>>=
set.seed(1)
@



\frame{\maketitle}

\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Review of formal Bayesian hypothesis testing
\item Likelihood ratio tests
\item Jeffrey-Lindley paradox
\item $p$-value interpretation
\end{itemize}

\end{frame}





\begin{frame}
\frametitle{Bayes tests = evaluate predictive models}
\small
  Consider a standard hypothesis test scenario:
  \[ H_0: \theta=\theta_0, \qquad H_1:\theta \ne \theta_0 \]
  \pause
  A Bayesian measure of the support for the null hypothesis is the Bayes Factor:
  \[ BF(H_0:H_1)  = \frac{p(y|H_0)}{p(y|H_1)} \pause = \frac{p(y|\theta_0)}{\int p(y|\theta)p(\theta|H_1) d\theta}   \]
  \pause 
  where $p(\theta|H_1)$ is the prior distribution for $\theta$ under the alternative hypothesis. \pause Thus the Bayes Factor measures the \alert{predictive ability} of the two Bayesian models. \pause Both models say $p(y|\theta)$ are the data model if we know $\theta$, but 
  \begin{enumerate}
  \item Model 0 says $\theta=\theta_0$ \pause and thus $p(y|\theta_0)$ is our predictive distribution for $y$ under model $H_0$ \pause while
  \item Model 1 says $p(\theta|H_1)$ is our uncertainty about $\theta$ \pause and thus 
  \[ p(y|H_1) = \int p(y|\theta)p(\theta|H_1) d\theta \]
  is our predictive distribution for $y$ under model $H_1$. 
  \end{enumerate}
\end{frame}


\begin{frame}[fragile]
\frametitle{Normal example}

Consider $y\sim N(\theta,1)$ and 
\[ H_0: \theta=0, \qquad H_1: \theta\ne 0 \]
\pause and we assume $\theta|H_1\sim N(0,C)$. \pause Thus,
  \[ BF(H_0:H_1)  = \frac{p(y|H_0)}{p(y|H_1)} \pause = \frac{p(y|\theta_0)}{\int p(y|\theta)p(\theta|H_1) d\theta} = \frac{N(y;0,1)}{N(y;0,1+C)}. \]
  \pause 
  Now, as $C\to\infty$, our predictions about $y$ become less sharp. 
\end{frame}
  
  
\begin{frame}
<<>>=
d <- data.frame(x = seq(-4, 4, length.out = 1001)) |>
  mutate(
    H0 = dnorm(x),
    H1 = dnorm(x, sd = 2)
  ) |>
  pivot_longer(
    H0:H1,
    names_to = "Hypothesis",
    values_to = "density"
  )

ggplot(d, aes(x        = x, 
              y        = density,
              color    = Hypothesis,
              linetype = Hypothesis)) +
  geom_line()
@
\end{frame}



\section{Likelihood Ratio Tests}
\frame{\frametitle{Likelihood Ratio Tests}
  Consider a likelihood $L(\theta)= p(y|\theta)$, then the likelihood ratio test statistic for testing $H_0:\theta\in \Theta_0$ and $H_1: \theta\in \Theta_0^c$ \pause with $\Theta = \Theta_0\cup \Theta_0^c$ is 
  \[ \lambda(y) = \frac{\mbox{sup}_{\Theta_0} L(\theta)}{\mbox{sup} _{\Theta} L(\theta)} \pause = \frac{L(\hat{\theta}_{0,MLE})}{L(\hat{\theta}_{MLE})} \]
  \pause where $\hat{\theta}_{MLE}$ and $\hat{\theta}_{0,MLE}$ are the (restricted) MLEs. \pause The likelihood ratio test (LRT) is any test that has a rejection region of the form $\{ y: \lambda(y)\le c\}$. (Casella \& Berger Def 8.2.1)
  
  \vspace{0.2in} \pause 
  
  Under certain conditions (see Casella \& Berger 10.3.3), as $n\to \infty$ 
  \[ -2\log \lambda(y) \to \chi^2_{\nu} \]
  \pause where $\nu$ us the difference between the number of free parameters specified by $\theta\in\theta_0$ and the number of free parameters specified by $\theta\in \Theta$. 
}



\subsection{Binomial example}
\frame{\frametitle{Binomial example}
  Consider a coin flipping experiment so that $Y_i \stackrel{iid}{\sim} Ber(\theta)$ and the null hypothesis $H_0:\theta=0.5$ versus the alternative $H_1:\theta\ne 0.5$. \pause Then 
  \[  
  \lambda(y) = \frac{\mbox{sup} _{\Theta_0} L(\theta)}{\mbox{sup} _{\Theta} L(\theta)}= \frac{0.5^n}{\hat{\theta}_{MLE}^{n\overline{y}}(1-\hat{\theta}_{MLE})^{n-n\overline{y}}}  = \frac{0.5^n}{\overline{y}^{n\overline{y}}(1-\overline{y})^{n-n\overline{y}}} 
   \]
   \pause and 
   $-2\log \lambda(y) \to \chi^2_1$ as $n\to\infty$ so 
   \[ \mbox{$p$-value} \approx P(\chi^2_1>-2\log \lambda(y)). \]
   \pause If $p$-value$ < a$, then we reject $H_0$ at significance level $a$. \pause Typically $a=0.05$. 
}

\begin{frame}[fragile]
\frametitle{Binomial example}
$Y\sim Bin(n,\theta)$ and, for the Bayesian analysis, $\theta|H_1 \sim Be(1,1)$ and $p(H_0)=p(H_1)=0.5$:
<<lrt_binomial,fig.width=8, warning=FALSE>>=
d = expand.grid(n=seq(10,30,by=10), ybar=seq(0,1,by=0.01))

# Bayes Factors
bf = function(n,ybar,a=1,b=1) exp(n*log(0.5)+lbeta(a,b)-lbeta(a+n*ybar,b+n-n*ybar))
d2 = ddply(d, .(n,ybar), summarize, bf=bf(n,ybar))
post_prob = function(bf, prior_odds=0.5) 1/(1+1/bf*prior_odds)
d2$statistic = post_prob(d2$bf)
d2$method = "Posterior probability"

# Pvalues
lrt = function(n,ybar) exp(n*log(0.5)-n*ybar*log(ybar)-n*(1-ybar)*log(1-ybar))
pvalue = function(lrt,df=1) 1-pchisq(-2*log(lrt),df)
d3 = ddply(d,.(n,ybar), summarize, lrt = lrt(n,ybar))
d3$statistic = pvalue(d3$lrt,1)
d3$method = "Likelihood ratio test pvalue"

# Both plots
ggplot(rbind(d2[,-3],d3[,-3]), aes(x=ybar, y=statistic, col=factor(n))) + 
  geom_line() + 
  facet_grid(~method) +
  theme_bw()
@
\end{frame}








\section{Jeffrey-Lindley paradox}
\begin{frame}
\frametitle{Do $p$-values and posterior probabilities agree?}
Suppose $n=10,000$ and $y=4,900$, \pause then the $p$-value is 
\[ \mbox{$p$-value} \approx P(\chi^2_1>-2 \log(0.135)) = 0.045 \]
\pause so we would reject $H_0$ at the 0.05 level. 

\vspace{0.2in} \pause

The posterior probability of $H_0$ is 
\[ p(H_0|y) \approx \frac{1}{1+1/10.8} = 0.96, \]
\pause so the probability of $H_0$ being true is 96\%. 

\vspace{0.2in} \pause 

It appears the Bayesian and LRT $p$-value completely disagree!
\end{frame}

\begin{frame}[fragile]
\frametitle{Binomial $\overline{y}=0.49$ with $n\to\infty$} 
<<paradox,fig.width=8>>=
paradox = expand.grid(n=10^(seq(0,5,by=0.1)), ybar=0.49)
paradox = ddply(paradox, .(n,ybar), summarize, pvalue=pvalue(lrt(n,ybar)), post_prob=post_prob(bf(n,ybar)))
m = melt(paradox, id=c("n","ybar"))
p = ggplot(m, aes(log10(n),value,col=variable)) + 
  geom_line() +
  theme_bw()
print(p)
@
\end{frame}

\subsection{Jeffrey-Lindley Paradox}
\frame{\frametitle{Jeffrey-Lindley Paradox}
  \begin{definition}
  The \alert{Jeffrey-Lindley Paradox} concerns a situation when comparing two hypotheses $H_0$ and $H_1$ given data $y$ \pause and find
  \begin{itemize}[<+->]\small
  \item a frequentist test result is significant leading to rejection of $H_0$, but
  \item our posterior belief in $H_0$ being true is high. 
  \end{itemize}
  \end{definition}
  
  \vspace{0.2in} \pause
  
  This can happen when 
  \begin{itemize}[<+->]\small
  \item the effect size is small, 
  \item $n$ is large, 
  \item $H_0$ is relatively precise, 
  \item $H_1$ is relatively diffuse, and
  \item the prior model odds is $\approx 1$. 
  \end{itemize}
}

\frame{\frametitle{Comparison}
  The test statistic with point null hypotheses:
  \[ \begin{array}{rl}
  \lambda(y) &= \frac{ p\left(y|\theta_0\right)}{p\left(y|\hat{\theta}_{MLE}\right)} \\ \\
  BF(H_0:H_1) &= \frac{ p\left(y|\theta_0\right)}{\int p(y|\theta)p(\theta|H_1) d\theta} \uncover<6->{= \frac{p(y|H_0)}{p(y|H_1)}}
  \end{array} \]
  \pause 
  
  A few comments:
  \begin{itemize}[<+->]\small
  \item The LRT chooses the best possible alternative value. 
  \item The Bayesian test penalizes for vagueness in the prior.
  \item The LRT can be interpreted as a Bayesian point mass prior exactly at the MLE. 
  \item Generally, $p$-values provide a measure of lack-of-fit of the data to the null model. 
  \item Bayesian tests compare predictive performance of two Bayesian models (model+prior). 
  \end{itemize}
}


\begin{frame}
\frametitle{Normal mean testing}

Let $y\sim N(\theta,1)$ and we are testing
\[ H_0:\theta=0 \qquad \mbox{vs} \qquad H_1:\theta\ne 0 \]
\pause
We can compute a two-sided $p$-value via
\[ \mbox{$p$-value} = 2\mathrm{\Phi}(-|y|) \]
where $\mathrm{\Phi}(\cdot)$ is the cumulative distribution function for a standard normal. 

\vspace{0.2in} \pause

Typically, we set our Type I error rate at level $a$, i.e. 
\[ P(\mbox{reject } H_0|H_0 \mbox{ true}) = a. \]
\pause
But, if we reject $H_0$, i.e. the $p$-value $ < a$, we should be interested in 
\[ P(H_0 \mbox{ true}|\mbox{reject } H_0) \pause = 1-\mbox{FDR} \]
where FDR is the False Discovery Rate.
\end{frame}



\section{$p$-value interpretation}
\subsection{App}
\begin{frame}[fragile]
\frametitle{$p$-value interpretation}

Let $y\sim N(\theta,1)$ and we are testing
\[ H_0:\theta=0 \qquad \mbox{vs} \qquad H_1:\theta\ne 0 \]

For the following activity, you need to tell me 
\begin{enumerate}
\item the observed $p$-value, 
\item the relative frequencies of null and alternative hypotheses, and
\item the distribution for $\theta$ under the alternative.
\end{enumerate}

\vspace{0.2in} \pause

Then this $p$-value app below will calculate (via simulation) the probability the null hypothesis is true.  

<<echo=TRUE, eval=FALSE>>=
shiny::runGitHub('jarad/pvalue')
@

\end{frame}


\subsection{App approach}
\begin{frame}
\frametitle{$p$-value app approach}

The idea is that a scientist performs a series of experiments. \pause For each experiment, 
\begin{itemize}[<+->]
\item whether $H_0$ or $H_1$ is true is randomly determined,
\item $\theta$ is sampled according to which hypothesis is true, and
\item the $p$-value is calculated. 
\end{itemize}

\vspace{0.2in} \pause

This process is repeated until a $p$-value of the desired value is achieved, e.g. $p$-value=0.05, \pause and the true hypothesis is recorded. \pause Thus, 
\[ P(H_0 \mbox{ true }|\mbox{ $p$-value }=0.05) \approx \frac{1}{K} \sum_{k=1}^K \mathrm{I}(H_0 \mbox{ true }|\mbox{ $p$-value }\approx 0.05).  \]

\vspace{0.2in} \pause

Thus, there is nothing Bayesian happening here except that the probability being calculated has the unknown quantity on the left and the known quantity on the right.

\end{frame}


\subsection{Prosecutor's Fallacy}
\begin{frame}
\frametitle{Prosecutor's Fallacy}

It is common for those using statistics to equate the following 
\[ \mbox{$p$-value} \stackrel{?}{\approx} P(\mbox{data}|H_0\mbox{ true}) \pause \ne P(H_0\mbox{ true}|\mbox{data}). \]
\pause 
but we can use Bayes rule to show us that these probabilities cannot be equated
\[ p(H_0|y) = \frac{p(y|H_0)p(H_0)}{p(y)} \pause = \frac{p(y|H_0)p(H_0)}{p(y|H_0)p(H_0)+p(y|H_1)p(H_1)} \]
\pause
This situation is common enough that it is called \href{http://en.wikipedia.org/wiki/Prosecutor's_fallacy}{The Prosecutor's Fallacy}.

\end{frame}


\subsection{ASA Statement on $p$-values}
\begin{frame}
\frametitle{ASA Statement on $p$-values}

\footnotesize

\url{https://amstat.tandfonline.com/doi/abs/10.1080/00031305.2016.1154108}

\vspace{0.1in} 

Principles:
\begin{enumerate}[<+->]
\item $P$-values can indicate how incompatible the data are with a specified
statistical model[, the model associated with the null hypothesis].
\item $P$-values do not measure the probability the studied hypothesis is true,
or the probability that the data were produced by random chance alone.
\item Scientific conclusions and business or policy decisions should not be 
based solely on whether a $p$-value passes a specific threshold.
\item Proper inference requires full reporting and transparency.
\item A $p$-value, or statistical significance, does not measure the size of an
effect or the importance of the result.
\item By itself, a $p$-value does not provide a good measure of evidence 
regarding a model or hypothesis.
\end{enumerate}
\end{frame}

\end{document}
