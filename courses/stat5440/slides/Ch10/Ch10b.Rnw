\documentclass[handout,aspectratio=169]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Introduction to Bayesian computation (cont.)}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=5, fig.height=3, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("tidyverse"); theme_set(theme_bw())
library("scales")
library("weights")
library("ars")
library("mcmcse")
@

<<set_seed>>=
set.seed(1)
@

\frame{\maketitle}


\begin{frame}
\frametitle{Outline}

Bayesian computation
\begin{itemize}
\item Adaptive rejection sampling
\item Importance sampling
\end{itemize}

\end{frame}


\section{Adaptive rejection sampling}
\begin{frame}[fragile]
\frametitle{Adaptive rejection sampling}

\begin{definition}
A function is concave if 
\[ f((1-t)x+t\, y) \ge (1-t) f(x) + t\, f (y) \]
for any $0\le t\le 1$.
\end{definition}

\vspace{0.2in} \pause

<<fig.width=12>>=
opar = par(mar=c(2,2,0,0)+.1)
curve(dnorm(x, log=T), -2.1, 1.1, axes=F, frame=T, lwd=2,
      xlab='', ylab='')
axis(1, c(-2,1), labels=c('x','y'))
axis(2, c(dnorm(c(-2,1), log=T)), labels=c('f(x)','f(y)'))
segments(-2,dnorm(-2, log=T), 1, dnorm(1, log=T))
@

\end{frame}


\begin{frame}
\frametitle{Log-concavity}

\begin{definition}
A function $f(x)$ is log-concave if $\log\, f(x)$ is concave. 
\end{definition}
\pause
\begin{lemma}
A function is log-concave if and only if $(\log\, f(x))'' \le 0 \,\forall\, x$.
\end{lemma}

\vspace{0.2in}\pause

For example, $X\sim N(0,1)$ has log-concave density since 
\[ \frac{d^2}{dx^2} \log\, e^{-x^2/2} = \frac{d^2}{dx^2} \frac{-x^2}{2} = \frac{d}{dx} -x = -1. \]
\end{frame}


\begin{frame}
\frametitle{Adaptive rejection sampling}

Adaptive rejection sampling can be used for distributions with log-concave densities. \pause It builds a piecewise linear envelope to the log density \pause by evaluating the log function and its derivative at a set of locations and constructing tangent lines, \pause e.g. 

\vspace{0.2in}

<<fig.width=10>>=
opar = par(mar=c(0,0,4,0)+.1, mfrow=c(1,2))
curve(dnorm(x, log=T), -4, 4, lwd=2, axes=F, frame=T, ylim=c(-10,0),
      main="log density")
deriv = function(x) -x

points = c(-2,.1,1.5)
xx = seq(-4, 4, by=.1)
for (p in points) {
  yy = deriv(p)*xx + dnorm(p,log=T)-deriv(p)*p
  lines(xx, yy, lty=2)
  #abline(dnorm(p,log=T)-deriv(p)*p, deriv(p), lty=2)
}

curve(dnorm(x), -4, 4, lwd=2, axes=F, frame=T, main="density", ylim=c(0,.5))
for (p in points) {
  yy = deriv(p)*xx + dnorm(p,log=T)-deriv(p)*p
  lines(xx, exp(yy), lty=2)
  #abline(dnorm(p,log=T)-deriv(p)*p, deriv(p), lty=2)
}
par(opar)
@
\end{frame}


\begin{frame}
\frametitle{Adaptive rejection sampling}

Pseudo-algorithm for adaptive rejection sampling:
\begin{enumerate}[<+->]
\item Choose starting locations $\theta$, call the set $\Theta$
\item Construct piece-wise linear envelope $\log g(\theta)$ to the log-density
  \begin{enumerate}
  \item Calculate $\log q(\theta|y)$ and $(\log q(\theta|y))'$.
  \item Find line intersections
  \end{enumerate}
\item Sample a proposed value $\theta^*$ from the envelope $g(\theta)$
  \begin{enumerate}
  \item Sample an interval
  \item Sample a truncated (and possibly negative of an) exponential r.v.
  \end{enumerate}
\item Perform rejection sampling
  \begin{enumerate}
  \item Sample $u \sim Unif(0,1)$
  \item Accept if $u\le q(\theta^*|y)/g(\theta^*)$.
  \end{enumerate}
\item If rejected, add $\theta^*$ to $\Theta$ and return to 2. 
\end{enumerate}

\end{frame}


\begin{frame}
\frametitle{Updating the envelope}

As values are proposed and rejected, the envelope gets updated:

\vspace{0.2in}

<<fig.width=10>>=
opar = par(mar=rep(0,4)+.1, mfrow=c(2,2))
deriv = function(x) -x

points = c(-2,1.5,.1, -3, -.6, 3.9)

for (i in 3:6) {
  curve(dnorm(x, log=T), -4, 4, lwd=2, axes=F, frame=T, ylim=c(-10,0))
  for (p in points[1:(i-1)]) abline(dnorm(p,log=T)-deriv(p)*p, deriv(p), lty=2)
  p = points[i]
  abline(dnorm(p,log=T)-deriv(p)*p, deriv(p), lty=2, col='red')
  abline(v=p, col='red')
}
par(opar)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Adaptive rejection sampling in R}

<<echo=TRUE, fig.width=8>>=
library(ars)
x = ars(n=1000, function(x) -x^2/2, function(x) -x) 
hist(x, prob=T, 100)
curve(dnorm, type='l', add=T)
@
\end{frame}



\begin{frame}
\frametitle{Adaptive rejection sampling summary}

\begin{itemize}
\item Can be used with log-concave densities
\item Makes rejection sampling efficient by updating the envelope
\end{itemize}

\vspace{0.2in} \pause

There is a vast literature on adaptive rejection sampling. \pause To improve upon the basic idea presented here you can 
\begin{itemize}
\item include a lower bound 
\item avoid calculating derivatives
\item incorporate a Metropolis step to deal with non-log-concave densities
\end{itemize}

\end{frame}



\subsection{Importance sampling}

\frame{\frametitle{Importance sampling}
  Notice that 
  \[ E[h(\theta)|y] = \int h(\theta) p(\theta|y) d\theta \pause = \int h(\theta) \frac{p(\theta|y)}{g(\theta)} g(\theta) d\theta \]
  \pause where $g(\theta)$ is a proposal distribution\pause, so that we approximate the expectation via 
  \[ E[h(\theta)|y] \approx \frac{1}{S} \sum_{s=1}^S w\left(\theta^{(s)}\right) h\left(\theta^{(s)}\right) \]
  \pause where $\theta^{(s)} \stackrel{iid}{\sim} g(\theta)$ \pause and 
  \[ w\left(\theta^{(s)}\right) = \frac{p\left(\left.\theta^{(s)}\right|y\right)}{g(\theta^{(s)})} \]
  \pause is known as the importance weight. 
}

\frame{\frametitle{Importance sampling}
  If the target distribution is known only up to a proportionality constant, then 
  \[ E[h(\theta)|y] = \frac{\int h(\theta) q(\theta|y) d\theta}{\int q(\theta|y) d\theta} \pause = \frac{\int h(\theta) \frac{q(\theta|y)}{g(\theta)} g(\theta) d\theta}{\int \frac{q(\theta|y)}{g(\theta)} g(\theta) d\theta} \]
  \pause where $g(\theta)$ is a proposal distribution\pause, so that we approximate the expectation via 
  \[ E[h(\theta)|y] \approx \frac{\frac{1}{S} \sum_{s=1}^S w\left(\theta^{(s)}\right) h\left(\theta^{(s)}\right)}{\frac{1}{S} \sum_{s=1}^S w\left(\theta^{(s)}\right)} = \sum_{s=1}^S \tilde{w}\left(\theta^{(s)}\right) h\left(\theta^{(s)}\right) \]
  \pause where $\theta^{(s)} \stackrel{iid}{\sim} g(\theta)$ and 
\[
\tilde{w}\left(\theta^{(s)}\right) = 
\frac{w\left(\theta^{(s)}\right)}{\sum_{j=1}^S w\left(\theta^{(j)}\right)}
\]
\pause 
is the \alert{normalized} importance weight. 
}

\begin{frame}
\frametitle{Example: Normal-Cauchy model}
  If $Y \sim N(\theta,1)$ and $\theta\sim Ca(0,1)$, then 
  \[ p(\theta|y) \propto e^{-(y-\theta)^2/2} \frac{1}{(1+\theta^2)} \]
  for all $\theta$. 
  
  \vspace{0.2in} \pause 
  
  If we choose a $N(y,1)$ proposal, we have
  \[ g(\theta)=\frac{1}{\sqrt{2\pi}}  e^{-(\theta-y)^2/2} \]
  \pause with 
  \[ w(\theta) = \frac{q(\theta|y)}{g(\theta)} = \frac{\sqrt{2\pi}}{(1+\theta^2)} \]

\end{frame}

\begin{frame}[fragile]
\frametitle{Normalized importance weights}
<<importance_sampling>>=
# Taken from Ch10a
q <- function(theta,y,log=FALSE) {
  out <- -(y-theta)^2/2-log(1+theta^2)
  if (log) return(out)
  return(exp(out))
}
g <- function(theta,y,log=FALSE) dnorm(theta,y,log=log)

py <- function(y,log=FALSE) {
  int <- integrate(function(x) q(x,y,log=FALSE),-Inf,Inf)
  if (int$message!="OK") {
    warning(paste("Could not compute marginal likelihood for y=", y, "\n"))
    return(NA)
  }
  int$value
}

y <- 1
n <- 1000
d <- data.frame(
  theta  = rnorm(n, y)
) |>
  mutate(
    weight = exp(q(theta, y, log=TRUE) - 
                 g(theta, y, log=TRUE)),
    weight = weight / sum(weight)
  )

ggplot(d,
       aes(
         x = theta,
         y = weight
       )) +
  geom_point() + 
  labs(y = "Normalized importance weight", x = expression(theta))
@
\end{frame}

\begin{frame}[fragile]
<<importance_sampling_mean, message=FALSE, fig.width=8, echo=TRUE>>=
library(weights)
theta  <- d$theta; weight <- d$weight
sum(weight*theta/sum(weight)) # Estimate mean
wtd.hist(theta, 100, prob=TRUE, weight=weight)
curve(q(x,y)/py(y), add=TRUE, col="red", lwd=2)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Resampling}
If an unweighted sample is desired, 
sample $\theta^{(s)}$ with replacement with probability equal to the normalized weights, $\tilde{w}\left(\theta^{(s)}\right)$.

<<resampling, warning=FALSE, echo=TRUE, fig.width=8>>=
# resampling
new_theta <- sample(theta, replace=TRUE, prob = weight) # internally normalized
hist(new_theta, 100, prob = TRUE, main = "Unweighted histogram of resampled draws"); curve(q(x,y)/py(y), add = TRUE, col="red", lwd=2)
@



\end{frame}


\frame{\frametitle{Heavy-tailed proposals}
Although any proposal can be used for importance sampling, 
proposals with tails as heavy as the target will be efficient
and have a CLT. 
  
  \vspace{0.2in} \pause
  
  For example, suppose our target is a standard Cauchy and our proposal is a standard normal, the weights are 
  \[ w\left(\theta^{(s)}\right) = \frac{p\left(\left.\theta^{(s)}\right|y\right)}{g(\theta^{(s)})} = \frac{\frac{1}{\pi(1+\theta^2)}}{\frac{1}{\sqrt{2\pi}}e^{-\theta^2/2}} \]
  For $\theta^{(s)}\stackrel{iid}{\sim} N(0,1)$, the weights for the largest $|\theta^{(s)}|$ will dominate the others.
}

\begin{frame}[fragile]
\frametitle{Importance weights for proposal with thin tails}
<<importance_sampling_weights>>=
set.seed(16)

d <- data.frame(
  theta  = rnorm(1000)
) |>
  mutate(
    unweight = dcauchy(theta) / dnorm(theta),
    weight   = unweight/sum(unweight)
  )

whmax  = which.max(d$weight)

ggplot(d,
      aes(
        x = theta,
        y = weight
      )) +
  geom_point() + 
  ylim(0, max(d$weight)) + 
  xlim(-d$theta[whmax], d$theta[whmax]) + 
  labs(x = expression(theta), 
       y = "Normalized importance weight")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Effective sample size}
We can get a measure of how efficient the sample is by computing the effective 
sample size (ESS), 
i.e. how many independent unweighted draws do we effectively have:
\[ ESS = \frac{1}{\sum_{s=1}^S (\tilde{w}\left(\theta^{(s)}\right))^2} \]
<<effective_size, echo=TRUE>>=
weight <- d$unweight         # Unnormalized weight
(n   <- length(d$weight))    # Number of samples
(ess <- 1/sum(d$weight^2))   # Effective sample size
ess/n                        # Effective sample proportion
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Effective sample size}
<<cumulative_effective_size, echo=TRUE, cache=TRUE>>=
set.seed(5)
theta          <- rnorm(1e4)
lweight        <- dcauchy(theta,log=TRUE) - dnorm(theta,log=TRUE)
cumulative_ess <- length(lweight)

for (i in 1:length(lweight)) {
  lw = lweight[1:i]
  w = exp(lw-max(lw))
  w = w/sum(w)
  cumulative_ess[i] = 1/sum(w^2)
}
@
\end{frame}


\begin{frame}
\frametitle{ESS - Light tail proposal}
<<ess-plot, dependson="cumulative_effective_size">>=
d <- data.frame(
  x = 1:length(cumulative_ess), 
  y = cumulative_ess)

ggplot(d,
       aes(
         x = x,
         y = y
       )) +
  geom_line() + 
  labs(x = "Number of samples", 
       y = "Effective sample size") 
@
\end{frame}

% \begin{frame}[fragile]
% \frametitle{The idea}
% <<accept_reject, eval=FALSE>>=
% f = function(x) dnorm(x)/(1-pnorm(5))
% q = function(x) dexp(x-5)
% M = f(5)/q(5)
% 
% plot.pt = function(x) {
%   u = runif(length(x))
%   points(x,u*M*q(x), 
%          pch=ifelse(M*q(x)*u<f(x), 19, 4),
%          col=ifelse(M*q(x)*u<f(x), "red", "blue"))
% }
% 
% par(mfrow=c(1,2), mar=c(5,4,4,2)+0.1)
% curve(M*dexp(x-5), 5, 6, col="blue", lwd=2, xlab="x", ylab="u*M*q(x)", ylim=c(0,M), main=paste("M=",round(M,2)))
% curve(f, col="red",lwd=2, add=T)
% legend("topright",c("f(x)","M*q(x)"), col=c("red","blue"), lwd=2)
% plot.pt(5+qexp(runif(1e2,0,pexp(1))))
% 
% 
% 
% M = f(5)/q(5)*10
% 
% plot.pt = function(x) {
%   u = runif(length(x))
%   points(x,u*M*q(x), 
%          pch=ifelse(M*q(x)*u<f(x), 19, 4),
%          col=ifelse(M*q(x)*u<f(x), "red", "blue"))
% }
% 
% curve(M*dexp(x-5), 5, 6, col="blue", lwd=2, xlab="x", ylab="u*M*q(x)", ylim=c(0,M), main=paste("M=",round(M,2)))
% curve(f, col="red",lwd=2, add=T)
% legend("topright",c("f(x)","M*q(x)"), col=c("red","blue"), lwd=2)
% plot.pt(5+qexp(runif(1e2,0,pexp(1))))
% @
% \end{frame}
% 
% \begin{frame}[fragile]
% \frametitle{The idea}
% <<accept_reject_plot, echo=FALSE>>=
% <<accept_reject>>
% @
% \end{frame}
% 
% \frame{\frametitle{Simulating extreme events}
% 	Suppose you are interested in simulating $X\sim N(0,1)\mathrm{I}(X>5)$. \pause We know that 
%  	\[ f(x) = \frac{\frac{1}{\sqrt{2\pi}} \exp(-x^2/2)}{1-\mathrm{\Phi}(5)}. \]
% 	\pause We can use a shifted exponential distribution as a proposal, i.e. $X^* = 5+Y$ where $Y\sim Exp(1)$. \pause We calculate
% 	{\small
% 	\[ M = \sup_x \frac{f(x)}{q(x)} \pause = \frac{\frac{1}{\sqrt{2\pi}}}{1-\mathrm{\Phi}(5)} \sup_x \frac{\exp(-x^2/2)}{\exp(-[x-5])} \pause = \frac{\frac{1}{\sqrt{2\pi}}}{1-\mathrm{\Phi}(5)} \exp(-5^2/2) \pause \approx 5.19 \]
% 	}
% }
% 
% \begin{frame}[fragile]
% <<accept_reject2, eval=FALSE>>=
% M = f(5)/q(5)
% ar = function() {
%   x = 0
%   while(x<5) {
%     u = runif(1)
%     x = rexp(1)
%     x = ifelse(u<f(x)/(M*q(x)), x, 0)
%   }
%   return(x)
% }
% 
% r = rdply(1e3, ar)
% ggplot(r, aes(x=V1))+
%   geom_histogram(aes(y=..density..), binwidth=.05)+
%   stat_function(fun=f, col="red")+
%   stat_function(fun=function(x) M*dexp(x-5), col="blue")
% @
% \end{frame}
% 
% \begin{frame}[fragile]
% <<accept_reject2_plot, echo=FALSE, cache=TRUE>>=
% <<accept_reject2>>
% @
% \end{frame}
% 
% \frame{\frametitle{Unknown normalizing constant}
% 	Suppose you are interested in simulating $X\sim N(0,1)\mathrm{I}(X>5)$. \pause We know that 
% 	\[ f(x)\propto f_2(x) = \exp(-x^2/2). \]
% 	\pause We calculate
% 	\[ M_2 = \sup_x \frac{f_2(x)}{q(x)} \pause = \sup_x \frac{\exp(-x^2/2)}{\exp(-[x-5])} \pause= \exp(-5^2/2) \pause \approx 3.73\times 10^{-6}  \]
%   \pause We can still draw $X^*\sim q(x)$ and accept with probability $f_2(x)/M_2 q(x)$ \pause since
% 	\[ M = \frac{\frac{1}{\sqrt{2\pi}}}{1-\mathrm{\Phi}(5)} M_2 \pause \implies \frac{f_2(x)}{M_2} = \frac{f(x)}{M}  \] 
% 	for all $x$. \pause But $M_2$ does not relate to the acceptance probability.
% }
% 
% \section{Summary}
% \frame{\frametitle{Summary}
% 	The accept-reject method is a way of obtaining samples from $f(x)$ \pause when
% 	\begin{itemize}
% 	\item the inverse cdf cannot be computed \pause or is expensive to compute \pause and
% 	\item when $f(x)$ can be evaluated at least up to a normalizing constant. \pause 
% 	\end{itemize}
% 	Based on 
% 	\begin{itemize}
% 	\item a draw from a proposal $q(x)$, \pause 
% 	\item a constant $M$ such that $f(x) \le M q(x)$\pause, and
% 	\item a uniform draw.
% 	\end{itemize}
% }


\section{Practical Monte Carlo}
\begin{frame}
\frametitle{Practical Monte Carlo}

As a practical matter, 
we typically obtain a single collection of samples, 
say $\theta^{(s)}$ for $s=1,\ldots,S$ and we want to address many 
scientific questions. 

\vspace{0.2in} \pause

For example,
\begin{itemize}
\item $E[\theta|y]$
\item Equal-tail 95\% CI for $\theta$
\item other functions of $\theta$
\end{itemize}

\vspace{0.2in} \pause

\alert{So how large should $S$ be?} 
Large enough so the Monte Carlo error on the worst estimated quantity
is sufficiently small.
\end{frame}


\begin{frame}[fragile]
\frametitle{Practical Monte Carlo - Monte Carlo error}

Calculate the Monte Carlo error. 

<<mc-error, echo=TRUE>>=
# Normal distribution
theta <- rnorm(1e3)

mcmcse::mcse(theta)         # expectation
mcmcse::mcse.q(theta, .025) # quantile
# mcmcse::mcse.q(theta, .975)
@

\end{frame}


\begin{frame}
\frametitle{Monte Carlo Error as a Function of Sample Size}
<<mc-sample-size, echo=FALSE>>=
y <- rnorm(1e5)
d <- tribble(
  ~S, ~se,
  1e1, mcse(y[1:1e1])$se,
  1e2, mcse(y[1:1e2])$se,
  1e3, mcse(y[1:1e3])$se,
  1e4, mcse(y[1:1e4])$se,
  1e5, mcse(y[1:1e5])$se,
)

ggplot(d, 
       aes(x = S,
           y = se)) +
  geom_line() +
  scale_y_log10() +
  scale_x_continuous(labels = scales::label_comma(),
                     trans = "log10")
@
\end{frame}

\end{document}
