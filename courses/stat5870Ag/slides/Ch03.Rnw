\documentclass[handout]{beamer}

\usepackage{verbatim,multicol}

% Theme settings
\usetheme{AnnArbor}\usecolortheme{beaver}
\setbeamertemplate{navigation symbols}{}


% Document settings
\newcommand{\lecturetitle}{Modeling assumptions}
\title[\lecturetitle]{STAT 401A - Statistical Methods for Research Workers}
\subtitle{\lecturetitle}
\author[Jarad Niemi]{Jarad Niemi (Dr. J)}
\institute[Iowa State]{Iowa State University}
\date[\today]{last updated: \today}


\setkeys{Gin}{width=0.6\textwidth}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

<<options, echo=FALSE>>=
opts_chunk$set(comment=NA, fig.width=6, fig.height=5, size='tiny', out.width='0.6\\textwidth', fig.align='center')
library(plyr)
library(ggplot2)
library(xtable)
@


\begin{document}

\begin{frame}
\maketitle
\end{frame}

\section{Normality assumptions}
\begin{frame}[fragile]
\frametitle{Normality assumptions}

In the paired t-test, we assume
\[ D_i \stackrel{iid}{\sim} N(\mu,\sigma^2). \]
In the two-sample t-test, we assume
\[ Y_{ij} \stackrel{ind}{\sim} N(\mu_j,\sigma^2). \]

\pause

<<visualize, echo=FALSE, out.width='\\textwidth'>>=
par(mfrow=c(2,2))
curve(dnorm(x,1), -4, 4, main="Paired t-test", xlab="Difference", ylab="Distribution", 
      lwd=2, axes=F, frame=T)
axis(1,0)
abline(v=0, col="gray", lty=2)
points(.8,0, pch=19)
segments(.8-1.3,0,.8+1.3,0, lwd=2)

curve(dnorm(x,-1), -4, 4, main="Two-sample t-test", xlab="", ylab="", lwd=2, axes=F, frame=T, col=2, lty=1)
curve(dnorm(x,1), add=TRUE, col=3, lwd=2, lty=2)
legend("bottomright", paste("Pop",1:2),col=2:3, lwd=2, lty=1:2)
@
\end{frame}



\begin{frame}
\frametitle{Normality assumptions}

In the paired t-test, we assume
\[ D_i \stackrel{iid}{\sim} N(\mu,\sigma^2). \]
In the two-sample t-test, we assume
\[ Y_{ij} \stackrel{ind}{\sim} N(\mu_j,\sigma^2). \]

\vspace{0.2in}

Key features of the normal distribution assumption:
\begin{itemize}[<+->]
\item Centered at the mean (expectation) $\mu$
\item Standard deviation describes the spread
\item Symmetric around $\mu$ (no skewness)
\item Non-heavy tails, i.e. outliers are rare (no kurtosis)
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Normality assumptions}
<<normal_distribution, echo=FALSE>>=
curve(dnorm, -3.1, 3.1, main='Probability density function', xlab='y', ylab='Probability density function, f(y)', lwd=2, axes=F, frame=TRUE)
abline(v=-3:3, lty=2, col='gray')
axis(1,-3:3,expression(mu-3*sigma, mu-2*sigma, mu-sigma, mu,mu+sigma,mu+2*sigma,mu+3*sigma))
arrows(-1:-3, dnorm(1:3), 1:3, dnorm(1:3), code=3)
text(0, dnorm(1:3), round(1-2*pnorm(-1:-3),3), pos=3)
@

\end{frame}


\subsection{Kurtosis (heavy-tailedness)}
\begin{frame}[fragile]
\frametitle{Kurtosis (heavy-tailedness)}
<<kurtosis, echo=FALSE>>=
v = c(Inf,30,15,5)
sigma = v/(v-2); sigma[1] = 1
mykurtosis = paste("Kurtosis=", round(6/(v-4),2))
curve(dnorm, -3, 3, main="t distribution", ylab='Probability density function, f(y)', xlab='y', lwd=2, axes=F, frame=TRUE)
for (i in 2:4) curve(dt(x/sigma[i],v[i])/sigma[i], add=TRUE, col=i, lty=i, lwd=2)
abline(v=-3:3, lty=2, col='gray')
legend("topright", mykurtosis, lwd=2, lty=1:4, col=1:4)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Kurtosis (heavy-tailedness)}
<<outliers, echo=FALSE>>=
v = 5
sigma = sqrt(v/(v-2))

curve(dnorm, -4, 4, main='Probability density function', xlab='y', ylab='Probability density function, f(y)', lwd=2, axes=F, frame=TRUE, col='gray')
curve(dt(x/sigma,v)/sigma, lwd=2, add=TRUE)
curve(dnorm, col="gray", add=TRUE)
abline(v=-3:3, lty=2, col='gray')
axis(1,-3:3,expression(mu-3*sigma, mu-2*sigma, mu-sigma, mu,mu+sigma,mu+2*sigma,mu+3*sigma))
arrows(-1:-3, dt(1:3/sigma,v)/sigma, 1:3, dt(1:3/sigma,v)/sigma, code=3)
text(0, dt(3:1/sigma,v)/sigma, round(1-2*pt(-3:-1,v),3), pos=3)
legend("topright", c("Normal", "Scaled t_5"), lwd=2, col=c("gray","black"))
@
\end{frame}






\begin{frame}[fragile]
\frametitle{Kurtosis (heavy-tailedness)}
<<kurtosis_samples, echo=FALSE>>=
v = c(Inf,30,15,5)
sigma = v/(v-2); sigma[1] = 1
d = data.frame(v=v, sigma=sigma, kurtosis=mykurtosis)
samps = ddply(d, .(v,kurtosis), function(x) data.frame(samples = x$sigma*rt(100,x$v))) 
qplot(samples, data=samps, facets=~kurtosis, binwidth=8/30)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Kurtosis (heavy-tailedness)}
<<kurtosis_samples_boxplot, echo=FALSE>>=
v = c(Inf,30,15,5)
sigma = v/(v-2); sigma[1] = 1
d = data.frame(v=v, sigma=sigma, kurtosis=mykurtosis)
samps = ddply(d, .(v,kurtosis), function(x) data.frame(samples = x$sigma*rt(100,x$v))) 
ggplot(samps, aes(factor(kurtosis), samples))+geom_boxplot()
@
\end{frame}




\subsection{Skewness}
\begin{frame}[fragile]
\frametitle{Skewness}
<<skewness, echo=FALSE>>=
skewness = function(mu,sigma) (exp(sigma^2)+2)*sqrt(exp(sigma^2)-1)
mydlnorm = function(x,sigma) dlnorm(x, -sigma^2/2, sigma) 
sigma = c(.5,1,1.5)
myskewness = paste("Skewness=", round(skewness(-sigma^2/2,sigma),2))
curve(mydlnorm(x,sigma[1]), 0, 4, main="Log-normal distribution", ylab='Probability density function, f(y)', xlab='y', lwd=2, axes=F, frame=TRUE, ylim=c(0,1.5))
curve(mydlnorm(x,sigma[2]), add=TRUE, col=2, lty=2, lwd=2)
curve(mydlnorm(x,sigma[3]), add=TRUE, col=3, lty=3, lwd=2)
legend("topright", myskewness, lwd=2, lty=1:3, col=1:3)
abline(v=1, lty=4, col='gray')
text(1,1,"Mean", pos=4, col="gray")
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Samples from skewed distributions}

<<skewness_samples, echo=FALSE>>=
d = data.frame(sigma=sigma, skewness=myskewness)
samps = ddply(d, .(sigma,skewness), function(x) data.frame(samples = rlnorm(100,-x$sigma^2/2, x$sigma))) 
qplot(samples, data=samps, facets=~sigma, binwidth=15/30)
@

\end{frame}




\subsection{Robustness}
\begin{frame}
\frametitle{Robustness}

\begin{definition}
A statistical procedure is \alert{robust to departures from a particular assumption} if it is valid even when the assumption is not met.
\end{definition}

\vspace{0.2in} \pause 

\begin{remark}
If a 95\% confidence interval is robust to departures from a particular assumption, the confidence interval should cover the true value about 95\% of the time.
\end{remark}
\end{frame}


\begin{frame}[fragile]
\frametitle{Robustness to skewness and kurtosis}

Percentage of 95\% confidence intervals that cover the true difference in means in an equal-sample two-sample t-test with non-normal populations (where the distributions are the same other than their means).

{\tiny
<<"robustness data", echo=FALSE, results='asis'>>=
options(width=170)
d = 
data.frame("sample size" = c(5,10,25,50,100),
           "strongly skewed" = c(95.5,95.5,95.3,95.1,94.8),
           "moderately skewed" = c(95.4,95.4,95.3,95.3,95.3),
           "mildly skewed" = c(95.2,95.2,95.1,95.1,95.0),
           "heavy-tailed" = c(98.3, 98.3, 98.2, 98.1, 98.0),
           "short-tailed" = c(94.5, 94.6, 94.9, 95.2, 95.6), 
           check.names=FALSE)
print(xtable(d, digits=c(NA,0,1,1,1,1,1), align='c|r|ccccc|'),
      include.row=FALSE)
@
}

\end{frame}


\begin{frame}[fragile]
\frametitle{Differences in variances}

<<different_variances, echo=FALSE>>=
par(mfrow=c(1,1))
sigma = c(1,2,4)
curve(dnorm, -5, 5, main="Normal distribution", xlab="", ylab="",
      axes=F, frame=T, type="n")
for (i in 1:3) curve(dnorm(x,0,sigma[i]), add=TRUE, lwd=2, col=i, lty=i)
legend("topright", paste("SD=",sigma), lwd=2, col=1:3, lty=1:3)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Differences in variances}

<<different_variances_samples, echo=FALSE>>=
d = adply(sigma,1,function(x) data.frame(sigma=x,y=rnorm(100,0,x)))
ggplot(d, aes(factor(sigma), y))+geom_boxplot()
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Robustness to differences in variances}

Percentage of 95\% confidence intervals that cover the true difference in means in an equal-sample two-sample t-test ($r=\sigma_1/\sigma_2$). 

{\small
<<"robustness data2", echo=FALSE, results='asis'>>=
options(width=170)
d = 
data.frame(n1=c(10,10,10,100,100,100), 
           n2=c(10,20,40,100,200,400), 
           "r=1/4" = c(95.2,83,71,94.8,86.5,71.6),
           "r=1/2" = c(94.2,89.3,82.6,96.2,88.3,81.5),
           "r=1"   = c(94.7,94.4,95.2,95.4,94.8,95.0),
           "r=2"   = c(95.2,98.7,99.5,95.3,98.8,99.5),
           "r=4"   = c(94.5,99.1,99.9,95.1,99.4,99.9),
           check.names=FALSE)
print(xtable(d, digits=c(NA,0,0,1,1,1,1,1), align='c|rr|ccccc|'),
      include.row=FALSE)
@
}

\end{frame}




\begin{frame}
\frametitle{Outliers}
\begin{definition}
A statistical procedure is \alert{resistant} if it does not change very much when a small part of the data changes, perhaps drastically.
\end{definition}

\vspace{0.2in} \pause 

Identify outliers:
\begin{enumerate}[<+->]
\item If recording errors, fix.
\item If outlier comes from a different population, remove and report.
\item If results are the same with and without outliers, report with outliers.
\item If results are different, use resistant analysis or report both analyses. 
\end{enumerate}


\end{frame}




\section{Independence}

\begin{frame}
\frametitle{Common ways for independence to be violated}



\begin{itemize}
\item Cluster effect
  \begin{itemize}
  \item e.g. pigs in a pen \pause
  \end{itemize}
\item Serial effect
  \begin{itemize}
  \item e.g. measurements in time with drifting scale \pause
  \end{itemize}
\item Spatial effect
  \begin{itemize}
  \item e.g. corn yield plots (drainage) \pause
  \end{itemize}
\end{itemize}

\end{frame}



\section{Transformations of the data}
\begin{frame}
\frametitle{Common transformations for data}

{\tiny From: \url{http://en.wikipedia.org/wiki/Data_transformation_(statistics)}}

\begin{definition}
In statistics, \alert{data transformation} refers to the application of a deterministic mathematical function to each point in a data set\pause â€” that is, each data point $y_i$ is replaced with the transformed value $z_i = f(y_i)$, where $f$ is a function. 
\end{definition}

\vspace{0.2in} \pause

The most common transformations are

\begin{itemize}[<+->]
\item If $y$ is a proportion, then $f(y) = sin^{-1}(\sqrt{y})$.
\item If $y$ is a count, then $f(y) = \sqrt{y}$. 
\item If $y$ is positive and right-skewed, then $f(y) = \log(y)$, the \emph{natural logarithm} of $y$. 
\end{itemize}

\pause

{\scriptsize
\begin{remark}
Since $\log(0)=-\infty$, the logarithm cannot be used directly when some $y_i$ are zero. In these cases, use $\log(y+c)$ where $c$ is something small relative to your data, e.g. half of the minimum non-zero value. 
\end{remark}
}

\end{frame}



\subsection{Log transformation}
\begin{frame}
\frametitle{Log transformation}

Consider two-sample data and let $z_{ij}=log(y_{ij})$. \pause Now, run a two-sample t-test on the z's. \pause Then we assume
\[ Z_{ij} \stackrel{ind}{\sim} N(\mu_j, \sigma^2) \]
\pause and the quantity $\overline{Z}_2-\overline{Z}_1$ \pause estimates the ``difference in population means on the (natural) log scale''. \pause The quantity $\exp\left(\overline{Z}_2-\overline{Z}_1\right)=e^{\overline{Z}_2-\overline{Z}_1}$ \pause estimates 
\[ \frac{\mbox{Median of population 2}}{\mbox{Median of population 1}} \]
on the original scale \pause or, equivalently, it estimates the \alert{multiplicative effect} of moving from population 1 to population 2. 

\end{frame}


\begin{frame}
\frametitle{Log transformation interpretation}

If we have a randomized experiment:
\begin{remark}
It is estimated that the response of an experimental unit to treatment 2 will be $\exp\left(\overline{Z}_2-\overline{Z}_1\right)$ times as large as its response to treatment 1.
\end{remark}

\vspace{0.2in} \pause

If we have an observational study:
\begin{remark}
It is estimated that the median for population 2 is $\exp\left(\overline{Z}_2-\overline{Z}_1\right)$ times as large as the median for population 1.
\end{remark}

\end{frame}



\begin{frame}
\frametitle{Confidence intervals with log transformation}


If $z_{ij}=log(y_{ij})$ and we assume
\[ Z_{ij} \stackrel{ind}{\sim} N(\mu_j, \sigma^2), \]
\pause then a $100(1-\alpha)\%$ two-sided confidence interval for $\mu_2-\mu_1$ is 
\[ (L,U) = \overline{Z}_2-\overline{Z}_1 \pm t_{n_1+n_2-2}(1-\alpha/2) SE\left(\overline{Z}_2-\overline{Z}_1\right). \]

\vspace{0.2in} \pause

A $100(1-\alpha)\%$ confidence interval for 
\[ \frac{\mbox{Median of population 2}}{\mbox{Median of population 1}} \]
\pause is $(e^L,e^U)$. 

\end{frame}



\subsection{Example}
\begin{frame}[fragile]
\frametitle{Miles per gallon data}

Untransformed:
<<echo=FALSE, fig.height=2.5, message=FALSE>>=
d = read.csv("mpg.csv")

grid <- with(d, seq(min(mpg), max(mpg), length = 100))
normaldens <- ddply(d, "country", function(df) {
  data.frame( 
    predicted = grid,
    density = dnorm(grid, mean(df$mpg), sd(df$mpg))
  )
})

ggplot(d, aes(x=mpg)) + geom_histogram(aes(y=..density..)) + facet_wrap(~country) + geom_line(data=normaldens, aes(x=predicted, y=density, color='red'), size=2)+guides(color=FALSE)
@

\pause

Logged:
<<echo=FALSE, fig.height=2.5, message=FALSE>>=
d$lmpg = log(d$mpg)
grid <- with(d, seq(min(lmpg), max(lmpg), length = 100))
normaldens <- ddply(d, "country", function(df) {
  data.frame( 
    predicted = grid,
    density = dnorm(grid, mean(df$lmpg), sd(df$lmpg))
  )
})
ggplot(d, aes(x=lmpg)) + geom_histogram(aes(y=..density..)) + facet_wrap(~country) + geom_line(data=normaldens, aes(x=predicted, y=density, color='red'), size=2)+guides(color=FALSE)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Miles per gallon data}

Untransformed:
<<echo=FALSE, fig.height=2.5, message=FALSE>>=
ggplot(d, aes(x=country, y=mpg)) + geom_boxplot()
@

Logged:
<<echo=FALSE, fig.height=2.5, message=FALSE>>=
ggplot(d, aes(x=country, y=lmpg)) + geom_boxplot() 
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Equal variances?}

We might also be concerned about the assumption of equal variances.

\vspace{0.2in} \pause

Untransformed:
<<echo=FALSE, results='asis'>>=
print(xtable(ddply(d, .(country), summarise, n=length(mpg), mean=mean(mpg), sd=sd(mpg))), include.rownames=F)
@
the ratio of sample standard deviations is around 1.05 and there are 3 times as many observations in the US.

\vspace{0.1in} \pause

Logged:
<<echo=FALSE, results='asis'>>=
print(xtable(ss<-ddply(d, .(country), summarise, n=length(lmpg), mean=mean(lmpg), sd=sd(lmpg))), include.rownames=F)
@
Now the ratio of standard deviations is 1.5 which argues for not using the logarithm.  

\end{frame}


\begin{frame}[fragile]
\frametitle{95\% two-sample CI for the ratio by hand}

{\tiny
<<echo=FALSE, results='asis'>>=
print(xtable(ss), include.rownames=F)
@

Choose group 2 to be Japan and group 1 to be the US: 
\[ \begin{array}{ll}
\alpha &= 0.05 \\
n_1+n_2-2 &= 249+79-2 = 326 \\
t_{n_1+n_2-2}(1-\alpha/2) &= t_{326}(0.975) = 1.96 \\
\overline{Z}_2 - \overline{Z}_1  &= 3.40-2.96 = 0.44 \\
s_p &= \sqrt{\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}} = \sqrt{\frac{(249-1)0.31^2+(79-1)0.21^2}{249+79-2}} = 0.29 \\
SE\left( \overline{Z}_2 - \overline{Z}_1 \right) &= s_p \sqrt{\frac{1}{n_1}+\frac{1}{n_2}} = 0.29 \sqrt{\frac{1}{249}+\frac{1}{79}} = 0.037 
\end{array}
\]

Thus a $95\%$ two-sided confidence interval for the difference (on the log scale) is 
\[ \begin{array}{rl}
(L,U) &= \overline{Z}_2 - \overline{Z}_1 \pm t_{n_1+n_2-2}(1-\alpha/2) SE\left( \overline{Z}_2 - \overline{Z}_1 \right) \\
&=  0.44 \pm 1.96\times 0.037 \\
&= (0.37, 0.51) 
\end{array} \]

and a $95\%$ two-sided confidence interval for the ratio (on the original scale) is 

\[ \left(e^L,e^U\right) = \left(e^{0.37},e^{0.51}\right) = (1.45,1,67) \]
}

\end{frame}


\begin{frame}[fragile]
\frametitle{Using R for t-test using logarithms}

<<>>=
t = t.test(log(mpg)~country, d, var.equal=TRUE)
t$estimate # On log scale
exp(t$estimate) # On original scale
exp(t$estimate[1]-t$estimate[2]) # Ratio of medians (Japan/US)
exp(t$conf.int) # Confidence interval for ratio of medians
@

\end{frame}


\frame[containsverbatim]{\frametitle{SAS code for t-test using logarithms}
{\small
\begin{verbatim}
DATA mpg;
  INFILE 'mpg.csv' DELIMITER=',' FIRSTOBS=2;
	INPUT mpg country $;

PROC TTEST DATA=mpg TEST=ratio;
	CLASS country;
	VAR mpg;
	run;
\end{verbatim}
}
}


\frame[containsverbatim]{
\frametitle{SAS output for t-test using logarithms}
{\tiny
\begin{verbatim}
                                        The TTEST Procedure
 
                                           Variable:  mpg

                                     Geometric     Coefficient
              country           N         Mean    of Variation     Minimum     Maximum

              Japan            79      29.8525          0.2111     18.0000     47.0000
              US              249      19.2051          0.3147      9.0000     39.0000
              Ratio (1/2)               1.5544          0.2928                        

                                Geometric                          Coefficient
country        Method                Mean       95% CL Mean       of Variation        95% CL CV

Japan                             29.8525     28.4887  31.2817          0.2111      0.1820   0.2514
US                                19.2051     18.4825  19.9560          0.3147      0.2882   0.3467
Ratio (1/2)    Pooled              1.5544      1.4452   1.6719          0.2928      0.2712   0.3183
Ratio (1/2)    Satterthwaite       1.5544      1.4636   1.6508                                     

                                                  Coefficients
                  Method           of Variation        DF    t Value    Pr > |t|

                  Pooled           Equal              326      11.91      <.0001
                  Satterthwaite    Unequal         193.33      14.46      <.0001

                                      Equality of Variances
 
                        Method      Num DF    Den DF    F Value    Pr > F

                        Folded F       248        78       2.17    0.0001
\end{verbatim}
}
}


\frame{\frametitle{Conclusion}

  Japanese median miles per gallon is 1.55 [95\% CI (1.46,1.65)] times as large as US median miles per gallon.
  
  \vspace{0.2in} 
  
  OR
  
  \vspace{0.2in} \pause
  
  Japenese median miles per gallon is 55\% [95\% CI (46\%,65\%)] larger than US median miles per gallon.
}


\section{Welch's two-sample t-test}
\begin{frame}
\frametitle{Unequal standard deviations}

The two-sample t-test tools assume either

\[ Y_{ij} \stackrel{ind}{\sim} N(\mu_j,\sigma^2) \qquad \mbox{ or } \qquad Z_{ij} \stackrel{ind}{\sim} N(\mu_j,\sigma^2) \]
depending on whether we were working on the original scale ($Y$) or log scale ($Z$), respectively.

\vspace{0.2in} \pause

But what if we don't believe the variances in the two populations are equal\pause, e.g. in the log transformed miles per gallon data set?

\vspace{0.2in} \pause

Instead compare 


\[ Y_{ij} \stackrel{ind}{\sim} N(\mu_j,\sigma_j^2) \qquad \mbox{ or } \qquad Z_{ij} \stackrel{ind}{\sim} N(\mu_j,\sigma_j^2), \]
i.e. the populations have unequal variances. \pause 
But still test $H_0:\mu_1=\mu_2$ vs $H_1:\mu_1\ne \mu_2$ or construct a confidence interval for $\mu_2-\mu_1$. 
\end{frame}



\begin{frame}[fragile]
\frametitle{Visualization of two normals with unequal standard deviations}

<<>>=
curve(dnorm, -3, 6, lwd=2)
curve(dnorm(x, 2, 2), lwd=2, col=2, lty=2, add=TRUE)
@

\end{frame}


\begin{frame}
\frametitle{Welch's SE with Satterthwaite's approximation to df}

Estimate of ($\mu_2-\mu_1$):
\[ \overline{Y}_2-\overline{Y}_1  \]
\pause
Standard error:
\[ SE_W\left(\overline{Y}_2-\overline{Y}_1 \right) =\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}} \]
\pause 
Degrees of freedom using the Satterthwaite's approximation:
\[ df_W = \frac{SE_W\left(\overline{Y}_2-\overline{Y}_1 \right)^4}{ \frac{SE\left(\overline{Y}_2\right)^4}{n_2-1}+\frac{SE\left(\overline{Y}_1\right)^4}{n_1-1} } \]
\pause 
where 
\[ SE\left(\overline{Y}_2\right)=\frac{s_2}{\sqrt{n_2}} \qquad \mbox{ and } \qquad SE\left(\overline{Y}_1\right)=\frac{s_1}{\sqrt{n_1}} \]
\pause
{\tiny (which is the same formula as in the paired t-test)}


\end{frame}



\begin{frame}
\frametitle{Welch's t-test and CI}

Welch's t-test has test statistic:
\[ t = \frac{\mbox{(Estimate-Parameter)}}{\mbox{SE(Estimate)}} \pause = \frac{\overline{Y}_2-\overline{Y}_1-(\mu_2-\mu_1)}{SE_W\left(\overline{Y}_2-\overline{Y}_1 \right)} \]
\pause 
which has a $t$ distribution with (approximately) $df_W$ degrees of freedom if the null hypothesis is true. \pause Calculate the pvalue
\begin{itemize}[<+->]
\item Two-sided ($H_1:\mu_2\ne \mu_1$): $p=2P(t_{df_W} < -|t|)$
\item One-sided ($H_1:\mu_2> \mu_1$): $p=P(t_{df_{W}} < -t)$
\item One-sided ($H_1:\mu_2< \mu_1$): $p=P(t_{df_{W}} < t)$
\end{itemize}

\vspace{0.2in} \pause 

Two-sided $100(1-\alpha)\%$ confidence interval for $\mu_2-\mu_1$:
\[ \overline{Y}_2-\overline{Y}_1 \pm t_{df_W}(1-\alpha/2) SE_W\left(\overline{Y}_2-\overline{Y}_1 \right) \]

\end{frame}



\begin{frame}
\frametitle{Are the variances equal?}

Suppose 
\[ Y_{ij} \stackrel{ind}{\sim} N(\mu_j,\sigma_j^2) \]
and you want to test $H_0: \sigma_1=\sigma_2$ vs $H_1: \sigma_1\ne\sigma_2$.

\vspace{0.2in} \pause 

You can use an $F$-test and its associated pvalue. \pause If the pvalue is small, e.g. less than 0.05, then we reject $H_0$. \pause If the pvalue is not small, then we fail to reject $H_0$, but this does not mean the variances are not equal. 

\vspace{.2in} \pause

{\tiny (Section 4.5.3) discusses another approach called Levene's test}

\end{frame}



\subsection{Example using original Japan vs US mpg data}
\begin{frame}[fragile]
\frametitle{Welch's test and CI using R}
<<welch>>=
var.test(mpg~country,d) # F-test
(t=t.test(mpg~country, d, var.equal=FALSE))
@

\end{frame}


\frame[containsverbatim]{\frametitle{SAS code for two-sample t-test}
\begin{verbatim}
DATA mpg;
    INFILE 'mpg.csv' DELIMITER=',' FIRSTOBS=2;
    INPUT mpg country $;

PROC TTEST DATA=mpg;
    CLASS country;
    VAR mpg;
    RUN;
\end{verbatim}
}



\frame[containsverbatim]{
\frametitle{SAS output for t-test}
{\tiny
\begin{verbatim}
                                       The TTEST Procedure

                                         Variable:  mpg

         country          N        Mean     Std Dev     Std Err     Minimum     Maximum

         Japan           79     30.4810      6.1077      0.6872     18.0000     47.0000
         US             249     20.1446      6.4147      0.4065      9.0000     39.0000
         Diff (1-2)             10.3364      6.3426      0.8190

  country       Method               Mean       95% CL Mean        Std Dev      95% CL Std Dev

  Japan                           30.4810     29.1130  31.8491      6.1077      5.2814   7.2429
  US                              20.1446     19.3439  20.9452      6.4147      5.8964   7.0336
  Diff (1-2)    Pooled            10.3364      8.7252  11.9477      6.3426      5.8909   6.8699
  Diff (1-2)    Satterthwaite     10.3364      8.7576  11.9152

                   Method           Variances        df    t Value    Pr > |t|

                   Pooled           Equal           326      12.62      <.0001
                   Satterthwaite    Unequal      136.87      12.95      <.0001

                                      Equality of Variances

                        Method      Num df    Den df    F Value    Pr > F

                        Folded F       248        78       1.10    0.6194
\end{verbatim}
}
}



\subsection{Example using logarithm of Japan vs US mpg data}
\begin{frame}[fragile]

<<welch_on_log>>=
var.test(log(mpg)~country,d)
(t = t.test(log(mpg)~country, d, var.equal=FALSE))
exp(t$conf.int)
@

\end{frame}




\frame[containsverbatim]{\frametitle{SAS code for t-test using logarithms}
{\small
\begin{verbatim}
DATA mpg;
  INFILE 'mpg.csv' DELIMITER=',' FIRSTOBS=2;
  INPUT mpg country $;

PROC TTEST DATA=mpg TEST=ratio;
	CLASS country;
	VAR mpg;
	run;
\end{verbatim}
}
}


\frame[containsverbatim]{
\frametitle{SAS output for t-test using logarithms}
{\tiny
\begin{verbatim}
                                        The TTEST Procedure
 
                                           Variable:  mpg

                                     Geometric     Coefficient
              country           N         Mean    of Variation     Minimum     Maximum

              Japan            79      29.8525          0.2111     18.0000     47.0000
              US              249      19.2051          0.3147      9.0000     39.0000
              Ratio (1/2)               1.5544          0.2928                        

                                Geometric                          Coefficient
country        Method                Mean       95% CL Mean       of Variation        95% CL CV

Japan                             29.8525     28.4887  31.2817          0.2111      0.1820   0.2514
US                                19.2051     18.4825  19.9560          0.3147      0.2882   0.3467
Ratio (1/2)    Pooled              1.5544      1.4452   1.6719          0.2928      0.2712   0.3183
Ratio (1/2)    Satterthwaite       1.5544      1.4636   1.6508                                     

                                                  Coefficients
                  Method           of Variation        DF    t Value    Pr > |t|

                  Pooled           Equal              326      11.91      <.0001
                  Satterthwaite    Unequal         193.33      14.46      <.0001

                                      Equality of Variances
 
                        Method      Num DF    Den DF    F Value    Pr > F

                        Folded F       248        78       2.17    0.0001
\end{verbatim}
}
}



\section{Summary}
\begin{frame}
\frametitle{Summary}

Two-sample $t$ tools assumptions
\begin{itemize}
\item Normality
  \begin{itemize}
  \item No skewness (take logs?)
  \item No heavy tails
  \end{itemize}
\item Equal variances
  \begin{itemize}
  \item Test: F-test or Levene's test
  \item Use Welch's two-sample t-test and CI
  \end{itemize}
\item Independence (use random effects or avoid)
  \begin{itemize}
  \item Cluster
  \item Serial
  \item Spatial
  \end{itemize}
\end{itemize}

\end{frame}



\end{document}
