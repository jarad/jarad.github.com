\documentclass[handout]{beamer}

\usepackage{verbatim,multicol,amsmath}

% Theme settings
\usetheme{AnnArbor}\usecolortheme{beaver}
\setbeamertemplate{navigation symbols}{}


% Document settings
\newcommand{\lecturetitle}{Nonparametric two-sample tests}
\title[\lecturetitle]{STAT 401A - Statistical Methods for Research Workers}
\subtitle{\lecturetitle}
\author[Jarad Niemi]{Jarad Niemi (Dr. J)}
\institute[Iowa State]{Iowa State University}
\date[\today]{last updated: \today}


\setkeys{Gin}{width=0.6\textwidth}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

<<options, echo=FALSE>>=
opts_chunk$set(comment=NA, fig.width=6, fig.height=5, size='tiny', out.width='0.6\\textwidth', fig.align='center', message=FALSE)
library(plyr)
library(ggplot2)
library(xtable)
@


\begin{document}

\begin{frame}
\maketitle
\end{frame}


\section{Nonparametric statistics}
\begin{frame}
\frametitle{Nonparametric statistics}

{\tiny \url{http://en.wikipedia.org/wiki/Parametric_statistics}}

\begin{definition}
\alert{Parametric statistics} assumes that the data have come from a certain probability distribution and makes inferences about the parameters of this distribution\pause, e.g. assuming the data come from a normal distribution and estimating the mean $\mu$.
\end{definition}

\vspace{0.2in} \pause

{\tiny \url{http://en.wikipedia.org/wiki/Nonparametric_statistics}}

\begin{definition}
\alert{Nonparametric statistics} make no assumptions about the probability distributions of the [data],\pause e.g. randomization and permutation tests.
\end{definition}

\end{frame}


\subsection{Central limit theorem}

\begin{frame}
\frametitle{Central limit theorem}

\begin{theorem}
Let $X_1,X_2,\ldots$ be a sequence of iid random variables with $E[X_i]=\mu$ and $0<V[X_i]=\sigma^2<\infty$. \pause Then 
\[ \frac{\overline{X}_n-\mu}{\sigma/\sqrt{n}} \stackrel{n\to\infty}{\longrightarrow} \pause N(0,1) \]
\pause 
where 
\[ \overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \]
i.e. the sample mean using the first $n$ variables.
\end{theorem}

\end{frame}


\begin{frame}
\frametitle{Central limit theorem}

\begin{lemma}
Let $X_1,X_2,\ldots$ be a sequence of iid random variables with $E[X_i]=\mu$ and $0<V[X_i]=\sigma^2<\infty$. \pause Then 
\[ \frac{\overline{X}_n-\mu}{s_n/\sqrt{n}} \stackrel{n\to\infty}{\longrightarrow} \pause N(0,1) \]
\pause 
where 
\[ \overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \qquad \mbox{ and } \qquad s_n^2 = \frac{1}{n-1} \sum_{i=1}^n \left(X_i-\overline{X}_n\right)^2  \]
i.e. the sample mean and variance using the first $n$ variables.
\end{lemma}

\end{frame}


\begin{frame}[fragile]
\frametitle{Bernoulli example}

Consider $X_i\stackrel{iid}{\sim} Ber(p)$, i.e. $X_i=1$ with probability $p$ and $X_i=0$ with probability $1-p$. \pause Then $E[X_i] = p$ and $0<V[X_i]=p(1-p)<\infty$. 
% So,
% \[ \frac{\overline{X}_n-p}{s_n/\sqrt{n}} \stackrel{n\to\infty}{\longrightarrow} \pause N(0,1). \]

\pause

<<bernoulli, echo=FALSE>>=
p = .5
res = ddply(data.frame(n=10^(2:4)), .(n), 
            function(x) rdply(1000, { y = rbinom(x$n, 1, p); data.frame(x = (mean(y)-p)/(sd(y)/sqrt(x$n)))  }))
ggplot(res, aes(x=x)) + geom_histogram(aes(y=..density..), alpha=0.4) + stat_function(fun=dnorm, col="red") + facet_wrap(~n)
@

\end{frame}


\section{Nonparametric approaches to paired data}
\begin{frame}[fragile]
\frametitle{Rusty leaves data}

<<rustyd_leaves_data, results='asis', echo=FALSE>>=
d = data.frame(year1=c(38,10,84,36,50,35,73,48), 
              year2=c(32,16,57,28,55,12,61,29))
d$diff = with(d, year1-year2)
d$"diff>0" = (d$diff>0)*1

print(xtable(d, digits=0), include.rownames=FALSE)
@

If there is no effect, then the ``diff$>$0'' column should be a 1 or 0 with probability 0.5, i.e. $X_i\stackrel{iid}{\sim} Ber(p)$ and $K=\sum_{i=1}^n X_i \sim Bin(n,p)$.

\end{frame}




\begin{frame}[fragile]
\frametitle{Sign test}
  The sign test calculates the probability of observing this many ones (or more extreme) if the null hypothesis is true. Here the hypotheses are 
  \[ H_0: p=0.5 \qquad H_1:p>0.5. \]
  \pause 
  For our one-sided hypothesis (removing leaves will decrease rusty leaves), the pvalue is the probability of observing 6, 7, or 8 ones. \pause This is 
	\[ {8\choose 6} 0.5^8 + {8\choose 7} 0.5^8 + {8\choose 8} 0.5^8 = 0.14 \]
	\pause 
  
<<sign_test>>=
K = sum(d[,4]) 
n = nrow(d)
sum(dbinom(K:8,8,.5))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Visualizing pvalues}

<<pvalue_visualization, echo=FALSE>>=
base_plot <- function(...) {
  xx = -1:10
  plot(xx-.5,dbinom(xx,8,.5), type="s", ylab="Bin(8,.5) probability mass function",...)
  segments(xx+.5, 0, xx+.5, dbinom(xx[-6],8,.5))
  abline(h=0)
}

fill_plot <- function(x,...) {
  rect(x-.5, 0, x+.5, dbinom(x,8,.5), col="red",...)
}

par(mfrow=c(1,3))
base_plot(main="H1: p<0.5")
fill_plot(0:K)

base_plot(main="H1: p!=0.5")
fill_plot(K:n)
fill_plot(0:(n-K))

base_plot(main="H1: p>0.5")
fill_plot(K:n)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Sign test using normal approximation}

  Recall that if $K\sim Bin(n,p)$, then $E[K] = np$ and $V[K] = np(1-p)$. \pause Thus, if $p=0.5$, then 
  \[ Z = \frac{K-(n/2)}{\sqrt{n/4}} \stackrel{n\to\infty}{\longrightarrow} N(0,1) \]
  and we can approximate the pvalue by calculating the area under the normal curve.
  
<<sign_test_approximation>>=
Z = (K-n/2)/(sqrt(n/4))
1-pnorm(Z)
@

The continuity correction accounts for the fact that K is discrete:

<<sign_test_approximation_with_continuity correction>>=
Z = (K-n/2-1/2)/(sqrt(n/4))
1-pnorm(Z)
@
\end{frame}


\begin{frame}
\frametitle{Continuity correction}

<<continuity_correction, echo=FALSE>>=
par(mfrow=c(1,1))
base_plot(main="Continuity correction")
fill_plot(6:8)
curve(dnorm(x, n/2, sqrt(n/4)), add=TRUE)
abline(v=6)
@

\end{frame}


\subsection{Wilcoxon signed-rank test}
\frame{\frametitle{Wilcoxon signed-rank test}
  Also known as the Wilcoxon signed-rank test: \pause
	\begin{enumerate}[<+->]
	\item Compute the difference in each pair.
	\item Drop zeros from the list.
	\item Order the absolute differences from smallest to largest and assign them their ranks.
	\item Calculate $S$: the sum of the ranks from the pairs for which the difference is positive. 
	\item Calculate $E[S] = n(n+1)/4$ where $n$ is the number of pairs.
	\item Calculate $SD[S]=[n(n+1)(2n+1)/24]^{1/2}$.
	\item Calculate $Z=(S-E[S]+c)/SD[S]$ where c, the continuity correction, is either 0.5 or -0.5.
	\item Calculate the pvalue comparing $Z$ to a standard normal.
	\end{enumerate}
}

\begin{frame}[fragile]
\frametitle{Signed rank test}
<<ranked_data, echo=FALSE, results='asis'>>=
#Signed rank test
d$absdiff = abs(d$diff)
ordr = order(d$absdiff)
d = d[ordr,]
d$rank = rank(d$absdiff)

#d$rank = as.character(d$rank)
tab = xtable(d, digits=c(NA,0,0,0,0,0,1))
print(tab, include.rownames=FALSE)
@
	
	\pause
	
	\begin{itemize}
	\item $S=32.5$
	\item $E[S]=18$
	\item $SD[S]=7.14$
	\item $Z=1.96$ (with continuity correction of -0.5)
	\item $p=0.02$
	\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Signed-rank test in R}

<<signed_rank_test>>=
# By hand
S = sum(d$rank[d$"diff>0"==1])
n = nrow(d)
ES = n*(n+1)/4
SDS = sqrt(n*(n+1)*(2*n+1)/24)
z = (S-ES-0.5)/SDS
1-pnorm(z)

# Using a function
wilcox.test(d$year1, d$year2, paired=T)
@

Divide this two-sided pvalue by 2 since the data are in agreement with the alternative hypothesis (fewer rusty leaves after removal).
\end{frame}



\frame[containsverbatim]{\frametitle{SAS code for paired nonparametric test}
{\small
\begin{verbatim}
DATA leaves;
  INPUT tree year1 year2;
  diff = year1-year2;
  DATALINES;
1 38 32
2 10 16
3 84 57
4 36 28
5 50 55
6 35 12
7 73 61
8 48 29
;

PROC UNIVARIATE DATA=leaves;
    VAR diff;
    RUN;
\end{verbatim}
}
}

\frame[containsverbatim]{\frametitle{SAS code for paired nonparametric tests}
{\tiny
\begin{verbatim}
                                    The UNIVARIATE Procedure
                                         Variable:  diff

                                             Moments

                 N                           8    Sum Weights                  8
                 Mean                     10.5    Sum Observations            84
                 Std Deviation      12.2007026    Variance            148.857143
                 Skewness           -0.1321468    Kurtosis            -1.2476273
                 Uncorrected SS           1924    Corrected SS              1042
                 Coeff Variation    116.197167    Std Error Mean      4.31359976


                                   Basic Statistical Measures

                         Location                    Variability

                     Mean     10.50000     Std Deviation           12.20070
                     Median   10.00000     Variance               148.85714
                     Mode       .          Range                   33.00000
                                           Interquartile Range     20.50000


                                   Tests for Location: Mu0=0

                        Test           -Statistic-    -----p Value------

                        Student's t    t  2.434162    Pr > |t|    0.0451
                        Sign           M         2    Pr >= |M|   0.2891
                        Signed Rank    S      14.5    Pr >= |S|   0.0469


                                    Quantiles (Definition 5)

                                     Quantile      Estimate

                                     100% Max          27.0
                                     99%               27.0
                                     95%               27.0
                                     90%               27.0
                                     75% Q3            21.0
                                     50% Median        10.0
                                     25% Q1             0.5
                                     10%               -6.0
                                     5%                -6.0
                                     1%                -6.0
                                     0% Min            -6.0


                                    The UNIVARIATE Procedure
                                         Variable:  diff

                                      Extreme Observations

                              ----Lowest----        ----Highest---

                              Value      Obs        Value      Obs

                                 -6        2            8        4
                                 -5        5           12        7
                                  6        1           19        8
                                  8        4           23        6
                                 12        7           27        3

\end{verbatim}
}
}

\frame{\frametitle{Conclusion}
	Removal of red cedar trees within 100 yards is associated with a significant reduction in rusty apple leaves (Wilcoxon signed rank test, p=0.023). 
}



\section{Wilcoxon Rank-Sum Test}
\begin{frame}[fragile]
\frametitle{Do these data look normal?}
<<mpg, echo=FALSE>>=
mpg = read.csv("mpg.csv")

ss = ddply(mpg, .(country), summarize, mn=mean(mpg), sd=sd(mpg))
attach(ss)

ggplot(mpg, aes(x=mpg))+
  geom_histogram(aes(y=..density..), data=subset(mpg,country=="Japan"), fill="red", alpha=0.5)+
  geom_histogram(aes(y=..density..), data=subset(mpg,country=="US"), fill="blue", alpha=0.5)+
  stat_function(fun=function(x) dnorm(x,mn[1],sd[1]), colour="red")+
  stat_function(fun=function(x) dnorm(x,mn[2],sd[2]), colour="blue")
@
\end{frame}


\begin{frame}
\frametitle{Rank-sum test}
  Also referred to as the Wilcoxon rank-sum test and the Mann-Whitney U test: \pause
	\begin{enumerate}[<+->]
	\item Transform the data to ranks 
	\item Calculate $U$, the sum of ranks of the group with a smaller sample size 
	\item Calculate $E[U] = n_1 \overline{R}$  
		\begin{enumerate}
		\item $n_1$: sample size of the smaller group 
		\item $\overline{R}$: average rank
		\end{enumerate}
	\item Calculate $SD(U) = s_R \sqrt{\frac{n_1 n_2}{(n_1+n_2)}}$
		\begin{enumerate}
		\item $n_2$: sample size of the larger group
		\item $s_R$: standard deviation of the ranks
		\end{enumerate}
	\item Calculate $Z=(U-E[U]+c)/SD(U)$ where c, the continuity correction, is either 0.5 or -0.5.
	\item Determine the pvalue using a standard normal distribution.
	\end{enumerate}
\end{frame}

\begin{frame}[fragile]
\frametitle{Example on a small dataset}
<<mpg_small, echo=FALSE, results='asis'>>=
set.seed(2)
id = sample(nrow(mpg),9)
sm = mpg[id,]

ordr = order(sm$mpg)
sm = sm[ordr,]
sm$mpg[5] = 26 # make tie
sm$rank = rank(sm$mpg)
rownames(sm) = 1:nrow(sm)

tab = xtable(sm, digits=c(NA,0,NA,1))
print(tab, include.rownames=FALSE)
@
	\pause
	\begin{itemize}[<+->]
	\item $U=22.5$
	\item $E[U] = 15$
	\item $SD[U]=3.86$
	\item $z=1.81$ (appropriate continuity correction is -0.5)
	\item $p=0.07$
	\end{itemize}
\end{frame}


\begin{frame}[fragile]\frametitle{Example on a small dataset}
<<mpg_small_by_hand>>=
n1 = sum(sm$country=="Japan")
n2 = sum(sm$country=="US")
U = sum(sm$rank[sm$country=="Japan"])
EU = n1*mean(sm$rank)
SDU = sd(sm$rank) * sqrt(n1*n2/(n1+n2))
Z = (U-.5-EU)/SDU
2*pnorm(-Z)

wilcox.test(mpg~country, sm)
@
\end{frame}



\subsection{Full data}
\begin{frame}[fragile]
\frametitle{Visual representation of Rank Sum Test}
<<visual_representation, echo=FALSE>>=
ordr = order(mpg$mpg)
mpg.ordered = mpg[ordr,]

par(mar=c(5,4,0,0)+.1)
plot(mpg.ordered$mpg, 1:nrow(mpg), col=mpg.ordered$country, pch=19, xlab="MPG", cex=0.7, ylab="Rank")
legend("topleft", c("Japan","US"), col=1:2, pch=19)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{R code and output for Rank Sum Test}
<<wilcoxon_rank_sum_test>>=
wilcox.test(mpg~country,mpg)
@
\end{frame}



\frame[containsverbatim]{
\frametitle{SAS code for Wilcoxon rank sum test}
\begin{verbatim}
DATA mpg;
    INFILE 'mpg.csv' DELIMITER=',' FIRSTOBS=2;
    INPUT mpg country $;

PROC NPAR1WAY DATA=mpg WILCOXON;
    CLASS country;
    VAR mpg;
    RUN;
\end{verbatim}

}



\frame[containsverbatim]{\frametitle{}
{\tiny
\begin{verbatim}

                                     The NPAR1WAY Procedure

                          Wilcoxon Scores (Rank Sums) for Variable mpg
                                 Classified by Variable country

                                    Sum of      Expected       Std Dev          Mean
             country       N        Scores      Under H0      Under H0         Score
             -----------------------------------------------------------------------
             US          249      33646.50      40960.50    733.579091    135.126506
             Japan        79      20309.50      12995.50    733.579091    257.082278

                               Average scores were used for ties.


                                    Wilcoxon Two-Sample Test

                                Statistic             20309.5000

                                Normal Approximation
                                Z                         9.9696
                                One-Sided Pr >  Z         <.0001
                                Two-Sided Pr > |Z|        <.0001

                                t Approximation
                                One-Sided Pr >  Z         <.0001
                                Two-Sided Pr > |Z|        <.0001

                           Z includes a continuity correction of 0.5.


                                      Kruskal-Wallis Test

                                Chi-Square               99.4068
                                DF                             1
                                Pr > Chi-Square           <.0001
\end{verbatim}
}
}

\frame{\frametitle{Conclusion}
	Average miles per gallon of Japanese cars are significantly different than average miles per gallon of American cars (Wilcoxon rank sum test, $p<0.0001$). 
}


\begin{frame}
\frametitle{Decision Tree}
\setkeys{Gin}{width=0.8\textwidth}
\begin{center}
\includegraphics{decisionTree}
\end{center}

\end{frame}

\end{document}
