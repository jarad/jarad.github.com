\documentclass[handout]{beamer}

\usepackage{verbatim,xmpmulti,multicol,multirow}

% Theme settings
\usetheme{AnnArbor}\usecolortheme{beaver}
\setbeamertemplate{navigation symbols}{}


% Document settings
\newcommand{\lecturetitle}{Inference Using $t$-Distributions}
\title[\lecturetitle]{STAT 401A - Statistical Methods for Research Workers}
\subtitle{\lecturetitle}
\author[Jarad Niemi]{Jarad Niemi (Dr. J)}
\institute[Iowa State]{Iowa State University}
\date[\today]{last updated: \today}


\setkeys{Gin}{width=0.6\textwidth}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

<<options, echo=FALSE>>=
opts_chunk$set(comment=NA, fig.width=6, fig.height=5, size='tiny', out.width='0.6\\textwidth', fig.align='center')
@


\begin{document}


\begin{frame}
\maketitle
\end{frame}


\section{Background}
\subsection{Random variables}
\begin{frame}
\frametitle{Random variables}

{\tiny From: \url{http://www.stats.gla.ac.uk/steps/glossary/probability_distributions.html}}

\begin{definition}
A \alert{random variable} is a function that associates a unique numerical value with every outcome of an experiment. 
\end{definition} 

\pause 

\begin{definition}
A \alert{discrete random variable} is one which may take on only a countable number of distinct values such as 0, 1, 2, 3, 4,...  Discrete random variables are usually (but not necessarily) counts. 
\end{definition}

\pause 

\begin{definition}
A \alert{continuous random variable} is one which takes an infinite number of possible values. Continuous random variables are usually measurements.
\end{definition}

\end{frame}


\begin{frame}
\frametitle{Random variables}

Examples:
\begin{itemize}[<+->]
\item Discrete random variables
  \begin{itemize}
  \item Coin toss: Heads (1) or Tails (0)
  \item Die roll: 1, 2, 3, 4, 5, or 6
  \item Number of Ovenbirds at a 10-minute point count
  \item RNAseq feature count
  \end{itemize}
\item Continuous random variables
  \begin{itemize}
  \item Pig average daily (weight) gain 
  \item Corn yield per acre
  \end{itemize}
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Statistical notation}

Let $Y$ be 1 if the coin toss is heads and 0 if tails, then 
\[ Y \sim Bin(n,p) \]
which means 
\begin{quote}
$Y$ is a binomial random variable with $n$ trials and probability of success $p$
\end{quote} 

\pause For example, if $Y$ is the number of heads observed when tossing a fair coin ten times, then $Y\sim Bin(10,0.5)$. 

\vspace{0.2in} \pause

Later we will be constructing $100(1-\alpha)\%$ confidence intervals, these intervals are constructed such that if $n$ of them are constructed then $Y\sim Bin(n, 1-\alpha)$ will cover the true value.

\end{frame}


\begin{frame}
\frametitle{Statistical notation}


Let $Y_i$ be the average daily (weight) gain in pounds for the $i$th pig, then 
\[ Y_i \stackrel{iid}{\sim} N(\mu,\sigma^2) \]
which means 
\begin{quote}
$Y_i$ are independent and identically distributed normal (Gaussian) random variables with expected value $E[Y_i]=\mu$ and variance $V[Y_i]=\sigma^2$ (standard deviation $\sigma$).
\end{quote}

\pause For example, if a litter of pigs is expected to gain 2 lbs/day with a standard deviation of 0.5 lbs/day and \emph{the knowledge of how much one pig gained does not affect what we think about how much the others have gained}, then $Y_i \stackrel{iid}{\sim} N(2,0.5^2)$.

\end{frame}


\subsection{Normal distribution}
\begin{frame}[fragile]
\frametitle{Normal (Gaussian) distribution}

A random variable $Y$ has a normal distribution, i.e. $Y\sim N(\mu,\sigma^2)$, with mean $\mu$ and variance $\sigma^2$ if draws from this distribution follow a bell curve centered at $\mu$ with spread determined by $\sigma^2$:

<<normal_distribution, fig.height=5, fig.align='center', echo=FALSE>>=
curve(dnorm, -3.1, 3.1, main='Probability density function', xlab='y', ylab='f(y)', lwd=2, axes=F, frame=TRUE)
abline(v=-3:3, lty=2, col='gray')
axis(1,-3:3,expression(mu-3*sigma, mu-2*sigma, mu-sigma, mu,mu+sigma,mu+2*sigma,mu+3*sigma))
arrows(-1:-3, dnorm(1:3), 1:3, dnorm(1:3), code=3)
text(0, dnorm(1:3), c("68%","95%","99.7%"), pos=3)
@
\end{frame}


\subsection{$t$-distribution}
\begin{frame}[fragile]
\frametitle{$t$-distribution}

A random variable $Y$ has a $t$-distribution, i.e. $Y\sim t_v$, with degrees of freedom $v$ if draws from this distribution follow a similar bell shaped pattern:

<<t_distribution, fig.height=5, fig.align='center', echo=FALSE>>=
curve(dnorm, -3.1, 3.1, main='Probability density function', xlab='y', ylab='f(y)', lwd=2, axes=F, frame=TRUE, col="gray")
curve(dt(x,3), -3.1, 3.1, add=TRUE, lwd=2)
abline(v=-3:3, lty=2, col='gray')
axis(1,-3:3)
legend("topright",expression(N(0,1),t[3]), lwd=2, col=c("gray","black"))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{$t$-distribution}

As $v\to \infty$, then $t_v \stackrel{d}{\to} N(0,1)$, i.e. as the degrees of freedom increase, a $t$ distribution gets closer and closer to a standard normal distribution, i.e. $N(0,1)$. \pause If $v>30$, the differences is negligible. \pause 

<<t30_distribution, fig.height=5, fig.align='center', echo=FALSE>>=
curve(dnorm, -3.1, 3.1, main='Probability density function', xlab='y', ylab='f(y)', lwd=2, axes=F, frame=TRUE, col="gray")
curve(dt(x,30), -3.1, 3.1, add=TRUE, lwd=2)
abline(v=-3:3, lty=2, col='gray')
axis(1,-3:3)
legend("topright",expression(N(0,1),t[30]), lwd=2, col=c("gray","black"))
@
\end{frame}




\begin{frame}[fragile]
\frametitle{$t$ critical value}

\begin{definition}
If $T\sim t_v$, a \alert{$t_v(1-\alpha/2)$ critical value} is the value such that $P(T<t_v(1-\alpha/2))=1-\alpha/2$ (or $P(T>t_v(1-\alpha))=\alpha/2$). 
\end{definition}

<<t_critical_value, fig.height=5, fig.align='center', echo=FALSE>>=
curve(dt(x,5), -3.1, 3.1, main=expression(paste('Probability density function ', t[5])), xlab='t', ylab='f(t)', lwd=2, axes=F, frame=T)
t_crit = qt(.9, 5)
axis(1,t_crit)
abline(v=t_crit, col="gray", lty=2)
text(-.5, .1, 0.9, cex=2)
text(2.5, .1, .1, cex=2)
arrows(2.4, .08, 2, 0.03)
@
\end{frame}




\section{Paired data}
\frame{\frametitle{Cedar-apple rust}
{\small
  Cedar-apple rust is a (non-fatal) disease that affects apple trees. Its most obvious symptom is rust-colored spots on apple leaves. Red cedar trees are the immediate source of the fungus that infects the apple trees. If you could remove all red cedar trees within a few miles of the orchard, you should eliminate the problem. In the first year of this experiment the number of affected leaves on 8 trees was counted; the following winter all red cedar trees within 100 yards of the orchard were removed and the following year the same trees were examined for affected leaves. 
	}
	
	\vspace{0.2in} \pause
	
	\begin{itemize}
	\item	Statistical hypothesis:
		\begin{itemize}
		\item[$H_0$:] Removing red cedar trees increases or maintains the same mean number of rusty leaves.
		\item[$H_1$:] Removing red cedar trees decreases the mean number of rusty leaves.
		\end{itemize}
	
	\vspace{0.2in} \pause
		
	\item Statistical question:
		\begin{itemize}
		\item[] What is the expected reduction of rusty leaves \alert{in our sample} between year 1 and year 2 (perhaps due to removal of red cedar trees)?
		\end{itemize}
	\end{itemize}
}

\begin{frame}[fragile]
\frametitle{Data}
	Here are the data
<<paired_data>>=
library(plyr)
y1 = c(38,10,84,36,50,35,73,48)
y2 = c(32,16,57,28,55,12,61,29)
leaves  = data.frame(year1=y1, year2=y2, diff=y1-y2)
leaves
summarize(leaves, n=length(diff), mean=mean(diff), sd=sd(diff))
@

\pause

Is this a statistically significant difference?
\end{frame}


\subsection{Paired t-test}
\frame{\frametitle{Assumptions}
	Let 
	\begin{itemize}
	\item $Y_{1j}$ be the number of rusty leaves on tree $j$ in year 1
	\item $Y_{2j}$ be the number of rusty leaves on tree $j$ in year 2
	\end{itemize}
	\pause Assume 
	\[ D_j=Y_{1j} - Y_{2j} \stackrel{iid}{\sim} N(\mu,\sigma^2) \]
	
	\vspace{0.2in} \pause
	
	Then the statistical hypothesis test is
	\begin{itemize}
	\item[$H_0$:] $\mu= 0$  ($\mu\le 0$)
	\item[$H_1$:] $\mu> 0$
	\end{itemize}
  while the statistical question is 'what is $\mu$?'
}

\frame{\frametitle{Paired t-test pvalue}
	Test statistic
	\[ t = \frac{\overline{D} - \mu}{SE(\overline{D})} \]
	\pause where $SE(\overline{D}) = s/\sqrt{n}$ with 
	\begin{itemize}
	\item $n$ being the number of observations (differences),
	\item $s$ being the sample standard deviation of the differences, and
  \item $\overline{D}$ being the average difference.
	\end{itemize}
	\pause If $H_0$ is true, then $\mu=0$ and $t\sim t_{n-1}$. \pause The pvalue is $P(t_{n-1}>t)$ since this is a one-sided test. \pause By symmetry, $P(t_{n-1}>t) = P(t_{n-1}<-t)$. 
	
	\vspace{0.2in} \pause 
	
	For these data, 
	\[ \overline{D}=10.5, \mbox{SE}(\overline{D})=4.31, t_{7}=2.43, \pause and \mbox{p}=0.02 \]
}

\frame{\frametitle{Confidence interval for $\mu$}
	The 100(1-$\alpha$)\% confidence interval has lower endpoint 
	\[ \overline{D} - t_{n-1}(1-\alpha) SE(\overline{D}) \]
	and upper endpoint at infinity
	
	\pause \vspace{0.2in} 
	
	For these data at 95\% confidence, $t_7(0.9)=1.89$ and thus the lower endpoint is
	\[ 10.5 - 1.89 \times 4.31 = 2.33 \]
	\pause So we are 95\% confident that the true difference in the number of rusty leaves is greater than 2.33. 
}

\subsection{SAS}
\frame[containsverbatim]{\frametitle{SAS code for paired t-test}
{\small
\begin{verbatim}
DATA leaves;
  INPUT tree year1 year2;
  DATALINES;
1 38 32
2 10 16
3 84 57
4 36 28
5 50 55
6 35 12
7 73 61
8 48 29
;

PROC TTEST DATA=leaves SIDES=U;
    PAIRED year1*year2;
    RUN;
\end{verbatim}
}
}



\frame[containsverbatim]{\frametitle{SAS output for paired t-test}
{\tiny 
\begin{verbatim}
                                       The TTEST Procedure

                                   Difference:  year1 - year2

                  N        Mean     Std Dev     Std Err     Minimum     Maximum

                  8     10.5000     12.2007      4.3136     -6.0000     27.0000

                     Mean       95% CL Mean        Std Dev      95% CL Std Dev

                  10.5000      2.3275  Infty       12.2007      8.0668  24.8317

                                       df    t Value    Pr > t

                                        7       2.43    0.0226
\end{verbatim}
}
}


\begin{frame}[fragile]
\frametitle{R output for paired t-test}
<<paired_t_test>>=
t.test(leaves$year1, leaves$year2, paired=TRUE, alternative="greater")
@

\end{frame}

\frame{\frametitle{Statistical Conclusion}
	Removal of red cedar trees within 100 yards is associated with a significant reduction in rusty apple leaves (paired t-test $t_7$=2.43, p=0.023). \pause The mean reduction in rust color leaves is 10.5 [95\% CI (2.33,$\infty$)]. 
}




\section{Two-sample t-test}
\frame{\frametitle{Do Japanese cars get better mileage than American cars?}
  \pause
	\begin{itemize}
	\item Statistical hypothesis:
		\begin{itemize}
		\item[$H_0$:] Mean mpg of Japanese cars is the same as mean mpg of American cars.
		\item[$H_1$:] Mean mpg of Japanese cars is different than mean mpg of American cars. 
		\end{itemize}
	
	\vspace{0.2in} \pause
	
	\item Statistical question:
	
	\begin{itemize}
	\item[] What is the difference in mean mpg between Japanese and American cars?
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	\item Data collection:
		\begin{itemize}
		\item Collect a random sample of Japanese/American cars
		\end{itemize}
	\end{itemize}
}


\begin{frame}[fragile]
\frametitle{}
<<mpg_data, message=FALSE>>=
mpg = read.csv("mpg.csv")
library(ggplot2)
ggplot(mpg, aes(x=mpg))+
  geom_histogram(data=subset(mpg,country=="Japan"), fill="red", alpha=0.5)+
  geom_histogram(data=subset(mpg,country=="US"), fill="blue", alpha=0.5)
@
\end{frame}




\frame{\frametitle{Assumptions}
	Let 
	\begin{itemize}
	\item $Y_{1j}$ represent the $j$th Japanese car
	\item $Y_{2j}$ represent the $j$th American car \pause
	\end{itemize}
	Assume 
	\[ Y_{1j}\stackrel{iid}{\sim} N(\mu_1, \sigma^2) \qquad Y_{2j}\stackrel{iid}{\sim} N(\mu_2, \sigma^2) \]
	
	\vspace{0.2in}  \pause
	
	Restate the hypotheses using this notation
	\begin{itemize}
	\item[$H_0$:] $\mu_1=\mu_2$ 
	\item[$H_1$:] $\mu_1\ne\mu_2$ \pause
	\end{itemize}
	Alternatively
	\begin{itemize}
	\item[$H_0$:] $\mu_1-\mu_2=0$ 
	\item[$H_1$:] $\mu_1-\mu_2\ne 0$
	\end{itemize}
}

\subsection{Test statistic}
\frame{\frametitle{Test statistic}
	The test statistic we use here is 
	\[ \frac{\overline{Y}_1-\overline{Y}_2 - (\mu_1-\mu_2)}{SE(\overline{Y}_1-\overline{Y}_2)} \]
	\pause where 
	\begin{itemize} 
	\item $\overline{Y}_1$ is the sample average mpg of the Japanese cars
	\item $\overline{Y}_2$ is the sample average mpg of the American cars
	\end{itemize}
	\pause and
	\[ SE(\overline{Y}_1-\overline{Y}_2) = s_p \sqrt{\frac{1}{n_1}+\frac{1}{n_2}} \pause \qquad s_p = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{(n_1+n_2-2)}} \]
	\pause where 
	\begin{itemize} 
	\item $s_1$ is the sample standard deviation of the mpg of the Japanese cars
	\item $s_2$ is the sample standard deviation of the mpg of the American cars
	\end{itemize}
}

\subsection{Pvalue}
\frame{\frametitle{Pvalue}
	If $H_0$ is true, then $\mu_1=\mu_2$ and the test statistic 
	\[ t=\frac{\overline{Y}_1-\overline{Y}_2 - (\mu_1-\mu_2)}{SE(\overline{Y}_1-\overline{Y}_2)} \sim t_{n_1+n_2-2} \]
	where 
	$t_{\nu}$ is a t-distribution with $\nu$ degrees of freedom. 
	
	\vspace{0.2in} \pause
	
	Pvalue is $P(\left|t_{n_1+n_2-2}\right|>|t|) \pause = P(t_{n_1+n_2-2}>\left|t\right|)+P(t_{n_1+n_2-2}<-\left|t\right|)$ \pause or as a picture
  
<<pvalue_figure, echo=FALSE, fig.height=3, fig.align='center'>>=
dt326 = function(x) dt(x,df=30)
xx = seq(-3,-1.5,0.1)
yy = dt326(xx)

par(mar=c(2,4,0,0)+.2)
curve(dt326, -3, 3, ylab="Probability density function")
polygon(c(xx,-1.5,-3), c(yy,0,0), col="red", border=NA)
polygon(-c(xx,-1.5,-3), c(yy,0,0), col="red", border=NA)
@
}

\begin{frame}[fragile]
\frametitle{Hand calculation}
	To calculate the quantity by hand, we need 6 numbers:
<<car_summary_statistics>>=
library(plyr)
ddply(mpg, .(country), summarize, n=length(mpg), mean=mean(mpg), sd=sd(mpg))
@
	
	Calculate 
	\[ \begin{array}{rll}
	s_p &= \sqrt{\frac{(79-1)\times 6.11^2+(249-1)\times 6.41^2}{79+249-2}} &= 6.34 \pause \\
	SE(\overline{Y}_1-\overline{Y}_2) &= 6.34 \sqrt{\frac{1}{79}+\frac{1}{249}} &= 0.82 \pause \\
	t &= \frac{30.5-20.1}{0.82} &= 12.6 
	\end{array} \]
	\pause Finally, we are interested in finding $P(|t_{326}|>|12.6|) = 2P(t_{326}<-|12.6|) \pause <0.0001$ which is found using a table or software.
\end{frame}

\subsection{Confidence interval}
\frame{\frametitle{Confidence interval}
	Alternatively, we can construct a 100(1-$\alpha$)\% confidence interval. \pause The formula is 
	\[ \overline{Y}_1-\overline{Y}_2 \pm t_{n_1+n_2-2}(1-\alpha/2) SE(\overline{Y}_1-\overline{Y}_2) \]
	where 
	\pause $\pm$ indicates plus and minus \pause and $t_{\nu}(1-\alpha/2)$ is the value such that $P(t_{\nu} < t_{\nu}(1-\alpha/2))=1-\alpha/2$. \pause If $\alpha=0.05$ and $\nu=326$, then $t_{\nu}(1-\alpha/2)= 1.97$. \pause
	
	\vspace{0.2in} 
	
	The 95\% confidence interval is 
	\[ 30.5-20.1\pm 1.97\times 0.82 = (8.73, 11.9) \]
	\pause We are 95\% confident that, on average, Japanese cars get between 8.73 and 11.9 more mpg than American cars.
}

\subsection{Using SAS}
\frame[containsverbatim]{\frametitle{SAS code for two-sample t-test}
\begin{verbatim}
DATA mpg;
    INFILE 'mpg.csv' DELIMITER=',' FIRSTOBS=2;
    INPUT mpg country $;

PROC TTEST DATA=mpg;
    CLASS country;
    VAR mpg;
    RUN;
\end{verbatim}
}



\frame[containsverbatim]{\frametitle{}
{\tiny
\begin{verbatim}
                                       The TTEST Procedure

                                         Variable:  mpg

         country          N        Mean     Std Dev     Std Err     Minimum     Maximum

         Japan           79     30.4810      6.1077      0.6872     18.0000     47.0000
         US             249     20.1446      6.4147      0.4065      9.0000     39.0000
         Diff (1-2)             10.3364      6.3426      0.8190

  country       Method               Mean       95% CL Mean        Std Dev      95% CL Std Dev

  Japan                           30.4810     29.1130  31.8491      6.1077      5.2814   7.2429
  US                              20.1446     19.3439  20.9452      6.4147      5.8964   7.0336
  Diff (1-2)    Pooled            10.3364      8.7252  11.9477      6.3426      5.8909   6.8699
  Diff (1-2)    Satterthwaite     10.3364      8.7576  11.9152

                   Method           Variances        df    t Value    Pr > |t|

                   Pooled           Equal           326      12.62      <.0001
                   Satterthwaite    Unequal      136.87      12.95      <.0001

                                      Equality of Variances

                        Method      Num df    Den df    F Value    Pr > F

                        Folded F       248        78       1.10    0.6194
\end{verbatim}
}
}


\begin{frame}[fragile]
\frametitle{R code/output for two-sample t-test}

<<two_sample_t_test>>=
t.test(mpg~country, data=mpg, var.equal=TRUE)
@

\end{frame}

\frame{\frametitle{Conclusion}
	Mean miles per gallon of Japanese cars is significantly different than mean miles per gallon of American cars (two-sample t-test t=12.62, $p<0.0001$). \pause Japanese cars get an average of 10.3 [95\% CI (8.7,11.9)] more miles per gallon than American cars.
}



\section{Tests and CIs}
\begin{frame}
\frametitle{Tests and CIs}

Goal: provide a generic framework for hypothesis test and confidence interval construction

\end{frame}

\subsection{Hypothesis testing}
\frame{\frametitle{Hypotheses}
  Three key features:
	\begin{itemize}[<+->]
	\item a test statistic calculated from data
	\item a sampling distribution for the test statistic under the null hypothesis
	\item a region that is as or more extreme (one-sided vs two-sided hypotheses)
	\end{itemize}
	
	\vspace{0.2in} \pause
  
	Calculate probability of being in the region:
  \begin{definition}
	A \alert{pvalue} is the probability of observing a test statistic as or more extreme than that observed, if the null hypothesis is true. 
	\end{definition}
  
  \vspace{0.2in} \pause
  \begin{itemize}
  \item If pvalue is less than or equal to $\alpha$, we reject the null hypothesis.
  \item If pvalue is greater than $\alpha$, we fail to reject the null hypothesis.
  \end{itemize}
}



\frame{
\frametitle{Hypothesis testing framework}
{\small
	Let's assume, we have 
	\begin{itemize}[<+->]
  \item a parameter $\mu$ and an estimate $\hat{\mu}$, 
	\item calculated a test statistic $t=(\hat{\mu}-\mu)/SE(\hat{\mu})$, and
	\item if the null hypothesis is true, $t$ has a $t_\nu$ sampling distribution. 
	\end{itemize}
	
  \pause
	
	Now, we can have one of three types of hypotheses:
	\begin{itemize}[<+->]
	\item Two-sided ($H_0:\mu=\mu_0$ vs $H_1: \mu\ne \mu_0$):
  \[ \mbox{pvalue} = P(t_\nu > |t|) + P(t_\nu<-|t|) = 2P(t_\nu<-|t|) \]
  \item One-sided ($H_0:\mu\le \mu_0$ vs $H_1: \mu> \mu_0$):
  \[ \mbox{pvalue} = P(t_\nu>t) = P(t_\nu<-t) \]
  \item One-sided ($H_0:\mu\ge \mu_0$ vs $H_1: \mu< \mu_0$):
  \[ \mbox{pvalue} = P(t_\nu<t) \]
  \end{itemize}
	
	\vspace{-0.2in} \pause
	
	\uncover<9->{$F(t)=P(t_\nu<t)$ is the \alert{cumulative distribution function} for a $t$ distribution with $\nu$ degrees of freedom.}
}
}

\begin{frame}[fragile]
\frametitle{Regions for hypothesis tests}
	
<<z, echo=FALSE>>=
xx = seq(-3,-1.5,by=0.1)
yy = dnorm(xx)
par(mar=c(2,4,0,0)+.1)
curve(dnorm, -3, 3, ylab="Probability density function", 
      axes=F, frame=T, cex.main=1.5)
polygon(c(xx,-1.5,-3), c(yy,0,0), col="red", border=NA)
polygon(-c(xx,-1.5,-3), c(yy,0,0), col="blue", border=NA)
axis(1, c(-1,0,1)*1.5, labels=c("-t","0","t"))
@
	
If test statistic is t, two-sided (both red and blue areas), one-sided with $\mu>0$ (blue area), one-sided with $\mu<0$ (one minus blue area).
\end{frame}


\frame{\frametitle{Paired t-test example}
	In the paired t-test example, we had a test statistic $t=(\overline{D}-0)/SE(\overline{D})=2.43$ with a $t_7$ sampling distribution if the null hypothesis is true.
	
	\vspace{0.2in} \pause
	
	Consider the following hypotheses ($\mu$ is the expected difference):
	\begin{itemize}[<+->]
	\item Two-sided ($H_0:\mu=0$ vs $H_1: \mu\ne 0$):
  \[ \mbox{pvalue} = 2P(t_7<-2.43) = 0.0454 \]
  \item One-sided ($H_0:\mu\le 0$ vs $H_1: \mu> 0$):
  \[ \mbox{pvalue} = \phantom{2}P(t_7<-2.43) = 0.0227\]
  \item One-sided ($H_0:\mu\ge 0$ vs $H_1: \mu< 0$):
  \[ \mbox{pvalue} = \phantom{2}P(t_7<2.43) = 0.9773 \]
  \end{itemize}
}


\frame{\frametitle{Two-sample t-test example}
  In a two-sample t-test, we might have a test statistic $t=-2$ with a $t_{30}$ sampling distribution if the null hypothesis is true.
	
	\vspace{0.2in} \pause
	
	Consider the following hypotheses ($\mu_1-\mu_2$ is the expected difference):
	\begin{itemize}[<+->]
	\item Two-sided ($H_0:\mu_1-\mu_2=0$ vs $H_1: \mu_1-\mu_2\ne 0$):
  \[ \mbox{pvalue} = 2P(t_{30}<-2) = 0.0546 \]
  \item One-sided ($H_0:\mu_1-\mu_2\le 0$ vs $H_1: \mu_1-\mu_2> 0$):
  \[ \mbox{pvalue} = \phantom{2}P(t_{30}<2) = 0.9727 \]
  \item One-sided ($H_0:\mu_1-\mu_2\ge 0$ vs $H_1: \mu_1-\mu_2< 0$):
  \[ \mbox{pvalue} = \phantom{2}P(t_{30}<-2) = 0.0273 \]
  \end{itemize}
}



\subsection{Confidence intervals}
\begin{frame}
\frametitle{Confidence interval construction}

Key steps in confidence interval construction for $\mu$:
\begin{enumerate}[<+->]
\item Calculate point estimate $\hat{\mu}$
\item Calculate standard error of the statistic $SE(\hat{\mu})$
\item Set error level $\alpha$ (usually 0.05)
\item Find the appropriate critical value
\item Construct the $100(1-\alpha)\%$ confidence interval
  \begin{itemize}
  \item Two-sided ($H_0:\mu=\mu_0$ vs $H_1: \mu\ne \mu_0$): $(L,U)$
  \[ (L,U) = \mbox{estimate} \pm \mbox{critical value}(1-\alpha/2)\times\mbox{standard error} \]
  \item One-sided ($H_0:\mu\le \mu_0$ vs $H_1: \mu> \mu_0$): $(L,\infty)$
  \[ L = \mbox{estimate} - \mbox{critical value}(1-\alpha)\times\mbox{standard error} \]
  \item One-sided ($H_0:\mu\ge \mu_0$ vs $H_1: \mu< \mu_0$): $(-\infty, U)$
  \[ U = \mbox{estimate} + \mbox{critical value}(1-\alpha)\times\mbox{standard error} \]
  \end{itemize}
\end{enumerate}
\end{frame}




\begin{frame}[fragile]
\frametitle{Critical values}
	A related quantity are critical values, e.g. $\alert{t_{\nu}(1-\alpha/2)}$.
	
	\pause
	
<<z2, echo=FALSE>>=
par(mar=c(4,4,0,0)+.1)
curve(dnorm, -3, 3, ylab="Probability density function", axes=F, frame=T, cex.main=1.5)
polygon(-c(xx,-1.5,-3), c(yy,0,0), col="red", border="red")
axis(1, 0)
axis(1, 1.5, labels=expression(t[nu](1-alpha/2)))
text(1.8, .02, expression(alpha/2), col="white", pos=3)
text(-.5, .02, expression(1-alpha/2), pos=3)
@
\pause 
Let $c=t_{\nu}(1-\alpha/2)$, \pause then we need $P(t_{\nu} < c) = 1-\alpha/2$, i.e. the inverse of the cumulative distribution function (quantile function).
\end{frame}



\frame{\frametitle{Paired t-test example}
  In the paired t-test example, we had an estimate $\hat{\mu} = 10.5$ and a standard error of $4.3136$ with $7$ degrees of freedom. 
  
	\vspace{0.2in} \pause
  
	The 95\%, i.e. $\alpha=0.05$, confidence intervals for $\mu$ are 
  \begin{itemize}[<+->]
  \item Two-sided ($t_7(.975)=2.364624$) 
  \[ 10.5\pm 2.364624\times 4.3136 = (0.30,20.7) \]
  \item One-sided (positive) ($t_7(.95)=1.894579$) 
  \[ (10.5- 1.894579\times 4.3136,\infty) = (2.33,\infty) \]
  \item One-sided (negative) ($t_7(.95)=1.894579$)
  \[ (-\infty,10.5+ 1.894579\times 4.3136) = (-\infty,18.7) \]
  \end{itemize}
}




\frame{\frametitle{Two-sample t-test example}
  In the two-sample t-test example, we had an estimate $\widehat{\mu_1-\mu_2} = 10.33643$ and a pooled standard error of $0.8190$ with $326$ degrees of freedom. 
  
  \vspace{0.2in} \pause
  
	The 90\%, i.e. $\alpha=0.10$, confidence intervals for $\mu_1-\mu_2$ \pause are 
  \begin{itemize}[<+->]
  \item Two-sided ($t_{326}(.95)=1.649541$) 
  \[ 10.33643\pm 1.649541\times 0.8190 = (9.0,11.7) \]
  \item One-sided (positive) ($t_{326}(.90)=1.285149$) 
  \[ (10.33643- 1.285149\times 0.8190,\infty) = (9.3,\infty) \]
  \item One-sided (negative) ($t_{326}(.90)=1.285149$)
  \[ (-\infty,10.33643+ 1.285149\times 0.8190) = (-\infty,11.4) \]
  \end{itemize}
}




\frame[containsverbatim]{\frametitle{Find critical values using SAS or R}
If $\alpha=0.05$, then $1-\alpha/2=0.975$.

\vspace{0.2in}

In SAS,
\begin{verbatim}
PROC IML;
  q = QUANTILE('T', 0.975, 7);
  PRINT q;
  QUIT;
\end{verbatim}

\vspace{0.2in}

In R,
\begin{verbatim}
q = qt(0.975,7)
\end{verbatim}

\vspace{0.2in}

Both obtain q=2.364. 
}


\begin{frame}
\frametitle{Equivalence of confidence intervals and pvalues}

{\small
\begin{theorem}
If the $100(1-\alpha)\%$ confidence interval does not contain $\mu_0$, then the associated hypothesis test would reject the null hypothesis at level $\alpha$, i.e. the pvalue will be less than $\alpha$. 
\end{theorem}

\pause

Examples: 
\begin{itemize}[<+->]
\item In the paired t-test example, the one-sided 95\% confidence interval for the difference was $(2.33,\infty)$ which does not include 0. Thus the pvalue for the one-sided hypothesis test (with alternative that the difference was greater than zero) was less than 0.05 (it was 0.02) and the null hypothesis was rejected.
\item In the two-sample t-test example, the two-sided 95\% confidence interval for the difference was $(9.0,11.7)$ which does not include 0. Thus the pvalue for the two-sided hypothesis test was less than 0.05 (it was $<0.0001$) and the null hypothesis was rejected.
\end{itemize}


\pause

\begin{remark}
Rather than reporting the pvalue, report the confidence interval as it provides the same information and more.
\end{remark}
}
\end{frame}


\subsection{Summary}
\begin{frame}
\frametitle{Summary}

Two main approaches to statistical inference:
\begin{itemize}
\item Statistical hypothesis (hypothesis test)
\item Statistical question (confidence interval)
\end{itemize}

\end{frame}


\end{document}
