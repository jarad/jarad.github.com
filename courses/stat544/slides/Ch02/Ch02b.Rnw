\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Parameter estimation (cont.)}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("MCMCpack")
library("rjags")
library("rstan")
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

# In order to have Arial font (default of stan_plot) 
# library("extrafont")
# loadfonts(device="postscript")
@

<<set_seed>>=
set.seed(2)
@

\frame{\titlepage}

\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Normal model, unknown mean
  \begin{itemize}
  \item Jeffreys prior
  \item Natural conjugate prior
  \item Posterior
  \end{itemize}
\item Normal model, unknown variance
  \begin{itemize}
  \item Jeffreys prior
  \item Natural conjugate prior
  \item Posterior
  \end{itemize}
\item JAGS/Stan
  \begin{itemize}
  \item Binomial model, unknown probability
  \item Normal model, unknown mean
  \item Normal model, unknown variance
  \end{itemize}
\end{itemize}

\end{frame}


% \section{Probability theory review}
% \frame{\frametitle{Relevant probability results}\tiny 
%   This is the only result I used:
% 
%   If $Y|m \sim N(m,C)$, the kernel of the density for $Y$ is 
%   \[ p(y| m) \propto \exp\left(-\frac{1}{2C} [y- m]^2 \right) \propto \exp\left(-\frac{1}{2C} \left[ y^2 -2y m\right] \right) \propto \exp\left(-\frac{1}{2} \left[ \frac{1}{C} y^2 -2y \frac{m}{C} \right] \right). \]
%   
%   \vspace{0.2in} \pause 
% 
%   These could be used, but I didn't. 
% 
%   If $Y_i\stackrel{ind}{\sim} N(m_i, C_i)$, then for constants $c_1$ and $c_2$ 
%   \[ c_1 Y_1+c_2 Y_2 \sim \pause  N\left(c_1m_1+c_2m_2, c_1^2C_1+c_2^2C_2\right) \qquad \mbox{and} \qquad \sum_{i=1}^n c_i Y_i \sim \pause N\left( \sum_{i=1}^n c_i m_i, \sum_{i=1}^n c_i^2 C_i \right). \]
%   
%   \pause 
%   If
%   \[ \left( \begin{array}{c} Y_1 \\ Y_2 \end{array} \right) \sim N\left( \left[ \begin{array}{c} m_1 \\ m_2 \end{array} \right], \left[ \begin{array}{cc} C_{1} & C_{12} \\ C_{21} & C_{2} \end{array} \right] \right), \mbox{ then } Y_1|Y_2 \sim \pause N\left(m_1+C_{12}C_{2}^{-1}(Y_2-m_2), C_{1}-C_{12}C_{2}^{-1}C_{21}\right). \]
%   
%   \pause
%   If $Y|\mu \sim N(\mu,s^2\mathrm{I})$ and $\mu\sim N(m,C)$, then $Cov(Y,\mu) = \pause Var(\mu)=C$, see Casella \& Berger exercise 12.11a. 
%   
%   \vspace{0.2in} \pause 
%   
%   If $Y|\sigma^2 \sim N(m,\sigma^2)$, then 
%   \[ E_y \left[s^2\right] = E\left[ \frac{1}{n} \sum_{i=1}^n (y_i-m)^2 \right] = \sigma^2 \]
% }




\section{Normal model, unknown mean}
\subsection{Jeffreys prior for $\mu$}
\frame{\frametitle{Jeffreys prior for $\mu$}
  \begin{theorem}
  If $Y_i\stackrel{iid}{\sim} N(\mu,s^2)$ ($s^2$ known), Jeffreys prior for $\mu$ is $p(\mu)\propto 1$. 
  \end{theorem}
  \pause
  \begin{proof}
  Since the normal distribution with unknown mean is an exponential family, use Casella \& Berger Lemma 7.3.11 {\tiny
  \[ \begin{array}{rl}
  -E_y\left[ \frac{\partial^2}{\partial \mu^2} \log p(y|\mu) \right] 
  &=  -E_y\left[ \frac{\partial^2}{\partial \mu^2} \left(-\log (2\pi s^2)/2 -\frac{1}{2s^2}\sum_{i=1}^n \left( y_i-\mu \right)^2 \right) \right] \\
  &=  -E_y\left[ \frac{\partial^2}{\partial \mu^2} \left(-\log (2\pi s^2)/2 -\frac{1}{2s^2}\left( \sum_{i=1}^n y_i^2 - 2\mu n\overline{y} + n\mu^2 \right)\right) \right] \\
  &=  -E_y\left[ \frac{\partial}{\partial \mu} \left(-\frac{1}{2s^2}\left( - 2 n\overline{y} + 2n\mu \right)\right) \right] \\
  &=  -E_y\left[  -\frac{1}{2s^2}\left( 2n \right) \right] \\
  &=  n/s^2 \pause \\ \\
  p(\mu) &\propto \sqrt{|\mathcal{I}(\mu)|} =  \sqrt{n/s^2} \pause \\
  &\propto 1
  \end{array} \]
  }
  \pause 
  So Jeffreys prior for $\mu$ is $p(\mu)\propto 1$.
  \end{proof}
}

\frame{\frametitle{Posterior propriety}
  Since $\int_{-\infty}^\infty 1 d\mu$ is not finite, we need to check posterior propriety. \pause
  \begin{theorem}
  For $n>0$, the posterior for a normal mean (known variance) using Jeffreys prior is proper. 
  \end{theorem} 
  \pause
  \begin{proof}
  The posterior is 
  \[ \begin{array}{rl}
  p(\mu|y) &\propto p(y|\mu)p(\mu) \pause \\
  &\propto \exp\left( -\frac{1}{2s^2} \sum_{i=1}^n (y_i-\mu)^2\right) \times 1 \pause \\
  &\propto \exp\left( -\frac{1}{2s^2} \left[ -2\mu n\overline{y}+n\mu^2 \right]\right) \pause \\
  &= \exp\left( -\frac{1}{2s^2/n} \left[ \mu^2 -2\mu \overline{y} \right]\right). 
  \end{array} \]
  \pause
  This is the kernel of a normal distribution with mean $\overline{y}$ and variance $s^2/n$ which is proper if $n>0$. 
  \end{proof}
}

\subsection{Natural conjugate prior}
\frame{\frametitle{Natural conjugate prior} 
\small

  Let $Y_i\stackrel{iid}{\sim} N(\mu,s^2)$ with $s^2$ known. The likelihood is 
  \[ \begin{array}{rl}
  L(\mu) &= \exp\left( -\frac{1}{2s^2/n} \left[ \mu^2 -2\mu \overline{y} \right]\right) \pause \\
  &\propto \exp\left( -\frac{1}{2} \left[ \frac{n}{s^2} \mu^2  -2\mu \frac{n}{s^2} \overline{y} \right] \right) \\
  \end{array} \]
  \pause
  This is the kernel of a normal distribution, so the natural conjugate prior is $\mu\sim N(m,C)$. \pause
  
  \[ \begin{array}{rl}
  p(\mu|y) &\propto p(y|\mu)p(\mu) = L(\mu)p(\mu) \pause \\
  &= \exp\left( -\frac{1}{2} \left[ \frac{n}{s^2} \mu^2  -2\mu \frac{n}{s^2} \overline{y} \right] \right) 
     \exp\left( -\frac{1}{2} \left[ \frac{1}{C} \mu^2  -2\mu \frac{1}{C}m \right] \right) \pause \\
  &= \exp\left( -\frac{1}{2} \left[ \left(\frac{1}{C}+ \frac{n}{s^2}\right) \mu^2  -2\mu \left(\frac{1}{C}m + \frac{n}{s^2} \overline{y} \right) \right] \right) \pause \\
  &= \exp\left( -\frac{1}{2\left(\frac{1}{C}+ \frac{n}{s^2}\right)^{-1}} \left[ \mu^2  -2\mu \frac{1}{\left(\frac{1}{C}+ \frac{n}{s^2}\right)} \left(\frac{1}{C}m + \frac{n}{s^2} \overline{y} \right) \right] \right)
  \end{array} \]
  \pause 
  This is the kernel of a $N(m',C')$ where 
  \[ C' = \left[C^{-1} + n/s^2\right]^{-1} \qquad 
  m' = C'\left[C^{-1}m + n/s^2\overline{y}\right] 
  %= \frac{C^{-1}}{C^{-1}+ n/s^2}m + \frac{n/s^2}{C^{-1}+ n/s^2}\overline{y}. 
  \]
}

\frame{\frametitle{Normal mean posterior comments}
  Let $P = 1/C$, $P'=1/C'$, and $Q=1/s^2$ be the relevant precisions (inverse variances), \pause then 
  
  \begin{itemize}
  \item The posterior precision is the sum of the prior and observation precisions.
  \[ P' = P + \sum_{i=1}^n Q = P+ nQ. \] \pause 
  \item The posterior mean is a precision weighted average of the prior and data. 
  \[ \begin{array}{rl}
  m' &=  \frac{1}{P'}\left[P m  + n Q \overline{y} \right] \pause \\
  &= \frac{P}{P'} m + n \frac{Q}{P'} \overline{y} \pause \\
  &= \frac{P}{P'} m + \sum_{i=1}^n \frac{Q}{P'} y_i \pause
  \end{array} \]
  \item Jeffreys prior/posterior are the limits of the conjugate prior/posterior as $C\to \infty$, i.e.
  \[ \lim_{C\to \infty} N(m,C) \stackrel{d}{\to}\, \propto 1 \qquad \lim_{C\to \infty} N(m',C') \stackrel{d}{\to} N(\overline{y}, s^2/n)  \]
  \end{itemize}
}

\begin{frame}[fragile]
\frametitle{Example}

Consider $Y_i\stackrel{ind}{\sim} N(\mu,1)$ and $\mu\sim N(0,1)$. 
<<unknown_mean, echo=TRUE>>=
# Prior
m = 0
C = 1; P = 1/C

# Data
mu = 1
s2 = 1; Q = 1/s2
n = 3
set.seed(6); (y = rnorm(n,mu,sqrt(1/Q))) 

# Posterior
nQ = n*Q
Pp = P+nQ
mp = (P*m+nQ*mean(y))/Pp
@
\end{frame}

\begin{frame}[fragile]
<<unknown_mean_plot>>=
# Plot 
curve(dnorm(x,mp,sqrt(1/Pp)), col="red", lwd=2, -2, 4,
      main="Normal model with unknown mean, normal prior",
      xlab=expression(mu), ylab="Density")
curve(dnorm(x,m,sqrt(1/P)), col="blue", lwd=2, add=TRUE)
curve(dnorm(x, mean(y), sqrt(1/nQ)), col="seagreen", lwd=2, add=TRUE)
for (i in 1:n) curve(dnorm(x, y[i], sqrt(1/Q)), col="seagreen", lty=2, add=TRUE)
legend("topright", c("Prior","Posterior","Likelihood"), col=c("blue","red","seagreen"), lwd=2)
@
\end{frame}



\section{Normal model, unknown variance}
\frame{\frametitle{}
  \begin{theorem}
  If $Y_i\stackrel{iid}{\sim} N(m,\sigma^2)$ ($m$ known), Jeffreys prior for $\sigma^2$ is $p(\sigma^2)\propto 1/\sigma^2$. 
  \end{theorem}
  \pause 
  \begin{proof}
  Since the normal distribution with unknown variance is an exponential family, use Casella \& Berger Lemma 7.3.11.{\tiny
  \[ \begin{array}{rl}
  -E_y\left[ \frac{\partial^2}{\partial  (\sigma^2)^2} \log p(y| \sigma^2) \right] 
  &=  -E_y \left[ \frac{\partial^2}{\partial  (\sigma^2)^2} -n\log (2\pi \sigma^2)/2 -\frac{1}{2\sigma^2}\sum_{i=1}^n \left( y_i- m \right)^2 \right] \\
  &=  -E_y \left[ \frac{\partial}{\partial  (\sigma^2)} -\frac{n}{2\sigma^2} +\frac{1}{2(\sigma^2)^2}\sum_{i=1}^n \left( y_i- m \right)^2 \right] \\
  &=  -E_y \left[ \frac{n}{2(\sigma^2)^2} -\frac{1}{(\sigma^2)^3}\sum_{i=1}^n \left( y_i- m \right)^2 \right] \\
  &=  -\frac{n}{2(\sigma^2)^2} +\frac{n}{(\sigma^2)^3} \sigma^2 \\
  &=  \frac{n}{2} (\sigma^2)^{-2}\\
  \pause \\
  p(\sigma^2) &\propto \sqrt{|\mathcal{I}(\sigma^2)|} \propto 1/\sigma^2
  \end{array} \] }
  \pause 
  So Jeffreys prior is $p(\sigma^2) \propto 1/\sigma^2$. 
  \end{proof}
}


\frame{\frametitle{Posterior propriety}
  Since $\int_0^\infty 1/\sigma^2 d \sigma^2$ is not finite, we need to check posterior propriety. \pause
  \begin{theorem}
  For $n>0$ and at least one $y_i\ne m$, the posterior for a normal variance (known mean) using Jeffreys prior is proper. 
  \end{theorem}
  \pause
  \begin{proof}
  The posterior is 
  \[ \begin{array}{rl}
  p(\sigma^2|y) &\propto p(y| \sigma^2)p( \sigma^2) \pause \\
  &= (2\pi\sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n [y_i-m]^2 \right) (\sigma^2)^{-1} \pause \\
  &\propto (\sigma^2)^{-n/2-1} \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n [y_i-m]^2 \right) 
  \end{array} \]
  \pause 
  This is the kernel of an inverse gamma distribution with shape $n/2$ and scale $\sum_{i=1}^n [y_i-m]^2/2$ \pause which will be proper so long as $n>0$ and at least one $y_i\ne m$. 
  \end{proof}
}

\frame{\frametitle{Natural conjugate prior} \small
  Let $Y_i\stackrel{iid}{\sim} N(m,\sigma^2)$ with $m$ known. The likelihood is 
  \[ \begin{array}{rl}
  L(\sigma^2) &\propto (\sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n [y_i-m]^2  \right) \\
  \end{array} \]
  \pause
  This is the kernel of an inverse gamma distribution, so the natural conjugate prior is $IG(a,b)$. \pause 
  
  \[ \begin{array}{rl}
  p(\sigma^2|y) &\propto p(y|\sigma^2)p(\sigma^2) \pause \\
  &= (\sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n [y_i-m]^2 \right) (\sigma^2)^{-a-1}\exp(-b/\sigma^2) \pause \\
  &= (\sigma^2)^{-(a+n/2)-1} \exp\left( -\frac{1}{\sigma^2}\left[ b+ \sum_{i=1}^n [y_i-m]^2/2 \right] \right) 
  \end{array} \]
  \pause
  This is the kernel of an inverse gamma distribution with shape $a+n/2$ and scale $b+\sum_{i=1}^n [y_i-m]^2/2$.
}



\begin{frame}[fragile]
\frametitle{Example}

Suppose $Y_i\stackrel{ind}{\sim} N(1,\sigma^2)$ and $\sigma^2\sim IG(1,1)$.
<<unknown_var, echo=TRUE>>=
# Prior
a = b = 1

# Data
m = 1
n = length(y)
y

# Posterior
ap = a+n/2
(bp = b+sum((y-m)^2)/2)
@
\end{frame}

\begin{frame}[fragile]
<<unknown_var_plot>>=
# Plot 
curve(dinvgamma(x,ap,bp), col="red", lwd=2, 0,3,
      main="Normal model with unknown variance, inverse gamma prior",
      xlab=expression(sigma**2), ylab="Density")
curve(dinvgamma(x,a,b), col="blue", lwd=2, add=TRUE)
vlike = Vectorize(function(x) prod(dnorm(y, m, sqrt(x)))/.1438483)
curve(vlike, col="seagreen", lwd=2, add=TRUE)
legend("topright", c("Prior","Posterior","Likelihood"), col=c("blue","red","seagreen"), lwd=2)
@
\end{frame}


\subsection{Summary}
\frame{\frametitle{Summary}\small
  Suppose $Y_i \sim N(\mu,\sigma^2)$.
  \begin{itemize}[<+->]
  \item $\mu$ unknown ($\sigma^2$ known)
    \begin{itemize}
    \item Jeffreys prior: $p(\mu)\propto 1$ (think of this as $N(0,\infty)$)
    \item Natural conjugate prior: $N(m,C)$ 
    \item Posterior $N(m',C')$ with
      \begin{itemize}
      \item $C' = [1/C + n\sigma^{-2}]^{-1}$
      \item $m' = C'[m/C + n\sigma^{-2}\overline{y}]$
      \end{itemize}
    \end{itemize}
  \item $\sigma^2$ unknown ($\mu$ known)
    \begin{itemize}
    \item Jeffreys prior: $p(\sigma^2) \propto 1/\sigma^2$ (think of this as $IG(0,0)$)
    \item Natural conjugate prior $IG(a,b)$
    \item Posterior $IG\left(a+n/2, b+\sum_{i=1}^n (y_i-\mu)^2/2\right)$
    \end{itemize}    
  \end{itemize}
}



\section{JAGS}

\begin{frame}
\frametitle{JAGS}

\href{http://mcmc-jags.sourceforge.net/}{Just another Gibbs sampler (JAGS)} ``is a program for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation  not wholly unlike \href{http://www.openbugs.net}{BUGS}.'' \pause We will use JAGS through its R interface \href{http://cran.r-project.org/web/packages/rjags/index.html}{\tt rjags}.

\vspace{0.2in} \pause

The basic workflow when using {\tt rjags} is 

\begin{enumerate}[<+->]
\item Define model and priors in a string
\item Assign data
\item Run JAGS, i.e. simulate from the posterior
\item Summarize as necessary, e.g. mean, median, credible intervals, etc
\end{enumerate}
\end{frame}

\subsection{Binomial model}
\begin{frame}[fragile]
\frametitle{Binomial model}

Let $Y\sim Bin(n,\theta)$ and $\theta\sim Be(1,1)$ and we observe $y=3$ successes out of $n=10$ attempts.

<<jags_binomial, echo=TRUE>>=
model = "
model 
{
  y     ~ dbin(theta,n)  # notice p then n
  theta ~ dbeta(a,b)
}
"

dat = list(n=10, y=3, a=1, b=1)

m = jags.model(textConnection(model), dat)
r = coda.samples(m, "theta", n.iter=1000)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Binomial model}

<<jags_binomial_summary, dependson='jags_binomial', echo=TRUE>>=
summary(r)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Binomial model}

<<jags_binomial_plot, dependson='jags_binomial', echo=TRUE, fig.height=4>>=
plot(r)
@
\end{frame}






\subsection{Normal model, unknown mean}
\begin{frame}[fragile]
\frametitle{Normal model, unknown mean}

Let $Y_i \ind N(\mu,s^2)$ and $\mu\sim N(0,1)$. 

<<jags_normal, echo=TRUE>>=
model = "
model 
{
  for (i in 1:n) {        # iterate over observations
    y[i] ~ dnorm(mu,1/s2) # precision instead of variance
  }
  mu ~ dnorm(m,1/C)       # cannot use improper prior in JAGS
}
"

dat = list(m=0,C=1,s2=1,y=y)
dat$n = length(dat$y)

m = jags.model(textConnection(model), dat)
r = coda.samples(m, "mu", n.iter=1000)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Normal model, unknown mean}

<<jags_normal_summary, dependson='jags_normal', echo=TRUE>>=
summary(r)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Normal model, unknown mean}

<<jags_normal_plot, dependson='jags_normal', echo=TRUE, fig.height=4>>=
plot(r)
@
\end{frame}



\subsection{Normal model, unknown variance}
\begin{frame}[fragile]
\frametitle{Normal model, unknown variance}

Let $Y\sim N(m,\sigma^2)$ and $\sigma^2\sim IG(1,1)$.

<<jags_normal_var, echo=TRUE>>=
model = "
model 
{
  for (i in 1:n) {      
    y[i] ~ dnorm(m,tau) # precision instead of variance
  }
  tau ~ dgamma(a,b)     # Inverse gamma is not a built in distribution
  sigma2 <- 1/tau       # Functions of parameters
}
"

dat = list(m=1,a=1,b=1,y=y)
dat$n = length(dat$y)

m = jags.model(textConnection(model), dat)
r = coda.samples(m, "sigma2", n.iter=1000)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Normal model, unknown variance}

<<jags_normal_var_summary, dependson='jags_normal_var', echo=TRUE>>=
summary(r)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Normal model, unknown variance}

<<jags_normal_var_plot, dependson='jags_normal_var', echo=TRUE, fig.height=4>>=
plot(r)
@
\end{frame}



\section{Stan}
\begin{frame}
\frametitle{Stan}

\href{http://mc-stan.org/}{Stan} ``is a probabilistic programming language implementing full Bayesian statistical inference.'' \pause We will use Stan through its R interface \href{http://mc-stan.org/rstan.html}{\tt rstan}.

\vspace{0.2in} \pause

The basic workflow when using {\tt rstan} is (almost exactly the same as for 
{\tt rjags}):

\begin{enumerate}
\item Define model and priors in a string \alert{and compile the model}.
\item Assign data
\item Run Stan, i.e. simulate from the posterior
\item Summarize as necessary, e.g. mean, median, credible intervals, etc
\end{enumerate}

\pause
But, additional coding is required for Stan.

\end{frame}




\subsection{Binomial model}
\begin{frame}[fragile]
\frametitle{Stan - Binomial model}

Let $Y\sim Bin(n,\theta)$ and $\theta\sim Be(1,1)$.

<<stan_binomial, echo=TRUE>>=
model = "
data {
  int<lower=0> n;              // define range and type 
  int<lower=0> a;              // and notice semicolons
  int<lower=0> b;
  int<lower=0,upper=n> y;
}
parameters {
  real<lower=0,upper=1> theta; 
}
model {
  y ~ binomial(n,theta);  
  theta ~ beta(a,b);
}
"

dat = list(n=10, y=3, a=1, b=1)

m = stan_model(model_code = model) # Only needs to be done once
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Stan - Binomial model sampling}

<<stan_binomial_sampling, dependson='stan_binomial', echo=TRUE, message=TRUE>>=
r = sampling(m, data=dat, iter = 11000, warmup = 1000, refresh=5000)
@
<<stan_binomial_results, dependson='stan_binomial_sampling', echo=TRUE, message=TRUE>>=
r
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - Binomial model}

<<stan_binomial_plot, dependson='stan_binomial_sampling', echo=TRUE, out.width='0.5\\textwidth'>>=
plot(r)
@
\end{frame}






\subsection{Normal model, unknown mean}
\begin{frame}[fragile]
\frametitle{Normal model, unknown mean}

Let $Y_i \ind N(\mu,s^2)$ and $\mu\sim N(0,1)$. 

<<stan_normal_mean, echo=TRUE>>=
model = "
data {
  int<lower=0> n;
  real y[n];            // vector
  real<lower=0> s2;
  real m;
  real<lower=0> C;
}
transformed data {      // run once
  real<lower=0> s;
  real<lower=0> sqrtC;
  s     = sqrt(s2);
  sqrtC = sqrt(C);
}
parameters {
  real mu;              // if used alone, implies a uniform prior 
}
model {
  y  ~ normal(mu,s);    // vectorized, i.e. assumed independent
  mu ~ normal(m,sqrtC); // standard deviation
}
"

dat = list(m=0,C=1,s2=1,y=y)
dat$n = length(dat$y)

m = stan_model(model_code = model)
@

<<stan_normal_mean_sampling, dependson='stan_normal_mean', echo=TRUE, message=TRUE>>=
r = sampling(m, data = dat, iter = 11000, warmup = 1000, refresh=5000)
@
<<stan_normal_mean_results, dependson='stan_normal_mean_sampling', echo=TRUE>>=
r
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Normal model, unknown mean}

<<stan_normal_mean_plot, dependson='stan_normal_mean_sampling', echo=TRUE, out.width='0.5\\textwidth'>>=
plot(r)
@
\end{frame}





\subsection{Normal model, unknown variance}
\begin{frame}[fragile]
\frametitle{Normal model, unknown variance}

Let $Y_i \ind N(m,\sigma^2)$ and $\sigma^2\sim IG(1,1)$. 

<<stan_normal_var, echo=TRUE>>=
model = "
data {
  int<lower=0> n;
  real y[n];              
  real m;
  real<lower=0> a;
  real<lower=0> b;
}
parameters {
  real<lower=0> sigma2;     // if used alone, implies a uniform prior on (0,Inf)
}
transformed parameters {    // deterministic function of parameters
  real<lower=0> sigma;
  sigma = sqrt(sigma2);
}
model {
  y  ~ normal(m,sigma); 
  sigma2 ~ inv_gamma(a,b);  // built in inverse gamma distribution
}
"

dat = list(a=1,b=1,m=1,y=y)
dat$n = length(dat$y)

m = stan_model(model_code = model)
@

<<stan_normal_var_sampling, dependson='stan_normal_var', echo=TRUE, message=TRUE>>=
r = sampling(m, data = dat, iter = 11000, warmup = 1000, refresh=5000)
@
<<stan_normal_var_results, dependson='stan_normal_var_sampling', echo=TRUE>>=
r
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Normal model, unknown variance}

<<stan_normal_var_plot, dependson='stan_normal_var_sampling', echo=TRUE, out.width='0.5\\textwidth'>>=
plot(r)
@
\end{frame}




\end{document}
