\documentclass[aspectratio=169]{beamer}

\usepackage{verbatim,multicol,amsmath}

\input{../frontmatter}
\input{../commands} 

\title{Parameter estimation}

\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=5, fig.height=3, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("tidyverse"); theme_set(theme_bw())
library("pscl") 
@

<<set_seed, echo=FALSE>>=
set.seed(2)
@


\begin{document}

\begin{frame}
\maketitle
\end{frame}


\section{Outline}
\begin{frame}
\frametitle{Outline}
\begin{itemize}
\item Parameter estimation
  \begin{itemize}
  \item Beta-binomial example
  \item Point estimation
  \item Interval estimation
  \item Simulation from the posterior
  \end{itemize}
\item Priors
  \begin{itemize}
  \item Subjective 
  \item Conjugate 
  \item Default 
  \item Improper
  \end{itemize}
\end{itemize}
\end{frame}


\section{Parameter estimation}
\frame{\frametitle{Parameter estimation}
  For point or interval estimation of a parameter $\theta$ in a model $M$ based on data $y$, \pause Bayesian inference is based off  
	\[ p(\theta|y) = \frac{p(y|\theta)p(\theta)}{\alert<7->{p(y)}} \uncover<7->{= \frac{p(y|\theta)p(\theta)}{\alert<7->{\int p(y|\theta)p(\theta) d \theta}}}\uncover<8->{\propto p(y|\theta) p(\theta)} \]
	
	\pause where
	\begin{itemize}[<+->]
	\item $p(\theta)$ is the \alert{prior} distribution for the parameter, 
	\item $p(\theta|y)$ is the \alert{posterior} distribution for the parameter, 
	\item $p(y|\theta)$ is the statistical \alert{model} (or \alert{likelihood}), and
	\item $p(y)$ is the \alert{prior predictive distribution} (or \alert{marginal likelihood}).
	\end{itemize}
}

\frame{\frametitle{Obtaining the posterior}
	The hard way: \pause 
	\begin{enumerate}
	\item Derive $p(y)$.\pause
	\item Derive $p(\theta|y) = p(y|\theta)p(\theta)/p(y)$.\pause
	\end{enumerate}
	
	The easy way:\pause
	\begin{enumerate}
	\item Derive $f(\theta) = p(y|\theta)p(\theta)$.\pause
	\item Recognize $f(\theta)$ as the \alert{kernel} of some distribution.\pause
	\end{enumerate}
	
	\begin{definition}
	The \alert{kernel} of a probability density (mass) function is the form of the pdf (pmf) with any terms not involving the random variable omitted. \pause
	\end{definition}
	
	For example, $\theta^{a-1}(1-\theta)^{b-1}$ is the kernel of a \href{http://en.wikipedia.org/wiki/Beta_distribution}{beta distribution}. 
}






\subsection{Beta-binomial example}
\frame{\frametitle{Derive the posterior - the hard way}
	Suppose $Y\sim Bin(n,\theta)$ and $\theta\sim Be(a,b)$, \pause then 
	\[ \begin{array}{ll}
	p(y) &= \int p(y|\theta)p(\theta) d\theta \pause \\
	&= \int {n\choose y} \theta^y(1-\theta)^{n-y}  \frac{\theta^{a-1} (1-\theta)^{b-1}}{\mbox{Beta}(a,b)} d\theta \pause \\
	&= {n\choose y} \frac{1}{\mbox{Beta}(a,b)} \int \theta^{a+y-1} (1-\theta)^{b+n-y-1} d\theta \pause \\
	&= {n\choose y} \frac{\mbox{Beta}(a+y,b+n-y)}{\mbox{Beta}(a,b)} \pause
	\end{array} \]
  which is known as the \href{http://en.wikipedia.org/wiki/Beta-binomial_distribution}{Beta-binomial distribution}. \pause
	\[ \begin{array}{ll}
	p(\theta|y) &= p(y|\theta)p(\theta) / p(y) \pause \\
	&= {n\choose y} \theta^y(1-\theta)^{n-y}  \frac{\theta^{a-1} (1-\theta)^{b-1}}{\mbox{Beta}(a,b)}  \left/ {n\choose y} \frac{\mbox{Beta}(a+y,b+n-y)}{\mbox{Beta}(a,b)} \right. \pause \\
	&= \frac{\theta^{a+y-1} (1-\theta)^{b+n-y-1}}{\mbox{Beta}(a+y,b+n-y)}
	\end{array} \]
\pause
Thus $\theta|y \sim Be(a+y, b+n-y)$.
	
}

\frame{\frametitle{Derive the posterior - the easy way}
	Suppose $Y\sim Bin(n,\theta)$ and $\theta\sim Be(a,b)$, \pause then 
	\[ \begin{array}{ll}
	p(\theta|y) &\propto p(y|\theta)p(\theta) \pause \\
	&\propto \theta^y(1-\theta)^{n-y} \theta^{a-1} (1-\theta)^{b-1} \pause \\
	&= \theta^{a+y-1} (1-\theta)^{b+n-y-1} \pause \\
	\end{array} \]
\pause
Thus $\theta|y \sim Be(a+y, b+n-y)$.
}


\begin{frame}
\frametitle{Interpretation of prior parameters}
When constructing the $Be(a,b)$ prior with the binomial likelihood which results
in the posterior 
\[ 
\theta|y \sim Be(a+y,b+n-y),
\]
\pause 
we can interpret the prior parameters in the following way:
\pause
\begin{itemize}[<+->]
\item $a$: prior successes
\item $b$: prior failures
\item $a+b$: prior sample size
\item $a/(a+b)$: prior mean
\end{itemize}
\pause
These interpretations may aid in construction of this prior for a given 
application.
\end{frame}



\begin{frame}
\frametitle{Posterior mean is a weighted average of prior mean and the MLE}

The posterior is $\theta|y \sim Be(a+y,b+n-y)$.
\pause
The posterior mean is 
\[ \begin{array}{rl} 
E[\theta|y] &= \frac{a+y}{a+b+n} \pause \\
&= \frac{a}{a+b+n} + \frac{y}{a+b+n} \pause \\
&= \frac{a+b}{a+b+n}\left(\frac{a}{a+b}\right) + \frac{n}{a+b+n}\left(\frac{y}{n}\right)
\end{array} \]
\pause
Thus, the posterior mean is a weighted average of the prior mean $a/(a+b)$ and 
the MLE $y/n$ with weights equal to the prior sample size ($a+b$) and the 
data sample size ($n$).
\end{frame}




\begin{frame}
\frametitle{Example data}

Assume $Y\sim Bin(n,\theta)$ and $\theta\sim Be(1,1)$ (which is equivalent to Unif(0,1)). 
\pause 
If we observe three successes ($y=3$) out of ten attempts ($n=10$). 
\pause 
Then our posterior is $\theta|y\sim Be(1+3,1+10-3) \stackrel{d}{=} Be(4,8)$.
\pause
The posterior mean is 
\[ 
E[\theta|y] = 
\frac{2}{12} \times \frac{1}{2} + \frac{10}{12} \times \frac{3}{10} = 
\frac{4}{12}.
\]

\vspace{0.5in} \pause

\begin{remark}
Note that a $Be(1,1)$ is equivalent to $p(\theta)=\I(0<\theta<1)$\pause, i.e. 

\[ p(\theta|y) \propto p(y|\theta)p(\theta) = p(y|\theta)\I(0<\theta<1) \]
\pause
so it may seem that a reasonable approach to a default prior is to replace $p(\theta)$ by a 1 (times the parameter constraint). \pause We will see later that this depends on the parameterization.
\end{remark}
\end{frame}





\begin{frame}[fragile]
\frametitle{Posterior distribution}

<<data, fig.width = 5.5>>=
n = 10
y = 3
a = b = 1
d = data.frame(x = seq(0,1,by=0.01)) %>% 
  mutate(prior = dbeta(x,a,b),
         "normalized likelihood" = dbeta(x, y, n-y),
         posterior = dbeta(x, a+y, b+n-y))

m <- d %>%
  pivot_longer(prior:posterior, 
               names_to = "Distribution",
               values_to = "density")

ggplot(m, 
       aes(
         x = x, 
         y = density, 
         group = Distribution, 
         linetype = Distribution, 
         color = Distribution)) +
  geom_line() +
  theme(legend.position="bottom") 
@
{\tiny Try it yourself at \url{https://jaradniemi.shinyapps.io/one_parameter_conjugate/}.}
\end{frame}






\frame{\frametitle{Point and interval estimation}
	Nothing inherently Bayesian about obtaining point and interval estimates. 
	
	\vspace{0.2in} \pause
	
	Point estimation requires specifying a loss (or utility) function. 
	
	\vspace{0.2in} \pause
	
	A $100(1-a)\%$ credible interval is any interval in the posterior that contains the parameter with probability $(1-a)$.
}




\subsection{Point estimation}
\frame{\frametitle{Point estimation}
	Define a loss (or utility) function $L\!\left(\theta,\hat{\theta}\right)=-U\!\left(\theta,\hat{\theta}\right)$ \pause where 
	\begin{itemize}
	\item $\theta$ is the parameter of interest \pause
	\item $\hat{\theta}=\hat{\theta}(y)$ is the estimator of $\theta$.  \pause
	\end{itemize}
	
	\vspace{0.1in} \pause 
	
	Find the estimator that minimizes the expected loss:
	\[ \hat{\theta}_{Bayes} = \mbox{argmin}_{\hat{\theta}} \,E\left[ \left. L\!\left(\theta,\hat{\theta}\right)\right|y \right] \] 
	\pause or maximizes expected utility. 
	
	\vspace{0.1in} \pause
	
	Common estimators: \pause
	\begin{itemize}
	\item Mean: $\hat{\theta}_{Bayes} = E[\theta|y]$ minimizes $L\!\left(\theta,\hat{\theta}\right) = \left(\theta-\hat{\theta}\right)^2$ \pause
	\item Median: $\int_{\hat{\theta}_{Bayes}}^\infty p(\theta|y) d\theta = \frac{1}{2}$ minimizes $L\!\left(\theta,\hat{\theta}\right) = \left|\theta-\hat{\theta}\right|$ \pause
	\item Mode: $\hat{\theta}_{Bayes} = \mbox{argmax}_\theta \, p(\theta|y)$ is obtained by minimizing $L\!\left(\theta,\hat{\theta}\right) = -\I\left(|\theta - \hat{\theta}|<\epsilon\right)$ as $\epsilon \to 0$, \pause also called \alert{maximum a posterior (MAP)} estimator.
	\end{itemize}
}


\frame{\frametitle{Mean minimizes squared-error loss}

	\begin{theorem}
	The mean minimizes expected squared-error loss. \pause
	\end{theorem}
	
	\begin{proof}
	Suppose $L\!\left(\theta,\hat{\theta}\right) = \left(\theta-\hat{\theta}\right)^2 \pause = \theta^2 -2\theta\hat{\theta} + \hat{\theta}^2$,
	\pause then 
	
	\[ \begin{array}{ll}
	E \left[\left. L\!\left(\theta,\hat{\theta}\right)\right|y\right] & = E\left[\theta^2|y\right] -2\hat{\theta}E[\theta|y] + \hat{\theta}^2 \pause \\ \\ 
	\frac{d}{d\hat{\theta}} E\left[\left. L\!\left(\theta,\hat{\theta}\right)\right|y\right] &= -2E[\theta|y] + 2\hat{\theta} \pause \stackrel{set}{=} 0 \pause \implies \hat{\theta} = E[\theta|y] \pause \\ \\
	\frac{d^2}{d\hat{\theta}^2} E\left[\left. L\!\left(\theta,\hat{\theta}\right)\right|y\right] &= 2 \pause
	\end{array} \]
	So $\hat{\theta} = E[\theta|y]$ minimizes expected squared-error loss. 
	\end{proof}
}



\begin{frame}[fragile]
\frametitle{Point estimation}

<<estimates, dependson='data'>>=
estimates = data.frame(mean = (a+y)/(a+b+n), 
              median = qbeta(.5, a+y, b+n-y),
              mode = (a+y-1)/(a+b+n-2))

ggplot(d, aes(x, posterior, group = 1)) +
  geom_line() +
  geom_vline(data = estimates %>% 
               pivot_longer(
                 everything(),
                 names_to = "estimator", 
                 values_to = "value"), 
             aes(
               xintercept = value, 
               color = estimator, 
               linetype = estimator),
             show.legend = TRUE) +
  theme(legend.position="bottom")
@

\end{frame}




\subsection{Interval estimation}
\frame{\frametitle{Interval estimation}
	\begin{definition}
	A $100(1-a)\%$ \alert{credible interval} is any interval (L,U) such that 
	\[ 1-a = \int_L^U p(\theta|y) d\theta. \]
	\end{definition}
	
	\vspace{0.2in} \pause 
	
	Some typical intervals \pause are 
	\begin{itemize}
	\item Equal-tailed: $a/2 = \int_{-\infty}^L p(\theta|y) d\theta = \int_U^\infty p(\theta|y) d\theta$ \pause
	\item One-sided: either $L=-\infty$ or $U=\infty$ \pause
	\item \alert{Highest posterior density (HPD)}: $p(L|y) = p(U|y)$ for a uni-modal posterior \pause which is also the shortest interval
	\end{itemize}
}



\begin{frame}[fragile]
\frametitle{Interval estimation}

<<intervals, dependson='data'>>=
library("pscl") 
hpd = pscl::betaHPD(a+y, b+n-y, 0.95)

interval = tibble::tribble(
  ~type, ~lb, ~ub, ~y,
  "equal", qbeta(.025, a+y, b+n-y), qbeta(.975, a+y, b+n-y), 0.1,
  "lower", 0, qbeta(.95, a+y, b+n-y), 0.2,
  "upper", qbeta(.05, a+y, b+n-y), 1, 0.3,
  "HPD", hpd[1], hpd[2], 0.4
)
# interval = data.frame(type = c('equal-tail',
#                                'lower tail',
#                                'higher tail',
#                                'highest posterior density (HPD)'),
#                       lb = c(qbeta(.025, a+y, b+n-y),
#                              0,
#                              qbeta(.05, a+y, b+n-y),
#                              hpd[1]),
#                       ub = c(qbeta(.975, a+y, b+n-y),
#                              qbeta(.95, a+y, b+n-y),
#                              1,
#                              hpd[2]),
#                       y = (1:4)/10)

ggplot(d, 
       aes(x = x, 
           y = posterior, 
           group = 1)) +
  geom_line(color='gray') +
  geom_segment(data = interval, 
               aes(
                 x = lb, 
                 xend = ub, 
                 y = y, 
                 yend = y,          
                 color = type, 
                 linetype = type)) + 
  theme(legend.position = "bottom") 
@

\end{frame}



\subsection{Simulation from the posterior}
\begin{frame}[fragile]
\frametitle{Simulation from the posterior}

An estimate of the full posterior can be obtained via simulation, i.e. 

<<simulation, dependson='data', echo=TRUE>>=
sim = data.frame(x = rbeta(10000, shape1 = a + y, shape2 = b + n - y))
@

<<simulation-plot, dependson='simulation', fig.width = 7>>=
ggplot(sim, aes(x=x)) + 
  geom_histogram(aes(y = after_stat(density)), binwidth=.01) + 
  stat_function(color = 'red', 
                fun = dbeta, 
                args = list(shape1 = a+y, 
                          shape2 = b+n-y)) +
  labs(x = expression(theta), 
       y = expression("p" * (theta ~ "|" ~ y))) +
  theme_bw()
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Estimates via simulation}

We can also obtain point and interval estimates using these simulations

<<quantiles, dependson='simulation', echo=TRUE>>=
round(c(mean = mean(sim$x), median = median(sim$x)),2)
round(quantile(sim$x, c(.025,.975)),2) # Equal-tail
round(c(quantile(sim$x, .05),1),2) # Upper
round(c(0,quantile(sim$x, .95)),2) # Lower
@

\end{frame}


\section{Priors}
\frame{\frametitle{Guess the probability}
What do you think the probability is?
  \begin{itemize}[<+->]
  \item A 6-sided die lands on 1.
  \item The first base pair in my chromosome 1 is A.
  \item Kansas City Chiefs win 2023 Super Bowl.
  \end{itemize}
}



\frame{\frametitle{What are priors?}
  \begin{definition}
  A \alert{prior probability distribution}, often called simply the \alert{prior}, of an uncertain quantity $\theta$ is the probability distribution that would express one's uncertainty about $\theta$ before the ``data'' is taken into account.
  \end{definition}
{\tiny \url{http://en.wikipedia.org/wiki/Prior_distribution}}
}

\subsection{Conjugate}
\frame{\frametitle{Priors}
  \begin{definition}
	A prior $p(\theta)$ is \alert{conjugate} if for $p(\theta)\in \mathcal{P}$ and $p(y|\theta)\in \mathcal{F}$, $p(\theta|y)\in \mathcal{P}$ where $\mathcal{F}$ and $\mathcal{P}$ are families of distributions.
	\end{definition}
	
	\vspace{0.05in} \pause  
	
	For example, the beta distribution ($\mathcal{P}$) is conjugate to the binomial distribution with unknown probability of success ($\mathcal{F}$) \pause since 
	\[ \theta \sim \alert{\mbox{Be}}(a,b) \qquad\mbox{and}\qquad \theta|y \sim \alert{\mbox{Be}}(a+y,b+n-y). \] 
	
	\pause
	
	\begin{definition}
	A \alert{natural} conjugate prior is a conjugate prior that has the same functional form as the likelihood.
	\end{definition}
	
	\vspace{0.05in} \pause 
	
	For example, the beta distribution is a natural conjugate prior since 
	\[ p(\theta) \propto \theta^{a-1}(1-\theta)^{b-1} \qquad \mbox{and} \qquad L(\theta) \propto \theta^y(1-\theta)^{n-y}. \]
}


\frame{\frametitle{Discrete priors are conjugate}
	\begin{theorem}
	Discrete priors are conjugate. \pause
	\end{theorem}
	
	\begin{proof}
	Suppose $p(\theta)$ is discrete, \pause i.e. 
	\[ P(\theta=\theta_i) = p_i \pause \qquad \sum_{i=1}^\I p_i = 1 \]
	\pause and $p(y|\theta)$ is the model. \pause Then, $P(\theta=\theta_i|y) = p_i'$ 
	is the posterior \pause with 
	\[ p_i' = \frac{p_i p(y|\theta_i)}{\sum_{j=1}^\I p_j p(y|\theta_j)} \propto p_i p(y|\theta_i). \]
	\end{proof}
}

\begin{frame}[fragile]
\frametitle{Discrete prior}
<<discrete>>=
dd = data.frame(theta = seq(0.01, 0.99, by=0.01)) %>%
  mutate(prior = dbeta(theta, 3, 2),
         prior = prior / sum(prior),
         posterior = prior * dbinom(y, n, theta),
         posterior = posterior / sum(posterior))

ggplot(dd %>% 
         pivot_longer(prior:posterior, 
                      names_to = "Distribution",
                      values_to = "density"),
       aes(x = theta, 
           y = density, 
           color = Distribution, 
           linetype = Distribution)) +
  geom_point() +
  theme(legend.position="bottom") 
@
\end{frame}


\frame{\frametitle{Discrete mixtures of conjugate priors are conjugate}
	\begin{theorem}
	Discrete mixtures of conjugate priors are conjugate. \pause
	\end{theorem}
	
	\begin{proof}
	Let $p_i = P(H_i)$ and $p_i(\theta) = p(\theta|H_i)$, 
	\[ \theta\sim \sum_{i=1}^\I p_i p_i(\theta) \pause \qquad \sum_{i=1}^\I p_i=1, \] \pause and $p_i(y) = \int p(y|\theta) p_i(\theta) d\theta$, \pause then 
  \[ \begin{array}{rl}
  p(\theta|y) 
  &= \frac{1}{p(y)} p(y|\theta)p(\theta) \pause
   = \frac{1}{p(y)} p(y|\theta) \sum_{i=1}^\I p_i p_i(\theta) \pause 
   = \frac{1}{p(y)} \sum_{i=1}^\I p_i p(y|\theta) p_i(\theta) \pause \\
  &= \frac{1}{p(y)} \sum_{i=1}^\I p_i p_i(y) p_i(\theta|y) \pause 
   = \sum_{i=1}^\I \frac{p_i p_i(y)}{p(y)}p_i(\theta|y) \pause 
   = \sum_{i=1}^\I \frac{p_i p_i(y)}{\sum_{j=1}^\I p_j p_j(y)} p_i(\theta|y)
  \end{array} \]
  where $p_i(\theta|y) = p(y|\theta)p_i(\theta)/p_i(y)$. 
	\end{proof}
}


\frame{\frametitle{Mixtures of conjugate priors are conjugate}
	Bottom line: if
	\[ \theta\sim \sum_{i=1}^\I p_i p_i(\theta) \pause \qquad \sum_{i=1}^\I p_i=1\] \pause and $p_i(y) = \int p(y|\theta) p_i(\theta) d\theta$, \pause then 
	\[ \theta|y \sim \sum_{i=1}^\I p_i' p_i(\theta|y) \pause \qquad p_i' \propto p_i p_i(y) \]
	\pause where $p_i(\theta|y)=p(y|\theta) p_i(\theta) / p_i(y)$. 
}



\begin{frame}
\frametitle{Mixture of beta distributions}

\small

  Recall, if $Y\sim Bin(n,\theta)$ and $\theta\sim \mbox{Be}(a,b)$, 
  \pause
  then the marginal likelihood is
  \[ \begin{array}{ll}
  p(y) &= \int p(y|\theta) p(\theta) d\theta \pause 
  = \int {n \choose y} \theta^y (1-\theta)^{n-y} \frac{\theta^{a-1}(1-\theta)^{b-1}}{\mbox{Beta}(a,b)} \\
  &= {n \choose y} \frac{1}{\mbox{Beta}(a,b)} \int  \theta^{a+y-1} (1-\theta)^{b+n-y-1} d\theta \\
  &= {n \choose y} \frac{\mbox{Beta}(a+y,b+n-y)}{\mbox{Beta}(a,b)} \quad y=0,\ldots,n 
  \end{array} \]
  which is called the beta-binomial distribution with parameters $a+y$ and 
  $b+n-y$.
  
  \pause
  
  If $Y\sim Bin(n,\theta)$ and 
  \[ \theta \sim p\, \mbox{Be}(a_1,b_1) + (1-p) \mbox{Be}(a_2,b_2), \]
  \pause
  then
  \[ \theta|y \sim p'\, \mbox{Be}(a_1+y,b_1+n-y) + (1-p') \mbox{Be}(a_2+y,b_2+n-y) \]
  \pause
  with 
  \[ 
  p' = \frac{p\, p_1(y)}{p\, p_1(y) + (1-p) p_2(y)} 
  \pause
  \qquad p_i(y) = {n \choose y} \frac{\mbox{Beta}(a_i+y,b_i+n-y)}{\mbox{Beta}(a_i,b_i)}
  \]
\end{frame}



\begin{frame}[fragile]
\frametitle{Mixture priors}
<<mixture>>=
p = 0.4
a = c(1,4)
b = c(3,2)
ppd = function(y,n,a,b) {
  exp(lchoose(n,y)+lbeta(a+y,b+n-y)-lbeta(a,b))
}
prior = function(theta,p,a,b) {
  p*dbeta(theta,a[1],b[1]) + (1-p)*dbeta(theta,a[2],b[2])
}
posterior = function(theta,p,a,b,y,n) {
  p = p*ppd(y,n,a[1],b[1])
  p = p/(p+(1-p)*ppd(y,n,a[2],b[2]))
  p*dbeta(theta,a[1]+y,b[2]+n-y) + (1-p)*dbeta(theta,a[2]+y,b[2]+n-y)
}

curve(posterior(x,p,a,b,y,n), col="red", lwd=2,
      main="Binomial, mixture of betas", ylab="Density", xlab=expression(theta))
curve(prior(x,p,a,b), col="blue", lwd=2, add=TRUE)

curve(p*dbeta(x,a[1],b[1]), col="blue", lty=2, add=TRUE)
curve((1-p)*dbeta(x,a[2],b[2]), col="blue", lty=2, add=TRUE)

curve(p*dbeta(x,a[1]+y,b[1]+n-y), col="red", lty=2, add=TRUE)
curve((1-p)*dbeta(x,a[2]+y,b[2]+n-y), col="red", lty=2, add=TRUE)

legend("topright", c("Prior","Posterior"), col=c("blue","red"), lwd=2)
@
\end{frame}



\subsection{Default priors}
\frame{\frametitle{Default priors}
  \begin{definition}
  A \alert{default} prior is used when a data analyst is unable or unwilling to specify an informative prior distribution. 
  \end{definition}
}


\frame{\frametitle{Default priors}
	Can we always use $p(\theta)\propto 1$? 
	
	\vspace{0.2in} \pause 
	
	Suppose we use $\phi = \log(\theta/[1-\theta])$, the log odds as our parameter, \pause and set $p(\phi) \propto 1$, \pause then the implied prior on $\theta$ \pause is 
	
	\[ \begin{array}{ll}
	p_\theta(\theta) \propto & 1 \left| \frac{d}{d\theta} \log(\theta/[1-\theta]) \right| \pause \\
	&= \frac{1-\theta}{\theta} \left[ \frac{1}{1-\theta} + \frac{\theta}{[1-\theta]^2} \right]  \\	
	&= \frac{1-\theta}{\theta} \left[ \frac{[1-\theta]+\theta}{[1-\theta]^2} \right]  \\
	&= \theta^{-1}[1-\theta]^{-1}
	\end{array} \]
	\pause 
	a Be(0,0), if that were a proper distribution, 
	\pause 
	and is different from setting $p(\theta)\propto 1$ which results in the 
	Be(1,1) prior.  
	\pause
	Thus, the constant prior is not invariant to the parameterization used.
}

\subsection{Jeffreys prior}
\begin{frame}
\frametitle{Fisher information background}

\small

\begin{definition}
\alert{Fisher information}, $\mathcal{I}(\theta)$, for a scalar parameter 
$\theta$ is the expectation of the second derivative of the log-likelihood, 
\pause
i.e.
\[
\mathcal{I}(\theta) = 
E\left[\left. \frac{\partial^2}{\partial\theta^2} \log p(y|\theta) \right|\theta\right].
\]
\end{definition}

\pause

\begin{theorem}[Casella \& Berger (2nd ed) Lemma 7.3.11]
For exponential families, 
\[
\mathcal{I}(\theta) = 
-E\left[\left. \left(\frac{\partial}{\partial\theta} \log p(y|\theta)\right)^2 \right|\theta\right].
\]
\end{theorem}

\pause

If $\theta=(\theta_1,\ldots,\theta_n)$, then the Fisher information is the expectation of the
Hessian matrix, which has the $i$th row and $j$th column that is the partial derivative
with respect to $\theta_i$ followed by the partial derivative with 
respect to $\theta_j$, of the log-likelihood.

\end{frame}



\frame{\frametitle{Jeffreys prior}
	\begin{definition}
	\alert{Jeffreys prior} is a prior that is invariant to parameterization \pause and is obtained  via 
	\[ p(\theta) \propto \sqrt{\mbox{det}\,  \mathcal{I}(\theta)} \]
	\pause where $\mathcal{I}(\theta)$ is the Fisher information. 
	\end{definition}
	
	\vspace{0.2in} \pause
	
	For example, for a binomial distribution $\mathcal{I}(\theta)=\frac{n}{\theta[1-\theta]}$, \pause so 
	\[ p(\theta) \propto \theta^{-1/2}(1-\theta)^{-1/2} =  \theta^{1/2-1}(1-\theta)^{1/2-1} \]
	\pause a Be(1/2,1/2) distribution. 
}



\begin{frame}
\frametitle{Fisher information}
  \begin{theorem}
  The Fisher information for $Y\sim Bin(n,\theta)$ is $\mathcal{I}(\theta) = \frac{n}{\theta(1-\theta)}$.
  \end{theorem}
  \begin{proof}
  Since the binomial is an exponential family, 
  
  \[ \begin{array}{ll}
  \mathcal{I}(\theta) &= -E_{y|\theta} \left[ \frac{\partial^2 }{\partial \theta^2} \log p(y|\theta) \right] \pause 
  = -E_{y|\theta} \left[ \frac{\partial^2 }{\partial \theta^2} \log {n\choose y} + y\log\theta + (n-y)\log(1-\theta)  \right] \pause \\
  &= -E_{y|\theta} \left[ \frac{\partial }{\partial \theta} \frac{y}{\theta}-\frac{n-y}{1-\theta}  \right]  
   = -E_{y|\theta} \left[ -\frac{y}{\theta^2}-\frac{n-y}{(1-\theta)^2}  \right]  
   = - \left[ -\frac{n\theta}{\theta^2}-\frac{n-n\theta}{(1-\theta)^2}  \right]  
   = \frac{n}{\theta}+\frac{n}{(1-\theta)} \\
  &= \frac{n}{\theta(1-\theta)}
  \end{array} \]
  \end{proof}
\end{frame}



\begin{frame}[fragile]
<<jeffreys>>=
n = 10
y = 3
a = b = .5
d = data.frame(x = seq(0,1, length.out = 1001)) %>% 
  mutate(prior = dbeta(x,a,b),
         "normalized likelihood" = dbeta(x, y, n-y),
         posterior = dbeta(x, a+y, b+n-y))

m <- d %>%
  pivot_longer(prior:posterior, 
               names_to = "Distribution", 
               values_to = "density")

ggplot(m, 
       aes(
         x = x, 
         y = density, 
         group = Distribution, 
         linetype = Distribution, 
         color = Distribution)) +
  geom_line() +
  theme_bw() +
  theme(legend.position="bottom")
@
\end{frame}

\subsection{Non-conjugate priors}
\frame{\frametitle{Non-conjugate priors}
  If $Y\sim Bin(n,\theta)$ and $p(\theta) = e^\theta/(e-1)$, then
  \[ 
  p(\theta|y) \propto f(\theta) = \theta^y (1-\theta)^{n-y} e^\theta
  \]
  which is not a known distribution. 
  
  \vspace{0.2in} \pause
  
  Options
  \begin{itemize}[<+->]
  \item Plot $f(\theta)$ (possibly multiplying by a constant). 
  \item Find $i = \int f(\theta) d\theta$, so that $p(\theta|y) = f(\theta)/i$. 
  \item Evaluate $f(\theta)$ on a grid and normalize by the grid spacing.
  \end{itemize}
}



\begin{frame}[fragile]
\frametitle{Plot of $f(\theta)$}
<<plot_f, dependson='data'>>=
f = function(theta) {
  theta^y*(1-theta)^(n-y)*exp(theta)
}
curve(f, col="red", lwd=2, 
      main="Binomial, nonconjugate prior", ylab="Density (proportional)", xlab=expression(theta))
legend("topright", c("Posterior"), col=c("red"), lwd=2)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Numerical integration}
Find $i = \int f(\theta) d\theta$, so that $p(\theta|y) = f(\theta)/i$. 

<<integrate, dependson=c('data','plot_f'), echo=TRUE>>=
(i = integrate(f, 0, 1))
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Nonconjugate prior, numerical integration}
<<nonconjugate_plot_normalized, dependson='integrate'>>=
curve(exp(x)/(exp(1)-1), col="blue", lwd=2, ylim=c(0,3), 
      main="Binomial, nonconjugate prior", ylab="Density", xlab=expression(theta))
curve(f(x)/i$value, add=TRUE, col="red", lwd=2)
legend("topright", c("Prior","Posterior"), col=c("blue","red"), lwd=2)
@
\end{frame}




\begin{frame}[fragile]
\frametitle{Nonconjugate prior, evaluated on a grid}
<<nonconjugate_grid, fig.width=9>>=
w = 0.01
theta = seq(w/2, 1-w/2, by=w)
d = f(theta)
d = d/sum(d)/w
plot(theta, d, type="l", col="red", lwd=2,  
      main="Binomial, nonconjugate prior", ylab="Density", xlab=expression(theta))
curve(exp(x)/(exp(1)-1), col="blue", lwd=2, add=TRUE)
legend("topright", c("Prior","Posterior"), col=c("blue","red"), lwd=2)
@

<<echo=TRUE>>=
theta[c(which(cumsum(d)*w>0.025)[1]-1, which(cumsum(d)*w>0.975)[1])] # 95\% CI
@
\end{frame}

\subsection{Improper priors}
\frame{\frametitle{Improper priors}
  \begin{definition}
	An unnormalized density, $f(\theta)$, is \alert{proper} if $\int f(\theta) d\theta = c < \infty$, and otherwise it is \alert{improper}. 
	\end{definition}
	
	\vspace{0.2in} \pause
	
	To create a normalized density from a proper unnormalized density, use 
	\[ p(\theta|y) = \frac{f(\theta)}{c} \pause \]
	to see that $p(\theta|y)$ is a proper normalized density \pause note that $c=\int f(\theta) d\theta$ is not a function of $\theta$\pause , then 
	\[ \int p(\theta|y) d\theta \pause = \int \frac{f(\theta)}{\int f(\theta) d\theta} d\theta \pause = \int \frac{f(\theta)}{c} d\theta \pause = \frac{1}{c} \int f(\theta) d\theta \pause = \frac{c}{c} \pause = 1 \]
}




\frame{\frametitle{Be(0,0) prior}
  Recall that $\mbox{Be}(a,b)$ is a proper probability distribution if $a>0,b>0$. 
  
  \vspace{0.2in} \pause
  
  Suppose $Y\sim Bin(n,\theta)$ and $p(\theta) \propto \theta^{-1}(1-\theta)^{-1}$, i.e. the kernel of a $Be(0,0)$ distribution. \pause This is an improper distribution.
  
  \vspace{0.2in} \pause
  
  The posterior, $\theta|y \sim Be(y,n-y)$, is proper if $0<y<n$.
}

\end{document}
