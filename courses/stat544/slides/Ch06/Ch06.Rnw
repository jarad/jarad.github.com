\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Model checking}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library(reshape2)
library(plyr)
library(dplyr)

library(ggplot2)
library(xtable)

library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
@

<<set_seed>>=
set.seed(1)
@



\frame{\maketitle}

\begin{frame}
\frametitle{Outline}

We assume $p(y|\theta)$ and $p(\theta)$, so it would be prudent to determine if these assumptions are reasonable. 

\vspace{0.2in} \pause

\begin{itemize}
\item (Prior) sensitivity analysis 
\item Posterior predictive checks
  \begin{itemize}
  \item Graphical checks
  \item Posterior predictive pvalues
  \end{itemize}
\end{itemize}
\end{frame}




\section{Prior sensitivity analysis}
\begin{frame}
\frametitle{Prior sensitivity analysis}

Since a prior specifies our prior belief, we may want to check to determine whether our conclusions would change if we held different prior beliefs. \pause Suppose a particular scientific question can be boiled down to 
\[ Y_i \stackrel{ind}{\sim} Ber(\theta) \]
\pause 
and that there is wide disagreement about the value for $\theta$ such that the following might reasonably characterize different individual beliefs before the experiment is run: \pause
\begin{itemize}
\item Skeptic: $\theta\sim Be(1,100)$ 
\item Agnostic: $\theta\sim Be(1,1)$
\item Believer: $\theta\sim Be(100,1)$
\end{itemize}
\pause
An experiment is run and the posterior under these different priors are compared. 

\end{frame}


\begin{frame}
\frametitle{Posteriors}

<<>>=
prior = data.frame(a=c(1,1,100), b=c(100,1,1), 
                   type=factor(c("skeptic","agnostic","believer"),
                               c("skeptic","agnostic","believer")),
                   n=10^rep(1:6, each=3))

p = 0.9
d = ddply(prior, .(type, n), function(x) {
  data.frame(xx=seq(0,1,by=0.01)) %>%
    mutate(yy=dbeta(xx, x$a+p*x$n, x$b+(1-p)*x$n))
})

ggplot(d, aes(x=xx,y=yy,col=type, linetype=type)) + 
  geom_line() +
  facet_wrap(~n, scales='free') + 
  labs(x=expression(theta), y=expression(paste("p(",theta,"|y)"))) +
  theme_bw() +
  theme(legend.position="bottom")
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Hierarchical variance prior}

Recall the normal hierarchical model
\[ y_i \stackrel{ind}{\sim} N(\theta_i,s_i^2), \quad \theta_i\stackrel{ind}{\sim} N(\mu,\tau^2) \]
\pause 
which results in the posterior distribution for $\tau$ of 
\[ p(\tau|y) \propto p(\tau) V_\mu^{1/2} \prod_{i=1}^\I (s_i^2+\tau^2)^{-1/2} \exp\left( -\frac{(y_{i}-\hat{\mu})^2}{2(s_i^2+\tau^2)} \right) \]

 \pause

As an attempt to be non-informative, consider an $IG(\epsilon,\epsilon)$ prior for $\tau^2$. \pause As an alternative, consider $\tau \sim Unif(0,C)$ or $\tau\sim Ca^+(0,C)$ \pause where $C$ is problem specific, but is chosen to be relatively large for the particular problem. 

 \pause

The \href{https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started}{8-schools example} has the following data:

<<results='asis'>>=
d = data.frame(y = c(28,  8, -3,  7, -1,  1, 18, 12),
               s = c(15, 10, 16, 11,  9, 11, 10, 18))
print(xtable(t(d), digits=0))
@


\end{frame}

\begin{frame}
\frametitle{Posterior for 8-schools example}
Reproduction of \href{http://projecteuclid.org/euclid.ba/1340371048}{Gelman 2006}:
<<analysis, echo=FALSE, fig.width=8, warning=FALSE>>=
dinvgamma = function(x, a, b) dgamma(1/x,a,b)/x^2
dsqrtinvgamma = function(x, a, b) dinvgamma(x^2, a, b)*2*x

tau_log_posterior = function(tau, y, si2) 
{
  spt = si2+tau^2
  Vmu = 1/sum(1/spt)
  mu = sum(y/spt)*Vmu
  0.5*log(Vmu)+sum(-0.5*log(spt)-(y-mu)^2/(2*spt))
}

V_tau_log_posterior = Vectorize(tau_log_posterior,"tau")



h = 0.001
e = 0.001

df = data.frame(x = seq(0,20-h, by=h)+h/2) %>%
  mutate(log_like = V_tau_log_posterior(x, d$y, d$s^2),
         'IG(1,1)'    = log(dsqrtinvgamma(x,1,1)),
         'IG(e,e)'    = log(dsqrtinvgamma(x,e,e)),
         'Unif(0,20)' = 0,
         'Ca^*(0,5)'  = dcauchy(x, 0, 10, log=TRUE)) %>%
  melt(id.var=c('x','log_like'), 
       variable.name = 'Distribution', 
       value.name = 'log_prior') %>%
  mutate(log_posterior = log_like + log_prior) %>%
  group_by(Distribution) %>% 
  mutate(prior = exp(log_prior-max(log_prior)),
         prior = prior/sum(prior)/h,
         posterior = exp(log_posterior-max(log_posterior)),
         posterior = posterior/sum(posterior)/h) %>%
  melt(measure.vars = c('prior','posterior'), 
       value.name = 'density')
  
ggplot(df, aes(x, density, color=variable, linetype=variable, group=variable)) +
  geom_line() + 
  facet_wrap(~Distribution, scales='free') +
  theme_bw() + 
  theme(legend.position="bottom")
@
\end{frame}


\begin{frame}
\frametitle{Summary}

For a default prior on a variance ($\sigma^2$) or standard deviation ($\sigma$), use 

\begin{enumerate}
\item Easy to remember.
  \begin{itemize}
  \item Half-Cauchy on the standard deviation ($\sigma \sim Ca^+(0,C)$). \pause
  \end{itemize}
\item Harder to remember
  \begin{itemize}
  \item Data-level variance 
    \begin{itemize}
    \item Use default prior ($p(\sigma^2)\propto 1/\sigma^2$) \pause 
    \end{itemize}
  \item Hierarchical standard deviation
    \begin{itemize}
    \item Use uniform prior (Unif$(0,C)$) if there are enough reps (5 or more) of that parameter. \pause 
    \item Use half-Cauchy prior ($Ca^+(0,C)$) otherwise.
    \end{itemize}
  \end{itemize}
\end{enumerate}

\vspace{0.2in} \pause

{\footnotesize

When assigning the values for $C$
\begin{itemize}
\item For a uniform prior (Unif$(0,C)$) make sure $C$ is large enough to capture any reasonable value for the standard deviation.
\item For a half-Cauchy prior ($Ca^+(0,C)$) err on the side of making $C$ too small since the heavy tails will let the data tell you if the standard deviation needs to be larger whereas a value of $C$ that is too large will put too much weight toward large values of the standard deviation and make the prior more informative.
\end{itemize}
}
\end{frame}


\section{Posterior predictive checks}
\begin{frame}
\frametitle{Posterior predictive checks}
  Let $y^{rep}$ be a replication of $y$, \pause then 
  \[ p(y^{rep}|y) = \int p(y^{rep}|\theta,y) p(\theta|y) d\theta \pause = \int p(y^{rep}|\theta) p(\theta|y) d\theta. \]
  \pause where $y$ is the observed data and $\theta$ are the model parameters.
  
  \vspace{0.2in} \pause
  
  To simulate a full replication:
  \begin{enumerate}[<+->]
  \item Simulate $\theta^{(j)}\sim p(\theta|y)$ and 
  \item Simulate $y^{rep,j} \sim p(y|\theta^{(j)})$.
  \end{enumerate}
  
  \vspace{0.2in} \pause 
  
  To assess model adequacy:
  \begin{itemize}
  \item Compare plots of replicated data to the observed data. \pause
  \item Calculate posterior predictive pvalues.
  \end{itemize}
\end{frame}



\subsection{Airline example{}}
\begin{frame}[fragile]
\frametitle{Airline accident data}
  Let 
  \begin{itemize}
  \item $y_i$ be the number of fatal accidents in year $i$
  \item $x_i$ be the number of 100 million miles flown in year $i$
  \end{itemize}
  \pause
  Consider the model 
  \[ Y_i \stackrel{ind}{\sim} Po(x_i\lambda) \qquad p(\lambda)\propto 1/\sqrt{\lambda}. \]
  \pause
\footnotesize
<<airline_accident_data, results='asis'>>=
d = data.frame(year=1976:1985, 
              fatal_accidents = c(24,25,31,31,22,21,26,20,16,22),
              passenger_deaths = c(734,516,754,877,814,362,764,809,223,1066),
              death_rate = c(0.19,0.12,0.15,0.16,0.14,0.06,0.13,0.13,0.03,0.15))
d$miles_flown = d$passenger_deaths/d$death_rate # 100 million miles
print(xtable(d, digits=0, include.rownames=FALSE))
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Posterior replications of the data}
  Under Jeffreys prior, the posterior is 
  \[ \lambda|y \sim Ga(0.5+n\overline{y}, n\overline{x}). \]
  So to obtain a replication of the data, do the following \pause
  \begin{enumerate}
  \item $\lambda^{(j)} \sim Ga(0.5+n\overline{y}, n\overline{x})$ \pause and 
  \item $y^{rep,j}_i \stackrel{ind}{\sim} Po(x_i\lambda^{(j)})$ for $i=1,\ldots,n$.
  \end{enumerate}

<<airline_fatalities>>=
# Jeffreys prior
a = 0.5 + sum(d$fatal_accidents)
b = 0   + sum(d$miles_flown)

# Replicate data
rep = rdply(19, { 
  tmp=d
  lambda = rgamma(1,a,b)
  tmp$fatal_accidents = rpois(nrow(tmp), lambda*tmp$miles_flown)
  tmp 
})
id = sample(20,1) # Choose a random spot to insert observed data set
if (id==20) {
  d$.n = 20
  rep = rbind(rep,d)
} else {
  rep$.n[rep$.n==id] = 20
  d$.n = id
  rep = rbind(rep,d) 
}
@
\end{frame}



\begin{frame}[fragile]
<<histograms, message=FALSE>>=
ggplot(rep, aes(x=fatal_accidents, fill=factor(1))) + 
  geom_histogram(binwidth=1) +
  facet_wrap(~.n) +
  theme_bw() +
  theme(legend.position="none")
@
\end{frame}



\begin{frame}[fragile]
<<histograms_observed, message=FALSE>>=
ggplot(rep, aes(x=fatal_accidents, fill=.n==id)) + 
  geom_histogram(binwidth=1) +
  facet_wrap(~.n) +
  theme_bw() +
  theme(legend.position="none")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{How might this model not accurately represent the data?}
  Let 
  \begin{itemize}
  \item $y_i$ be the number of fatal accidents in year $i$
  \item $x_i$ be the number of 100 million miles flown in year $i$
  \end{itemize}
  
  Consider the model 
  \[ Y_i \stackrel{ind}{\sim} Po(x_i\lambda) \qquad p(\lambda)\propto 1/\sqrt{\lambda}. \]
  
\footnotesize
<<results='asis'>>=
print(xtable(d, digits=0, include.rownames=FALSE))
@
\end{frame}



\begin{frame}[fragile]
<<scatterplot>>=
# change random data location
old_id = id
id = sample(20,1) # Choose a random spot to insert observed data set
rep$.n[rep$.n==old_id] = 21
rep$.n[rep$.n==id    ] = old_id
rep$.n[rep$.n==21    ] = id


g = ggplot(rep, aes(x=year, y=fatal_accidents, col=factor(1))) + 
  geom_point() + 
  facet_wrap(~.n) +
  theme_bw() +
  theme(legend.position="none")
g
@
\end{frame}


\begin{frame}[fragile]
<<scatterplot_with_regression_line>>=
g = g + stat_smooth(method="lm", formula=y~x)
g
@
\end{frame}


\begin{frame}[fragile]
<<scatterplot_with_observed>>=
g + aes(col=.n==id)
@
\end{frame}



\subsection{Posterior predictive pvalues}
\begin{frame}
\frametitle{Posterior predictive pvalues}
  To quantify the discrepancy between observed and replicated data: \pause
  \begin{enumerate}
  \item Define a test statistic $T(y,\theta)$. \pause
  \item Define the posterior predictive pvalue
  \[ p_B = P(T(y^{rep},\theta)\ge T(y,\theta)|y) \]
  \pause where $y^{rep}$ and $\theta$ are random. \pause Typically this pvalue is calculated via simulation, \pause i.e. 
  \[ \begin{array}{rl}
  p_B
  &= E_{y^{rep},\theta}[\mathrm{I}(T(y^{rep},\theta)\ge T(y,\theta))|y] \pause \\
  &= \int \int \mathrm{I}(T(y^{rep},\theta)\ge T(y,\theta)) p(y^{rep}|\theta) p(\theta|y) dy^{rep} d\theta \pause \\
  &\approx \frac{1}{J} \sum_{j=1}^J \mathrm{I}(T(y^{rep,j},\theta^{(j)})\ge T(y,\theta^{(j)})) 
  \end{array} \]
  \pause where $\theta^{(j)} \sim p(\theta|y)$ and $y^{rep,j} \sim p(y|\theta^{(j)})$. 
   
  \end{enumerate}
  \pause Small \alert{or large} pvalues are (possible) cause for concern. 
\end{frame}






\begin{frame}[fragile]
\frametitle{Posterior predictive pvalue for slope}
  Let 
  \[ Y_i^{obs} = \beta_0^{obs}+\beta_1^{obs}\, i \]
  where 
  \begin{itemize}
  \item $Y_i^{obs}$ is the observed number of fatal accidents in year $i$ \pause and 
  \item $\beta_1^{obs}$ be the test statistic. 
  \end{itemize}
  
  \vspace{0.2in} \pause 

  Now, generate replicate data $y^{rep}$ and fit 
  \[ Y_i^{rep} = \beta_0^{rep} + \beta_1^{rep} i.\]
  \pause
  Now compare $\beta_1^{obs}$ to the distribution of $\beta_1^{rep}$.

<<slope, cache=TRUE>>=
rep = rdply(1000, { 
  tmp=d
  lambda = rgamma(1,a,b)
  tmp$fatal_accidents = rpois(nrow(d), lambda*d$miles_flown)
  data.frame(beta1 = coefficients(lm(fatal_accidents~year, tmp))[2])
})
observed_slope = coefficients(lm(fatal_accidents~year,d))[2]
@
\end{frame}



\begin{frame}[fragile]
<<slope_plot, dependson="slope", fig.width=8, echo=TRUE>>=
mean(rep$beta1>observed_slope) 
ggplot(rep, aes(x=beta1)) + geom_histogram(binwidth=0.1) + 
  geom_vline(xintercept=observed_slope, color="red") +
  theme_bw()
@
\end{frame}


\subsection{Update model}
\begin{frame}
\frametitle{Consider a linear model for the $\lambda_i$}
  Consider the model 
  \[ \begin{array}{rl}
  Y_i &\stackrel{ind}{\sim} Po(x_i \lambda_i) \pause  \\
  \log(\lambda_i) &= \beta_0+\beta_1 i
  \end{array} \]
  \pause where
  \begin{itemize}[<+->]
  \item $Y_i$ is the number of fatal accidents in year $i$
  \item $x_i$ is the number of 100 million miles flown in year $i$
  \item $\lambda_i$ is the accident rate in year $i$
  \end{itemize}
  
  \vspace{0.2in} \pause
  
  Here the $\lambda_i$ are a deterministic function of year, but (possibly) different each year.
\end{frame}




\begin{frame}[fragile]
\frametitle{Stan linear model for accident rate}
<<stan, echo=TRUE, results='hide', cache=TRUE>>=
model = "
data {
  int<lower=0> n;
  int<lower=0> y[n];
  vector<lower=0>[n] x;
}
transformed data {
  vector[n] log_x;
  log_x = log(x); # both x and logx need to be vectors 
}
parameters {
  real beta[2];
}
transformed parameters {
  vector[n] log_lambda;
  for (i in 1:n) log_lambda[i] = beta[1] + beta[2]*i;
}
model {
  y ~ poisson_log(log_x + log_lambda); # _log indicates mean on log scale
}
"

m = stan_model(model_code = model)
r = sampling(m, list(n=nrow(d), y=d$fatal_accidents, x=d$miles_flown))
@
\end{frame}

\begin{frame}[fragile]
<<stan_posteriors, dependson='stan'>>=
r
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Posterior predictive pvalue: slope}
<<stan_rep, dependson='stan', cache=TRUE>>=
log_lambda = extract(r, "log_lambda")[["log_lambda"]]
reps = adply(log_lambda, 1, function(a) {
  d = data.frame(yrep = rpois(nrow(d), d$miles_flown * exp(a)),
             year = 1:10)
})

rep_slopes = daply(reps, .(iterations), function(d) {
  slopes=coefficients(lm(yrep~year, d))[2]
})
@

<<echo=TRUE, dependson='stan_rep'>>=
# Posterior predictive pvalue: slope
mean(rep_slopes>observed_slope)
@

<<fig.width=8, dependson='stan_rep'>>=
ggplot(data.frame(slopes=rep_slopes), aes(x=slopes)) + 
  geom_histogram(binwidth=0.1) + 
  geom_vline(xintercept = observed_slope, color='red') +
  theme_bw()
@
\end{frame}




\end{document}
