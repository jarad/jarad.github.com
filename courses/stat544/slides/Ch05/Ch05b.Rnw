\documentclass[handout,aspectratio=169]{beamer}

\usepackage{verbatim}

\input{../frontmatter}
\input{../commands}

\title{Hierarchical models (cont.)}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA,
               fig.width=5, fig.height=3,
               size='tiny',
               out.width='0.8\\textwidth',
               fig.align='center',
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<packages, message=FALSE, warning=FALSE, cache=FALSE>>=
library("tidyverse"); theme_set(theme_bw())
library(RColorBrewer)
library(xtable)

# library(rstan)
# rstan_options(auto_write = TRUE)
# options(mc.cores = parallel::detectCores())
@

<<set-seed>>=
set.seed(1)
@

\frame{\maketitle}

\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Theoretical justification for hierarchical models
  \begin{itemize}
  \item Exchangeability
  \item de Finetti's theorem
  \item Application to hierarchical models
  \end{itemize}
\item Normal hierarchical model
  \begin{itemize}
  \item Posterior
  \item Simulation study
  \item Shrinkage
  \end{itemize}
\end{itemize}
\end{frame}


\section{Theoretical justification for hierarchical models}
\subsection{Exchangability}
\begin{frame}
\frametitle{Exchangeability}
  \begin{definition}
	The set $Y_1,Y_2,\ldots,Y_n$ is \alert{exchangeable} if the joint probability $p(y_1,\ldots,y_n)$ is invariant to permutation of the indices. That is, for any permutation $\pi$,
	\[ p(y_1,\ldots,y_n) = p(y_{\pi_1},\ldots,y_{\pi_n}). \]
	\end{definition}

	\vspace{0.2in} \pause

	An exchangeable but not iid example: \pause
	\begin{itemize}
	\item Consider an urn with one red ball and one blue ball with probability 1/2 of drawing either. \pause
	\item Draw without replacement from the urn. \pause
	\item Let $Y_i=1$ if the $i$th ball is red and otherwise $Y_i=0$. \pause
	\item Since $1/2=P(Y_1=1,Y_2=0) \pause = P(Y_1=0,Y_2=1)=1/2$\pause , $Y_1$ and $Y_2$ are exchangeable. \pause
	\item But $0=P(Y_2=1|Y_1=1) \pause \ne P(Y_2=1)=1/2$ \pause and thus $Y_1$ and $Y_2$ are not independent.
	\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Exchangeability}
	\begin{theorem}
	All independent and identically distributed random variables are exchangeable.
	\end{theorem} \pause
	\begin{proof}
	Let $y_i \stackrel{iid}{\sim} p(y)$\pause, then
	\[
	p(y_1,\ldots,y_n) \pause = \prod_{i=1}^n p(y_i) \pause = \prod_{i=1}^n p(y_{\pi_i}) \pause = p(y_{\pi_1},\ldots,y_{\pi_n})
	\]
	\end{proof}

  \pause

  \begin{definition}
	The sequence $Y_1,Y_2,\ldots$ is \alert{infinitely exchangeable} if, for any $n$, $Y_1,Y_2,\ldots,Y_n$ are exchangeable.
	\end{definition}
\end{frame}



\subsection{de Finetti's theorem}
\begin{frame}
\frametitle{de Finetti's theorem}
\small
	\begin{theorem}
	A sequence of random variables ($y_1,y_2,\ldots$) is infinitely exchangeable iff\pause, for all $n$,
	\[ p(y_1,y_2,\ldots,y_n) = \int \prod_{i=1}^n p(y_i|\theta) P(d\theta), \]
	for some measure $P$ on $\theta$.
	\end{theorem}
	\pause If the distribution on $\theta$ has a density, we can replace $P(d\theta)$ with $p(\theta)d\theta$.

	\vspace{0.1in} \pause

	This means that there must exist \pause
	\begin{itemize}
	\item a parameter $\theta$\pause,
	\item a likelihood $p(y|\theta)$ \pause such that $y_i \stackrel{ind}{\sim} p(y|\theta)$\pause, and
	\item a distribution $P$ on $\theta$.
	\end{itemize}
\end{frame}



\subsection{Hierarchical models}
\begin{frame}
\frametitle{Application to hierarchical models}
	Assume $(y_1,y_2,\ldots)$ are infinitely exchangeable\pause, then by de Finetti's theorem for the $(y_1,\ldots,y_n)$ that you actually observed\pause, there exists
	\begin{itemize}
	\item a parameter $\theta$\pause,
	\item a distribution $p(y|\theta)$ \pause such that $y_i\stackrel{ind}{\sim} p(y|\theta)$\pause, and
	\item a distribution $P$ on $\theta$.
	\end{itemize}
	\pause Assume $\theta=(\theta_1,\theta_2,\ldots)$ \pause with $\theta_i$ infinitely exchangeable. \pause By de Finetti's theorem for $(\theta_1,\ldots,\theta_n)$\pause, there exists
	\begin{itemize}
	\item a parameter $\phi$\pause,
	\item a distribution $p(\theta|\phi)$ \pause such that $\theta_i\stackrel{ind}{\sim} p(\theta|\phi)$\pause, and
	\item a distribution $P$ on $\phi$.
	\end{itemize}
	\pause Assume $\phi=\phi$ \pause with $\phi \sim p(\phi)$.
\end{frame}


\subsection{Covariate information}
\begin{frame}
\frametitle{Exchangeability with covariates}
	Suppose we observe $y_i$ observations and $x_i$ covariates for each unit $i$.  Now we assume $(y_1,y_2,\ldots)$ are infinitely exchangeable given $x_i$, then by de Finetti's theorem for the $(y_1,\ldots,y_n)$, there exists
	\begin{itemize}
	\item a parameter $\theta$,
	\item a distribution $p(y|\theta,\alert{x})$  such that $y_i\stackrel{ind}{\sim} p(y|\theta,\alert{x_i})$, and
	\item a distribution $P$ on $\theta$ given $\alert{x}$.
	\end{itemize}
	 Assume $\theta=(\theta_1,\theta_2,\ldots)$  with $\theta_i$ infinitely exchangeable given $\alert{x}$.  By de Finetti's theorem for $(\theta_1,\ldots,\theta_n)$, there exists
	\begin{itemize}
	\item a parameter $\phi$,
	\item a distribution $p(\theta|\phi,\alert{x})$  such that $\theta_i\stackrel{ind}{\sim} p(\theta|\phi,\alert{x_i})$, and
	\item a distribution $P$ on $\phi$ given $\alert{x}$.
	\end{itemize}
	 Assume $\phi=\phi$  with $\phi \sim p(\phi|\alert{x})$.
\end{frame}

\section{Summary}
\begin{frame}
\frametitle{Summary}
	Hierarchical model:
	\[ y_i\stackrel{ind}{\sim} p(y|\theta_i), \qquad \theta_i \stackrel{ind}{\sim} p(\theta|\phi), \qquad \phi \sim p(\phi) \]
	\pause Hierarchical linear model:
	\[ y_i\stackrel{ind}{\sim} p(y|\theta_i,x_i), \qquad \theta_i \stackrel{ind}{\sim} p(\theta|\phi,x_i), \qquad \phi \sim p(\phi|x) \]

	\vspace{0.2in} \pause

Although hierarchical models are typically written using the conditional
independence notation above\pause,
the assumptions underlying the model are exchangeability \pause
and functional forms for the priors.
\end{frame}



\section{Normal hierarchical models}
\begin{frame}
\frametitle{Normal hierarchical models}
  Suppose we have the following model
	\[ \begin{array}{rl}
	y_{ij} &\stackrel{ind}{\sim} N(\theta_i, \sigma^2) \pause \\
	\theta_i &\stackrel{iid}{\sim} N(\mu,\tau^2) \pause \\

	\end{array} \]
	\pause with $j=1,\ldots,n_i$, $i=1,\ldots,\I$, and $n=\sum_{i=1}^\I n_i$. \pause This is a normal hierarchical model.

	\vspace{0.2in} \pause

	Make the following assumptions for computational reasons:
	\begin{itemize}
	\item Let $\sigma^2=s^2$ be known.
	\item Assume $p(\mu,\tau) \propto p(\mu|\tau)p(\tau) \propto p(\tau)$,
	i.e. assume an improper uniform prior on $\mu$.
	\end{itemize}
\end{frame}



\subsection{Posterior}
\begin{frame}
\frametitle{Posterior distribution}
	The posterior is
	\[ p(\theta,\mu,\tau|y) \pause \propto p(y|\theta)p(\theta|\mu,\tau) p(\mu|\tau)p(\tau) \]
	\pause but the decomposition
	\[ p(\theta,\mu,\tau|y) = p(\theta|\mu,\tau,y) p(\mu|\tau,y) p(\tau|y) \]
	\pause where
	\[ \begin{array}{ll}
	p(\theta|\mu,\tau,y) &\propto p(y|\theta)p(\theta|\mu,\tau) \pause \\
	p(\mu|\tau,y) & \propto \int p(y|\theta)p(\theta|\mu,\tau) d\theta\, p(\mu|\tau) \pause \\
	p(\tau|y) &\propto \int p(y|\theta)p(\theta|\mu,\tau)p(\mu|\tau) d\theta d\mu \, p(\tau)
	\end{array} \]
	\pause will aide computation \pause via
	\begin{enumerate}[1.]
	\item $\tau^{(k)} \sim p\left(\tau|y\right)$ \pause
	\item $\mu^{(k)} \sim p\left(\mu|\tau^{(k)},y\right)$ \pause
	\item $\theta_i^{(k)} \sim p\left(\theta|\mu^{(k)},\tau^{(k)},y\right)$ for $i=1,\ldots,\I$.
	\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Posterior distributions}
	The necessary conditional and marginal posteriors are presented in section 5.4 of BDA. Let
  \[ \overline{y}_{i\cdot } = \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij} \qquad \mbox{and} \qquad s_i^2 =s^2/n_i \]
\pause
Then
	\[ \begin{array}{rlrl}
	p(\tau|y) &\multicolumn{3}{c}{\propto p(\tau) V_\mu^{1/2} \prod_{i=1}^\I (s_i^2+\tau^2)^{-1/2} \exp\left( -\frac{(\overline{y}_{i\cdot }-\hat{\mu})^2}{2(s_i^2+\tau^2)} \right)} \pause \\
	\mu|\tau,y &\sim N(\hat{\mu},V_\mu)  \pause\\
	\theta_i|\mu,\tau,y &\sim N(\hat{\theta}_i,V_i) \pause\\ \\
	V_\mu^{-1} &= \sum_{i=1}^\I \frac{1}{s_i^2+\tau^2} \pause &
	\hat{\mu} &= V_\mu\left( \sum_{i=1}^\I \frac{\overline{y}_{\cdot i}}{s_i^2+\tau^2} \right) \pause\\
	V_i^{-1} &= \frac{1}{s_i^2}+\frac{1}{\tau^2} \pause &
	\hat{\theta}_i &= V_i \left( \frac{\overline{y}_{i\cdot }}{s_i^2}+\frac{\mu}{\tau^2} \right)
	\end{array} \]
\end{frame}


\subsection{Simulation study}
\begin{frame}
\frametitle{Simulation study}
Common to both simulation scenarios:
	\begin{itemize}
	\item $\I=10$
	\item $n_i=9$ for all $i$
	\item $s=1$ thus $s_i = 1/3$ for all $i$
	\end{itemize}

	\vspace{0.2in} \pause

Scenarios:
	\begin{enumerate}[1.]
	\item Common mean: $\theta_i=0$ for all $i$ \pause
	\item Group-specific means: $\theta_i=i-(\I/2+.5)$
	\end{enumerate}


	\vspace{0.2in} \pause

  Use $\tau \sim Ca^+(0,1)$.
\end{frame}

\begin{frame}[fragile]
\frametitle{Simulation study}
<<data, echo=TRUE>>=
J <- 10
n_per_group <- 9
n <- rep(n_per_group,J)
sigma <- 1
N <- sum(n)
group <- rep(1:J, each=n_per_group)

set.seed(1)
df <- bind_rows(data.frame(group = factor(group),
                           simulation = "common_mean",
                           y = rnorm(N                )), # All means are the same
                data.frame(group = factor(group),
                           simulation = "group_specific_mean",
                           y = rnorm(N, group-(J/2+.5)))) # Each group has its own mean
@
\end{frame}

\begin{frame}[fragile]
<<data-plot, dependson='data'>>=
ggplot(df, aes(x = group, y = y, color = group)) +
  geom_point() +
  facet_wrap(~simulation) +
  theme_bw()
@
\end{frame}




\begin{frame}[fragile]
\frametitle{Summary statistics}
\tiny
<<exploratory-analysis, dependson='data', results='asis'>>=
d_sum <- df |>
  group_by(simulation, group) |>
  summarize(
    n       = n(),
    mean    = mean(y),
    sd      = sd(y),
    .groups = "drop"
  )

tab <- xtable(d_sum)
print(tab,
      hline.after=c(-1,0,J,2*J),
      include.rownames = FALSE)
@
\end{frame}


\subsection{Sampling on a grid}
\begin{frame}
\frametitle{Sampling on a grid}

Consider samping from an arbitrary unnormalized density $f(\tau)\propto p(\tau|y)$ using the following approach
\begin{enumerate}[<+->]
\item Construct a step-function approximation to this density:
  \begin{enumerate}
  \item Determine an interval $[L,U]$ such that outside this interval $f(\tau)$ is small.
  \item Set an interval half-width $h$ to generate a grid of $M$ points ($x_1,\ldots,x_M$) in this interval, i.e.
  \[ x_1 = L+h \mbox{ and } x_m = x_{m-1}+2h \quad \forall\, 1<m\le M. \]
  \item Evaluate the density on this grid, i.e. $f(x_m)$.
  \item Normalize interval weights, i.e. $w_m = f(x_m)\left/\sum_{i=1}^M f(x_i)\right.$ \\(to constructed a normalized density, divide each $w_m$ by $2h$.).
  \end{enumerate}
\item Sampling from this approximation:
  \begin{enumerate}
  \item Sample an interval $m$ with probability $w_m$.
  \item Sample uniformly within this interval, i.e. $\tau\sim \mbox{Unif}(x_m-h, x_m+h)$.
  \end{enumerate}
\end{enumerate}
\end{frame}


\begin{frame}[fragile]
\frametitle{Approximation to $p(\tau|y)$ when $\tau\sim Ca^+(0,1)$}
<<analysis, dependson='exploratory-analysis', echo=FALSE, fig.width=8>>=
tau_log_posterior <- function(tau, ybar, sj2, C=1)
{
  if (tau<=0) return(-Inf)
  spt = sj2+tau^2
  Vmu = 1/sum(1/spt)
  mu = sum(ybar/spt)*Vmu
  0.5*log(Vmu)+sum(-0.5*log(spt)-(ybar-mu)^2/(2*spt))+dcauchy(tau, 0, C, log=TRUE)
}

V_tau_log_posterior <- Vectorize(tau_log_posterior,"tau")

h <- 0.1
post <- d_sum |>
  group_by(simulation) |>
  reframe(
    x = seq(0, 10-2*h, by = 2*h) + h,
    f = exp(V_tau_log_posterior(x, mean, sd^2)),
    p = f/(sum(f)*2*h)
  )

ggplot(post, aes(x=x-h,y=p)) +
  geom_step() +
  facet_wrap(~simulation) +
  theme_bw()
@
\end{frame}

<<group-posteriors, dependson='exploratory-analysis', echo=FALSE>>=
rposterior <- function(n_samples, y, gp)
{
  ybar <- by(y,gp,mean)
  n_groups <- nlevels(gp)

  # Used throughout
  sj2 <- rep(sigma/n_per_group, n_groups)

  # Sample from tau|y
  half_width <- 0.05
  tau_xx <- seq(0,10,by=2*half_width)+half_width
  tau_log_post <- V_tau_log_posterior(tau_xx, ybar, sj2)
  tau_post <- exp(tau_log_post)
  tau <- tau_xx[sample(1:length(tau_xx), n_samples, replace=T, prob=exp(tau_log_post))]+
         runif(n_samples, -half_width, half_width)

  # Sample from mu|tau,y
  Vmu <- muhat <- rep(NA,n_samples)
  for (i in 1:n_samples)
  {
    spt <- sj2 + tau[i]^2
    Vmu[i] <- 1/sum(1/spt)
    muhat[i] <- sum(ybar/spt)*Vmu[i]
  }
  mu <- rnorm(n_samples, muhat, sqrt(Vmu))

  # Sample from theta|mu,tau,y
  theta <- matrix(NA, n_samples, n_groups)
  for (i in 1:n_samples)
  {
    tau2 <- tau[i]^2
    Vjs <- 1/(1/sj2+1/tau2)
    thetahat <- (ybar/sj2+mu[i]/tau2)*Vjs
    theta[i,] <- rnorm(n_groups, thetahat, sqrt(Vjs))
  }

  data.frame(
    rep = 1:n_samples,
    tau = tau,
    mu = mu,
    theta = theta)
}

res <- df |>
  group_by(simulation) |>
  reframe(
    rposterior(1e4, y, group)
  )

for(i in 1:length(res)) attr(res[[i]],"name") <- names(res)[i]
@

\begin{frame}[fragile]
\frametitle{Hyperparameters: group-to-group mean variability}
Recall $\theta_i \stackrel{ind}{\sim} N(\mu,\tau^2)$:
<<tau, dependson='group-posteriors', echo=FALSE, fig.width=6>>=
ggplot(res |>
         pivot_longer(mu:tau,
                      names_to = "variable",
                      values_to = "value"),
       aes(x = value)) +
  geom_histogram(aes(y = after_stat(density)), binwidth=0.1) +
  facet_grid(simulation ~ variable,
             scales="free_x")
@
\end{frame}



% \begin{frame}[fragile]
% \frametitle{Group-specific means}
% Recall $\theta_i \stackrel{ind}{\sim} N(\mu,\tau^2)$:
% <<theta, fig.width=8, echo=FALSE>>=
% m = melt(ldply(res, function(l) { data.frame(theta=l$theta) }))
% ggplot(m, aes(x=value,fill=variable)) +
%   geom_density(alpha=0.5) +
%   facet_wrap(~simulation)
% @
% \end{frame}


<<independent-analysis, echo=FALSE, results='hide'>>=
independent <- d_sum |>
  group_by(simulation, group) |>
  reframe(
    value = rnorm(1e4, mean, 1/sqrt(n))
  ) |>
  mutate(
    variable = paste("theta.", group, sep=''),
    variable = factor(variable, levels = paste("theta.", 1:10, sep='')),
    model = "independent"
  ) |>
  dplyr::select(-group)


m <- res |>
  dplyr::select(simulation, starts_with("theta")) |>
  pivot_longer(-simulation,
               names_to  = "variable",
               values_to = "value") |>
  mutate(model = "hierarchical")
@


\begin{frame}[fragile]
\frametitle{Group-specific means}
Recall
\begin{itemize}
\item Common mean: $E[Y_{ij}] = \mu$
\item Group-specific mean: $E[Y_{ij}] = i - 10/2 + 0.5$
\end{itemize}

\vspace{0.1in}

<<theta, dependson="independent-analysis", echo=FALSE, fig.width=8>>=
ggplot(bind_rows(independent, m) |>
         mutate(variable = factor(variable, levels = paste0("theta.",1:10))),
       aes(x = value, fill = variable)) +
  geom_density(alpha = 0.5) +
  facet_grid(model ~ simulation)
@
\end{frame}






\subsection{Summary}
\begin{frame}
\frametitle{Extensions}

\begin{itemize}
\item Unknown data variance:
\[ y_{ij} \sim N(\theta_i,\sigma^2), \, \theta_i\sim N(\mu,\tau^2)\]
\pause or
\[ y_{ij} \sim N(\theta_i,\sigma^2), \, \theta_i\sim N(\mu,\sigma^2\tau^2)\]
\item \pause Alternative distributions:
  \begin{itemize}
  \item Heavy-tailed:
\[ y_{ij} \sim N(\theta_i,\sigma^2), \, \theta_i\sim t_\nu(\mu,\tau^2) \]
  \item \pause Peak at zero:
\[ y_{ij} \sim N(\theta_i,\sigma^2), \, \theta_i\sim \mbox{Laplace}(\mu,\tau^2) \]
  \item \pause Point mass at zero:
\[ y_{ij} \sim N(\theta_i,\sigma^2), \, \theta_i\sim \pi \delta_0 + (1-\pi) N(\mu,\tau^2) \]
  \end{itemize}

\end{itemize}
\end{frame}

\begin{frame}\frametitle{Summary}
	Hierarchical models
	\begin{itemize}
	\item allow the data to inform us about similarities across groups \pause
	\item provide data driven shrinkage toward a grand mean \pause
		\begin{itemize}
		\item lots of shrinkage when means are similar \pause
		\item little shrinkage when means are different
		\end{itemize}
	\end{itemize}

	\vspace{0.2in} \pause

	Computation used the decomposition
	\[ p(\theta,\mu,\tau|y) = p(\theta|\mu,\tau,y) p(\mu|\tau,y) p(\tau|y) \]
	\pause which allowed for simulation from $\tau$ then $\mu$ and then $\theta$ to obtain samples from the posterior.
\end{frame}




\end{document}
