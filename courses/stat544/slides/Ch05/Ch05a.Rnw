\documentclass[handout,aspectratio=169]{beamer}

\usepackage{verbatim}

\input{../frontmatter}
\input{../commands}

\title{Hierarchical models}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=5, fig.height=3, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("tidyverse"); theme_set(theme_bw())
library("xtable")
library("GGally")
library("RColorBrewer")

library("rstan")
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
@

<<set-seed>>=
set.seed(1)
@

\frame{\maketitle}


\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Motivating example
  \begin{itemize}
  \item Independent vs pooled estimates
  \end{itemize}
\item Hierarchical models
  \begin{itemize}
  \item General structure
  \item Posterior distribution
  \end{itemize}
\item Binomial hierarchial model
  \begin{itemize}
  \item Posterior distribution
  \item Prior distributions
  \end{itemize}
\item Stan analysis of binomial hierarchical model
  \begin{itemize}
  \item informative prior
  \item default prior
  \item integrating out $\theta$
  \item across seasons
  \end{itemize}
\end{itemize}

\end{frame}

\section{Modeling}
\frame{\frametitle{Andre Dawkin's three-point percentage}
Let $Y_s = \sum_{j=1}^{n_s} Y_{sj}$ be
the number 3-pointers Andre Dawkin's makes in season $s$, 
\pause 
and assume
\[ 
Y_s \ind Bin(n_s,\theta_s) \pause
\quad \mbox{or, equivalently,} \quad
Y_{sj} \ind Ber(\theta_s) \, j=1,\ldots,n_s
\]
  where
  \begin{itemize}[<+->]
  \item $n_s$ are the number of 3-pointers attempted and
  \item $\theta_s$ is the probability of making a 3-pointer in season $i$.
  \end{itemize}

  \vspace{0.2in} \pause

  Do these models make sense?
  \begin{itemize}[<+->]
  \item The 3-point percentage every season is the same, i.e. $\theta_s=\theta$.
  \item The 3-point percentage every season is independent of other seasons.
  \item The 3-point percentage a season should be similar to other seasons.
  \end{itemize}
}


\frame{\frametitle{Andre Dawkin's three-point percentage}
Let $Y_g= \sum_{j=1}^{n_g} Y_{gj}$ be the number of 3-pointers Andre Dawkin's 
makes in \alert{game} $g$, 
and assume
\[ 
Y_g \ind Bin(n_g,\theta_g) 
\quad \mbox{or, equivalently,} \quad
Y_{gj} \ind Ber(\theta_g) \, j=1,\ldots,n_g
\]
  where
  \begin{itemize}
  \item $n_g$ are the number of 3-pointers attempted in \alert{game} $i$ and
  \item $\theta_g$ is the probability of making a 3-pointer in \alert{game} $i$.
  \end{itemize}

  \vspace{0.2in} \pause

  Do these models make sense?
  \begin{itemize}
  \item The 3-point percentage every \alert{game} is the same, i.e. $\theta_g=\theta$.
  \item The 3-point percentage every \alert{game} is independent of other \alert{games}.
  \item The 3-point percentage a \alert{game} should be similar to other \alert{games}.
  \end{itemize}
}

<<dawkins-data, results='hide'>>=
d <- structure(list(date = structure(c(16017, 16021, 16024, 16027, 
16028, 16033, 16036, 16038, 16042, 16055, 16058, 16067, 16070, 
16074, 16077, 16081, 16083, 16088, 16092, 16095, 16097, 16102, 
16105, 16109), class = "Date"), opponent = c("davidson", "kansas", 
"florida atlantic", "unc asheville", "east carolina", "vermont", 
"alabama", "arizona", "michigan", "gardner-webb", "ucla", "eastern michigan", 
"elon", "notre dame", "georgia tech", "clemson", "virginia", 
"nc state", "miami", "florida state", "pitt", "syracuse", "wake forest", 
"boston college"), made = c(0L, 0L, 5L, 3L, 0L, 3L, 0L, 1L, 2L, 
4L, 1L, 6L, 5L, 1L, 1L, 0L, 1L, 3L, 2L, 3L, 6L, 4L, 4L, 0L), 
    attempts = c(0L, 0L, 8L, 6L, 1L, 9L, 2L, 1L, 2L, 8L, 5L, 
    10L, 7L, 4L, 5L, 4L, 1L, 7L, 6L, 6L, 7L, 9L, 7L, 1L), 
game = 1:24), 
row.names = c(NA, -24L), class = "data.frame")
@

<<dawkins-summary, dependson="dawkins-data">>=
d2 <- bind_rows(d,
               
               # Add a total row
               tibble(date     = NA,
                      game     = max(d$game)+1,
                      opponent = 'Total',
                      made     = sum(d$made),
                      attempts = sum(d$attempts))) %>%

  # Add posterior parameters
  mutate(a        = 0.5 + made,
         b        = 0.5 + attempts-made,
         lcl      = qbeta(0.025,a,b),
         ucl      = qbeta(0.975,a,b),
         Estimate = ifelse(opponent=="Total", "Combined", "Individual"),
         game     = 1:n())
@



\begin{frame}[fragile]
\frametitle{Andre Dawkin's 3-point percentage}
{\tiny
<<results='asis'>>=
print(xtable(d |> 
        mutate(date = format(date,"%Y-%m-%d"))) , 
      include.rownames = FALSE)
@
}
\end{frame}


\begin{frame}[fragile]
\frametitle{Andre Dawkin's 3-point percentage}
<<dawkins-plot, out.width='0.7\\textwidth', dependson="dawkins-summary">>=
ggplot(d2,
       aes(x     = lcl,
           xend  = ucl,
           y     = game,
           yend  = game,
           color = Estimate))+
  geom_segment(lwd=1) +
  scale_color_brewer(palette='Set1') +
  labs(x=expression(theta), title='95% Credible Intervals') +
  scale_y_reverse()
@
\end{frame}


\section{Hierarchical models}
\frame{\frametitle{Hierarchical models}
  Consider the following model
	\[ \begin{array}{ll}
	y_{ij} &\ind p(y|\theta_i) \pause \\
	\theta_i &\ind p(\theta|\phi) \pause \\
	\phi &\sim p(\phi)
	\end{array} \]
  \pause where
  \begin{itemize}
  \item $y_{ij}$ for $i=1\ldots,n_i$ are the observed data \pause,
	\item $\theta=(\theta_1,\ldots,\theta_n)$ and $\phi$ are parameters\pause, and
	\item only $\phi$ has a prior that is set.
	\end{itemize}

	\vspace{0.2in} \pause

	This is a hierarchical or multilevel model.
}

\subsection{Posteriors}
\frame{\frametitle{Posterior distribution for hierarchical models}
	The joint posterior distribution of interest in hierarchical models is
{\small
\[ \begin{array}{rl}
p(\theta,\phi|y) \pause \propto p(y|\theta,\phi)p(\theta,\phi) \pause =
p(y|\theta)p(\theta|\phi)p(\phi)\pause =
\Big[ \prod_{i=1}^n p(y_i|\theta_i)  
p(\theta_i|\phi)\Big] p(\phi).
\end{array} \]
}
where $p(y_i|\theta_i) = \prod_{j=1}^{n_i} p(y_{ij}|\theta_i)$.
	\pause The joint posterior distribution can be decomposed via
  \[ p(\theta,\phi|y) = p(\theta|\phi,y)p(\phi|y) \]
  where
{\small
  \[ \begin{array}{rl}
  p(\theta|\phi,y) &\propto p(y|\theta)p(\theta|\phi) \pause = \prod_{i=1}^n p(y_i|\theta_i)p(\theta_i|\phi) \pause \propto \prod_{i=1}^n p(\theta_i|\phi,y_i) \pause \\
  p(\phi|y) &\propto p(y|\phi)p(\phi) \pause \\
  p(y|\phi) &=
  \int p(y|\theta)p(\theta|\phi) d\theta \pause \\
  &= \int \cdots \int  \prod_{i=1}^n  \left[ p(y_i|\theta_i)p(\theta_i|\phi) \right] d\theta_1 \cdots d\theta_n \pause \\
  &= \prod_{i=1}^n \int p(y_i|\theta_i)p(\theta_i|\phi) d\theta_i \pause \\
  &= \prod_{i=1}^n p(y_i|\phi)
  \end{array} \]
}
}

\section{Binomial hierarchical model}
\frame{\frametitle{Three-pointer example}
Let $Y_{i,g}$ be an indicator that 3-point attempt $i$ in game $g$ was successful for $i = 1,\ldots,n_g$ \pause and $Y_g = \sum_{i=1}^{n_g} Y_{i,g}$. 
	\pause 
	Assume
	\[ \begin{array}{ll}
	Y_{i,g} &\ind Ber(\theta_g) \pause 
	\quad \mbox{or, equivalently} \quad
	Y_g \ind Bin(n_g, \theta_g)
	\pause \\
	\theta_g&\ind Be(\alpha,\beta) \pause \\
	\alpha,\beta &\sim p(\alpha,\beta)
	\end{array} \]

	\vspace{0.2in} \pause

	In this example,
	\begin{itemize}
	\item $\phi=(\alpha,\beta)$\pause\,
	\item $Be(\alpha,\beta)$ describes the variability in 3-point percentage across games\pause, and
	\item we are going to learn about this variability.
	\end{itemize}
}

\begin{frame}
\frametitle{Decomposed posterior}
  \[ Y_g \ind Bin(n_g,\theta_g) \quad \theta_g\ind Be(\alpha,\beta) \quad \alpha,\beta \sim p(\alpha,\beta) \]
  Conditional posterior for $\theta$:
\[ p(\theta|\alpha,\beta,y) = \prod_{i=1}^n p(\theta_g|\alpha,\beta,y_g) \pause = \prod_{i=1}^n Be(\theta_g|\alpha+y_g,\beta+n_g-y_g) \]
\pause
Marginal posterior for $(\alpha,\beta)$:
{\small
\[ \begin{array}{rl}
p(\alpha,\beta|y) &\propto p(y|\alpha,\beta)p(\alpha,\beta) \pause \\
p(y|\alpha,\beta) &= \prod_{i=1}^n p(y_g|\alpha,\beta) \pause = \prod_{i=1}^n \int p(y_g|\theta_g)p(\theta_g|\alpha,\beta) d\theta_g \pause \\
% &= \prod_{i=1}^n \int Bin(y_g|n_g,\theta_g)Be(\theta_g|\alpha,\beta) d\theta_g \pause \\
% &= \prod_{i=1}^n \int_0^1 {n_g\choose y_g} \theta_g^{y_g} (1-\theta_g)^{n_g-y_g} \frac{\theta_g^{\alpha-1}(1-\theta_g)^{\beta-1}}{B(\alpha,\beta)} d\theta_g \pause \\
% &= \prod_{i=1}^n {n_g\choose y_g}\frac{1}{B(\alpha,\beta)} \int_0^1 \theta_g^{\alpha+y_g-1} (1-\theta_g)^{\beta+n_g-y_g-1}  d\theta_g \pause \\
&= \prod_{i=1}^n {n_g\choose y_g}\frac{B(\alpha+y_g,\beta+n_g-y_g)}{B(\alpha,\beta)}
\end{array} \]
}
\pause
Thus $y_g|\alpha,\beta \ind \mbox{Beta-binomial}(n_g,\alpha,\beta)$.
\end{frame}



\subsection{Prior}
\begin{frame}
\frametitle{A prior distribution for $\alpha$ and $\beta$}
  Recall the interpretation: \pause
  \begin{itemize}[<+->]
  \item $\alpha$: prior successes
  \item $\beta$: prior failures
  \end{itemize}

  \vspace{0.2in} \pause

  A more natural parameterization is
  \begin{itemize}[<+->]
  \item prior expectation: $\mu = \frac{\alpha}{\alpha+\beta}$
  \item prior sample size: $\eta = \alpha + \beta$
  \end{itemize}

  \vspace{0.2in} \pause

  Place priors on these parameters or transformed to the real line:
  \begin{itemize}[<+->]
  \item $\text{logit } \mu = \log(\mu/[1-\mu]) = \log(\alpha/\beta)$
  \item $\log \eta$
  \end{itemize}
\end{frame}



\begin{frame}[fragile]
\frametitle{A prior distribution for $\alpha$ and $\beta$}
  It seems reasonable to assume the expectation ($\mu$) and size ($\eta$) are 
  independent \emph{a priori}:
  \[ p(\mu,\eta) = p(\mu)p(\eta) \]

    \vspace{0.1in} \pause

Let's construct a prior that has 
\begin{itemize}
\item $P(0.1<\mu<0.5) \approx 0.95$ since most college basketball players have 
a three-point percentage between 10\% and 50\% \pause and
\item is somewhat diffuse for $\eta$ but has more mass for smaller values. 
\end{itemize}

<<informative-prior>>=
a = 6
b = 14
e = 1/20
@

  Let's assume an informative prior for $\mu$ and $\eta$ \pause perhaps
  \begin{itemize}
  \item $\mu \sim Be(\Sexpr{a},\Sexpr{b})$
  \item $\eta \sim Exp(\Sexpr{e})$
  \end{itemize}

  \vspace{0.1in} \pause

<<informative-prior-print, dependson="informative-prior", echo=TRUE>>=
<<informative-prior>>
@


\end{frame}



\begin{frame}[fragile]
\frametitle{Prior draws}
<<informative-prior-plot, dependson='informative-prior', echo=TRUE>>=
n <- 1e4

prior_draws <- data.frame(mu  = rbeta(n, a, b),
                         eta = rexp(n, e)) %>%
  mutate(alpha = eta*   mu,
         beta  = eta*(1-mu))

prior_draws %>%
  tidyr::gather(parameter, value) %>%
  group_by(parameter) %>%
  summarize(lower95 = quantile(value, prob = 0.025),
            median  = quantile(value, prob = 0.5),
            upper95 = quantile(value, prob = 0.975))

cor(prior_draws$alpha, prior_draws$beta)
@
\end{frame}



% \begin{frame}[fragile]
% <<proper_prior_plot, dependson='proper_prior', out.width='0.8\\textwidth'>>=
% ggpairs(prior_draws, lower=list(continuous='density'))
% @
% \end{frame}



\section{Stan}
\begin{frame}[fragile]
\frametitle{}
<<stan, echo=TRUE>>=
model_informative_prior = "
data {
  int<lower=0> G;    // data
  int<lower=0> n[G];
  int<lower=0> y[G];
  real<lower=0> a;   // prior
  real<lower=0> b;
  real<lower=0> e;
}
parameters {
  real<lower=0,upper=1> mu;
  real<lower=0> eta;
  real<lower=0,upper=1> theta[G];
}
transformed parameters {
  real<lower=0> alpha;
  real<lower=0> beta;

  alpha = eta*   mu ;
  beta  = eta*(1-mu);
}
model {
  mu    ~ beta(a,b);
  eta   ~ exponential(e);

  // implicit joint distributions
  theta ~ beta(alpha,beta);
  y     ~ binomial(n,theta);
}
"
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Stan}
<<stan-run, dependson=c('informative-prior','stan',"dawkins-data"), echo=TRUE, results='hide'>>=
dat = list(y = d$made, n = d$attempts, G = nrow(d), a = a, b = b, e = e)
m = stan_model(model_code = model_informative_prior)
r = sampling(m, dat, c("mu","eta","alpha","beta","theta"),
             iter = 10000)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{}
<<stan-post, dependson="stan-run", echo=TRUE>>=
r
@
\end{frame}


\begin{frame}[fragile]
\frametitle{stan}
<<stan0plot, dependson="stan-run", message=TRUE, echo=TRUE, out.width='0.5\\textwidth'>>=
plot(r, pars=c('eta','alpha','beta'))
@
\end{frame}

\begin{frame}[fragile]
\frametitle{stan}
<<stan-plot2, dependson="stan-run", echo=TRUE, out.width='0.7\\textwidth'>>=
plot(r, pars=c('mu','theta'))
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Comparing independent and hierarchical models}
<<quantiles, dependson="stan-run", out.width='0.7\\textwidth'>>=
d <- d %>%
  mutate(model = "independent",
         lcl      = qbeta(0.025, 0.5 + made,0.5 + attempts - made),
         ucl      = qbeta(0.975, 0.5 + made,0.5 + attempts - made))

tmp = data.frame(summary(r)$summary[,c(4,8)])
new_d = mutate(d,
               model = "hierarchical",
               lcl = tmp[5:28,1],
               ucl = tmp[5:28,2])

e = 0.2
ggplot(bind_rows(d, new_d), aes(x=lcl,
                     xend=ucl,
                     y=game+e*(model=="hierarchical"),
                     yend=game+e*(model=="hierarchical"),
                     color=model))+
  geom_segment(lwd=1, alpha=0.5) +
  labs(x=expression(theta), y="game", title='95% Credible Intervals') +
  scale_color_brewer(palette='Set1') +
  scale_y_reverse()
@
\end{frame}




\frame{\frametitle{A prior distribution for $\alpha$ and $\beta$}
  In Bayesian Data Analysis (3rd ed) page 110, several priors are discussed

  \vspace{0.2in} \pause

	\begin{itemize}
	\item $(\log(\alpha/\beta), \log(\alpha+\beta)) \propto 1$ leads to an improper posterior.

  \vspace{0.2in} \pause

  \item $(\log(\alpha/\beta), \log(\alpha+\beta)) \sim Unif([-10^{10},10^{10}] \times [-10^{10},10^{10}])$ \pause while proper and seemingly vague is a very informative prior.

  \vspace{0.2in} \pause

  \item $(\log(\alpha/\beta), \log(\alpha+\beta)) \propto \alpha\beta(\alpha+\beta)^{-5/2}$ \pause which leads to a proper posterior \pause and is equivalent to $p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2}$.
	\end{itemize}
}



\subsection{default prior}
\begin{frame}[fragile]
\frametitle{Stan - default prior}
<<stan2-run, echo=TRUE, results='hide', dependson="stan-run">>=
model_default_prior <- "
data {
  int<lower=0> G;
  int<lower=0> n[G];
  int<lower=0> y[G];
}
parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
  real<lower=0,upper=1> theta[G];
}

model {
  // default prior
  target += -5*log(alpha+beta)/2;

  // implicit joint distributions
  theta ~ beta(alpha,beta);
  y     ~ binomial(n,theta);
}
"

m2 <- stan_model(model_code = model_default_prior)
r2 <- sampling(m2, dat, c("alpha","beta","theta"), iter = 10000,
               control = list(adapt_delta = 0.9))
@
\end{frame}


\subsection{beta-binomial}
\begin{frame}
\frametitle{Marginal posterior for $\alpha,\beta$}

An alternative to jointly sampling $\theta,\alpha,\beta$ is to
\begin{enumerate}
\item sample $\alpha,\beta\sim p(\alpha,\beta|y)$\pause, and then
\item sample $\theta_g \ind p(\theta_g|\alpha,\beta,y_g) \stackrel{d}{=} Be(\alpha+y_g,\beta+n_g-y_g)$.
\end{enumerate}

\vspace{0.2in} \pause

The marginal posterior for $\alpha,\beta$ is
\[ p(\alpha,\beta|y) \propto p(y|\alpha,\beta)p(\alpha,\beta) = \left[ \prod_{g=1}^G \mbox{Beta-binomial}(y_g|n_g,\alpha,\beta) \right] p(\alpha,\beta) \]

\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - beta-binomial}
<<stan3-run, echo=TRUE, results='hide',dependson="stan-run">>=
# Marginalized (integrated) theta out of the model
model_marginalized <- "
data {
  int<lower=0> G;
  int<lower=0> n[G];
  int<lower=0> y[G];
}
parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
}
model {
  target += -5*log(alpha+beta)/2;
  y     ~ beta_binomial(n,alpha,beta);
}
generated quantities {
  real<lower=0,upper=1> theta[G];
  for (i in 1:G)
    theta[i] = beta_rng(alpha+y[i],beta+n[i]-y[i]);
}
"

m3 <- stan_model(model_code = model_marginalized)
r3 <- sampling(m3, dat, iter = 10000)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Stan - beta-binomial}
<<stan3-post, dependson="stan3-run">>=
r3
@
\end{frame}


\begin{frame}[fragile]
\frametitle{}

Posterior samples for $\alpha$ and $\beta$

<<stan3-alpha-beta, dependson="stan3-run", echo=FALSE>>=
samples <- extract(r3, c("alpha","beta"))

ggpairs(data.frame(log_alpha = log(as.numeric(samples$alpha)),
                   log_beta  = log(as.numeric(samples$beta))),
        lower  = list(continuous='density')) 
@
\end{frame}














\begin{frame}[fragile]
\frametitle{Comparing all models}
<<quantiles2, dependson=c("stan-run","stan3-run")>>=
d$prior     <- "independent"
new_d$prior <- "informative"

# d = d %>%
#   mutate(lcl = qbeta(0.025,a,b),
#          ucl = qbeta(0.975,a,b))
  
tmp <- data.frame(summary(r3)$summary[,c(4,8)])
new_d2 <- mutate(new_d,
                prior = "default",
                lcl = tmp[3:26,1],
                ucl = tmp[3:26,2])

bind_d <- bind_rows(d,new_d,new_d2)
bind_d$prior <- factor(bind_d$prior, c('informative','default','independent'))

e = 0.2
ggplot(bind_d,
       aes(x     = lcl,
           xend  = ucl,
           y     = game+e*(prior=="independent")-e*(prior=="informative"),
           yend  = game+e*(prior=="independent")-e*(prior=="informative"),
           color = prior))+
  geom_segment(lwd=1, alpha=0.5) +
  labs(x=expression(theta), y="game", title='95% Credible Intervals') +
  scale_color_brewer(palette='Set1') +
  scale_y_reverse()
@
\end{frame}





\begin{frame}[fragile]
\frametitle{Posterior sample for $\theta_{22}$}
<<stan3-theta, dependson="stan3-run", echo=TRUE, fig.width=6>>=
game <- 22
theta22 <- extract(r3, "theta")$theta[,game]

hist(theta22, 100,
     main=paste("Posterior for game against", d$opponent[game], "on", d$date[game]),
     xlab="3-point probability",
     ylab="Posterior")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{$\theta$s are not independent in the posterior}
<<stan3-thetas, dependson=c('stan3-run','stan3-theta'), fig.width=6>>=
theta23 <- extract(r3, "theta")$theta[,23]
ggpairs(data.frame(theta22=theta22,theta23=theta23), 
        lower=list(continuous='density')) 
@
\end{frame}



\subsection{3-point percentage across seasons}
\begin{frame}[fragile]
\frametitle{3-point percentage across seasons}

An alternative to modeling game-specific 3-point percentage is to model 
season-specific 3-point percentage. 
\pause 
The model is exactly the same, but the data changes. \pause

<<season-data, results='asis'>>=
d <- data.frame(season = 1:4, y = c(36,64,67,64), n = c(95,150,171,152))
print(xtable(d, digits=0), include.rownames = FALSE)
@

Due to the low number of seasons (observations), 
we will use an informative prior for $\alpha$ and $\beta$.

\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - beta-binomial}
<<stan-season, echo=TRUE, results='hide'>>=
model_seasons <- "
data {
  int<lower=0> G; int<lower=0> n[G]; int<lower=0> y[G];
  real<lower=0> a; real<lower=0> b; real<lower=0> e;
}
parameters {
  real<lower=0,upper=1> mu;
  real<lower=0> eta;
}
transformed parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
  alpha = eta *    mu;
  beta  = eta * (1-mu);
}
model {
  mu  ~ beta(a,b);
  eta ~ exponential(e);
  y   ~ beta_binomial(n,alpha,beta);
}
generated quantities {
  real<lower=0,upper=1> theta[G];
  for (g in 1:G) theta[g] = beta_rng(alpha+y[g], beta+n[g]-y[g]);
}
"
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Run stan}
<<stan-season-run, dependson=c('season-data','stan-season'), echo=TRUE>>=
dat       <- list(G = nrow(d), y = d$y, n = d$n, a = a, b = b, e = e)
m4        <- stan_model(model_code = model_seasons)
r_seasons <- sampling(m4, dat, iter = 10000,
                     c("alpha","beta","mu","eta","theta"))
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Stan - hierarchical model for seasons}

<<stan-season-summary, dependson="stan-season-run">>=
r_seasons
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - hierarchical model for seasons}

<<stan-season-posteriors, dependson="stan-season">>=
posterior <- extract(r_seasons, "theta") %>%
  as.data.frame() %>%
  tidyr::gather(parameter, value) %>%
  mutate(parameter = as.factor(parameter))

ggplot(posterior,
       aes(x = value,
           fill = parameter)) +
  geom_density(alpha = 0.5) 
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Stan - hierarchical model for seasons}

Probabilities that 3-point percentage is greater in season 4 than in the other seasons:

<<stan-season-comparison, echo=TRUE, dependson="stan-season">>=
theta = extract(r_seasons, "theta")[[1]]
mean(theta[,4] > theta[,1])
mean(theta[,4] > theta[,2])
mean(theta[,4] > theta[,3])
@

\end{frame}



\subsection{Summary}
\begin{frame}
\frametitle{Summary - hierarchical models}
Two-level hierarchical model:
\[ y_{ij}\ind p(y|\theta_i) \qquad \theta_i \ind p(\theta|\phi) \qquad \phi\sim p(\phi) \phantom{|\psi\qquad \psi\sim p(\psi)} \]

\vspace{0.2in} \pause

Conditional independencies:
\begin{itemize}
\item $y_{ij} \independent y_{ij'} | \theta$ for $j\ne j'$
\item $y_{ij} \independent y_{i'j'} | \theta$ for $i\ne i'$ and any $j,j'$
\item $\theta_i \independent \theta_{i'} | \phi$ for $i\ne i'$
\item $y_{ij} \independent \phi | \theta$ for any $i,j$
\item $y_{ij} \independent y_{i'j'} | \phi$ for $i\ne i'$ and any $j,j'$
\item $\theta_i \independent \theta_{i'} | \phi, y$ for $i\ne i'$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Summary - extension to more levels}
Three-level hierarchical model:
	\[ y\sim p(y|\theta) \qquad \theta\sim p(\theta|\phi) \qquad \phi\sim p(\phi|\psi) \qquad \psi\sim p(\psi) \]

\vspace{0.2in} \pause

  When deriving posteriors, remember the conditional independence structure, \pause e.g.
	\[ p(\theta,\phi,\psi|y) \propto p(y|\theta) p(\theta|\phi) p(\phi|\psi) p(\psi) \]
\end{frame}



\end{document}
