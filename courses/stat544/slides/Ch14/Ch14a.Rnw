\documentclass[handout,aspectratio=169]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Bayesian linear regression}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=5, fig.height=3, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library(reshape2)
library(plyr)
library(ggplot2); theme_set(theme_bw())
library(xtable)
library(coda)
library(LearnBayes)
library(arm)
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}

\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Linear regression
  \begin{itemize}
  \item Classical regression
  \item Default Bayesian regression
  \item Conjugate subjective Bayesian regression
  \end{itemize}
\item Simulating from the posterior
  \begin{itemize}
  \item Inference on functions of parameters
  \item Posterior for optimum of a quadratic
  \end{itemize}
\end{itemize}

\end{frame}

\section{Linear regression}
\frame{\frametitle{Linear Regression}
  Basic idea
	\begin{itemize}
	\item understand the relationship between response $y$ and explanatory variables $x=(x_1,\ldots,x_k)$ \pause
	\item based on data from experimental units index by $i$. \pause
	\end{itemize}
	
	If we assume 
	\begin{itemize}
	\item linearity, independence, normality, and constant variance, 
	\end{itemize}
	
	\pause then we have 
	\[ y_i \stackrel{ind}{\sim} N(\beta_1 x_{i1} + \cdots + \beta_k x_{ik},\sigma^2) \]
	\pause where $x_{i1}=1$ if we want to include an intercept. \pause In matrix notation, we have 
	\[ y \sim N(X\beta, \sigma^2 \I) \]
	\pause where $y=(y_1,\ldots,y_n)^\top  $, $\beta=(\beta_1,\ldots,\beta_k)^\top  $, and $X$ is an $n \times k$ full-rank matrix with each row being $x_i = (x_{i1},\ldots,x_{ik})$.
}

\subsection{Classical regression}
\begin{frame}
\frametitle{Classical regression}
  How do you find confidence intervals for $\beta$? 
  
  \vspace{0.2in} \pause
  
  What is the MLE for $\beta$? \pause 
  \[ \hat{\beta}=\hat{\beta}_{MLE} = (X^\top   X)^{-1}X^\top   y \]
  
  \vspace{0.2in} \pause
  
  What is the sampling distribution for $\hat{\beta}$?
  \[ \hat{\beta} \sim t_{n-k}(\beta, s^2(X^\top   X)^{-1}) \]
  where $s^2 = SSE/[n-k]$ and $SSE = (Y-X\hat{\beta})^\top   (Y-X\hat{\beta})$. 
  
  \vspace{0.2in} \pause
  
  What is the sampling distribution for $s^2$? 
  \[ \frac{[n-k]s^2}{\sigma^2} \sim \chi^2_{n-k} 
  %\pause \implies \frac{1}{s^2} \sim \mbox{Inv-}\chi^2(n-k,\sigma^2) 
  \]

\end{frame}

\subsection{Default Bayesian inference}
\frame{\frametitle{Default Bayesian regression}
	Assume the standard noninformative prior
	\[ p(\beta,\sigma^2) \propto 1/\sigma^2 \]
	\pause then the posterior is 
	\[ \begin{array}{rl}
	p(\beta,\sigma^2|y) &= p(\beta|\sigma^2,y) p(\sigma^2|y) \pause \\
	\beta|\sigma^2,y &\sim N(\hat{\beta}, \sigma^2 V_\beta) \pause \\
	\sigma^2|y &\sim \mbox{Inv-}\chi^2(n-k,s^2) \pause \\
	\beta|y &\sim t_{n-k}(\hat{\beta}, s^2V_{\beta}) \pause \\
	\\
	\hat{\beta} &= (X^\top  X)^{-1}X^\top  y \pause \\
	V_\beta &= (X^\top  X)^{-1} \pause \\
	s^2 &= \frac{1}{n-k}(y-X\hat{\beta})^\top  (y-X\hat{\beta})
	\end{array} \]
	\pause The posterior is proper if $n>k$ and rank$(X)=k$. 
}



\begin{frame}
\frametitle{Comparison to classical regression}


\small

In classical regression, we have fixed, but unknown, true parameters $\beta_0$ and $\sigma_0^2$ and quantify our uncertainty about these parameters using the sampling distribution of the error variance and regression coefficients, i.e.  
  \[ \frac{[n-k]s^2}{\sigma_0^2} \sim \chi^2_{n-k}  \]
and 
\[ \hat{\beta} \sim t_{n-k}(\beta_0, s^2[X^\top  X]^{-1}).  \]

\vspace{0.2in} \pause

In the default Bayesian regression, we still have the fixed, but unknown, true parameters, but quantify our uncertainty about these parameters using prior and posterior distributions, i.e. 
\[ \left.\frac{s^2[n-k]}{\sigma^2}\right|y \sim \chi^2_{n-k} \]
and 
\[ \beta|y \sim t_{n-k}(\hat{\beta}, s^2[X^\top  X]^{-1}). \]

\end{frame}


\subsection{Cricket chirps}
\begin{frame}[fragile]
\frametitle{Cricket chirps}
	As an example, consider the relationship between the number of cricket chirps (in 15 seconds) and temperature (in Fahrenheit). From example in {\tt LearnBayes::blinreg}.
<<chirp-data, fig.width=6>>=
chirps=c(20,16.0,19.8,18.4,17.1,15.5,14.7,17.1,15.4,16.2,15,17.2,16,17,14.1)
temp=c(88.6,71.6,93.3,84.3,80.6,75.2,69.7,82,69.4,83.3,78.6,82.6,80.6,83.5,76.3)

g <- ggplot(data.frame(chirps = chirps, temp = temp),
       aes(x = temp, y = chirps)) + 
  geom_point() +
  labs(
    x = "Temperature (F)",
    y = "Chirps (in 15 seconds)"
  )

g
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Default Bayesian regression}

<<chirp-lm, echo=TRUE, dependson="chirp-data">>=
summary(m <- lm(chirps ~ temp))
confint(m) # Credible intervals
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Default Bayesian regression - Full posteriors}
<<dependson="chirp-lm">>=
beta_hat = m$coefficients
sd       = summary(m)$coefficients[,2]
df       = summary(m)$df[2]
s2       = summary(m)$sigma^2

opar = par(mfrow=c(1,3))
for (i in 1:2) {
  f = function(x) dt((x-beta_hat[i])/sd[i], df)/sd[i]
  curve(f, 
        beta_hat[i]-3*sd[i], 
        beta_hat[i]+3*sd[i], 
        lwd=2, 
        main=paste("beta",i,sep=''))
}       
sigma = 1/sqrt(rgamma(10000,df/2, df*s2/2))
hist(sigma,100)
par(opar)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Cricket chirps}
<<chirp-posterior-plot, dependson="chirp-data">>=
g + geom_smooth(method = "lm")
@
\end{frame}



%\begin{comment}
\subsection{Subjective Bayesian inference}
\frame{\frametitle{Fully conjugate subjective Bayesian inference}
	If we assume the following normal-gamma prior,
	\[ \beta|\sigma^2 \sim N(m_0, \sigma^2 C_0) \qquad \sigma^2 \sim \mbox{Inv-}\chi^2(v_0,s_0^2) \]
	\pause then the posterior is 
	\[ \beta|\sigma^2,y \sim N(m_n, \sigma^2 C_n) \qquad \sigma^2|y \sim \mbox{Inv-}\chi^2(v_n,s_n^2) \]
	\pause with
	\[ \begin{array}{rl}
	m_n &= m_0 + C_0X^\top  (XC_0X^\top  +\I)^{-1}(y-X m_0) \\
	C_n &= C_0-C_0X^\top  (XC_0X^\top  +\I)^{-1}XC_0 \\
	v_n &= v_0+n \\
	v_ns_n^2 &= v_0s_0^2+(y-X m_0)^\top  (XC_0X^\top  +\I)^{-1}(y-X m_0) 
	\end{array} \]
}
%\end{comment}


\begin{frame}
\frametitle{Information about chirps per 15 seconds}

Let
\begin{itemize}
\item $Y_i$ is the average number of chirps per 15 seconds and 
\item $X_i$ is the temperature in Fahrenheit.
\end{itemize}

\vspace{0.2in} \pause

And we assume 
\[ Y_i \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_i, \sigma^2) \]
then 
\begin{itemize}
\item $\beta_0$ is the expected number of chirps at 0 degrees Fahrenheit
\item $\beta_1$ is the expected increase in number of chirps (per 15 seconds) for each degree increase in Fahrenheit.
\end{itemize}
\vspace{0.2in} \pause
Based on prior experience the prior $\beta_1\sim N(0,1)$ might be reasonable. 

\end{frame}


\begin{frame}[fragile]
\frametitle{Subjective Bayesian regression}
<<subjective, echo=TRUE, warning=FALSE>>=
m = arm::bayesglm(chirps~temp,   # Default prior for \beta_0 is N(0,Inf)
                  prior.mean=0,  # E[\beta_1]
                  prior.scale=1, # V[\beta_1]
                  prior.df=Inf)  # normal prior
summary(m)
@
\end{frame}





\begin{frame}[fragile]
\frametitle{Subjective vs Default}
<<t-distribution, message=FALSE, echo=TRUE>>=
# Subjective analysis
m$coefficients
confint(m)

# compared to default analysis
tmp = lm(chirps~temp)
tmp$coefficients
confint(tmp)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Subjective vs Default}
<<t-distribution_fit, dependson=c('chirp-data','chirp-lm'), message=FALSE>>=
g +
  geom_smooth(method='lm',formula=y~x, se=FALSE, color='red', linetype=2, linewidth=2) + 
  geom_abline(intercept=m$coefficients[1], slope=m$coefficients[2], color='blue', linetype=3, linewidth=2)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Shrinkage (as $V[\beta_1]$ gets smaller)}
<<shrinkage, dependson='chirp-data'>>=
d = ddply(data.frame(V=10^seq(-2,2,by=0.2)), .(V), function(x) {
  m = bayesglm(chirps~temp, prior.mean=0, prior.scale=x$V, prior.df=Inf)
  data.frame(beta0=m$coefficients[1], beta1=m$coefficients[2])
})
tmp = melt(d, id="V", value.name="estimate")
ggplot(tmp, aes(V, estimate)) +
  geom_line() +
  scale_x_log10() + 
  facet_wrap(~variable, scales='free') +
  theme_bw()
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Shrinkage (as $V[\beta_1]$ gets smaller)}
<<shrinkage2, dependson=c('shrinkage','chirp-data')>>=
ggplot(data.frame(chirps=chirps, temperature=temp), aes(temp, chirps)) + 
  geom_point() + 
  geom_abline(data=d, aes(intercept=beta0, slope=beta1, color=V)) + 
  scale_colour_gradient(trans = "log10") +
  theme_bw()
@
\end{frame}




\subsection{Simulation}
\begin{frame}
\frametitle{Simulating from the posterior}
  Although the full posterior for $\beta$ and $\sigma^2$ is available, \pause the decomposition 
  \[ p(\beta,\sigma^2|y) = p(\beta|\sigma^2,y) p(\sigma^2|y) \]
  suggests an approach to simulating from the posterior via
  \begin{enumerate}[<+->]
  \item $(\sigma^2)^{(j)} \sim \mbox{Inv-}\chi^2(n-k,s^2)$ and
  \item $\beta^{(j)}\sim N(\hat{\beta}, (\sigma^2)^{(j)} V_\beta)$.
  \end{enumerate}
  
  \vspace{0.2in}\pause
  
  This also provides an approach to obtaining posteriors for any function $\gamma=f(\beta,\sigma^2)$ of the parameters \pause via
  \[ \begin{array}{rl}
  p(\gamma|y) &= \int \int p(\gamma|\beta,\sigma^2,y) p(\beta|\sigma^2,y) p(\sigma^2|y) d\beta d\sigma^2 \pause \\
  &= \int \int p(\gamma|\beta,\sigma^2) p(\beta|\sigma^2,y) p(\sigma^2|y) d\beta d\sigma^2 \pause \\
  &= \int \int \I(\gamma=f(\beta,\sigma^2)) p(\beta|\sigma^2,y) p(\sigma^2|y) d\beta d\sigma^2 \\
  \end{array} \]
  \pause by adding the step  
  \begin{enumerate}
  \setcounter{enumi}{2}
  \item $\gamma^{(j)} = f(\beta^{(j)},(\sigma^2)^{(j)})$.
  \end{enumerate}
\end{frame}


<<potato_data>>=
d = structure(list(site = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L), .Label = c("Site 1", "Site 2", 
"Site 3", "Site 4"), class = "factor"), year = c(1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 
1995L), irrigated = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("no", "yes"), class = "factor"), 
    N.rate = c(0L, 0L, 0L, 0L, 50L, 50L, 50L, 50L, 100L, 100L, 
    100L, 100L, 150L, 150L, 150L, 150L, 200L, 200L, 200L, 200L, 
    250L, 250L, 250L, 250L, 0L, 0L, 0L, 0L, 50L, 50L, 50L, 50L, 
    100L, 100L, 100L, 100L, 150L, 150L, 150L, 150L, 200L, 200L, 
    200L, 200L, 250L, 250L, 250L, 250L, 0L, 0L, 0L, 0L, 50L, 
    50L, 50L, 50L, 100L, 100L, 100L, 100L, 150L, 150L, 150L, 
    150L, 200L, 200L, 200L, 200L, 250L, 250L, 250L, 250L, 0L, 
    0L, 0L, 0L, 50L, 50L, 50L, 50L, 100L, 100L, 100L, 100L, 150L, 
    150L, 150L, 150L, 200L, 200L, 200L, 200L, 250L, 250L, 250L, 
    250L, 0L, 0L, 0L, 0L, 50L, 50L, 50L, 50L, 100L, 100L, 100L, 
    100L, 150L, 150L, 150L, 150L, 200L, 200L, 200L, 200L, 250L, 
    250L, 250L, 250L, 0L, 0L, 0L, 0L, 50L, 50L, 50L, 50L, 100L, 
    100L, 100L, 100L, 150L, 150L, 150L, 150L, 200L, 200L, 200L, 
    200L, 250L, 250L, 250L, 250L, 0L, 0L, 0L, 0L, 50L, 50L, 50L, 
    50L, 100L, 100L, 100L, 100L, 150L, 150L, 150L, 150L, 200L, 
    200L, 200L, 200L, 250L, 250L, 250L, 250L, 0L, 0L, 0L, 0L, 
    50L, 50L, 50L, 50L, 100L, 100L, 100L, 100L, 150L, 150L, 150L, 
    150L, 200L, 200L, 200L, 200L, 250L, 250L, 250L, 250L), mean.yield = c(24, 
    24, 24, 24, 26, 26, 26, 26, 24, 24, 24, 24, 26, 26, 26, 26, 
    28, 28, 28, 28, 22, 22, 22, 22, 29.5, 29.5, 29.5, 29.5, 38, 
    38, 38, 38, 40, 40, 40, 40, 41.5, 41.5, 41.5, 41.5, 41, 41, 
    41, 41, 41.5, 41.5, 41.5, 41.5, 28, 28, 28, 28, 28.5, 28.5, 
    28.5, 28.5, 30, 30, 30, 30, 29, 29, 29, 29, 29.5, 29.5, 29.5, 
    29.5, 28, 28, 28, 28, 39, 39, 39, 39, 47, 47, 47, 47, 49, 
    49, 49, 49, 46, 46, 46, 46, 48, 48, 48, 48, 45, 45, 45, 45, 
    22, 22, 22, 22, 26, 26, 26, 26, 24, 24, 24, 24, 25, 25, 25, 
    25, 26, 26, 26, 26, 23, 23, 23, 23, 22, 22, 22, 22, 30, 30, 
    30, 30, 32, 32, 32, 32, 31, 31, 31, 31, 33, 33, 33, 33, 32.5, 
    32.5, 32.5, 32.5, 24, 24, 24, 24, 25, 25, 25, 25, 27, 27, 
    27, 27, 25, 25, 25, 25, 25, 25, 25, 25, 25.5, 25.5, 25.5, 
    25.5, 31, 31, 31, 31, 29.5, 29.5, 29.5, 29.5, 30.5, 30.5, 
    30.5, 30.5, 31, 31, 31, 31, 31, 31, 31, 31, 30.5, 30.5, 30.5, 
    30.5), yield = c(23.4, 24.2, 23.2, 25.6, 26.3, 25.2, 26.5, 
    26.7, 24.6, 23.7, 25.5, 24.4, 25.4, 23.8, 27.1, 26, 28, 28.9, 
    28.8, 28.6, 22.9, 22.8, 22.1, 20, 30.1, 29.4, 29.3, 28, 37.5, 
    38.4, 39.4, 37.9, 40.4, 39.9, 38.6, 39.6, 41.1, 41.4, 42.6, 
    42.3, 40.8, 40.7, 41.7, 41.6, 40.8, 40.8, 41.9, 42.3, 27.9, 
    28.9, 28.4, 27.4, 28.8, 27.4, 29.9, 30.5, 29.6, 29, 30.6, 
    29.9, 31.4, 29, 29.7, 29, 28.8, 29.7, 27.7, 31, 28.2, 30.2, 
    28.5, 27.3, 39.6, 38.1, 37.7, 39.3, 46.6, 47, 47.1, 46.4, 
    48.4, 48.9, 50.2, 47.5, 46.6, 46.3, 47.1, 45.7, 48.4, 48.3, 
    47.5, 49.2, 46.2, 45.7, 46.6, 45.6, 20.7, 21.4, 20.8, 21.5, 
    25.4, 26, 25.1, 26.2, 23.3, 25.8, 24.7, 24.9, 25.4, 26.7, 
    24.4, 24.5, 27.4, 25.3, 25.8, 25.6, 22.7, 22.7, 23.5, 22.8, 
    21.5, 23.3, 21.8, 21.8, 29.9, 30.7, 29.9, 30, 31.3, 31.7, 
    32.1, 31.4, 31.5, 29.5, 31.3, 29.5, 32.7, 32.5, 32.3, 32.9, 
    30.6, 33.7, 30.8, 32, 22.9, 23.2, 26.1, 24, 23.7, 23.4, 25.5, 
    25, 26.7, 26.1, 25.5, 25.9, 26, 24.4, 23.6, 26.9, 25.4, 24.8, 
    26.1, 25.9, 24.9, 27.7, 25.2, 24.1, 30.9, 31.2, 33.3, 31.1, 
    30, 29.4, 29.2, 29.5, 31.3, 32.6, 31.5, 31.7, 29.8, 32, 31.2, 
    29.5, 31.5, 30.8, 32.5, 30.2, 30.1, 29.6, 30.3, 30.9)), .Names = c("site", 
"year", "irrigated", "N.rate", "mean.yield", "yield"), class = "data.frame", row.names = c(NA, 
-192L))
@

\subsection{Posterior for global maximum}
\begin{frame}[fragile]
\frametitle{Posterior for global maximum}
Consider this potato yield data set
<<potato_plot>>=
ggplot(d, aes(x=N.rate, y=yield)) + 
  geom_point() + 
  geom_smooth(method='lm',formula=y~x+I(x^2)) +
  theme_bw()
@
with a goal of estimating the optimal nitrogen rate.
\end{frame}


\begin{frame}[fragile]
\frametitle{Posterior for global maximum}
Let 
\begin{itemize}
\item $Y_i$ be the potato yield and 
\item $X_i$ be the nitrogen rate. 
\end{itemize}
\pause
We assume the model 
\[  Y_i \stackrel{ind}{\sim} N(\beta_0+\beta_1 X_i + \beta_2 X_i^2, \sigma^2)  \]
\pause
Assuming this quadratic curve is correct, the maximum occurs at $\gamma=-\beta_1/[2\beta_2]$. \pause
<<potato_max, echo=TRUE>>=
m = LearnBayes::blinreg(d$yield, cbind(1,d$N.rate, d$N.rate^2), 1e4)
beta1 = m$beta[,2]; beta2 = m$beta[,3]; gamma = -beta1/(2*beta2)
round(quantile(gamma, c(.025,.5,.975)))
@
This does not require any data asymptotics or approximations, e.g. delta method.
\end{frame}



\begin{frame}
\frametitle{Summary}

\begin{itemize}
\item Model: $y\sim N(X\beta,\sigma^2 \mathrm{I})$
\item Default Bayesian analysis corresponds exactly to classical regression analysis
\[ p(\beta,\sigma^2)\propto 1/\sigma^2 \implies \]
\[ \beta|\sigma^2,y \sim N(\hat{\beta}, \sigma^2 [X^\top  X]^{-1}), \sigma^2|y \sim \mbox{Inv-}\chi^2(n-k,s^2) \]
\item Conjugate subjective Bayesian analysis:
\[ \beta|\sigma^2 \sim N(m_0, \sigma^2 C_0), \sigma^2 \sim \mbox{Inv-}\chi^2(v_0,s_0^2) \implies \]
\[ \beta|\sigma^2,y \sim N(m_n, \sigma^2 C_n), \sigma^2|y \sim \mbox{Inv-}\chi^2(v_n,s_n^2) \]
\item Obtain functions of parameters and their uncertainty by simulating the parameters from their joint posterior, calculating the function, and taking posterior quantiles.
\end{itemize}

\end{frame}



\frame{\frametitle{Computation}
  For numerical stability and efficiency, the QR decomposition can be used to calculate posterior quantities. \pause 
  \begin{definition}
  For an $n\times k$ matrix $X$, a \alert{QR decomposition} is $X = QR$ for an $n\times k$ matrix $Q$ with orthonormal columns and a $k\times k$ upper triangular matrix $R$. 
	\end{definition}
	
	\pause The quantities of interest are 
	\[ \begin{array}{rl}
	V_\beta &= (X^\top  X)^{-1} \pause = ([QR]^\top  QR)^{-1} \pause = (R^\top  Q^\top  QR)^{-1} \pause = (R^\top  R)^{-1} \pause \\
	&= R^{-1}[R^\top  ]^{-1} \pause \\
	\\
	\hat{\beta} &= (X^\top  X)^{-1}X^\top  y \pause = R^{-1}[R^\top  ]^{-1}R^\top  Q^\top  y \pause = R^{-1}Q^\top  y \pause \\
	R\hat{\beta} &= Q^\top  y
	\end{array} \]
	\pause The last equation is useful because R is upper triangular and therefore the system of linear equations can be solved without requiring the inverse of R.
}


\begin{frame}[fragile]
\frametitle{Cricket chirps}
<<regression, echo=TRUE>>=
library(MASS)
X = cbind(1,temp)
n = nrow(X)
k = ncol(X)
y = matrix(chirps,n,1)

qr = qr(X); Q = qr.Q(qr); R = qr.R(qr)
stopifnot(all.equal(X, Q%*%R),  
          all.equal(rep(1,k), colSums(Q^2)),
          all.equal(diag(nrow=k), t(Q)%*%Q))

# Check for posterior propriety
stopifnot(n>k,qr$rank==k) 

# Calculate posterior hyperparameters
Rinv = solve(qr.R(qr))
vbeta = Rinv%*%t(Rinv)
betahat = qr.solve(qr,y)
df = n-k
e = qr.resid(qr,y)
s2 = sum(e^2)/df

# Simulate from the posterior
n.sims = 10000
sigma = sqrt(1/rgamma(n.sims, df/2, df*s2/2))
beta = matrix(betahat, n.sims, k, byrow=T) + sigma * mvrnorm(n.sims, rep(0,k), vbeta)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Cricket chirps}
<<posterior_simulations, message=FALSE>>=
ggplot(melt(data.frame(sigma=sigma, intercept=beta[,1], chirps=beta[,2])), 
       aes(x=value, y=after_stat(density))) +
  geom_histogram() + 
  facet_wrap(~variable, scales='free')
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Monte Carlo error}
<<quantiles, echo=TRUE>>=
# sigma^2
sqrt(df*s2/qchisq(c(.975,.025),df)) # Exact
quantile(sigma,c(.025,.975)) # MC

# beta
confint(lm(chirps~temp)) # Exact
t(apply(beta, 2, quantile, probs=c(.025,.975))) # MC 
@
\end{frame}




% \section{Simulation study}
% \begin{frame}[fragile]
% <<cache=TRUE>>=
% n = 101
% p = 100
% b = rnorm(p)
% X = matrix(rnorm(n*p), n, p)
% y = rnorm(X%*%b)
% 
% # OLS
% b_ols = lm(y~0+X)$coefficients
% mean((b_ols-b)^2)
% 
% # Bayes, standard normal prior
% b_bayes = bayesglm(y~0+X, prior.mean=0, prior.scale=1, prior.df=Inf)$coefficients
% mean((b_bayes-b)^2)
% @
% \end{frame}
% 
% 
% \subsection{Summary}
% \frame{\frametitle{Summary}
% 	\begin{itemize}
% 	\item Standard objective Bayesian regression matches standard point estimates and confidence intervals.
% 	\item For stability and efficiency, need to be thoughtful about matrix computations. 
% 	\end{itemize}
% }




\end{document}
