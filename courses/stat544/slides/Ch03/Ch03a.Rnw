\documentclass[aspectratio=169,handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Multiparameter models}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=5, fig.height=3, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("tidyverse"); theme_set(theme_bw())
library("gridExtra")
@

<<set-seed>>=
set.seed(2)
@

\frame{\titlepage}


\begin{frame}
\frametitle{Outline}
\begin{itemize}
\item Independent beta-binomial
  \begin{itemize}
  \item Independent posteriors
  \item Comparison of parameters
  \item JAGS
  \end{itemize}
\item Probability theory results
  \begin{itemize}
  \item Scaled Inv-$\chi^2$ distribution
  \item $t$-distribution
  \item Normal-Inv-$\chi^2$ distribution
  \end{itemize}
\item Normal model with unknown mean and variance
  \begin{itemize}
  \item Jeffreys prior
  \item Natural conjugate prior
  \end{itemize}
% \item Theoretical justification for simulation
%   \begin{itemize}
%   \item Strong Law of Large Numbers
%   \item Central limit theorem
%   \end{itemize}
\end{itemize}
\end{frame}






\section{Independent binomials}
\subsection{3-point success in basketball}
\frame{\frametitle{Motivating example}
  	Is Andre Dawkins 3-point percentage higher in 2013-2014 than each of the
  	past years?
		
		\vspace{0.2in} \pause
		
		\begin{center}
		\begin{tabular}{llrr}
		Season & Year & Made & Attempts \\
		\hline
		1 & 2009-2010 & 36 & 95  \\
		2 & 2010-2011 & 64 & 150 \\
    3 & 2011-2012 & 67 & 171 \\
  	4 & 2013-2014 & 64 & 152 \\
		\hline
		\end{tabular}
		\end{center}
}

\subsection{Binomial model}
\frame{\frametitle{Binomial model}
	Assume an independent binomial model,
	\[ Y_s \stackrel{ind}{\sim} Bin(n_s,\theta_s), \uncover<6->{\mbox{ i.e. },\,    p(y|\theta) = \prod_{s=1}^S p(y_s|\theta_s) = \prod_{s=1}^S  {n_s\choose y_s} \theta_s^{y_s} (1-\theta_s)^{n_s-y_s}}  \]
	\pause where 
	\begin{itemize}
	\item $y_s$ is the number of 3-pointers made in season $s$ \pause
	\item $n_s$ is the number of 3-pointers attempted in season $s$ \pause 
	\item $\theta_s$ is the unknown 3-pointer success probability in season $s$ \pause
	\item $S$ is the number of seasons \pause \pause
	\item $\theta = (\theta_1,\theta_2,\theta_3,\theta_4)'$ and $y=(y_1,y_2,y_3,y_4)$
	\end{itemize}
	\pause and assume independent beta priors distribution:
	\[ p(\theta) = \prod_{s=1}^S p(\theta_s) \pause = \prod_{s=1}^S \frac{\theta_s^{a _s-1}(1-\theta_s)^{b _s-1}}{Beta(a_s,b_s)} \mathrm{I}(0<\theta_s<1).  \]
}

\frame{\frametitle{Joint posterior}
	Derive the posterior according to Bayes rule: \pause 
	\[ \begin{array}{ll}
	p(\theta|y) &\pause \propto p(y|\theta)p(\theta) \pause \\
	&= \prod_{s=1}^S p(y_s|\theta_s) \prod_{s=1}^S p(\theta_s)  \pause \\
	&= \prod_{s=1}^S p(y_s|\theta_s) p(\theta_s) \pause \\
	&\propto \prod_{s=1}^S \mbox{Beta}(\theta_s|a _s+y_s, b _s+n_s-y_s) \pause 
	\end{array} \]
	So the posterior for each $\theta_s$ is exactly the same as if we treated each season independently. 
}

\begin{frame}[fragile]
\frametitle{Joint posterior}
<<joint-posterior>>=
a <- b <- 1
d <- data.frame(year = 1:4, 
                y = c(36, 64, 67, 64), 
                n = c(95, 150, 171, 152)) |>
  mutate(a = a + y, b = b + n - y) 

p <- d|>
  expand_grid(x = seq(0, 1, length = 1001)) %>%
  mutate(density = dbeta(x, a, b))

ggplot(p %>% mutate(year = factor(year)), aes(x = x, y = density, color = year, linetype = year)) +
  geom_line() +
  labs(x = expression(theta), y = "Posterior")
@
\end{frame}

\section{Monte Carlo estimates}
\begin{frame}[fragile]
\frametitle{Monte Carlo estimates - code}
Estimating means, medians, and quantiles (credible intervals).
<<theta-sims, echo=TRUE, dependson="joint-posterior", eval=FALSE>>=
sim = d |>
  expand_grid(rep = 1:1e3) |>
  mutate(theta = rbeta(n(), a, b)) 

hpd = function(theta, a, b, p=.95) {
  h      = dbeta((a-1)/(a+b-2),a,b)
  ftheta = dbeta(theta,a,b)
  r      = uniroot(function(x) mean(ftheta>x)-p,c(0,h))
  range(theta[which(ftheta>r$root)])
}

# expectations
sim |> 
  group_by(year) %>%
  summarize(
    mean = mean(theta),
    median = median(theta),
    ciL  = quantile(theta, c(.025,.975))[1],
    ciU  = quantile(theta, c(.025,.975))[2],
    hpdL = hpd(theta,a[1],b[1])[1],
    hpdU = hpd(theta,a[1],b[1])[2])
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Monte Carlo estimates}
Estimated means, medians, and quantiles (credible intervals).
<<theta-sims-results, echo=FALSE, dependson="theta-sims", eval=TRUE>>=
<<theta-sims>>
@
\end{frame}


\frame{\frametitle{Comparing probabilities across years}
	The scientific question of interest here is whether Dawkins's 3-point 
	percentage is higher in 2013-2014 than in each of the previous years. 
	\pause 
	Using probability notation, this is 
	\[ P(\theta_4>\theta_s|y)\mbox{ for } s=1,2,3.\]
	\pause which can be approximated via Monte Carlo as 
	\[ P(\theta_4>\theta_s|y) = E_{\theta|y}[\mathrm{I}(\theta_4>\theta_s)]\approx \frac{1}{M} \sum_{m=1}^M \mathrm{I}\left(\theta_4^{(m)} > \theta_s^{(m)}\right) \]
	\pause where
	\begin{itemize}
	\item $\theta_s^{(m)} \stackrel{ind}{\sim} Be(a_s + y_s,b_s+n_s-y_s)$ \pause 
	\item $\mathrm{I}(A)$ is in indicator function that is 1 if $A$ is true and zero otherwise. 
	\end{itemize}
}

\begin{frame}[fragile]
\frametitle{Estimated probabilities}
<<theta-results, dependson='theta_sim', echo=TRUE>>=
s <- sim |> 
  select(rep, year, theta) |>
  mutate(year = paste0("theta_",year)) |>
  pivot_wider(
    id_cols = rep,
    names_from = year,
    values_from = theta
  )

# Probabilities that season 4 percentage is higher than other seasons
mean(s$theta_4 > s$theta_1)
mean(s$theta_4 > s$theta_2)
mean(s$theta_4 > s$theta_3)
@
\end{frame}


\subsection{JAGS}
\begin{frame}[fragile]
\frametitle{Using JAGS}
<<jags, echo=TRUE>>=
library("rjags")
independent_binomials = "
model {
  for (i in 1:N) { 
    y[i] ~ dbin(theta[i],n[i]) 
    theta[i] ~ dbeta(1,1)
  }
}
"

d = list(y=c(36,64,67,64), n=c(95,150,171,152), N=4)
m = jags.model(textConnection(independent_binomials), d)
res = coda.samples(m, "theta", 1000)
@
\end{frame}

\begin{frame}[fragile]
<<jags-summary, dependson='jags', echo=TRUE>>=
summary(res)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{JAGS Analysis}
<<jags-results, dependson='jags', echo=TRUE>>=
# Extract sampled theta values
theta = as.matrix(res[[1]]) # with only 1 chain, all values are in the first list element

# Calculate probabilities that season 4 percentage is higher than other seasons
mean(theta[,4] > theta[,1])
mean(theta[,4] > theta[,2])
mean(theta[,4] > theta[,3])
@
\end{frame}






\section{Probability theory}
\begin{frame}
\frametitle{Background probability theory}

\begin{itemize}
\item Scaled Inv-$\chi^2$ distribution
\item Location-scale $t$-distribution
\item Normal-Inv-$\chi^2$ distribution
\end{itemize}

\end{frame}


\subsection{Scaled-inverse $\chi$-square}
\begin{frame}
\frametitle{Scaled-inverse $\chi^2$-distribution}
  If $\sigma^2\sim IG(a,b)$ with shape $a$ and scale $b$, 
  \pause
  then $\sigma^2\sim \mbox{Inv-}\chi^2(v, z^2)$ with
  degrees of freedom $v$ and scale $z^2$ 
  \pause 
  have the following 
  \begin{itemize}[<+->]
  \item $a=v/2$ and $b=vz^2/2$, \pause or, equivalently,
  \item $v=2a$ and $z^2=b/a$. 
  \end{itemize}
   
  \vspace{0.2in} \pause 
  
  Deriving from the inverse gamma, 
  the scaled-inverse $\chi^2$ has 
  \begin{itemize}[<+->]
  \item Mean: $vz^2/(v-2)$ for $v>2$
  \item Mode: $vz^2/(v+2)$
  \item Variance: $2v^2(z^2)^2/[(v-2)^2(v-4)]$ for $v>4$
  \end{itemize}
   
  \vspace{0.2in} \pause 
  
  So $z^2$ is a point estimate and $v\to \infty$ means the variance decreases, 
  since, for large $v$, 
  \[ \frac{2v^2(z^2)^2}{(v-2)^2(v-4)} \approx \frac{2v^2(z^2)^2}{v^3} = \frac{2(z^2)^2}{v}.   \]
\end{frame}


\begin{frame}[fragile]
\frametitle{Scaled-inverse $\chi^2$-distribution}
<<scaled-inverse-chi-square, echo=TRUE>>=
dinvgamma = function(x, shape, scale, ...) dgamma(1/x, shape = shape, rate  = scale,       ...) / x^2
dsichisq  = function(x, dof, scale, ...) dinvgamma(  x, shape = dof/2, scale = dof*scale/2, ...)
@

<<scaled-inverse-chi-square-plot, dependson='scaled-inverse-chi-square', out.width='0.7\\textwidth'>>=
d = expand.grid(v = c(10,5,1), s2 = c(1,2,3), x = seq(.01,5,.01)) |>
  mutate(density = dsichisq(x, dof = v, scale = s2),
         v_f  = as.factor(v),
         s2_f = as.factor(s2))

levels(d$s2_f) = paste('s2=',levels(d$s2_f))

ggplot(d, aes(x,density, color=v_f, linetype=v_f, group=v_f)) + 
  geom_line() + 
  facet_grid(s2_f~.) +
  theme_bw()
@
\end{frame}



\subsection{$t$-distribution}
\begin{frame}
\frametitle{Location-scale $t$-distribution}
  The $t$-distribution is a location-scale family (Casella \& Berger Thm 3.5.6), \pause i.e. if $T_v$ has a standard $t$-distribution with $v$ degrees of freedom and pdf
  \[ f_t(t) = \frac{\mathrm{\Gamma}([v+1]/2)}{\mathrm{\Gamma}(v/2)\sqrt{v\pi}}\left(1+t^2/v\right)^{-(v+1)/2}, \]
  \pause then $X=m+zT_v$ has pdf 
  \[ f_X(x) = f_t([x-m]/z)/z \pause = \frac{\mathrm{\Gamma}([v+1]/2)}{\mathrm{\Gamma}(v/2)\sqrt{v\pi}z}\left(1+\frac{1}{v}\left[\frac{x-m}{z}\right]^2\right)^{-(v+1)/2}. \]
  \pause This is referred to as a $t$ distribution with $v$ degrees of freedom, location $m$, and scale $z$; it is written as $t_{v}(m,z^2)$. \pause Also,
\[ 
t_v(m,z^2) \stackrel{v\to\infty}{\longrightarrow} N(m,z^2).
\]
\end{frame}



\begin{frame}[fragile]
\frametitle{$t$ distribution as $v$ changes}
<<t-distribution>>=
dtms = function(x,v,m=0,s=1) dt((x-m)/s,v)/s

d = expand.grid(v=c(1,10,100), 
               x = seq(-3,6,by=.1)) |>
  group_by(v) |>
  mutate(density = dtms(x, v)) |>
  ungroup() |>
  mutate(v = factor(v))

ggplot(d, aes(x=x,y=density, group=v, linetype=v, color=v)) +
  geom_line() + 
  theme(legend.position='bottom') +
  labs(x = "t")
@
\end{frame}






\begin{frame}
\frametitle{Normal-Inv-$\chi^2$ distribution}

Let $\mu|\sigma^2 \sim N(m,\sigma^2/k)$ and $\sigma^2 \sim \mbox{Inv-}\chi^2(v,z^2)$, \pause then the kernel of this joint density is

\[ \begin{array}{rl}
p(\mu,\sigma^2) &= p(\mu|\sigma^2)p(\sigma^2) \pause \\
&\propto (\sigma^2)^{-1/2} e^{-\frac{1}{2\sigma^2/k}(\mu-m)^2} (\sigma^2)^{-\frac{v}{2}-1} e^{-\frac{vz^2}{2\sigma^2}} \pause \\
&= (\sigma^2)^{-(v+3)/2} e^{-\frac{1}{2\sigma^2}\left[ k(\mu-m)^2 + vz^2 \right]}
\end{array} \]

\vspace{0.2in} \pause

In addition, 
the marginal distribution for $\mu$ is 
  \[ \begin{array}{rl}
  p(\mu) &= \int p(\mu|\sigma^2) p(\sigma^2) d\sigma^2 \pause = \cdots \\
  &= \frac{\mathrm{\Gamma}([v+1]/2)}{\mathrm{\Gamma}(v/2)\sqrt{v\pi}z/\sqrt{k}}\left(1+\frac{1}{v}\left[\frac{\mu-m}{z/\sqrt{k}}\right]^2\right)^{-(v+1)/2}. \pause 
  \end{array} \]
with $\mu \in \mathbb{R}$.
\pause
Thus $\mu \sim t_{v}(m, z^2/k)$. 
  
\end{frame}




\section{Univariate normal}
\begin{frame}[fragile]
\frametitle{Univariate normal model}
Suppose $Y_i\stackrel{ind}{\sim} N(\mu,\sigma^2)$. 

<<normal-model,fig.width=6>>=
opar = par(mar = c(0, 0, 2, 0)+0.1)
curve(dnorm, -3, 3, lwd=2, axes=F, frame=TRUE, 
      xlab="y", ylab=expression(p(y~"|"~mu,sigma^2)), main="Normal model")
abline(v=0, lty=2)
text(0,0,expression(mu),pos=4)
x = 1
arrows(-x,dnorm(-x),x,dnorm(x),code=3, lty=2)
text(x/2,dnorm(x), expression(sigma^2), pos=3)
@
\end{frame}

\subsection{Confidence interval}
\begin{frame}
\frametitle{Confidence interval for $\mu$}
\small

Let \pause
\[ \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i \qquad \mbox{and} \qquad S^2 = \frac{1}{n-1} \sum_{i=1}^n (Y_i-\overline{Y})^2.\]
  
\pause 
Then, 
\[ T_{\visible<4->{n-1}} = \frac{\overline{Y}-\mu}{S/\sqrt{n}} \pause \sim t_{n-1} \]
  
\pause and an equal-tail $100(1-\alpha)$\% confidence interval can be constructed via 
  
\[ \begin{array}{rl}
1-\alpha &= P\left(-t_{n-1,1-\alpha/2} \le T_{n-1}\le t_{n-1,1-\alpha/2}\right) \pause \\
&= P\left( \overline{Y} -\frac{t_{n-1,1-\alpha/2} S}{\sqrt{n}} \le \mu \le \overline{Y} +\frac{ t_{n-1,1-\alpha/2}S}{\sqrt{n}} \right) 
\end{array} \]
  where $t_{n-1,1-\alpha/2}$ is the t-critical value, 
  \pause
  i.e. $P(T_{n-1} > t_{n-1,1-\alpha/2}) = \alpha/2$.
  
\pause 
Thus 
\[ \overline{y} \pm t_{n-1,1-\alpha/2} s/\sqrt{n}\]
is an equal-tail $100(1-\alpha)$\% confidence interval with
$\overline{y}$ and $s$ the observed values of $\overline{Y}$ and $S$.
  
\end{frame}
  

\subsection{Priors}
\begin{frame}
\frametitle{Default priors}

Jeffreys prior can be shown to be $p(\mu,\sigma^2) \propto (1/\sigma^2)^{3/2}$. \pause But alternative methods, e.g. reference prior, find that $p(\mu,\sigma^2) \propto 1/\sigma^2$ is a more appropriate prior. 

\vspace{0.2in} \pause

The posterior under the reference prior is 
\[ \begin{array}{rl}
p(\mu,\sigma^2|y) &\propto (\sigma^2)^{-n/2}\exp\left( -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i\phantom{{}-\overline{y}{}+\overline{y}}-\mu)^2 \right) \times \frac{1}{\sigma^2} \pause \\
&= (\sigma^2)^{-n/2}\exp\left( -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\overline{y}+\overline{y}-\mu)^2 \right) \times \frac{1}{\sigma^2} \pause \\
& \vdots \\
&= (\sigma^2)^{-(n-1+3)/2} \exp\left( -\frac{1}{2\sigma^2} \left[ n(\mu-\overline{y})^2 + (n-1) s^2 \right]\right)  
\end{array} \]
\pause Thus
\[ \mu|\sigma^2,y \sim N(\overline{y}, \sigma^2/n) \qquad \sigma^2|y \sim \mbox{Inv-}\chi^2(n-1,s^2). \]
\end{frame}


\begin{frame}
\frametitle{Marginal posterior for $\mu$}

The marginal posterior for $\mu$ is 

\[ \mu|y \sim t_{n-1}(\overline{y}, s^2/n). \]
  
\vspace{0.2in} \pause
  
An equal-tailed $100(1-\alpha)$\% credible interval can be obtained via 
  
\[ \overline{y} \pm t_{n-1,1-\alpha/2} s/\sqrt{n}. \]
  
\pause 

This formula is exactly the same as the formula for a 
$100(1-\alpha/2)$\% confidence interval.
\pause
But the interpretation of this credible interval is a statement about your 
belief when your prior belief is represented by the prior 
$p(\mu,\sigma^2) \propto 1/\sigma^2$.
\end{frame}


\begin{frame}
\frametitle{Predictive distribution}
  Let $\tilde{y} \sim N(\mu,\sigma^2)$. \pause The predictive distribution is 
  \[ p(\tilde{y}|y) = \int \int p(\tilde{y}|\mu,\sigma^2)p(\mu|\sigma^2,y) p(\sigma^2|y) d\mu d\sigma^2 \]
  \pause
  The easiest way to derive this is to write $\tilde{y} = \mu + \epsilon$ with 
  \[ \mu|\sigma^2,y \sim N(\overline{y}, \sigma^2/n) \qquad \epsilon|\sigma^2,y \sim N(0,\sigma^2) \]
  independent of each other. \pause Thus
  \[ \tilde{y}|\sigma^2,y \sim N(\overline{y}, \sigma^2[1+1/n]). \]
  \pause
  with $\sigma^2|y \sim \mbox{Inv-}\chi^2(n-1,s^2)$. 
  \pause 
  Now, we can use the Normal-Inv-$\chi^2$ theory, to find that 
  \[ \tilde{y}|y \sim t_{n-1}(\overline{y}, s^2[1+1/n]). \]
\end{frame}



\begin{frame}
\frametitle{Conjugate prior for $\mu$ and $\sigma^2$}
  The joint conjugate prior for $\mu$ and $\sigma^2$ is 
  \[ \mu|\sigma^2\phantom{,y} \sim N(m,\sigma^2/k) \qquad \sigma^2\phantom{,y} \sim \mbox{Inv-}\chi^2(v,z^2) \]
  \pause where $z^2$ serves as a prior guess about $\sigma^2$ and $v$ controls how certain we are about that guess. 
  
  \vspace{0.2in} \pause 

  The posterior under this prior is 
  \[ \mu|\sigma^2,y \sim N(m',\sigma^2/k') \qquad \sigma^2|y \sim \mbox{Inv-}\chi^2(v',(z')^2) \]
  \pause where 
  \[ \begin{array}{rl}
  k' &= k+n \\
  m' &= [km + n\overline{y}]/k' \\
  v' &= v+n \\
  v'(z')^2 &= vz^2 + (n-1)S^2 + \frac{kn}{k'}(\overline{y}-m)^2
  \end{array} \]
\end{frame}



\begin{frame}
\frametitle{Marginal posterior for $\mu$}
  The marginal posterior for $\mu$ is 
  \[ \mu|y \sim t_{v'}(m', (z')^2/k'). \]
  
  \vspace{0.2in} \pause
  
  An equal-tailed $100(1-\alpha)$\% credible inteval can be obtained via 
  \[ m' \pm t_{v',1-\alpha/2} z'/\sqrt{k'}. \]
\end{frame}



\begin{frame}
\frametitle{Marginal posterior via simulation}

An alternative to deriving the closed form posterior for $\mu$ is to simulate from the distribution. \pause Recall that 
\[  \mu|\sigma^2,y \sim N(m',\sigma^2/k') \qquad \sigma^2|y \sim \mbox{Inv-}\chi^2(v',(z')^2) \]
\pause
To obtain a simulation from the posterior distribution $p(\mu,\sigma^2|y)$, 
calculate $m', k', v',$ and $z'$ and then 
\begin{enumerate}[<+->]
\item simulate $\sigma^2 \sim \mbox{Inv-}\chi^2(v',(z')^2)$ and
\item using the simulated $\sigma^2$, simulate $\mu\sim  N(m',\sigma^2/k')$. 
\end{enumerate}
\pause
Not only does this provide a sample from the joint distribution for $\mu,\sigma$ but it also (therefore) provides a sample from the marginal distribution for $\mu$. \pause The integral was suggestive:
\[  p(\mu|y) = \int p(\mu|\sigma^2,y) p(\sigma^2|y) d\sigma^2 \]
\end{frame}



\begin{frame}
\frametitle{Predictive distribution via simulation}

Similarly, we can obtain the predictive distribution via simulation. \pause Recall that 
\[  p(\tilde{y}|y) = \int \int p(\tilde{y}|\mu,\sigma^2) p(\mu|\sigma^2,y) p(\sigma^2|y) d\mu d\sigma^2 \]
\pause
To obtain a simulation from the predictive distribution $p(\tilde{y}|y)$,
calculate $m', k', v',$ and $z'$
\begin{enumerate}[<+->]
\item simulate $\sigma^2 \sim \mbox{Inv-}\chi^2(v',(z')^2)$,
\item using this $\sigma^2$, simulate $\mu\sim  N(m',\sigma^2/k')$, and
\item using these $\mu$ and $\sigma^2$, simulate $\tilde{y}\sim N(\mu,\sigma^2)$.
\end{enumerate}
\end{frame}




\begin{frame}
\frametitle{Summary of normal inference}
\begin{itemize}
\item Default analysis
  \begin{itemize}
  \item Prior: (think $\mu\sim N(0,\infty)$ and $\sigma^2 \sim \mbox{Inv-}\chi^2(0,0)$)
  \[ p(\mu,\sigma^2) \propto 1/\sigma^2 \] 
  \item Posterior:
  \[ \mu|\sigma^2,y \sim N(\overline{y}, \sigma^2/n) ,\,  \sigma^2|y \sim  \mbox{Inv-}\chi^2(n-1,S^2) ,\,  \mu|y \sim t_{n-1}(\overline{y}, S^2/n) \]
  \end{itemize}
\item Conjugate analysis
  \begin{itemize}
  \item Prior: 
  \[ \mu|\sigma^2 \sim N(m, \sigma^2/k) ,\,  \sigma^2 \sim  \mbox{Inv-}\chi^2(v,z^2) ,\,  \mu \sim t_{v}(m, z^2/k) \] 
  \item Posterior:
  \[ \mu|\sigma^2,y \sim N(m', \sigma^2/k') ,\,  \sigma^2|y \sim  \mbox{Inv-}\chi^2(v',(z')^2) ,\,  \mu|y \sim t_{v'}(m', (z')^2/k') \]
  with 
  \[ k' = k+n, m' = [km+n\overline{y}]/k', v'=v+n, \]
  \[ v'(z')^2 = vz^2 + (n-1)S^2 + \frac{kn}{k'}(\overline{y}-m)^2\]
  \end{itemize}
\end{itemize}
\end{frame}


\end{document}
