\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Multiparameter models (cont.)}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results="hide", echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library(plyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
@

<<set_seed>>=
set.seed(2)
@

\frame{\titlepage}


\begin{frame}
\frametitle{Outline}
\begin{itemize}
\item Multinomial
\item Multivariate normal
  \begin{itemize}
  \item Unknown mean
%  \item Unknown covariance
  \item Unknown mean and covariance
  \end{itemize}
\end{itemize}

\vspace{0.2in}\pause

In the process, we'll introduce the following distributions
\begin{itemize}
\item Multinomial
\item Dirichlet
\item Multivariate normal
\item Inverse Wishart (and Wishart)
\item normal-inverse Wishart distribution
\end{itemize}

\end{frame}


\section{Multinomial}
\begin{frame}
\frametitle{Motivating examples}

Multivariate count data:
\begin{itemize}
\item Item-response (Likert scale)

\includegraphics{likert-scale-1}

\item Voting

\includegraphics[width=1in]{Candidates}
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Multinomial distribution}

Suppose there are $K$ categories and each individual independently chooses category $k$ with probability $\pi_k$ such that $\sum_{k=1}^K \pi_k=1$. \pause 
Let $y_k$ be the number of individuals who choose category $k$ with $n = \sum_{k=1}^K y_k$ begin the total number of individuals.

\vspace{0.2in} \pause 

Then $Y = (Y_1,\ldots,Y_n)$ has a multinomial distribution, i.e. $Y\sim Mult(n,\pi)$, \pause with probability mass function (pmf)
\[ 
p(y) = n! \prod_{k=1}^k \frac{\pi_k^{y_k}}{y_k!}.
\]

\end{frame}



\begin{frame}
\frametitle{Properties of the multinomial distribution}

The multinomial distribution with pmf:
\[ p(y) = n! \prod_{k=1}^k \frac{\pi_k^{y_k}}{y_k!} \]
has the following properties:
\pause
\begin{itemize}
\item $E[Y_k] = n\pi_k$ \pause
\item $V[Y_k] = n\pi_k(1-\pi_k)$ \pause 
\item $Cov[Y_k,Y_{k'}] = -n\pi_k\pi_{k'}$ for $k\ne k'$
\end{itemize}

\vspace{0.2in} \pause

Marginally, each component of a multinomial distribution is a binomial distribution with $Y_k \sim Bin(n,\pi_k)$. 

\end{frame}


\begin{frame}
\frametitle{Dirichlet distribution}

Let $\pi=(\pi_1,\ldots,\pi_K)$ have a Dirichlet distribution, i.e. $\pi \sim Dir(a)$, with concentration parameter $a=(a_1,\ldots,a_K)$ where $a_k>0$ for all $k$. 

\vspace{0.2in} \pause

The probability density function (pdf) for $\pi$ is 
\[
p(\pi) = \frac{1}{\Beta(a)} \prod_{k=1}^K \pi_k^{a_k-1}
\]
where Beta($a$) is the multinomial beta function, i.e. 
\[
\Beta(a) = \frac{\prod_{k=1}^K \mG(a_k)}{\mG(\sum_{k=1}^K a_k)}.
\]

\end{frame}


\begin{frame}
\frametitle{Properties of the Dirichlet distribution}

The Dirichlet distribution with pdf 
\[
p(\pi) \propto \prod_{k=1}^K \pi_k^{a_k-1}
\]
has the following properties (where $a_0 = \sum_{k=1}^K a_k$): \pause
\begin{itemize}
\item $E[\pi_k] = \frac{a_k}{a_0}$
\item $V[\pi_k] = \frac{a_k(a_0-a_k)}{a_0^2(a_0+1)}$
\item $Cov[\pi_k,\pi_{k'}] = \frac{-a_k a_{k'}}{a_0^2(a_0+1)}$
\end{itemize}

\vspace{0.2in} \pause

Marginally, each component of a Dirichlet distribution is a beta distribution with $\pi_k \sim Be(a_k,a_0-a_k)$. 

\end{frame}


\begin{frame}
\frametitle{Bayesian inference}

The conjugate prior for a multinomial distribution, i.e. $Y\sim Mult(n,\pi)$, with unknown probability vector $\pi$ is a Dirichlet distribution. \pause
The Jeffreys prior is a Dirichlet distribution with $a_k=0.5$ for all $k$. \pause
Some argue that for large $K$, this prior will put too much mass on rare categories and would suggest the Dirichlet prior with $a_k=1/K$ for all $k$.

\vspace{0.2in} \pause 

The posterior under a Dirichlet prior is \pause 
\[ \begin{array}{rl}
p(\pi|y) &\propto p(y|\pi) p(\pi) \pause \\
&\propto \left[ \prod_{k=1}^K \pi_k^{y_k} \right] \left[ \prod_{k=1}^K \pi_k^{a_k-1} \right] \pause \\
&= \prod_{k=1}^K \pi_k^{a_k+y_k-1}
\end{array} \]
\pause
Thus $\pi|y \sim Dir(a+y)$.
\end{frame}


\section{Multivariate normal}
\begin{frame}
\frametitle{Multivariate normal distribution}

Let $Y=(Y_1,\ldots,Y_K)$ have a multivariate normal distribution, i.e. $Y\sim N_K(\mu,\mS)$ with mean $\mu$ and variance-covariance matrix $\mS$.

\vspace{0.2in} \pause

The probability density function (pdf) for $Y$ is 
\[
p(y) = (2\pi)^{-k/2}|\mS|^{-1/2} \exp\left(-\frac{1}{2}(y-\mu)^\top \mS^{-1} (y-\mu) \right)
\]

\end{frame}


\begin{frame}[fragile]
\frametitle{Bivariate normal contours}

<<multivariate_normal, fig.show='asis'>>=
# Modified from http://stats.stackexchange.com/questions/24380/how-to-get-ellipse-region-from-bivariate-normal-distributed-data
sigma = matrix(c(1,.8,.8,1), nrow=2)
sigma.inv = solve(sigma)
ellipse <- function(s,t) {u<-c(s,t); u %*% sigma.inv %*% u / 2}

e = 3
n = 50
x = seq(-e,e,length=n)
y = seq(-e,e,length=n)
z <- mapply(ellipse, as.vector(rep(x,n)), as.vector(outer(rep(0,n), y, `+`)))
contour(x,y, matrix(z,n,n), levels=(0:10), col = terrain.colors(11), 
        main=paste('Contours of a bivariate normal with correlation of',sigma[1,2]))
@

\end{frame}


\begin{frame}
\frametitle{Properties of the multivariate normal distribution}

The multivariate normal distribution with pdf 
\[
p(y) = (2\pi)^{-k/2}|\mS|^{-1/2} \exp\left(-\frac{1}{2}(y-\mu)^\top \mS^{-1} (y-\mu) \right)
\]
has the following properties: \pause
\begin{itemize}
\item $E[y_k] = \mu_k$
\item $V[y_k] = \mS_{kk}$
\item $Cov[y_k,y_{k'}] = \mS_{k,k'}$ \pause
\item Marginally, each component of a multivariate normal distribution is a normal distribution with $Y_k \sim N(\mu,\mS_{kk})$. 
\item Conditional distributions are also normal, i.e. if 
\[
\left( \begin{array}{c} Y_1 \\ Y_2 \end{array}\right) \sim N\left(\left[ \begin{array}{c} \mu_1 \\ \mu_2 \end{array} \right],\left[ \begin{array}{cc} \mS_{11} & \mS_{12} \\ \mS_{21} & \mS_{22} \end{array} \right]\right)
\]
then 
\[ 
Y_1|Y_2 = y_2 \sim N\left(\mu_1 + \mS_{12}\mS_{22}^{-1}(y_2-\mu_2), \mS_{11}-\mS_{12}\mS_{22}^{-1}\mS_{21}\right).
\]
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Representing independence in a multivariate normal}

Let $Y\sim N(\mu,\mS)$ with precision matrix $\mO = \mS^{-1}$. \pause
\begin{itemize}
\item If $\mS_{k,k'}=0$, then $Y_k$ and $Y_{k'}$ are independent of each other.
\item If $\mO_{k,k'}=0$, then $Y_k$ and $Y_{k'}$ are conditionally independent of each other given $Y_j$ for $j\ne k,k'$.
\end{itemize}

\vspace{1in}

Graphs

\vspace{1in}

\end{frame}




\subsection{Unknown mean}
\begin{frame}
\frametitle{Default inference with an unknown mean}

Let $Y_i\ind N(\mu,S)$ with default prior $p(\mu)\propto 1$, then 
\[ \begin{array}{rl}
p(\mu|y) &\propto p(y|\mu)p(\mu) \pause \\
&\propto \exp\left(-\frac{1}{2}\sum_{i=1}^n (y_i-\mu)^\top S^{-1} (y_i-\mu) \right) \pause \\
&= \exp\left(-\frac{1}{2} tr(S^{-1} S_0) \right)
\end{array} \]
where 
\[ S_0 = \sum_{i=1}^n (y_i-\mu)(y_i-\mu)^\top. \]
\pause
This posterior is proper if $n\ge K$ and, in that case, is 
\[ 
\mu|y \sim N(\overline{y}, S/n).
\]
\end{frame}



\begin{frame}
\frametitle{Conjugate inference with an unknown mean}

Let $Y_i\ind N(\mu,S)$ with conjugate prior $\mu \sim N_K(m,C)$
\[ \begin{array}{rl}
p(\mu|y) \propto& p(y|\mu)p(\mu) \pause \\
&\propto \exp\left(-\frac{1}{2}\sum_{i=1}^n (y_i-\mu)^\top S^{-1} (y_i-\mu) \right) \\
&\times \exp\left(-\frac{1}{2}\mu-m)^\top C^{-1} (\mu-m) \right) \pause \\
=& \exp\left(-\frac{1}{2} (\mu-m')^{\top}C'^{-1}(\mu-m') \right)
\end{array} \]
and thus 
\[ \mu|y \sim N(m',C') \]
where 
\[ \begin{array}{rl}
C' &= \left[C^{-1} + nS^{-1}\right]^{-1} \\
m' &= C'\left[ C^{-1}m + n S^{-1}\overline{y} \right]
\end{array} \]
\end{frame}

%\subsection{Unknown covariance}
\begin{frame}
\frametitle{Inverse Wishart distribution}

Let the $K\times K$ matrix $\mS$ have an inverse Wishart distribution, i.e. $\mS \sim IW(v,W^{-1})$, with degrees of freedom $v>K-1$ and positive definite scale matrix $W$. 

\vspace{0.2in} \pause

The pdf for $\mS$ is 
\[ 
p(\mS) \propto |W|^{v-K-1}/2\exp\left( -\frac{1}{2} tr\left(W\mS^{-1}\right) \right).
\]



\end{frame}


\begin{frame}
\frametitle{Properties of the inverse Wishart distribution}

The inverse Wishart distribution with pdf 
\[ 
p(\mS) \propto |W|^{v-K-1}/2\exp\left( -\frac{1}{2} tr\left(W\mS^{-1}\right) \right).
\]
has the following properties:
\begin{itemize}
\item $E[\mS] = (v-K-1)^{-1} W$.
\item Marginally, $\sigma_k^2 = \mS_{kk} \sim \chi^2(v,W_{kk})$.
\item If a $K\times K$ matrix $W$ has a Wishart distribution, i.e. $W\sim Wishart(v,S)$,  then $W^{-1} \sim IW(v,S^{-1})$. 
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Normal-inverse Wishart distribution}

A multivariate generalization of the normal-scaled-inverse-$\chi^2$ distribution is the normal-inverse Wishart distribution. \pause
For a vector $\mu\in \mathbb{R}^K$ and $K\times K$ matrix $\mS$, the normal-inverse Wishart distribution is 
\[ \begin{array}{rl}
\mu|\mS &\sim N(m,\mS/c) \\
\mS &\sim IW(v,W^{-1})
\end{array} \]

\vspace{0.2in} \pause

The marginal distribution for $\mu$, i.e. 
\[ 
p(\mu) = \int p(\mu|\mS)p(\mS) d\mS,
\]
\pause
is a multivariate t-distribution, i.e. 
\[ 
\mu \sim t_{v-K+1}(m,W/[c(v-K+1)]).
\]

\end{frame}




\subsection{Unknown mean and covariance}



\begin{frame}
\frametitle{Conjugate inference with unknown mean and covariance}

Let $Y_i\ind N(\mu,\mS)$ with conjugate prior 
\[ 
\mu|\mS \sim N(m,\mS/c) \quad \mS \sim IW(v,W^{-1})
\]
which has pdf \[ 
p(\mu,\mS) \propto |\mS|^{-((v+K)/2+1)} \exp\left(-\frac{1}{2} tr(W\mS^{-1}) - \frac{c}{2}(\mu-m)^\top \mS^{-1}(\mu-m)\right).
\]

\vspace{0.1in} \pause

The posterior is a normal-inverse Wishart with parameters
\[ \begin{array}{rl}
c' &= c+n \\
v' &= v+n \\
m' &= \frac{k}{k+n}m + \frac{n}{k+n}\overline{y} \\
W' &= W + S + \frac{kn}{k+n}(\overline{y}-m)(\overline{y}-m)^\top
\end{array} \]
where 
\[ 
S = \sum_{i=1}^n (y_i-\overline{y})(y_i-\overline{y})^\top.
\]
\end{frame}


\begin{frame}
\frametitle{Default inference with unknown mean and covariance}

\begin{itemize}
\item The prior $\mS \sim IW(K+1,\I)$ is non-informative in the sense that marginally each correlation has a uniform distribution on (-1,1). \pause
\item The prior 
\[ p(\mu,\mS) \propto |\mS|^{-(K+1)/2}, \]
\pause
which can be thought of as a normal-inverse-Wishart distribution with $c\to 0, v\to -1, and |W|\to 0$, \pause results in the posterior distribution
\[ \begin{array}{rl} 
\mu|\mS,y &\sim N(\overline{y}, \mS/n) \\
\mS|y &\sim IW(n-1,S^{-1}).
\end{array}
\]
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Issues with the inverse Wishart distribution}
{\scriptsize

\begin{itemize}
\item Marginals of the IW have an IG (or scaled-inverse-$\chi^2$) distribution and therefore inherit the low density near zero resulting in a (possible) bias for small variances toward larger values.
\item Due to the above issue, and the relationship between the variances and the correlations (\url{http://www.themattsimpson.com/2012/08/20/prior-distributions-for-covariance-matrices-the-scaled-inverse-wishart-prior/}), the correlations can be biased:
\begin{itemize}
\item small variances imply small correlations
\item large variances imply large correlations
\end{itemize}
\end{itemize}

\vspace{0.2in} \pause

Remedies:
\begin{itemize}
\item Don't blindly use $\I$ for the scale matrix in an IW, instead use a reasonable diagonal matrix for your data set.
\item Use the scaled Inverse wishart distribution (see pg 74)
\item Use the separation strategy, i.e. $\mS = DCD$ where $D$ is diagonal and $C$ is a correlation matrix, where you specify the standard deviations (or variances) and correlations separately. \pause In this case, Gelman recommends putting the LKJ prior (see page 582) on the correlation matrix.
\end{itemize}
}
\end{frame}

\end{document}
