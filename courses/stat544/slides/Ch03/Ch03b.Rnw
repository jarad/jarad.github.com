\documentclass[aspectratio=169,handout]{beamer}

\input{../frontmatter}
\input{../commands}

\newcommand{\bk}{{\bold k}}

\title{Multiparameter models (cont.)}

\begin{document}

%\section{Temp??} \begin{comment}

<<options, results="hide", echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=5, fig.height=3, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("tidyverse"); theme_set(theme_bw())
library("gridExtra")
library("mvtnorm") # for dmvnorm
@

<<set-seed>>=
set.seed(2)
@

\frame{\titlepage}


\begin{frame}
\frametitle{Outline}
\begin{itemize}
\item Multinomial
\item Multivariate normal
  \begin{itemize}
  \item Unknown mean
%  \item Unknown covariance
  \item Unknown mean and covariance
  \end{itemize}
\end{itemize}

\vspace{0.2in}\pause

In the process, we'll introduce the following distributions
\begin{itemize}
\item Multinomial
\item Dirichlet
\item Multivariate normal
\item Inverse Wishart (and Wishart)
\item normal-inverse Wishart distribution
\end{itemize}

\end{frame}


\section{Multinomial}
\begin{frame}
\frametitle{Motivating examples}

Multivariate count data:
\begin{itemize}
\item Item-response (Likert scale)

\includegraphics{likert-scale-1}

\item Voting

\includegraphics[width=1in]{Candidates}
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Multinomial distribution}

Suppose there are $K$ categories and each individual independently chooses category $k$ with probability $\pi_k$ such that $\sum_{k=1}^K \pi_k=1$. 
\pause 
Let 
\begin{itemize}
\item  $Y_k\in \{0,1,\ldots,n\}$ be the number of individuals who choose category $k$ 
\pause
\item with $n = \sum_{k=1}^K Y_k$ being the total number of individuals.
\end{itemize}

\vspace{0.2in} \pause 

Then $Y = (Y_1,\ldots,Y_K)$ has a multinomial distribution, i.e. $Y\sim Mult(n,\pi)$, \pause with probability mass function (pmf)
\[ 
p(y) = n! \prod_{k=1}^k \frac{\pi_k^{y_k}}{y_k!}.
\]

\end{frame}



\begin{frame}
\frametitle{Properties of the multinomial distribution}

The multinomial distribution with pmf:
\[ p(y) = n! \prod_{k=1}^k \frac{\pi_k^{y_k}}{y_k!} \]
has the following properties:
\pause
\begin{itemize}
\item $E[Y_k] = n\pi_k$ \pause
\item $Var[Y_k] = n\pi_k(1-\pi_k)$ \pause 
\item $Cov[Y_k,Y_{k'}] = -n\pi_k\pi_{k'}$ for $k\ne k'$
\end{itemize}

\vspace{0.2in} \pause

Marginally, each component of a multinomial distribution is a binomial distribution with $Y_k \sim Bin(n,\pi_k)$. 

\end{frame}


\begin{frame}
\frametitle{Dirichlet distribution}

Let $\pi=(\pi_1,\ldots,\pi_K)$ have a Dirichlet distribution, i.e. $\pi \sim Dir(a)$, with concentration parameter $a=(a_1,\ldots,a_K)$ where $a_k>0$ for all $k$. 

\vspace{0.2in} \pause

The probability density function (pdf) for $\pi$ is 
\[
p(\pi) = \frac{1}{\Beta(a)} \prod_{k=1}^K \pi_k^{a_k-1}
\]
with $\sum_{k=1}^K \pi_k = 1$ 
\pause 
and $Beta(a)$ is the beta function, i.e. 
\[
\Beta(a) = \frac{\prod_{k=1}^K \Gamma(a_k)}{\Gamma(\sum_{k=1}^K a_k)}.
\]

\end{frame}


\begin{frame}
\frametitle{Properties of the Dirichlet distribution}

The Dirichlet distribution with pdf 
\[
p(\pi) \propto \prod_{k=1}^K \pi_k^{a_k-1}
\]
has the following properties (where $a_0 = \sum_{k=1}^K a_k$): \pause
\begin{itemize}
\item $E[\pi_k] = \frac{a_k}{a_0}$
\item $Var[\pi_k] = \frac{a_k(a_0-a_k)}{a_0^2(a_0+1)}$
\item $Cov[\pi_k,\pi_{k'}] = \frac{-a_k a_{k'}}{a_0^2(a_0+1)}$
\end{itemize}

\vspace{0.2in} \pause

Marginally, each component of a Dirichlet distribution is a beta distribution with $\pi_k \sim Be(a_k,a_0-a_k)$. 

\end{frame}


\begin{frame}
\frametitle{Bayesian inference}

The conjugate prior for a multinomial distribution, i.e. $Y\sim Mult(n,\pi)$, with unknown probability vector $\pi$ is a Dirichlet distribution. \pause
The Jeffreys prior is a Dirichlet distribution with $a_k=0.5$ for all $k$. \pause
Some argue that for large $K$, this prior will put too much mass on rare categories and would suggest the Dirichlet prior with $a_k=1/K$ for all $k$.

\vspace{0.2in} \pause 

The posterior under a Dirichlet prior is \pause 
\[ \begin{array}{rl}
p(\pi|y) &\propto p(y|\pi) p(\pi) \pause \\
&\propto \left[ \prod_{k=1}^K \pi_k^{y_k} \right] \left[ \prod_{k=1}^K \pi_k^{a_k-1} \right] \pause \\
&= \prod_{k=1}^K \pi_k^{a_k+y_k-1}
\end{array} \]
\pause
Thus $\pi|y \sim Dir(a+y)$.
\end{frame}


\section{Multivariate normal}
\begin{frame}
\frametitle{Multivariate normal distribution}

Let $Y=(Y_1,\ldots,Y_K)$ have a multivariate normal distribution, i.e. $Y\sim N_K(\mu,\Sigma)$ with mean $\mu$ and variance-covariance matrix $\Sigma$.

\vspace{0.2in} \pause

The probability density function (pdf) for $Y$ is 
\[
p(y) = (2\pi)^{-k/2}|\Sigma|^{-1/2} \exp\left(-\frac{1}{2}(y-\mu)^\top \Sigma^{-1} (y-\mu) \right)
\]

\end{frame}


\begin{frame}[fragile]
\frametitle{Bivariate normal contours}

<<multivariate-normal, fig.show='asis'>>=
# Modified from http://stats.stackexchange.com/questions/24380/how-to-get-ellipse-region-from-bivariate-normal-distributed-data
rho <- 0.8

d <- expand.grid(
  x = seq(-3, 3, length = 101),
  y = seq(-3, 3, length = 101)
) |>
  rowwise() |>
  mutate(density = dmvnorm(c(x,y), sigma = matrix(c(1,rho,rho,1), nrow=2)))
  
ggplot(d, aes(x=x, y=y, z=density)) +
  geom_contour() +
  labs(x = expression(y[1]), y = expression(y[2]),
       title = paste0("Bivariate normal: correlation ", rho))
@

\end{frame}


\begin{frame}
\frametitle{Properties of the multivariate normal distribution}

\small

The multivariate normal distribution has the following properties: 
\pause
\begin{itemize}
% \item Marginally, $Y_k \sim N_1(\mu_k, \Sigma_{kk})$ \pause where 
%   \begin{itemize}
%   \item $\mu_k$ is the $k$th element of $\mu$ and 
%   \item $\Sigma_{kk}$ is the $k$th diagonal element of $\Sigma$. \pause
%   \end{itemize}
\item For any subvector $Y_{\bk}$ of $Y$ where $\bk \subset \{1,2,\ldots,K\}$ 
with $|\bk|=d$, 
\pause
we have $Y_{\bk} \sim N_d(\mu_{\bk},\Sigma_{\bk,\bk})$ 
\pause where 
  \begin{itemize}
  \item $\mu_{\bk}$ contains the corresponding elements from $\mu$ and 
  \item $\Sigma_{\bk,\bk}$ is the submatrix of $\Sigma$ constructed by 
  extracting rows $\bk$ and columns $\bk$. \pause 
  \item $Cov[Y_{\bk},Y_{\bk'}] = \Sigma_{\bk,\bk'}$ is the submatrix of $\Sigma$
  constructed by extracting rows $\bk$ and columns $\bk'$. \pause
  \end{itemize}
\item Conditional distributions are also normal, i.e. for $\bk \cap \bk' = \emptyset$
\[ \left( \begin{array}{c} Y_{\bk} \\ Y_{\bk'} \end{array}\right) \sim 
N\left(\left[ \begin{array}{c} \mu_{\bk} \\ \mu_{\bk'} \end{array} \right],
\left[ \begin{array}{cc} 
\Sigma_{\bk,\bk} & \Sigma_{\bk,\bk'} \\ 
\Sigma_{\bk',\bk} & \Sigma_{\bk',\bk'} \end{array} 
\right]\right)
\]
then 
\[ 
Y_{\bk}|Y_{\bk'} = y_{\bk'} \sim 
N\left(\mu_{\bk} + \Sigma_{\bk,\bk'} \Sigma_{\bk',\bk'}^{-1}(y_{\bk'}-\mu_{\bk'}), 
\Sigma_{\bk,\bk}-\Sigma_{\bk,\bk'}\Sigma_{\bk',\bk'}^{-1}\Sigma_{\bk',\bk}\right).
\]
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{Representing independence in a multivariate normal}

Let $Y\sim N(\mu,\Sigma)$ with precision matrix $\Omega = \Sigma^{-1}$. \pause
\begin{itemize}
\item If $\Sigma_{k,k'}=0$, then $Y_k$ and $Y_{k'}$ are independent of each other.
\item If $\Omega_{k,k'}=0$, then $Y_k$ and $Y_{k'}$ are conditionally independent of each other given $Y_j$ for $j\ne k,k'$.
\end{itemize}

% \vspace{1in}
% 
% Graphs
% 
% \vspace{1in}

\end{frame}




\subsection{Unknown mean}
\begin{frame}
\frametitle{Default inference with an unknown mean}

Let $Y_i\ind N_K(\mu,S)$ with default prior $p(\mu)\propto 1$
where $Y_i = (Y_{i1},\ldots,Y_{iK})$, then 
\[ \begin{array}{rl}
p(\mu|y) &\propto p(y|\mu)p(\mu) \pause \\
&\propto \exp\left(-\frac{1}{2}\sum_{i=1}^n (y_i-\mu)^\top S^{-1} (y_i-\mu) \right) \pause \\
&= \exp\left(-\frac{1}{2} tr(S^{-1} S_0) \right)
\end{array} \]
where 
\[ S_0 = \sum_{i=1}^n (y_i-\mu)(y_i-\mu)^\top. \]
\pause
This posterior is proper if $n\ge 1$ (text has a typo) and, 
in that case, is 
\[ 
\mu|y \sim N_K\left(\overline{y}, S/n\right).
\]
where this $\overline{y} = (\overline{y}_1,\ldots,\overline{y}_K)$ has elements
\[ 
\overline{y}_k = \frac{1}{n} \sum_{i=1}^n \overline{y}_{ik}.
\]
\end{frame}



\begin{frame}
\frametitle{Conjugate inference with an unknown mean}

Let $Y_i\ind N(\mu,S)$ with conjugate prior $\mu \sim N_K(m,C)$
\[ \begin{array}{rl}
p(\mu|y) \propto& p(y|\mu)p(\mu) \pause \\
\propto& \exp\left(-\frac{1}{2}\sum_{i=1}^n (y_i-\mu)^\top S^{-1} (y_i-\mu) \right) \\
&\times \exp\left(-\frac{1}{2}\mu-m)^\top C^{-1} (\mu-m) \right) \pause \\
=& \exp\left(-\frac{1}{2} (\mu-m')^{\top}C'^{-1}(\mu-m') \right)
\end{array} \]
and thus 
\[ \mu|y \sim N(m',C') \]
where 
\[ \begin{array}{rl}
C' &= \left[C^{-1} + nS^{-1}\right]^{-1} \\
m' &= C'\left[ C^{-1}m + n S^{-1}\overline{y} \right].
\end{array} \]
\end{frame}




%\subsection{Unknown covariance}
\begin{frame}
\frametitle{Inverse Wishart distribution}

Let the $K\times K$ matrix $\Sigma$ have an inverse Wishart distribution, 
i.e. $\Sigma \sim IW(v,W^{-1})$, 
with degrees of freedom $v>K-1$ and positive definite scale matrix $W$. 

\vspace{0.2in} \pause

The pdf for $\Sigma$ is 
\[ 
p(\Sigma) \propto 
|\Sigma|^{-(v+K+1)/2}
\exp\left( -\frac{1}{2} tr\left(W\Sigma^{-1}\right) \right).
\]



\end{frame}


\begin{frame}
\frametitle{Properties of the inverse Wishart distribution}

The inverse Wishart distribution with pdf 
\[ 
p(\Sigma) \propto 
|\Sigma|^{-(v+K+1)/2}
\exp\left( -\frac{1}{2} tr\left(W\Sigma^{-1}\right) \right).
\]
has the following properties:
\begin{itemize}[<+->]
\item $E[\Sigma] = (v-K-1)^{-1} W$ for $v>K+1$.
\item Marginally, $\sigma_k^2 = \Sigma_{kk} \sim Inv-\chi^2(v,W_{kk})$.
\item If a $K\times K$ matrix $\Sigma^{-1}$ has a Wishart distribution, 
i.e. $\Sigma^{-1}\sim Wishart(v,W)$,  then $\Sigma \sim IW(v,W^{-1})$. 
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Normal-inverse Wishart distribution}

A multivariate generalization of the normal-scaled-inverse-$\chi^2$ distribution is the normal-inverse Wishart distribution. \pause
For a vector $\mu\in \mathbb{R}^K$ and $K\times K$ matrix $\Sigma$, the normal-inverse Wishart distribution is 
\[ \begin{array}{rl}
\mu|\Sigma &\sim N(m,\Sigma/c) \\
\Sigma &\sim IW(v,W^{-1})
\end{array} \]

\vspace{0.2in} \pause

The marginal distribution for $\mu$, i.e. 
\[ 
p(\mu) = \int p(\mu|\Sigma)p(\Sigma) d\Sigma,
\]
\pause
is a multivariate t-distribution, i.e. 
\[ 
\mu \sim t_{v-K+1}(m,W/[c(v-K+1)]).
\]

\end{frame}




\subsection{Unknown mean and covariance}



\begin{frame}
\frametitle{Conjugate inference with unknown mean and covariance}

Let $Y_i\ind N(\mu,\Sigma)$ with conjugate prior 
\[ 
\mu|\Sigma \sim N(m,\Sigma/c) \quad \Sigma \sim IW(v,W^{-1})
\]
which has pdf \[ 
p(\mu,\Sigma) \propto |\Sigma|^{-((v+K)/2+1)} \exp\left(-\frac{1}{2} tr(W\Sigma^{-1}) - \frac{c}{2}(\mu-m)^\top \Sigma^{-1}(\mu-m)\right).
\]

\vspace{0.1in} \pause

The posterior is a normal-inverse Wishart with parameters
\[ \begin{array}{rl}
c' &= c+n \\
v' &= v+n \\
m' &= \frac{c}{c'}m + \frac{n}{c'}\overline{y} \\
W' &= W + S + \frac{cn}{c'}(\overline{y}-m)(\overline{y}-m)^\top
\end{array} \]
where 
\[ 
S = \sum_{i=1}^n (y_i-\overline{y})(y_i-\overline{y})^\top.
\]
\end{frame}


\begin{frame}
\frametitle{Default inference with unknown mean and covariance}

\begin{itemize}
\item The prior $\Sigma \sim IW(K+1,\I)$ is non-informative in the sense that marginally each correlation has a uniform distribution on (-1,1). \pause
\item The prior 
\[ p(\mu,\Sigma) \propto |\Sigma|^{-(K+1)/2}, \]
\pause
which can be thought of as a normal-inverse-Wishart distribution with $c\to 0$, 
$v\to -1$, and $|W|\to 0$, \pause results in the posterior distribution
\[ \begin{array}{rl} 
\mu|\Sigma,y &\sim N(\overline{y}, \Sigma/n) \\
\Sigma|y &\sim IW(n-1,S^{-1}).
\end{array}
\]
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Issues with the inverse Wishart distribution}
{\scriptsize

\begin{itemize}
\item Marginals of the IW have an IG (or scaled-inverse-$\chi^2$) distribution and therefore inherit the low density near zero resulting in a (possible) bias for small variances toward larger values.
\item Due to the above issue, and the relationship between the variances and the correlations (\url{http://www.themattsimpson.com/2012/08/20/prior-distributions-for-covariance-matrices-the-scaled-inverse-wishart-prior/}), the correlations can be biased:
\begin{itemize}
\item small variances imply small correlations
\item large variances imply large correlations
\end{itemize}
\end{itemize}

\vspace{0.2in} \pause

Remedies:
\begin{itemize}
\item Don't blindly use $\I$ for the scale matrix in an IW, instead use a reasonable diagonal matrix for your data set.
\item Use the scaled Inverse wishart distribution (see pg 74)
\item Use the separation strategy, i.e. $\Sigma = \Delta \Lambda \Delta$ where $\Delta$ 
is diagonal and $\Lambda$ is a correlation matrix, where you specify the standard deviations (or variances) and correlations separately. \pause In this case, Gelman recommends putting the LKJ prior (see page 582) on the correlation matrix.
\end{itemize}
}
\end{frame}

\end{document}
