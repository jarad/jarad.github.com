\documentclass[handout,aspectratio=169]{beamer}

\usepackage{verbatim}

\input{../frontmatter}
\input{../commands}

\title{Data Asymptotics}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA,
               fig.width=5, fig.height=3,
               size='tiny',
               out.width='0.8\\textwidth',
               fig.align='center',
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@


<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("tidyverse")
theme_set(theme_bw())
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}

\section{Normal approximation to the posterior}
\begin{frame}
\frametitle{Normal approximation to the posterior}

Suppose $p(\theta|y)$ is unimodal and roughly symmetric\pause, then a Taylor series expansion of the logarithm of the posterior around the posterior mode $\hat{\theta}$ \pause is
\[ \log p(\theta|y) = \log p(\hat{\theta}|y) - \frac{1}{2} (\theta-\hat{\theta})^\top \left[ -\frac{d^2}{d\theta^2} \log p(\theta|y) \right]_{\theta=\hat{\theta}} (\theta-\hat{\theta}) + \cdots \]
\pause
where the linear term in the expansion is zero because the derivative of the log-posterior density is zero at its mode.

\vspace{0.2in} \pause

Discarding the higher order terms,
this expansion provides a normal approximation to the posterior, i.e.
\[ p(\theta|y) \stackrel{d}{\approx} N(\hat{\theta}, \J(\hat{\theta})^{-1}) \]
\pause
where $\J(\hat{\theta})$ is the sum of the prior and observed information, i.e.
\[
\J(\hat{\theta}) = -\frac{d^2}{d\theta^2} \log p(\theta)_{|_{\theta=\hat{\theta}}}
-\frac{d^2}{d\theta^2} \log p(y|\theta)_{|_{\theta=\hat{\theta}}}.
\]
\end{frame}


\subsection{Example}
\begin{frame}
\frametitle{Binomial probability}

Let $y \sim Bin(n,\theta)$ and $\theta \sim Be(a,b)$, \pause then $\theta|y\sim \pause Be(a+y,b+n-y)$ \pause and the posterior mode is
\[ \hat{\theta}=\frac{y'}{n'} = \frac{a+y-1}{a+b+n-2}. \]
\pause
Thus
\[ \J(\hat{\theta}) = \frac{n'}{\hat{\theta}(1-\hat{\theta})}. \]
\pause
Thus
\[ p(\theta|y) \stackrel{d}{\approx} N\left( \hat{\theta}, \frac{\hat{\theta}(1-\hat{\theta})}{n'} \right). \]

\end{frame}


\begin{frame}[fragile]
\frametitle{Binomial probability}

<<beta_normal_approximation, eval=FALSE, echo=TRUE>>=
a <- b <- 1     # Prior
n <- 10; y <- 3 # Data (attempts, successes)

yp <- a + y - 1; np <- a + b + n - 2
theta_hat = yp / np # mode of a beta

d <- data.frame(x = seq(0, 1, length = 1001)) |>
  mutate(beta   = dbeta(x,a+y,b+n-y),
         normal = dnorm(x, theta_hat, sqrt(theta_hat*(1-theta_hat)/np))) |>
  pivot_longer(beta:normal, names_to = "Distribution", values_to = "density")

ggplot(d, aes(x = x, y = density, color = Distribution, linetype = Distribution)) +
  geom_line()
@

\url{https://youtu.be/cRhD9FbSb34}

\end{frame}

\begin{frame}[fragile]
\frametitle{Binomial probability}

<<beta_normal_approximation_plot>>=
<<beta_normal_approximation>>
@
\end{frame}



\section{Large-sample theory}
\begin{frame}
\frametitle{Large-sample theory}

Consider a model $y_i\stackrel{iid}{\sim} p(y|\theta_0)$ for some true value $\theta_0$.

\vspace{0.2in} \pause

\begin{itemize}
\item Does the posterior distribution converge to $\theta_0$?
\item Does a point estimator (mode) converge to $\theta_0$?
\item What is the limiting posterior distribution?
\end{itemize}

\end{frame}



\subsection{Convergence of the posterior distribution}
\begin{frame}
\frametitle{Convergence of the posterior distribution}

Consider a model $y_i\stackrel{iid}{\sim} p(y|\theta_0)$ for some true value $\theta_0$. \pause

\begin{theorem}
If the parameter space $\Theta$ is discrete and $Pr(\theta=\theta_0)>0$, then $Pr(\theta=\theta_0|y)\to 1$ as $n\to\infty$.
\end{theorem}
\pause
\begin{theorem}
If the parameter space $\Theta$ is continuous and $A$ is a neighborhood around $\theta_0$ with $Pr(\theta\in A)>0$, then $Pr(\theta\in A|y)\to 1$ as $n\to\infty$.
\end{theorem}

\end{frame}


\begin{frame}[fragile]
<<convergence_of_the_posterior_distribution_discrete, echo=TRUE, eval=FALSE>>=
n <- 1000
theta0 <- 0.3
d <- data.frame(
    n     = 1:n,
    y     = rbinom(n, 1, prob = 0.3))

dt <- expand_grid(d, theta = seq(0.1, 0.9, by = 0.1)) |>
  mutate(
    log_prob = dbinom(y, 1, prob = theta, log = TRUE),
  ) |>
  group_by(theta) |>
  arrange(n) |>
  mutate(
    log_prob = cumsum(log_prob)
  ) |>
  group_by(n) |>
  mutate(
    log_prob = log_prob - max(log_prob),
    prob     = exp(log_prob),
    prob     = prob / sum(prob),
    theta    = factor(theta)
  )

ggplot(dt, aes(x = n, y = prob,
              color = theta, group = theta)) +
  geom_line() +
  labs(
    x = "Sample size",
    y = "Posterior probability",
    title = "Posterior convergence for discrete distribution"
  )
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Posterior distribution convergence of a discrete distribution}
<<convergence_of_the_posterior_distribution_discrete-plot, echo=FALSE, eval=TRUE>>=
<<convergence_of_the_posterior_distribution_discrete>>
@
\end{frame}


\begin{frame}[fragile]
<<convergence_of_the_posterior_distribution_continuous, echo=TRUE, eval=FALSE>>=
a <- b <- 1 # prior
e <- 0.05   # window half-width

# Calculate P(theta0 - e < \theta < theta0 + e | y)
dc <- d |> mutate(y = cumsum(y),
                  prob = pbeta(theta0 + e, a + y, b + n - y) -
                         pbeta(theta0 - e, a + y, b + n - y))

# Plot calculated probability as a function of sample size
ggplot(dc, aes(x = n, y = prob)) +
  geom_line() +
  labs(
    x     = "Sample size",
    y     = "Probability within 'epsilon'",
    title = "Posterior convergence around truth"
  ) +
  ylim(0,1)
@
\end{frame}

\begin{frame}
\frametitle{Posterior distribution convergence of a continuous distribution}
<<convergence_of_the_posterior_distribution_continuous-plot, echo=FALSE, eval=TRUE>>=
<<convergence_of_the_posterior_distribution_continuous>>
@
\end{frame}



\subsection{Consistency of Bayesian point estimates}
\begin{frame}
\frametitle{Consistency of Bayesian point estimates}

Suppose $y_i\stackrel{iid}{\sim} p(y|\theta_0)$ where $\theta_0$ is a particular value for $\theta$.

\vspace{0.2in} \pause

Recall that an estimator is consistent, i.e. $\hat{\theta}\stackrel{p}{\to} \theta_0$, if
\[ \lim_{n\to\infty} P(|\hat{\theta}-\theta_0|<\epsilon) = 1. \]

\vspace{0.2in} \pause

Recall, under regularity conditions that $\hat{\theta}_{MLE}\stackrel{p}{\to} \theta_0$. \pause If Bayesian estimators converge to the MLE, then they have the same properties.

\end{frame}



\begin{frame}
\frametitle{Binomial example}

Consider $y \sim Bin(n,\theta)$ with true value $\theta=\theta_0$ and prior $\theta\sim Be(a,b)$. \pause Then $\theta|y \sim Be(a+y, b+n-y)$.

\vspace{0.2in} \pause

Recall that $\hat{\theta}_{MLE} = y/n$. \pause The following estimators are all consistent

\begin{itemize}[<+->]
\item Posterior mean: $\frac{a+y}{a+b+n}$
\item Posterior median: $\approx \frac{a+y-1/3}{a+b+n-2/3}$ for $\alpha,\beta>1$
\item Posterior mode: $\frac{a+y-1}{a+b+n-2}$
\end{itemize}
\pause since as $n\to\infty$, these all converge to $\hat{\theta}_{MLE}=y/n$.

\end{frame}


\begin{frame}[fragile]
<<beta_binomial_consistency, echo=TRUE, eval=FALSE>>=
# Calculate posterior mean, median, and mode
dbc <- dc |>
  mutate(
    mean   = (a + y) / (a + b + n),
    median = qbeta(0.5, a + y, a + b + n - y),
    mode   = (a + y - 1) / (a + b + n - 2)
  ) |>
  pivot_longer(mean:mode, names_to = "Estimator", values_to = "estimate")

# Plot estimates vs sample size
ggplot(dbc, aes(x = n, y = estimate,
              color = Estimator, linetype = Estimator, group = Estimator)) +
  geom_line() +
  geom_hline(yintercept = theta0) +
  labs(
    x = "Sample size",
    y = "Estimate",
    title = "Binomial: Bayesian Estimator Convergence"
  )
@
\end{frame}

\begin{frame}
\frametitle{Binomial: Bayesian Estimator Convergence}
<<beta_binomial_consistency-plot, echo=FALSE>>=
<<beta_binomial_consistency>>
@
\end{frame}



\begin{frame}
\frametitle{Normal example}

Consider $Y_i\stackrel{iid}{\sim} N(\theta,1)$ with known and prior $\theta\sim N(c,1)$. \pause Then
\[ \theta|y \sim N\left( \frac{1}{n+1} c +\frac{n}{n+1}\overline{y}, \frac{1}{n+1} \right) \]

\pause

Recall that $\hat{\theta}_{MLE} = \overline{y}$. \pause Since the posterior mean converges to the MLE, then the posterior mean (as well as the median and mode) are consistent.
\end{frame}


<<normal_consistency, echo=FALSE, eval=FALSE>>=
mu0 <- 10 # true mean
m <- 0; v <- 1 # prior

# Calculate posterior mean/median/mode (which is same as MLE)
d <- data.frame(
  n = 200,
  y = rnorm(n, mu0, 1)
) |>
  mutate(ybar = cumsum(y)/n,
         yhat = ybar*n/(n+v) + v*m/(n+v))

# Plot MLE vs sample size
ggplot(d, aes(x = n, y = yhat)) +
  geom_line(color = 'red') +
  geom_hline(yintercept = mu0) +
  labs(
    x = "Sample size",
    y = "Estimate",
    title = "Normal: Bayesian Estimator Convergence"
  )
@



\subsection{Asymptotic normality}
\begin{frame}
\frametitle{Asymptotic normality}
{\small

Consider the Taylor series expansion of the log posterior
{\footnotesize
\[
\log p(\theta|y) =
\log p(\hat{\theta}|y) -
\frac{1}{2} (\theta-\hat{\theta})^\top \left[-\frac{d^2}{d\theta^2} \log p(\theta|y)\right]_{\theta=\hat{\theta}}(\theta-\hat{\theta}) +
R \]
}

\pause where the linear term is zero because the derivative at the posterior mode $\hat{\theta}$ is zero and $R$ represents all higher order terms.

\vspace{0.2in} \pause

With iid observations, the coefficient for the quadratic term can be written as
\[
-\frac{d^2}{d\theta^2} [\log p(\theta|y)]_{\theta=\hat{\theta}} =
-\frac{d^2}{d\theta^2} \log p(\theta)_{\theta=\hat{\theta}} -
\sum_{i=1}^n \frac{d^2}{d\theta^2} [\log p(y_i|\theta)]_{\theta=\hat{\theta}}\]
\pause
where
\[ E_y\left[ -\frac{d^2}{d\theta^2} [\log p(y_i|\theta)]_{\theta=\hat{\theta}}\right] = \I(\theta_0) \]
\pause
where $\I(\theta_0)$ is the expected Fisher information \pause and thus, by the LLN, the second term converges to $n\I(\theta_0)$.
}
\end{frame}



\begin{frame}
\frametitle{Bernstein-von Mises Theorem}

For large $n$, we have
\[
\log p(\theta|y) \approx
\log p(\hat{\theta}|y) -
\frac{1}{2} (\theta-\hat{\theta})^\top \left[ n\I(\theta_0) \right](\theta-\hat{\theta})  \]
where $\hat{\theta}$ is the posterior mode.

\vspace{0.2in} \pause

If $\hat{\theta}\to\theta_0$ as $n\to\infty$, $\I(\hat{\theta}) \to \I(\theta_0)$ as $n\to\infty$ \pause and we have
\[ p(\theta|y) \propto \exp\left(-\frac{1}{2} (\theta-\hat{\theta})^\top \left[ n\I(\hat{\theta}) \right](\theta-\hat{\theta}) \right). \]
\pause
Thus, as $n\to\infty$
\[ \theta|y \stackrel{d}{\to} N\left(\hat{\theta}, \frac{1}{n}\I(\hat{\theta})^{-1}\right) \]
\pause
Thus, the posterior distribution is asymptotically normal.

\end{frame}


\begin{frame}
\frametitle{Binomial example}
Suppose $y\sim Bin(n,\theta)$ and $\theta\sim Be(a,b)$.
<<binomial_example, fig.width=6>>=
theta0 <- 0.3

d <- expand_grid(
  expand.grid(n = 10^(1:3), a = 10^(0:2)),
  x = seq(0, 1, length = 101)) |>
  mutate(
    b = a,
    y = theta0 * n, # Imagine we observed exactly theta0 * n successes
    mode = (a+y-1)/(a+b+n-2),
    "True posterior" = dbeta(x, a+y, b+n-y),
    "Normal approximation" = dnorm(x, mode, sqrt(mode*(1-mode)/n)),
    
    # For facet labels
    af = paste("a = b =",a), 
    nf = paste("n =", n)
  ) |>
  pivot_longer(`True posterior`:`Normal approximation`,
               names_to = "Distribution",
               values_to = "density") 


ggplot(d, aes(x=x, y=density,color=Distribution)) +
  geom_line() +
  facet_grid(nf~af) +
  geom_vline(xintercept=theta0, linetype=3) +
  theme_bw()
@
\end{frame}


\section{What can go wrong?}
\begin{frame}
\frametitle{What can go wrong?}

\begin{itemize}[<+->]
\item Not unique to Bayesian statistics
  \begin{itemize}
  \item Unidentified parameters
  \item Number of parameters increase with sample size
  \item Aliasing
  \item Unbounded likelihoods
  \item Tails of the distribution
  \item True sampling distribution is not $p(y|\theta)$
  \end{itemize}
\item Unique to Bayesian statistics
  \begin{itemize}
  \item Improper posterior
  \item Prior distributions that exclude the point of convergence
  \item Convergence to the edge of the (prior) parameter space 
  \end{itemize}
\end{itemize}

\end{frame}



\subsection{Truncated priors}
\begin{frame}
\frametitle{Truncated priors}

Suppose 
\[
Y\sim Bin(n,\theta)
\]
and the true value for $\theta$ is 
\[\theta_0=0.3.\]
\pause 
Your belief is that there is no way $\theta$ is less than 0.5 and thus you 
assign a truncated beta distribution for a prior, i.e.
\[ \theta \sim Be(a,b)\I(\theta > 0.5). \]
\pause
The posterior is then 
\[ \theta|y \sim Be(a + y, b + n - y)\I(\theta > 0.5). \]
\pause
The following occurs:
\begin{itemize}[<+->]
\item the posterior will not converge to a neighborhood around $\theta_0$,
\item no Bayesian estimators will converge to $\theta_0$, and
\item the posterior will not converge to a normal distribution. 
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{True sampling distribution is not $p(y|\theta)$}

Suppose that $f(y)$, the true sampling distribution, does not correspond to 
$p(y|\theta)$ for any $\theta = \theta_0$.

\vspace{0.2in} \pause

Then the posterior $p(\theta|y)$ converges to a $\theta_0$ that is the smallest in Kullback-Leibler divergence to the true $f(y)$ \pause where
\[ KL(f(y)||p(y|\theta)) = E\left[ \log\left(\frac{f(y)}{p(y|\theta)} \right)\right] = \int \log \left(\frac{f(y)}{p(y|\theta)} \right) f(y) dy. \]
\pause
That is, we do about the best that we can given that we have assumed the wrong sampling distribution $p(y|\theta)$.
\end{frame}






\end{document}
