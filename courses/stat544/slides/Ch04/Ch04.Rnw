\documentclass[handout,aspectratio=169]{beamer}

\usepackage{verbatim}

\input{../frontmatter}
\input{../commands}

\title{Data Asymptotics}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=5, fig.height=3, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@


<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("tidyverse")
theme_set(theme_bw())
@

<<set_seed>>=
set.seed(2)
@

\frame{\maketitle}

\section{Normal approximation to the posterior}
\begin{frame}
\frametitle{Normal approximation to the posterior}

Suppose $p(\theta|y)$ is unimodal and roughly symmetric\pause, then a Taylor series expansion of the logarithm of the posterior around the posterior mode $\hat{\theta}$ \pause is 
\[ \log p(\theta|y) = \log p(\hat{\theta}|y) - \frac{1}{2} (\theta-\hat{\theta})^\top \left[ -\frac{d^2}{d\theta^2} \log p(\theta|y) \right]_{\theta=\hat{\theta}} (\theta-\hat{\theta}) + \cdots \] 
\pause 
where the linear term in the expansion is zero because the derivative of the log-posterior density is zero at its mode.

\vspace{0.2in} \pause

Discarding the higher order terms, 
this expansion provides a normal approximation to the posterior, i.e. 
\[ p(\theta|y) \stackrel{d}{\approx} N(\hat{\theta}, \J(\hat{\theta})^{-1}) \]
\pause 
where $\J(\hat{\theta})$ is the sum of the prior and observed information, i.e. 
\[ 
\J(\hat{\theta}) = -\frac{d^2}{d\theta^2} \log p(\theta)_{|_{\theta=\hat{\theta}}} 
-\frac{d^2}{d\theta^2} \log p(y|\theta)_{|_{\theta=\hat{\theta}}}. 
\]
\end{frame}


\subsection{Example}
\begin{frame}
\frametitle{Binomial probability}

Let $y \sim Bin(n,\theta)$ and $\theta \sim Be(a,b)$, \pause then $\theta|y\sim \pause Be(a+y,b+n-y)$ \pause and the posterior mode is 
\[ \hat{\theta}=\frac{y'}{n'} = \frac{a+y-1}{a+b+n-2}. \] 
\pause 
Thus
\[ \J(\hat{\theta}) = \frac{n'}{\hat{\theta}(1-\hat{\theta})}. \]
\pause 
Thus 
\[ p(\theta|y) \stackrel{d}{\approx} N\left( \hat{\theta}, \frac{\hat{\theta}(1-\hat{\theta})}{n'} \right). \]

\end{frame}


\begin{frame}[fragile]
\frametitle{Binomial probability}

<<beta_normal_approximation, eval=FALSE, echo=TRUE>>=
a <- b <- 1     # Prior
n <- 10; y <- 3 # Data (attempts, successes)

yp = a + y - 1; np = a + b + n - 2
theta_hat = yp / np

d <- data.frame(x = seq(0, 1, length = 1001)) %>%
  mutate(beta   = dbeta(x,a+y,b+n-y),
         normal = dnorm(x, theta_hat, sqrt(theta_hat*(1-theta_hat)/np))) %>%
  pivot_longer(beta:normal, names_to = "Distribution", values_to = "density")

ggplot(d, aes(x = x, y = density, color = Distribution, linetype = Distribution)) +
  geom_line()
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Binomial probability}

<<beta_normal_approximation_plot>>=
<<beta_normal_approximation>>
@
\end{frame}



\section{Large-sample theory}
\begin{frame}
\frametitle{Large-sample theory}

Consider a model $y_i\stackrel{iid}{\sim} p(y|\theta_0)$ for some true value $\theta_0$.

\vspace{0.2in} \pause

\begin{itemize}
\item Does the posterior distribution converge to $\theta_0$?
\item Does a point estimator (mode) converge to $\theta_0$?
\item What is the limiting posterior distribution?
\end{itemize}

\end{frame}



\subsection{Convergence of the posterior distribution}
\begin{frame}
\frametitle{Convergence of the posterior distribution}

Consider a model $y_i\stackrel{iid}{\sim} p(y|\theta_0)$ for some true value $\theta_0$. \pause 

\begin{theorem}
If the parameter space $\Theta$ is discrete and $Pr(\theta=\theta_0)>0$, then $Pr(\theta=\theta_0|y)\to 1$ as $n\to\infty$. 
\end{theorem}
\pause 
\begin{theorem}
If the parameter space $\Theta$ is continuous and $A$ is a neighborhood around $\theta_0$ with $Pr(\theta\in A)>0$, then $Pr(\theta\in A|y)\to 1$ as $n\to\infty$. 
\end{theorem}

\end{frame}


\begin{frame}[fragile]
<<convergence_of_the_posterior_distribution_discrete, fig.width=10, echo=TRUE>>=
n <- 1000
theta0 <- 0.3
d <- expand_grid(n     = 1:n, 
                 y     = rbinom(n, 1, prob = 0.3), 
                 theta = seq(0.1, 0.9, by = 0.1)) |>
  mutate(
    log_prob = dbinom(y, 1, prob = theta, log = TRUE),
  ) |>
  group_by(theta) |>
  arrange(n) |>
  mutate(
    log_prob = cumsum(log_prob)
  ) %>%
  group_by(n) |>
  mutate(
    log_prob = log_prob - max(log_prob),
    prob     = exp(log_prob),
    prob     = prob / sum(prob)
  )

ggplot(d, aes(x = n, y = prob, color = theta)) + 
  geom_line()
@
\end{frame}


\begin{frame}[fragile]
<<convergence_of_the_posterior_distribution_continuous, fig.width=10, echo=TRUE>>=
a = b = 1
e = 0.05
p = rep(NA,n)
for (i in 1:n) {
  yy = sum(y[1:i])
  zz = i-yy
  p[i] = diff(pbeta(theta0+c(-e,e), a+yy, b+zz))
}
plot(p, type="l", ylim=c(0,1), ylab="Posterior probability of neighborhood", 
     xlab="n", main="Continuous parameter space")
@
\end{frame}



\subsection{Consistency of Bayesian point estimates}
\begin{frame}
\frametitle{Consistency of Bayesian point estimates}

Suppose $y_i\stackrel{iid}{\sim} p(y|\theta_0)$ where $\theta_0$ is a particular value for $\theta$.

\vspace{0.2in} \pause

Recall that an estimator is consistent, i.e. $\hat{\theta}\stackrel{p}{\to} \theta_0$, if 
\[ \lim_{n\to\infty} P(|\hat{\theta}-\theta_0|<\epsilon) = 1. \]

\vspace{0.2in} \pause

Recall, under regularity conditions that $\hat{\theta}_{MLE}\stackrel{p}{\to} \theta_0$. \pause If Bayesian estimators converge to the MLE, then they have the same properties. 

\end{frame}



\begin{frame}
\frametitle{Binomial example}

Consider $y \sim Bin(n,\theta)$ with true value $\theta=\theta_0$ and prior $\theta\sim Be(a,b)$. \pause Then $\theta|y \sim Be(a+y, b+n-y)$. 

\vspace{0.2in} \pause

Recall that $\hat{\theta}_{MLE} = y/n$. \pause The following estimators are all consistent

\begin{itemize}[<+->]
\item Posterior mean: $\frac{a+y}{a+b+n}$
\item Posterior median: $\approx \frac{a+y-1/3}{a+b+n-2/3}$ for $\alpha,\beta>1$
\item Posterior mode: $\frac{a+y-1}{a+b+n-2}$
\end{itemize}
\pause since as $n\to\infty$, these all converge to $\hat{\theta}_{MLE}=y/n$. 

\end{frame}


\begin{frame}[fragile]
<<beta_binomial_consistency, fig.width=10, echo=TRUE>>=
a = b = 1
n = 1000
theta0 = 0.5
y = rbinom(n, 1, theta0)
yy = cumsum(y)
nn = 1:n
plot(0,0, type="n", xlim=c(0,n), ylim=c(0,1), xlab="Number of flips", ylab="Estimates")
abline(h=theta0)
lines((a+yy)/(a+b+nn), col=2)
lines((a+yy-1/3)/(a+b+nn-2/3), col=3)
lines((a+yy-1)/(a+b+nn-2), col=4)
legend("topright",c("Truth","Mean","Median","Mode"), col=1:4, lty=1)
@
\end{frame}



\begin{frame}
\frametitle{Normal example}

Consider $Y_i\stackrel{iid}{\sim} N(\theta,1)$ with known and prior $\theta\sim N(c,1)$. \pause Then 
\[ \theta|y \sim N\left( \frac{1}{n+1} c +\frac{n}{n+1}\overline{y}, \frac{1}{n+1} \right) \]

\pause 

Recall that $\hat{\theta}_{MLE} = \overline{y}$. \pause Since the posterior mean converges to the MLE, then the posterior mean (as well as the median and mode) are consistent.

<<normal_consistency, fig.width=10>>=
c = 0
n = 200; nn = 1:n
theta0 = 10
y = rnorm(n, theta0, 1)
ybar = cumsum(y)/nn
yhat = ybar*nn/(nn+1) + c/(nn+1)
plot(yhat, type="l", col="red", ylim=range(ybar, yhat), xlab="n", ylab="Estimates")
abline(h=theta0)
lines(ybar, lty=2)
legend("bottomright", c("Truth","MLE","Posterior mean"), col=c("black","black","red"), lty=c(1,2,1))
@

\end{frame}



\section{Asymptotic normality}
\begin{frame}
\frametitle{Asymptotic normality}
{\small

Consider the Taylor series expansion of the log posterior
{\footnotesize 
\[ 
\log p(\theta|y) = 
\log p(\hat{\theta}|y) - 
\frac{1}{2} (\theta-\hat{\theta})^\top \left[-\frac{d^2}{d\theta^2} \log p(\theta|y)\right]_{\theta=\hat{\theta}}(\theta-\hat{\theta}) + 
R \]
}

\pause where the linear term is zero because the derivative at the posterior mode $\hat{\theta}$ is zero and $R$ represents all higher order terms.

\vspace{0.2in} \pause

With iid observations, the coefficient for the quadratic term can be written as 
\[ 
-\frac{d^2}{d\theta^2} [\log p(\theta|y)]_{\theta=\hat{\theta}} = 
-\frac{d^2}{d\theta^2} \log p(\theta)_{\theta=\hat{\theta}} - 
\sum_{i=1}^n \frac{d^2}{d\theta^2} [\log p(y_i|\theta)]_{\theta=\hat{\theta}}\]
\pause 
where 
\[ E_y\left[ -\frac{d^2}{d\theta^2} [\log p(y_i|\theta)]_{\theta=\hat{\theta}}\right] = \I(\theta_0) \]
\pause 
where $\I(\theta_0)$ is the expected Fisher information \pause and thus, by the LLN, the second term converges to $n\I(\theta_0)$. 
}
\end{frame}



\begin{frame}
\frametitle{Asymptotic normality}

For large $n$, we have
\[ 
\log p(\theta|y) \approx
\log p(\hat{\theta}|y) - 
\frac{1}{2} (\theta-\hat{\theta})^\top \left[ n\I(\theta_0) \right](\theta-\hat{\theta})  \]
where $\hat{\theta}$ is the posterior mode. 

\vspace{0.2in} \pause 

If $\hat{\theta}\to\theta_0$ as $n\to\infty$, $\I(\hat{\theta}) \to \I(\theta_0)$ as $n\to\infty$ \pause and we have 
\[ p(\theta|y) \propto \exp\left(-\frac{1}{2} (\theta-\hat{\theta})^\top \left[ n\I(\hat{\theta}) \right](\theta-\hat{\theta}) \right). \]
\pause 
Thus, as $n\to\infty$
\[ \theta|y \stackrel{d}{\to} N\left(\hat{\theta}, \frac{1}{n}\I(\hat{\theta})^{-1}\right) \]
\pause 
Thus, the posterior distribution is asymptotically normal. 

\end{frame}


\begin{frame}
\frametitle{Binomial example}
Suppose $y\sim Bin(n,\theta)$ and $\theta\sim Be(a,b)$.
<<binomial_example, fig.width=8>>=
d = expand.grid(n=10^(1:3), a=10^(0:2))
tmp = ddply(d, .(n,a), function(x) {
  d = with(x, data.frame(a=a,b=a,n=n,x=seq(0,1,length=1001)))
  mutate(d, 
         y = 0.3*n,
         mode = (a+y-1)/(a+b+n-2),
         "Posterior" = dbeta(x, a+y, b+n-y),
         "Normal approximation" = dnorm(x, mode, sqrt(mode*(1-mode)/n)))
})

plot_d = melt(tmp, 
              measure.vars=c("Posterior","Normal approximation"),
              variable.name = c("Distribution"),
              value.name = "Density")

ggplot(mutate(plot_d, af = paste("a = b =",a), nf = paste("n =", n)),
              aes(x=x, y=Density,color=Distribution)) + 
  geom_line() + 
  facet_grid(nf~af) + 
  geom_vline(xintercept=0.3, linetype=3) + 
  theme_bw()
@
\end{frame}


\subsection{What can go wrong?}
\begin{frame}
\frametitle{What can go wrong?}

\begin{itemize}[<+->]
\item Not unique to Bayesian statistics
  \begin{itemize}
  \item Unidentified parameters
  \item Number of parameters increase with sample size
  \item Aliasing
  \item Unbounded likelihoods
  \item Tails of the distribution
  \item True sampling distribution is not $p(y|\theta)$
  \end{itemize}
\item Unique to Bayesian statistics
  \begin{itemize}
  \item Improper posterior 
  \item Prior distributions that exclude the point of convergence
  \item Convergence to the edge of the parameter space (prior)
  \end{itemize}
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{True sampling distribution is not $p(y|\theta)$}

Suppose that $f(y)$ the true sampling distribution does not correspond to $p(y|\theta)$ for some $\theta=\theta_0$. 

\vspace{0.2in} \pause

Then the posterior $p(\theta|y)$ converges to a $\theta_0$ that is the smallest in Kullback-Leibler divergence to the true $f(y)$ \pause where 
\[ KL(f(y)||p(y|\theta)) = E\left[ \log\left(\frac{f(y)}{p(y|\theta)} \right)\right] = \int \log \left(\frac{f(y)}{p(y|\theta)} \right) f(y) dy. \]
\pause
That is, we do about the best that we can given that we have assumed the wrong sampling distribution $p(y|\theta)$. 
\end{frame}






\end{document}
