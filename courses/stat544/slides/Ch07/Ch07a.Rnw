\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Bayesian hypothesis testing}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library(reshape2)
library(plyr)
library(ggplot2)
library(xtable)
library(rstan)
@

<<set_seed>>=
set.seed(1)
@



\frame{\maketitle}

\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Scientific method
  \begin{itemize}
  \item Statistical hypothesis testing
  \item Simple vs composite hypotheses
  \end{itemize}
\item Simple Bayesian hypothesis testing 
  \begin{itemize}
  \item All simple hypotheses
  \item All composite hypotheses
  \end{itemize}
\item Propriety 
  \begin{itemize}
  \item Posterior 
  \item Prior predictive distribution
  \end{itemize}
\item Bayesian hypothesis testing with mixed hypotheses (models)
  \begin{itemize}
  \item Prior model probability
  \item Prior for parameters in composite hypotheses
    \begin{itemize}
    \item \alert{WARNING: do not use non-informative priors}
    \end{itemize}
  \item Posterior model probability
  \end{itemize}
\end{itemize}

\end{frame}


\setkeys{Gin}{width=0.8\textwidth}
\frame{\frametitle{Scientific method}
  \begin{center}
  \includegraphics{Ch07a-scientific_method}
  \end{center}
  
{\tiny \url{http://www.wired.com/wiredscience/2013/04/whats-wrong-with-the-scientific-method/}}
}

\section{Statistical hypothesis testing}
\begin{frame}
\frametitle{Statistical hypothesis testing}

\begin{definition}
A \alert{simple hypothesis} specifies the value for all parameters while a \alert{composite hypothesis} does not. 
\end{definition}

\vspace{0.2in} \pause

Let $Y_i \stackrel{ind}{\sim} Ber(\theta)$ and 
\begin{itemize}
\item $H_0: \theta=0.5$ (simple)
\item $H_1: \theta\ne 0.5$ (composite)
\end{itemize}

\end{frame}


\frame{\frametitle{Prior probabilities on simple hypotheses}
  What is your prior probability for the following hypotheses:
  \begin{itemize}[<+->] \small
  \item a coin flip has exactly 0.5 probability of landing heads
  \item a fertilizer treatment has zero effect on plant growth
  \item inactivation of a mouse growth gene has zero effect on mouse hair color
  \item a butterfly flapping its wings in Australia has no effect on temperature in Ames
  \item guessing the color of a card drawn from a deck has probability 0.5
  \end{itemize}
  
  \vspace{0.2in} \pause 
  
  Many null hypotheses have zero probability \emph{a priori}, so why bother performing the hypothesis test?
}



\subsection{All simple hypotheses}
\begin{frame}
\frametitle{Bayesian hypothesis testing with all simple hypotheses}

Let $Y\sim p(y|\theta)$ and $H_j: \theta = \theta_j$ for $j=1,\ldots,J$. \pause Treat this as a discrete prior on the $\theta_j$, \pause i.e. 
\[ P(\theta=\theta_j) = p_j. \]
The posterior is then 
\[ P(\theta=\theta_j|y) = \frac{p_j p(y|\theta_j)}{\sum_{k=1}^J p_k p(y|\theta_k)} \propto p_j p(y|\theta_j). \]

\vspace{0.2in} \pause

For example, suppose $Y_i\stackrel{ind}{\sim} Ber(\theta)$ \pause and $P(\theta=j/10) = 1/11$ \pause for $j=0,\ldots,10$. \pause The posterior is 
\[ P(\theta=j/10|y) \propto \frac{1}{11} \prod_{i=1}^n (j/10)^{y_i}(1-j/10)^{1-y_i} \pause = \frac{1}{11} (j/10)^{n\overline{y}}(1-j/10)^{n(1-\overline{y})} \]
\pause
If $j=0$ ($j=10$), any $y_i=1$ ($y_i=0$) will make the posterior probability zero.

\end{frame}



\begin{frame}[fragile]
\frametitle{Discrete prior example}

<<discrete_data,echo=TRUE>>=
n = 13; y = rbinom(n,1,.45); sum(y)
@

<<discrete_data_discrete_prior, dependson='discrete_data', fig.width=10>>=
d = ddply(data.frame(theta = seq(0,1,by=0.1), prior=1/11), 
          .(theta, prior), 
          function(x) {
            data.frame(posterior = x$prior * x$theta^sum(y)*(1-x$theta)^(n-sum(y)))
          })
d$posterior = d$posterior/sum(d$posterior)
ggplot(melt(d, id.var="theta"), aes(x=theta, y=value, color=variable)) + 
  geom_point(size=5) +
  theme_bw()
@

\end{frame}



\subsection{All composite hypotheses}
\begin{frame}
\frametitle{Bayesian hypothesis testing with all composite hypotheses}

Let $Y\sim p(y|\theta)$ and $H_j: \theta \in (E_{j-1},E_j]$ for $j=1,\ldots,J$. \pause Just calculate the area under the curve, i.e. 
\[ P(H_j|y) = \int_{E_{j-1}}^{E_j} p(\theta|y) d\theta \]

\vspace{0.2in} \pause

For example, suppose $Y_i\stackrel{ind}{\sim} Ber(\theta)$ \pause and $E_j = j/10$ \pause for $j=0,\ldots,10$. \pause Now, assume 
\[ \theta \sim Be(1,1) \pause \quad \mbox{and thus} \quad \theta|y \sim Be(1+n\overline{y},1+n[1-\overline{y}]). \]
\end{frame}



\begin{frame}[fragile]
\frametitle{Beta example}
<<discrete_data_continuous_prior, dependson='discrete_data', fig.width=10>>=
Ej = seq(0,1,by=0.1)
vlines = data.frame(x=Ej, xend=Ej, y=0, yend = dbeta(Ej,1+sum(y), 1+n-sum(y)))
heights = data.frame(x=Ej[-1]-.05, y=0.4, label = round(diff(pbeta(Ej,1+sum(y), 1+n-sum(y))),2))
ggplot(vlines, aes(x=x,xend=xend,y=y,yend=yend)) + 
  stat_function(fun=function(x) dbeta(x, 1+sum(y), 1+n-sum(y))) + 
  geom_segment() +
  annotate("text", x=heights$x, y=heights$y, label=heights$label) +
  theme_bw()
@

\end{frame}



\section{Posterior propriety}
\subsection{Tonelli's Theorem}
\begin{frame}
\frametitle{Tonelli's Theorem (successor to Fubini's Theorem)}

\begin{theorem}
Tonelli's Theorem states that if $\mathcal{X}$ and $\mathcal{Y}$ are $\sigma$-finite measure spaces and $f$ is non-negative and measureable, then 
\[ \int_\mathcal{X} \int_\mathcal{Y} f(x,y) dy dx =  \int_\mathcal{Y} \int_\mathcal{X}f(x,y) dx dy  \]
i.e. you can interchange the integrals (or sums). 
\end{theorem}

\vspace{0.2in} \pause

On the following slides, the use of this theorem will be indicated by TT. 

\end{frame}


\subsection{Proper priors}
\frame{\frametitle{Proper priors with discrete data}
\small
  \begin{theorem}
  If the prior is proper and the data are discrete, then the posterior is always proper. 
  \end{theorem}
  
  \vspace{0.2in} \pause 
	
	\begin{proof}
	Let $p(\theta)$ be the prior and $p(y|\theta)$ be the statistical model. \pause Thus, we need to show that 
	\[ p(y) = \int_{\Theta} p(y|\theta) p(\theta) d\theta < \infty \quad \forall y. \]
	\pause For discrete $y$, we have 
	\[ \begin{array}{ll} 
	p(y) &\le \sum_{z\in \mathcal{Y}} p(z) \pause =  \sum_{z\in \mathcal{Y}} \int_{\Theta} p(z|\theta) p(\theta) d\theta \pause \stackrel{TT}{=}  \int_{\Theta} \sum_{z\in \mathcal{Y}} p(z|\theta) p(\theta) d\theta \pause \\ \\
  &= \int_{\Theta} p(\theta) d\theta \pause = 1. 
	\end{array} \]
	\pause Thus the posterior is always proper if $y$ is discrete and the prior is proper. 
	\end{proof}
}

\frame{\frametitle{Proper priors with continuous data}
\small
	\begin{theorem}
	If the prior is proper and the data are continuous, then the posterior is almost always proper. 
	\end{theorem}
	
	\pause 
	
	\begin{proof}
	Let $p(\theta)$ be the prior and $p(y|\theta)$ be the statistical model. Thus, we need to show that 
	\[ p(y) = \int_{\Theta} p(y|\theta) p(\theta) d\theta < \infty \quad \mbox{for almost all } y. \]
	\pause For continuous $y$, we have 
	\[ \begin{array}{ll} 
	\int_{\mathcal{Y}} p(z) dz \pause =  \int_{\mathcal{Y}} \int_{\Theta} p(z|\theta) p(\theta) d\theta dz \pause\stackrel{TT}{=}  \int_{\Theta} \int_{\mathcal{Y}} p(z|\theta) dz \, p(\theta) d\theta \pause= \int_{\Theta} p(\theta) d\theta \pause = 1 
	\end{array} \]
	\pause thus $p(y)$ is finite except on a set of measure zero, \pause i.e. $p(y)$ is almost always proper. 
	\end{proof}
}



\subsection{Propriety of prior predictive distributions}
\begin{frame}
\frametitle{Proper prior predictive distributions}

In the previous derivations, we showed that 
\[ \sum_{z\in \mathcal{Y}} p(z) = 1 \qquad \mbox{and} \qquad \int_\mathcal{Y} p(z) dz = 1 \]
for discrete and continuous data, respectively. 

\vspace{0.2in} \pause

Thus, when the prior is proper, the prior predictive distribution is also proper. 

\end{frame}



\frame{\frametitle{Improper prior predictive distributions}
  \begin{theorem}
  If $p(\theta)$ is improper, then $p(y) = \int p(y|\theta) p(\theta) d\theta$ is improper. 
  \end{theorem}
  
  \vspace{0.2in} \pause
  
  \begin{proof}
  \[ \begin{array}{rl}
  \int p(y) dy &= \pause \int \int p(y|\theta) p(\theta) d\theta dy \pause \stackrel{TT}{=} \int p(\theta)  \int p(y|\theta) dy d\theta \pause \\
  &= \int p(\theta) d\theta 
  \end{array} \]
  \pause \pause since $p(\theta)$ is improper, so is $p(y)$. \pause A similar result holds for discrete $y$ replacing the integral with a sum.
  \end{proof}
}



\section{Bayesian hypothesis testing}
\begin{frame}
\frametitle{Bayesian hypothesis testing}

To evaluate the relative plausibility of a hypothesis (model)\pause, we use the posterior model probability: 
\[ p(H_j|y) \pause = \frac{p(y|H_j)p(H_j)}{p(y)} \pause = \frac{p(y|H_j)p(H_j)}{\sum_{k=1}^J p(y|H_k)p(H_k)} \pause \propto p(y|H_j)p(H_j).  \]
\pause
where $p(H_j)$ is the prior model probability \pause and 
\[ p(y|H_j) = \int p(y|\theta)p(\theta|H_j) d\theta \]
\pause is the marginal likelihood under model $H_j$ \pause and $p(\theta|H_j)$ is the prior for parameters $\theta$ when model $H_j$ is true. 

\end{frame}


\begin{frame}
\frametitle{Marginal likelihood}

The marginal likelihood calculation differs for simple vs composite hypotheses:
\begin{itemize}
\item Simple hypotheses can be considered to have a Dirac delta function for a prior, \pause e.g. if $H_0:\theta=\theta_0$ then $\theta|H_0 \sim \delta_{\theta_0}$. \pause Then the marginal likelihood is 
\[
p(y|H_0) = \int p(y|\theta)p(\theta|H_0) d\theta = p(y|\theta_0).
\]
\item \pause Composite hypotheses have a continuous prior and thus 
\[
p(y|H_j) = \int p(y|\theta)p(\theta|H_j) d\theta.
\]
\end{itemize}

\end{frame}



\frame{\frametitle{Two models}
  If we only have two models: $H_0$ and $H_1$, then 
  \[ p(H_0|y) = \frac{p(y|H_0)p(H_0)}{p(y|H_0)p(H_0)+p(y|H_1)p(H_1)} \pause = \frac{1}{1+\frac{p(y|H_1)}{p(y|H_0)}\frac{p(H_1)}{p(H_0)}} \]
  \pause 
  where 
  \[ \frac{p(H_1)}{p(H_0)} = \frac{p(H_1)}{1-p(H_1)} \]
  is the prior odds in favor of $H_1$ \pause and 
  \[ BF(H_1:H_0) = \frac{p(y|H_1)}{p(y|H_0)} = \frac{1}{BF(H_0:H_1)} \]
  is the Bayes Factor for model $H_1$ relative to $H_0$. 
}

\subsection{Binomial model}
\frame{\frametitle{Binomial model}
  Consider a coin flipping experiment so that $Y_i \stackrel{ind}{\sim} Ber(\theta)$ and the null hypothesis $H_0:\theta=0.5$ versus the alternative $H_1:\theta\ne 0.5$ \pause and $\theta|H_1\sim Be(a,b)$. \pause
  \[ \begin{array}{rl}
  BF(H_0:H_1) &= \frac{0.5^n}{\int_0^1 \theta^{n\overline{y}}(1-\theta)^{n(1-\overline{y})} \frac{\theta^{a-1}(1-\theta)^{b-1}}{Beta(a,b)} d\theta } \pause \\
  &= \frac{0.5^n}{\frac{1}{Beta(a,b)} \int_0^1 \theta^{a+n\overline{y}-1}(1-\theta)^{b+n-n\overline{y}-1} \theta} \pause\\
  &= \frac{0.5^n}{\frac{Beta(a+n\overline{y},b+n-n\overline{y})}{Beta(a,b)}} \pause \\
  &= \frac{0.5^n Beta(a,b)}{Beta(a+n\overline{y},b+n-n\overline{y})}
  \end{array} \]
  \pause and with $p(H_0)=p(H_1)$ the posterior model probability is 
    \[ P(H_0|y) = \frac{1}{1+\frac{1}{BF(H_0:H_1)}}. \]
}




\begin{frame}[fragile]
\frametitle{Sample size and sample average}
$P(H_0) = P(H_1) = 0.5$ and $\theta|H_1 \sim Be(1,1)$:
<<fig.width=8>>=
d = expand.grid(n=seq(10,30,by=10), ybar=seq(0,1,by=0.01))
bf = function(n,ybar,a=1,b=1) exp(n*log(0.5)+lbeta(a,b)-lbeta(a+n*ybar,b+n-n*ybar))
d = ddply(d, .(n,ybar), summarize, bf=bf(n,ybar))
post_prob = function(bf, prior_odds=0.5) 1/(1+1/bf*prior_odds)
ggplot(d, aes(x=ybar, y=post_prob(bf), color=factor(n))) + 
  geom_line() +
  labs(y=expression(paste("p(",H[0],"|y)"))) + 
  ylim(0,1) + 
  scale_color_discrete(name="n") +
  theme_bw()
@
\end{frame}


\begin{frame}
\frametitle{``Non-informative'' prior}

Recall that $\theta \sim Be(a,b)$ has 
\begin{itemize}
\item $a$ prior successes and
\item $b$ prior failures.
\end{itemize}

\pause

Thus, in some sense $a,b \to 0$ puts minimal prior data into the analysis. 

\vspace{0.2in} \pause If $\theta|H_1 \sim Be(e,e)$, then 

\pause 

\[ BF(H_0:H_1) = \frac{0.5^n Be(e,e)}{Be(e+n\overline{y},b+n-n\overline{y})} \stackrel{e\to 0}{\longrightarrow} \infty \quad \mbox{for any } \overline{y}\in (0,1)\]
since $Be(e,e) \stackrel{e\to 0}{\longrightarrow} \infty$.

\end{frame}


\begin{frame}[fragile]
\frametitle{Limit of proper prior}
$P(H_0) = P(H_1) = 0.5$ and $\theta|H_1 \sim Be(e,e)$:
<<fig.width=8>>=
d = expand.grid(e=10^(0:-4), ybar=seq(0,1,by=0.01), n=20)
bf = function(n,ybar,a=1,b=1) exp(n*log(0.5)+lbeta(a,b)-lbeta(a+n*ybar,b+n-n*ybar))
d = ddply(d, .(n,ybar,e), summarize, bf=bf(n,ybar,e,e))
post_prob = function(bf, prior_odds=0.5) 1/(1+1/bf*prior_odds)
ggplot(d, aes(x=ybar, y=post_prob(bf), color=factor(e))) + 
  geom_line() +
  labs(y=expression(paste("p(",H[0],"|y)"))) + 
  ylim(0,1) + 
  scale_color_discrete(name="e") +
  theme_bw()
@
\end{frame}



\frame{\frametitle{Normal example}
  Consider the model $Y\stackrel{ind}{\sim} N(\theta,1)$ and the hypothesis test
  \begin{itemize}[<+->]
  \item $H_0: \theta=0$ versus
  \item $H_1: \theta\ne 0$ with prior $\theta|H_1 \sim N(0,C)$.
  \end{itemize}
  
  \vspace{0.2in} \pause 
  
  The predictive distribution under $H_1$ is \pause
  \[ p(y|H_1) = \int p(y|\theta) p(\theta|H_1) d\theta \pause = N(y;0,1+C) \]
  \pause and the Bayes factor is 
  
  \[ BF(H_0:H_1) = \frac{N(y;0,1)}{N(y;0,1+C)}. \]
  \pause The Bayes factor will increase as $C\to \infty$ for any $y$ \pause and this only gets worse if you use an improper prior.
}

\begin{frame}[fragile]
\frametitle{Normal example}
<<normal_bayes_factor, fig.width=8>>=
d = ddply(expand.grid(y=seq(0,5,by=1), C=10^seq(0,4,by=0.1)), .(y,C), summarize,
          post_prob_H0 = 1/(1+1/exp(dnorm(y,0,1,log=TRUE)-dnorm(y,0,1+C,log=TRUE))))
ggplot(d, aes(sqrt(C), post_prob_H0, color=factor(y))) + 
  geom_line() + 
  labs(x = expression(sqrt(C)), y = expression(paste("p(",H[0],"|y)"))) + 
  scale_color_discrete(name="y") +
  theme_bw()
@
\end{frame}


\begin{frame}
\frametitle{Summary}

\begin{itemize}
\item Treat hypothesis testing as parameter estimation
  \begin{itemize}
  \item All simple hypotheses: discrete prior
  \item All composite hypotheses: continuous prior
  \end{itemize}
\item Formal Bayesian hypothesis testing 

(simple and composite hypotheses)

  \begin{itemize}
  \item Specify prior model probabilities
  \item Specify parameter priors for composite hypotheses
  
  \alert{WARNING: Do not use non-informative priors!}
  
  \item Calculate Bayes Factors or posterior model probabilities
  \end{itemize}
\end{itemize}

\end{frame}


\frame{\frametitle{Scientific method updated}
\setkeys{Gin}{width=0.8\textwidth}
  \begin{quote}
  All models are wrong, but some are useful.
  \end{quote}
  George Box 1987 \pause 

  \begin{center}
  \includegraphics{Ch07a-scientific_method_updated}
  \end{center}
  
{\tiny \url{http://www.wired.com/wiredscience/2013/04/whats-wrong-with-the-scientific-method/}}
}



% 
% 
% 
% \section{Likelihood Ratio Tests}
% \frame{\frametitle{Likelihood Ratio Tests}
%   Consider a likelihood $L(\theta|y)= p(y|\theta)$, then the liklihood ratio test statistic for testing $H_0:\theta\in \Theta_0$ and $H_1: \theta\in \Theta_0^c$ \pause with $\Theta = \Theta_0\cup \Theta_0^c$ is 
%   \[ \lambda(y) = \frac{\mbox{sup}_{\Theta_0} L(\theta|y)}{\mbox{sup} _{\Theta} L(\theta|y)} \pause = \frac{L(\hat{\theta_0}_{MLE}|y)}{L(\hat{\theta}_{MLE}|y)} \]
%   \pause where $\hat{\theta}_{MLE}$ and $\hat{\theta_0}_{MLE}$ are the (restricted) MLEs. \pause The likelihood ratio test (LRT) is any test that has a rejection region of the form $\{ y: \lambda(y)\le c\}$. (Casella \& Berger Def 8.2.1)
%   
%   \vspace{0.2in} \pause 
%   
%   Under certain conditions (see Casella \& Berger 10.3.3), as $n\to \infty$ 
%   \[ -2\log \lambda(y) \to \chi^2_{\nu} \]
%   \pause where $\nu$ us the difference between the number of free parameters specified by $\theta\in\theta_0$ and the number of free parameters specified by $\theta\in \Theta$. 
% }
% 
% 
% 
% \subsection{Binomial example}
% \frame{\frametitle{Binomial example}
%   Consider a coin flipping experiment so that $Y_i \stackrel{iid}{\sim} Ber(\theta)$ and the null hypothesis $H_0:\theta=0.5$ versus the alternative $H_1:\theta\ne 0.5$. \pause Then 
%   \[  
%   \lambda(y) = \frac{\mbox{sup} _{\Theta_0} L(\theta|y)}{\mbox{sup} _{\Theta} L(\theta|y)}= \frac{0.5^n}{\hat{\theta}_{MLE}^{n\overline{y}}(1-\hat{\theta}_{MLE})^{n-n\overline{y}}}  = \frac{0.5^n}{\overline{y}^{n\overline{y}}(1-\overline{y})^{n-n\overline{y}}} 
%    \]
%    \pause and 
%    $-2\log \lambda(y) \to \chi^2_1$ as $n\to\infty$ so 
%    \[ pvalue \approx P(\chi^2_1>-2\log \lambda(y)). \]
%    \pause If $pvalue < \alpha$, then we reject $H_0$ at level $\alpha$. \pause Typically $\alpha=0.05$. 
% }
% 
% \begin{frame}[fragile]
% \frametitle{Binomial example}
% <<lrt_binomial,fig.width=8, warning=FALSE>>=
% d = expand.grid(n=seq(10,30,by=10), ybar=seq(0,1,by=0.01))
% lrt = function(n,ybar) exp(n*log(0.5)-n*ybar*log(ybar)-n*(1-ybar)*log(1-ybar))
% pvalue = function(lrt,df=1) 1-pchisq(-2*log(lrt),df)
% d = ddply(d,.(n,ybar), summarize, lrt = lrt(n,ybar))
% p_lrt = ggplot(d, aes(x=ybar, y=pvalue(lrt,1), col=factor(n)))+geom_line()+labs(y="pvalue")+geom_abline(intercept=0.05,slope=0,col="gray")
% print(p_lrt)
% @
% \end{frame}
% 
% 


% 
% 
% 
% 
% \section{Jeffrey-Lindley paradox}
% \begin{frame}
% \frametitle{Do pvalues and posterior probabilities agree?}
% Suppose $n=10,000$ and $y=4,900$, \pause then the pvalue is 
% \[ pvalue \approx P(\chi^2_1>-2 \log(0.135)) = 0.045 \]
% \pause so we would reject $H_0$ at the 0.05 level. 
% 
% \vspace{0.2in} \pause
% 
% The posterior probability of $H_0$ is 
% \[ p(H_0|y) \approx \frac{1}{1+1/10.8} = 0.96, \]
% \pause so the probability of $H_0$ being true is 96\%. 
% 
% \vspace{0.2in} \pause 
% 
% It appears the Bayesian and LRT pvalue completely disagree!
% \end{frame}
% 
% \begin{frame}[fragile]
% \frametitle{Binomial $\overline{y}=0.49$ with $n\to\infty$} 
% <<paradox,fig.width=8>>=
% paradox = expand.grid(n=10^(seq(0,5,by=0.1)), ybar=0.49)
% paradox = ddply(paradox, .(n,ybar), summarize, pvalue=pvalue(lrt(n,ybar)), post_prob=post_prob(bf(n,ybar)))
% m = melt(paradox, id=c("n","ybar"))
% p = ggplot(m, aes(log10(n),value,col=variable))+geom_line() 
% print(p)
% @
% \end{frame}
% 
% \subsection{Jeffrey-Lindley Paradox}
% \frame{\frametitle{Jeffrey-Lindley Paradox}
%   \begin{definition}
%   The \alert{Jeffrey-Lindley Paradox} concerns a situation when comparing two hypotheses $H_0$ and $H_1$ given data $y$ \pause and find
%   \begin{itemize}[<+->]\small
%   \item a frequentist test result is significant leading to rejection of $H_0$, but
%   \item the posterior probability of $H_0$ is high. 
%   \end{itemize}
%   \end{definition}
%   
%   \vspace{0.2in} \pause
%   
%   This can happen when 
%   \begin{itemize}[<+->]\small
%   \item the effect size is small, 
%   \item $n$ is large, 
%   \item $H_0$ is relatively precise, 
%   \item $H_1$ is relative diffuse, and
%   \item the prior model odds is $\approx 1$. 
%   \end{itemize}
% }
% 
% \frame{\frametitle{Comparison}
%   The test statistic with point null hypotheses:
%   \[ \begin{array}{rl}
%   \lambda(y) &= \frac{ p\left(y|\theta_0\right)}{p\left(y|\hat{\theta}_{MLE}\right)} \\ \\
%   BF(H_0:H_1) &= \frac{ p\left(y|\theta_0\right)}{\int p(y|\theta)p(\theta|H_1) d\theta} \uncover<6->{= \frac{p(y|H_0)}{p(y|H_1)}}
%   \end{array} \]
%   \pause 
%   
%   A few comments:
%   \begin{itemize}[<+->]\small
%   \item The LRT chooses the best possible alternative value. 
%   \item The Bayesian test penalizes for vagueness in the prior.
%   \item The LRT can be interpreted as a Bayesian point mass prior exactly at the MLE. 
%   \item Generally, pvalues provide a measure of lack-of-fit of the null model. 
%   \item Bayesian tests compare predictive performance of two Bayesian models (model+prior). 
%   \end{itemize}
% }
% 
% \section{Priors for hypothesis testing}


% 
% \subsection{Binomial model}
% \begin{frame}[fragile]
% \frametitle{If $Y\sim Bin(n,\theta)$, what is $P(\theta>0.5)$?} 
% <<binomial_composite, fig.width=14>>=
% n = 10
% y = 3
% a = b = 1
% d = data.frame(x=seq(0,1,by=0.01))
% d$y = dbeta(d$x, a+y, b+n-y)
% shade = rbind(c(0.5,0), d[d$x>=0.5,], c(1,0))
% p = ggplot(d, aes(x,y)) + geom_line() + geom_polygon(data=shade, aes(x, y))
% 1-pbeta(0.5, a+y, b+n-y)
% print(p)
% @
% \end{frame}
% 
% 
% \section{Summary}
% \frame{\frametitle{Summary}
%   \begin{itemize}[<+->] \small
%   \item Bayesian hypothesis testing requires no asymptotics.
%   \item Bayesian hypothesis testing can easily handle more than 2 hypotheses.
%   \item The prior predictive distribution can often be hard to analytically evaluate.
% 
%   \vspace{0.2in} 
%   
%   \item From a Bayesian perspective, most point null hypotheses have probability zero.
%   \item Bayesian hypothesis testing is really looking at (prior) predictive performance of the models.
%   \item These predictions require reasonable proper priors.  
%   
%   \vspace{0.2in} 
%   
%   \item When all hypotheses are composite, prefer estimation.
%   \item When all hypotheses are point null, prefer estimation with a discrete prior. 
%   \end{itemize}
% }
% 
% \section{Testing}












\end{document}
