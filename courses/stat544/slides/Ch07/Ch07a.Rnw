\documentclass[handout,aspectratio=169]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Bayesian hypothesis testing}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA,
               fig.width=5, fig.height=3,
               size='tiny',
               out.width='0.8\\textwidth',
               fig.align='center',
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library("reshape2")
library("plyr")
library("ggplot2"); theme_set(theme_bw())
library("xtable")
library("rstan")
@

<<set_seed>>=
set.seed(1)
@



\frame{\maketitle}

\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Scientific method
  \begin{itemize}
  \item Statistical hypothesis testing
  \item Simple vs composite hypotheses
  \end{itemize}
\item Simple Bayesian hypothesis testing
  \begin{itemize}
  \item All simple hypotheses
  \item All composite hypotheses
  \end{itemize}
\item Propriety
  \begin{itemize}
  \item Posterior
  \item Prior predictive distribution
  \end{itemize}
\item Bayesian hypothesis testing with mixed hypotheses (models)
  \begin{itemize}
  \item Prior model probability
  \item Prior for parameters in composite hypotheses
    \begin{itemize}
    \item \alert{WARNING: do not use non-informative priors}
    \end{itemize}
  \item Posterior model probability
  \end{itemize}
\end{itemize}

\end{frame}


\setkeys{Gin}{width=0.8\textwidth}
\frame{\frametitle{Scientific method}
  \begin{center}
  \includegraphics{Ch07a-scientific_method}
  \end{center}

{\tiny \url{http://www.wired.com/wiredscience/2013/04/whats-wrong-with-the-scientific-method/}}
}

\section{Statistical hypothesis testing}
\begin{frame}
\frametitle{Statistical hypothesis testing}

\begin{definition}
A \alert{simple hypothesis} specifies the value for all parameters of interest
while a \alert{composite hypothesis} does not.
\end{definition}

\vspace{0.2in} \pause

Let $Y_i \stackrel{ind}{\sim} Ber(\theta)$ and
\begin{itemize}
\item $H_0: \theta=0.5$ (simple)
\item $H_1: \theta\ne 0.5$ (composite)
\end{itemize}

\end{frame}


\frame{\frametitle{Prior probabilities on simple hypotheses}
  What is your prior probability for the following hypotheses:
  \begin{itemize}[<+->] \small
  \item a coin flip has exactly 0.5 probability of landing heads
  \item a fertilizer treatment has zero effect on plant growth
  \item inactivation of a mouse growth gene has zero effect on mouse hair color
  \item a butterfly flapping its wings in Australia has no effect on temperature in Ames
  \item guessing the color of a card drawn from a deck has probability 0.5
  \end{itemize}

  \vspace{0.2in} \pause

  Many null hypotheses have zero probability \emph{a priori}, so why bother performing the hypothesis test?
}



\subsection{All simple hypotheses}
\begin{frame}
\frametitle{Bayesian hypothesis testing with all simple hypotheses}

Let $Y\sim p(y|\theta)$ and $H_j: \theta = d_j$ for $j=1,\ldots,J$. \pause Treat this as a discrete prior on the $d_j$, \pause i.e.
\[ P(\theta=d_j) = p_j. \]
The posterior is then
\[ P(\theta=d_j|y) = \frac{p_j p(y|d_j)}{\sum_{k=1}^J p_k p(y|d_k)} \propto p_j p(y|d_j). \]

\vspace{0.2in} \pause

For example,
suppose $Y_i\stackrel{ind}{\sim} Ber(\theta)$ \pause and $P(\theta=d_j) = 1/11$
where $d_j=j/10$
\pause
for $j=0,\ldots,10$.
\pause
The posterior is
\[ P(\theta=d_j|y)
\propto \frac{1}{11} \prod_{i=1}^n (d_j)^{y_i}(1-d_j)^{1-y_i} \pause
= (d_j)^{n\overline{y}}(1-d_j)^{n(1-\overline{y})}
\]
\pause
If $j=0$ ($j=10$), any $y_i=1$ ($y_i=0$) will make the posterior probability
of $H_0$ ($H_1$) zero.

\end{frame}



\begin{frame}[fragile]
\frametitle{Discrete prior example}

<<discrete_data,echo=TRUE>>=
n = 13; y = rbinom(n,1,.45); sum(y)
@

<<discrete_data_discrete_prior, dependson='discrete_data', fig.width=6>>=
d = ddply(data.frame(theta = seq(0,1,by=0.1), prior=1/11),
          .(theta, prior),
          function(x) {
            data.frame(posterior = x$prior * x$theta^sum(y)*(1-x$theta)^(n-sum(y)))
          })
d$posterior = d$posterior/sum(d$posterior)
ggplot(melt(d, id.var="theta"),
       aes(x=theta, y=value, color=variable)) +
  geom_point(size=5) +
  labs(x = expression(theta),
       y = "Probability")
@

\end{frame}



\subsection{All composite hypotheses}
\begin{frame}
\frametitle{Bayesian hypothesis testing with all composite hypotheses}

Let $Y\sim p(y|\theta)$ and $H_j: \theta \in (E_{j-1},E_j]$ for $j=1,\ldots,J$. \pause Just calculate the area under the curve, i.e.
prior probabilities are
\[
P(H_j) =
\pause
P(E_{j-1} < \theta < E_j) =
\pause
\int_{E_{j-1}}^{E_j} p(\theta) d\theta.
\]
\pause
and posterior probabilities are
\[
P(H_j|y) =
\pause
P(E_{j-1} < \theta < E_j|y) =
\pause
\int_{E_{j-1}}^{E_j} p(\theta|y) d\theta
\]

\vspace{0.2in} \pause

For example, suppose $Y_i\stackrel{ind}{\sim} Ber(\theta)$ \pause and $E_j = j/10$ \pause for $j=0,\ldots,10$.
\pause
Now, assume
\[
\theta \sim Be(1,1) \pause
\pause \quad \mbox{and thus} \quad
\theta|y \pause \sim Be(1+n\overline{y},1+n[1-\overline{y}]). \]
\pause

\end{frame}



\begin{frame}[fragile]
\frametitle{Beta example}
The posterior probabilities are
<<discrete_data_continuous_prior, dependson='discrete_data',fig.width=6>>=
Ej = seq(0,1,by=0.1)
vlines = data.frame(x=Ej, xend=Ej, y=0, yend = dbeta(Ej,1+sum(y), 1+n-sum(y)))
heights = data.frame(x=Ej[-1]-.05, y=0.4, label = round(diff(pbeta(Ej,1+sum(y), 1+n-sum(y))),2))
ggplot(vlines,
       aes(x   = x,
           y    = y)) +
  geom_segment(aes(xend = x, yend = yend)) +
  stat_function(fun=function(x) dbeta(x, 1+sum(y), 1+n-sum(y))) +
  annotate("text", x=heights$x, y=heights$y, label=heights$label) +
  labs(
    x = expression(theta),
    y = expression(paste("p(",theta,"|y)"))
  ) +
  scale_x_continuous(breaks = seq(0, 1, by = 0.1))
@

\end{frame}



\section{Posterior propriety}
\subsection{Tonelli's Theorem}
\begin{frame}
\frametitle{Tonelli's Theorem (successor to Fubini's Theorem)}

\begin{theorem}
Tonelli's Theorem states that if $\mathcal{X}$ and $\mathcal{Y}$ are $\sigma$-finite measure spaces and $f$ is non-negative and measureable, then
\[ \int_\mathcal{X} \int_\mathcal{Y} f(x,y) dy dx =  \int_\mathcal{Y} \int_\mathcal{X}f(x,y) dx dy  \]
i.e. you can interchange the integrals (or sums).
\end{theorem}

\vspace{0.2in} \pause

On the following slides, the use of this theorem will be indicated by TT.

\end{frame}


\subsection{Proper priors}
\frame{\frametitle{Proper priors with discrete data}
\small
  \begin{theorem}
  If the prior is proper and the data are discrete, then the posterior is always proper.
  \end{theorem}

  \pause

	\begin{proof}
	Let $p(\theta)$ be the prior and $p(y|\theta)$ be the statistical model. \pause Thus, we need to show that
	\[ p(y) = \int_{\Theta} p(y|\theta) p(\theta) d\theta < \infty \quad \forall y. \]
	\pause For discrete $y$, we have
	\[ \begin{array}{ll}
	p(y) &\le \sum_{z\in \mathcal{Y}} p(z) \pause =  \sum_{z\in \mathcal{Y}} \int_{\Theta} p(z|\theta) p(\theta) d\theta \pause \stackrel{TT}{=}  \int_{\Theta} \sum_{z\in \mathcal{Y}} p(z|\theta) p(\theta) d\theta \pause \\ \\
  &= \int_{\Theta} p(\theta) d\theta \pause = 1.
	\end{array} \]
	\pause Thus the posterior is always proper if $y$ is discrete and the prior is proper.
	\end{proof}
}

\frame{\frametitle{Proper priors with continuous data}
\small
	\begin{theorem}
	If the prior is proper and the data are continuous, then the posterior is almost always proper.
	\end{theorem}

	\pause

	\begin{proof}
	Let $p(\theta)$ be the prior and $p(y|\theta)$ be the statistical model. Thus, we need to show that
	\[ p(y) = \int_{\Theta} p(y|\theta) p(\theta) d\theta < \infty \quad \mbox{for almost all } y. \]
	\pause For continuous $y$, we have
	\[ \begin{array}{ll}
	\int_{\mathcal{Y}} p(z) dz \pause =  \int_{\mathcal{Y}} \int_{\Theta} p(z|\theta) p(\theta) d\theta dz \pause\stackrel{TT}{=}  \int_{\Theta} \int_{\mathcal{Y}} p(z|\theta) dz \, p(\theta) d\theta \pause= \int_{\Theta} p(\theta) d\theta \pause = 1
	\end{array} \]
	\pause thus $p(y)$ is finite except on a set of measure zero, \pause i.e. $p(\theta|y)$ is almost always proper.
	\end{proof}
}



\subsection{Propriety of prior predictive distributions}
\begin{frame}
\frametitle{Proper prior predictive distributions}

In the previous derivations when the prior is proper, we showed that
\[ \sum_{z\in \mathcal{Y}} p(z) = 1
\qquad \mbox{and} \qquad
\int_\mathcal{Y} p(z) dz = 1 \]
for discrete and continuous data, respectively.

\vspace{0.2in} \pause

\begin{corollary}
When the prior is proper, the prior predictive distribution is also proper.
\end{corollary}

\end{frame}



\frame{\frametitle{Improper prior predictive distributions}
  \begin{theorem}
  If $p(\theta)$ is improper, then $p(y) = \int p(y|\theta) p(\theta) d\theta$ is improper.
  \end{theorem}

  \vspace{0.2in} \pause

  \begin{proof}
  \[ \begin{array}{rl}
  \int p(y) dy &= \pause \int \int p(y|\theta) p(\theta) d\theta dy \pause \stackrel{TT}{=} \int p(\theta)  \int p(y|\theta) dy d\theta \pause \\
  &= \int p(\theta) d\theta
  \end{array} \]
  \pause \pause since $p(\theta)$ is improper, so is $p(y)$. \pause A similar result holds for discrete $y$ replacing the integral with a sum.
  \end{proof}
}



\section{Bayesian hypothesis testing}
\begin{frame}
\frametitle{Bayesian hypothesis testing}

To evaluate the relative plausibility of a hypothesis (model)\pause, we use the posterior model probability:
\[ p(H_j|y) \pause = \frac{p(y|H_j)p(H_j)}{p(y)} \pause = \frac{p(y|H_j)p(H_j)}{\sum_{k=1}^J p(y|H_k)p(H_k)} \pause \propto p(y|H_j)p(H_j).  \]
\pause
where $p(H_j)$ is the prior model probability \pause and
\[ p(y|H_j) = \int p(y|\theta)p(\theta|H_j) d\theta \]
\pause is the marginal likelihood under model $H_j$ \pause and $p(\theta|H_j)$ is the prior for parameters $\theta$ when model $H_j$ is true.

\end{frame}


\begin{frame}
\frametitle{Marginal likelihood}

The marginal likelihood calculation differs for simple vs composite hypotheses:
\begin{itemize}
\item Simple hypotheses can be considered to have a Dirac delta function for a prior, \pause e.g. if $H_0:\theta=\theta_0$ then $\theta|H_0 \sim \delta_{\theta_0}$. \pause Then the marginal likelihood is
\[
p(y|H_0) = \int p(y|\theta)p(\theta|H_0) d\theta = p(y|\theta_0).
\]
\item \pause Composite hypotheses have a continuous prior and thus
\[
p(y|H_j) = \int p(y|\theta)p(\theta|H_j) d\theta.
\]
\end{itemize}

\end{frame}



\frame{\frametitle{Two models}
  If we only have two models: $H_0$ and $H_1$, then
  \[ p(H_0|y) = \frac{p(y|H_0)p(H_0)}{p(y|H_0)p(H_0)+p(y|H_1)p(H_1)} \pause = \frac{1}{1+\frac{p(y|H_1)}{p(y|H_0)}\frac{p(H_1)}{p(H_0)}} \]
  \pause
  where
  \[ \frac{p(H_1)}{p(H_0)} = \frac{p(H_1)}{1-p(H_1)} \]
  is the prior odds in favor of $H_1$ \pause and
  \[ BF(H_1:H_0) = \frac{p(y|H_1)}{p(y|H_0)} = \frac{1}{BF(H_0:H_1)} \]
  is the Bayes Factor for model $H_1$ relative to $H_0$.
}

\subsection{Binomial model}
\frame{\frametitle{Binomial model}
  Consider a coin flipping experiment so that $Y_i \stackrel{ind}{\sim} Ber(\theta)$ and the null hypothesis $H_0:\theta=0.5$ versus the alternative $H_1:\theta\ne 0.5$ \pause and $\theta|H_1\sim Be(a,b)$. \pause
  \[ \begin{array}{rl}
  BF(H_0:H_1) &= \frac{0.5^n}{\int_0^1 \theta^{n\overline{y}}(1-\theta)^{n(1-\overline{y})} \frac{\theta^{a-1}(1-\theta)^{b-1}}{Beta(a,b)} d\theta } \pause \\
  &= \frac{0.5^n}{\frac{1}{Beta(a,b)} \int_0^1 \theta^{a+n\overline{y}-1}(1-\theta)^{b+n-n\overline{y}-1} \theta} \pause\\
  &= \frac{0.5^n}{\frac{Beta(a+n\overline{y},b+n-n\overline{y})}{Beta(a,b)}} \pause \\
  &= \frac{0.5^n Beta(a,b)}{Beta(a+n\overline{y},b+n-n\overline{y})}
  \end{array} \]
  \pause and with $p(H_0)=p(H_1)$ the posterior model probability is
    \[ P(H_0|y) = \frac{1}{1+\frac{1}{BF(H_0:H_1)}}. \]
}




\begin{frame}[fragile]
\frametitle{Sample size and sample average}
$P(H_0) = P(H_1) = 0.5$ and $\theta|H_1 \sim Be(1,1)$:
<<sample-size,fig.width=6>>=
d = expand.grid(n=seq(10,30,by=10), ybar=seq(0,1,by=0.01))
bf = function(n,ybar,a=1,b=1) exp(n*log(0.5)+lbeta(a,b)-lbeta(a+n*ybar,b+n-n*ybar))
d = ddply(d, .(n,ybar), summarize, bf=bf(n,ybar))
post_prob = function(bf, prior_odds=0.5) 1/(1+1/bf*prior_odds)
ggplot(d, aes(x=ybar, y=post_prob(bf), color=factor(n))) +
  geom_line() +
  labs(y=expression(paste("p(",H[0],"|y)"))) +
  ylim(0,1) +
  scale_color_discrete(name="n")
@
\end{frame}


\begin{frame}
\frametitle{``Non-informative'' prior}

Recall that $\theta \sim Be(a,b)$ has
\begin{itemize}
\item $a$ prior successes and
\item $b$ prior failures.
\end{itemize}

\pause

Thus, in some sense $a,b \to 0$ puts minimal prior data into the analysis.

\vspace{0.2in} \pause If $\theta|H_1 \sim Be(e,e)$, then

\pause

\[ BF(H_0:H_1) = \frac{0.5^n Be(e,e)}{Be(e+n\overline{y},e+n-n\overline{y})} \stackrel{e\to 0}{\longrightarrow} \infty \quad \mbox{for any } \overline{y}\in (0,1)\]
since $Be(e,e) \stackrel{e\to 0}{\longrightarrow} \infty$.

\end{frame}


\begin{frame}[fragile]
\frametitle{Limit of proper prior}
$P(H_0) = P(H_1) = 0.5$ and $\theta|H_1 \sim Be(e,e)$:
<<proper-prior-limit,fig.width=6>>=
d = expand.grid(e=10^(0:-4), ybar=seq(0,1,by=0.01), n=20)
bf = function(n,ybar,a=1,b=1) exp(n*log(0.5)+lbeta(a,b)-lbeta(a+n*ybar,b+n-n*ybar))
d = ddply(d, .(n,ybar,e), summarize, bf=bf(n,ybar,e,e))
post_prob = function(bf, prior_odds=0.5) 1/(1+1/bf*prior_odds)
ggplot(d, aes(x=ybar, y=post_prob(bf), color=factor(e))) +
  geom_line() +
  labs(y=expression(paste("p(",H[0],"|y)"))) +
  ylim(0,1) +
  scale_color_discrete(name="e")
@
\end{frame}



\frame{\frametitle{Normal example}
  Consider the model $Y\sim N(\theta,1)$ and the hypothesis test
  \begin{itemize}[<+->]
  \item $H_0: \theta=0$ versus
  \item $H_1: \theta\ne 0$ with prior $\theta|H_1 \sim N(0,C)$.
  \end{itemize}

  \vspace{0.2in} \pause

  The predictive distribution under $H_1$ is \pause
  \[ p(y|H_1) = \int p(y|\theta) p(\theta|H_1) d\theta \pause = N(y;0,1+C) \]
  \pause and the Bayes factor is

  \[ BF(H_0:H_1) = \frac{N(y;0,1)}{N(y;0,1+C)}. \]
  \pause The Bayes factor will increase as $C\to \infty$ for any $y$ \pause and this only gets worse if you use an improper prior.
}

\begin{frame}[fragile]
\frametitle{Normal example}
<<normal_bayes_factor>>=
d = ddply(expand.grid(y=seq(0,5,by=1), C=10^seq(0,4,by=0.1)), .(y,C), summarize,
          post_prob_H0 = 1/(1+1/exp(dnorm(y,0,1,log=TRUE)-dnorm(y,0,1+C,log=TRUE))))
ggplot(d, aes(sqrt(C), post_prob_H0, color=factor(y))) +
  geom_line() +
  labs(x = expression(sqrt(C)), y = expression(paste("p(",H[0],"|y)"))) +
  scale_color_discrete(name="y")
@
\end{frame}


\begin{frame}
\frametitle{Summary}

\begin{itemize}
\item Treat hypothesis testing as parameter estimation
  \begin{itemize}
  \item All simple hypotheses: discrete prior
  \item All composite hypotheses: continuous prior
  \end{itemize}
\item Formal Bayesian hypothesis testing

(simple and composite hypotheses)

  \begin{itemize}
  \item Specify prior model probabilities
  \item Specify parameter priors for composite hypotheses

  \alert{WARNING: Do not use non-informative priors!}

  \item Calculate Bayes Factors or posterior model probabilities
  \end{itemize}
\end{itemize}

\end{frame}


\frame{\frametitle{Scientific method updated}
\setkeys{Gin}{width=0.6\textwidth}
  \begin{quote}
  All models are wrong, but some are useful.
  \end{quote}
  George Box 1987 \pause

  \begin{center}
  \includegraphics{Ch07a-scientific_method_updated}
  \end{center}

{\tiny \url{http://www.wired.com/wiredscience/2013/04/whats-wrong-with-the-scientific-method/}}
}


\end{document}
