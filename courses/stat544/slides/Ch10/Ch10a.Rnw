\documentclass[handout,aspectratio=169]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Introduction to Bayesian Computation}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=5, fig.height=3, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
# library("reshape2")
# library("plyr")
# library("dplyr")
# library("ggplot2") 
library("tidyverse"); theme_set(theme_bw())

<<set-seed>>=
set.seed(1)
@

\frame{\maketitle}


\section{Bayesian computation}
\subsection{Goals}
\begin{frame}
\frametitle{Bayesian computation}

Goals:
\begin{itemize}[<+->]
\item $E_{\theta|y}[h(\theta)|y] = \int h(\theta) p(\theta|y) d\theta$
\item $p(y) = \int p(y|\theta)p(\theta) d\theta = E_\theta[p(y|\theta)]$
\end{itemize}

\vspace{0.2in} \pause

Approaches:
\begin{itemize}
\item Numerical integration
\item Stochastic (Monte Carlo) integration
  \begin{itemize}
  \item Theoretical justification
  \item Gridding
  \item Inverse CDF
  \item Accept-reject
  \end{itemize}
\end{itemize}
\end{frame}



\subsection{Numerical integration - overview}
\begin{frame}
\frametitle{Numerical integration}
Numerical integration where   
\[ 
E[h(\theta)|y] 
= \int h(\theta)p(\theta|y) d\theta 
\approx \frac{1}{S} \sum_{S=1}^S w_s h\left(\theta^{(s)}\right)p\left(\left.\theta^{(s)}\right|y\right) 
\]
\begin{itemize}
\item $\theta^{(s)}$ are selected points, 
\item $w_s$ is the weight given to the point $\theta^{(s)}$, and
\item the error can be bounded. 
\end{itemize}
\end{frame}



\subsection{Stochastic integration}
\begin{frame}
\frametitle{Stochastic integration - overview}

Monte Carlo (simulation) methods where \[ E[h(\theta)|y] = \int h(\theta)p(\theta|y) d\theta \approx \frac{1}{S} \sum_{S=1}^S w_s h\left(\theta^{(s)}\right) \]
  and
\begin{itemize}
\item $\theta^{(s)} \stackrel{ind}{\sim} g(\theta)$ (for some proposal distribution $g$),
\item $w_s = p(\theta^{(s)}|y)/g(\theta^{(s)})$, 
\item and we have SLLN and CLT.
\end{itemize}
\end{frame}




\subsection{Numerical integration example}
\begin{frame}
\frametitle{Example: Normal-Cauchy model}
  Let $Y\sim N(\theta,1)$ with $\theta\sim Ca(0,1)$. \pause The posterior is 
  \[ p(\theta|y) \propto p(y|\theta)p(\theta) \propto \frac{\exp(-(y-\theta)^2/2)}{1+\theta^2}=q(\theta|y) \]
  \pause which is not a known distribution. \pause We might be interested in
  \begin{enumerate}
  \item  normalizing this posterior, i.e. calculating
  \[ c(y) = \int q(\theta|y) d\theta \]
  \item or in calculating the posterior mean, i.e. 
  \[ E[\theta|y] = \int \theta p(\theta|y) d\theta = \int \theta \frac{q(\theta|y)}{c(y)} d\theta. \]
  \end{enumerate}
\end{frame}



\begin{frame}[fragile]
\frametitle{Normal-Cauchy: marginal likelihood }
<<data, echo=TRUE>>=
y <- 1 # Data
@

<<normal_cauchy_posterior, dependson='data', echo=TRUE>>=
q <- function(theta, y, log = FALSE) {
  out <- - (y - theta)^2 / 2 - log(1 + theta^2)
  if (log) return(out)
  return(exp(out))
}

# Find normalizing constant for q(theta|y)
w     <- 0.1                             # grid width
theta <- seq(-5, 5, by = w) + y
(cy <- sum(q(theta,y) * w))              # grid-based estimate 

integrate(function(x) q(x,y), -Inf, Inf) # numerical integration
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Normal-Cauchy: distribution}
<<normal_cauchy_posterior_plot, dependson=c('data','normal_cauchy_posterior'), echo=FALSE, fig.width=6>>=
ggplot(
  data = data.frame(
    theta = theta, 
    pdf   = q(theta,y)/cy),
  mapping = aes(
    x = theta,
    y = pdf,
    xend = theta,
    yend = 0)) +
  # geom_point() +
  geom_segment() +
  geom_line()
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Posterior expectation - Reimann Integration}
\[ E[h(\theta)|y] \approx \sum_{s=1}^S w_s h\left(\theta^{(s)}\right)p\left(\left.\theta^{(s)}\right|y\right) = \sum_{s=1}^S w_s h\left(\theta^{(s)}\right) \frac{q\left(\left.\theta^{(s)}\right|y\right)}{c(y)} \]
<<normal_cauchy_posterior_expectation, dependson=c('data','normal_cauchy_posterior'), echo=TRUE>>=
h <- function(theta) theta # expectation

sum(w * h(theta) * q(theta,y) / cy)
@

\end{frame}



<<normal_cauchy_posterior_mean, dependson=c('normal_cauchy_posterior'), echo=FALSE>>=
# Calculate marginal likelihood, p(y)
py <- function(y, log = FALSE) {
  int <- integrate(function(x) q(x, y, log = FALSE), -Inf, Inf)
  if (int$message!="OK") {
    warning(paste("Could not compute marginal likelihood for y=", y, "\n"))
    return(NA)
  }
  int$value 
}

# Find posterior expectation
post_expectation <- function(y) {
  qy  <- py(y)
  int <- integrate(function(x) x * q(x,y) / qy, -Inf, Inf)
  if (int$message != "OK") {
    warning(paste("Could not compute posterior expectation for y=", y, "\n"))
    return(NA)
  }
  int$value
}

res <- data.frame(y = seq(from = -5,to = 5, by = 0.1)) |>
  rowwise(y) |>
  summarize(Cauchy = post_expectation(y), .groups = "drop") |>
  mutate(`N(0,1)` = y/2,
         `improper uniform` = y)
@




\begin{frame}[fragile]
\frametitle{Posterior expectation as a function of observed data}
<<normal_cauchy_posterior_mean_plot, dependson='normal_cauchy_posterior_mean'>>=
ggplot(
  data = pivot_longer(
    res, 
    -y, 
    names_to = "Prior", 
    values_to = "value"),
  aes(x = y, 
      y = value, 
      color = Prior))+
  geom_line() + 
  labs(
    x = "y",
    y = "Posterior expectation") 
@
\end{frame}



\subsection{Monte Carlo methods}
\begin{frame}
\frametitle{Convergence review}

\tiny

Three main notions of convergence
of a sequence of random variables $X_1,X_2,\ldots$ and a random variable $X$:
\begin{itemize}[<+->]
\item Convergence in distribution ($X_n \stackrel{d}{\to} X$):
\[ \lim_{n\to\infty} F_n(X) = F(x). \]
\item Convergence in probability (WLLN, $X_n \stackrel{p}{\to} X$):
\[ \lim_{n\to\infty} P(|X_n-X|\ge \epsilon) = 0. \]
\item Almost sure convergence (SLLN, $X_n \stackrel{a.s.}{\longrightarrow} X$):
\[ P\left(\lim_{n\to\infty} X_n = X\right) = 1. \]
\end{itemize}

\pause

Implications:
\begin{itemize}[<+->]
\item Almost sure convergence implies convergence in probability.
\item Convergence in probability implies convergence in distribution.
\end{itemize}

\pause

Here,
\begin{itemize}[<+->]
\item $X_n$ will be our approximation to an integral and 
$X$ the true (constant) value of that integral \pause or
\item $X_n$ will be a standardized approximation and $X$ will be $N(0,1)$.
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Monte Carlo integration}
\footnotesize
Consider evaluating the integral
\[ E[h(\theta)] = \int_{\Theta} h(\theta) p(\theta) d\theta \]
\pause using the Monte Carlo estimate
\[ \hat{h}_S = \frac{1}{S} \sum_{s=1}^S h\left(\theta^{(s)}\right) \]
\pause where $\theta^{(s)} \stackrel{ind}{\sim} p(\theta)$.  \pause We know
\begin{itemize}
\item SLLN: $\hat{h}_S \stackrel{a.s.}{\longrightarrow} E[h(\theta)]$. \pause
\item CLT: if $h^2$ has finite expectation, \pause then
\[ \frac{\hat{h}_S-E[h(\theta)]}{\sqrt{v_S/S}} \stackrel{d}{\to} N(0,1) 
\]
where
\[
v_S = Var[h(\theta)] \pause \approx 
\frac{1}{S} \sum_{s=1}^S \left[ h\left(\theta^{(s)}\right) - \hat{h}_S \right]^2 \]
\pause
or any other consistent estimator.
\end{itemize}
\end{frame}





\subsection{Definite integral}
\frame{\frametitle{Definite integral}
	Suppose you are interested in evaluating
	\[ \mathrm{I} = \int_0^1 e^{-\theta^2/2} d\theta. \]
	\pause Then set
	\begin{itemize}
	\item $h(\theta) = e^{-\theta^2/2}$ \pause and
	\item $p(\theta) = 1$\pause, i.e. $\theta\sim \mbox{Unif}(0,1)$. \pause
	\end{itemize}
  \pause
  and approximate by a Monte Carlo estimate via
  \begin{enumerate}[<+->]
  \item For $s=1,\ldots,S$,
    \begin{enumerate}
    \item sample $\theta^{(s)} \stackrel{ind}{\sim} Unif(0,1)$ and
    \item calculate $h\left(\theta^{(s)}\right)$.
    \end{enumerate}
  \item Calculate
  \[ \mathrm{I} \approx \frac{1}{S} \sum_{s=1}^S h(\theta^{(s)}). \]
  \end{enumerate}
}



\begin{frame}[fragile]
\frametitle{Monte Carlo sampling randomly infills}
<<uniform>>=
f <- function(theta) exp(-theta^2/2)

# Monte Carlo integration
set.seed(1)
d <- data.frame(
  theta = runif(1e3)
) |>
  mutate(
    ptheta = f(theta),
    
    # Cumulative mean
    cumsum  = cumsum(ptheta),
    id      = 1:n(),
    cummean = cumsum / id,
    
    # Cumulative variance
    cumth2 = cumsum(ptheta^2),
    cumvar = cumth2/id - cummean^2,
    
    # Interval
    ucl    = cummean + qnorm(.975) * sqrt(cumvar/id),
    lcl    = cummean - qnorm(.975) * sqrt(cumvar/id)
  )

subsample <- bind_rows(
  d |> slice_sample(n = 20) |> mutate(`Number of samples` = 20),
  d |> slice_sample(n = 200) |> mutate(`Number of samples` = 200)
)

ggplot(subsample,
       aes(x    = theta,
           y    = 0)
) +
  geom_segment(aes(xend = theta, yend = ptheta)) +
  stat_function(fun = f) +
  facet_wrap(~`Number of samples`, ncol = 1,
             labeller = label_both) +
  labs(y = expression(f(theta)),
       x = expression(theta))
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Strong law of large numbers}
<<uniform2, dependson='uniform'>>=
# Monte Carlo integration
g <- ggplot(d |> filter(id > 2), 
       aes(x = id,
           y = cummean)) +
  geom_line(color = "gray") +
  geom_hline(yintercept = h <- diff(pnorm(c(0,1)))*sqrt(2*pi),
             color = "red") +
  labs(y = "Estimate",
       x = "Number of samples",
       title = "Monte Carlo Estimate")

g
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Central limit theorem}
<<uniform3, dependson='uniform2'>>=
g +
  geom_line(aes(y = lcl)) +
  geom_line(aes(y = ucl)) +
  labs(title = "Monte Carlo Central Limit Theorem Uncertainty")
@
\end{frame}




\subsection{Infinite bounds}
\frame{\frametitle{Infinite bounds}
	Suppose $\theta\sim N(0,1)$ and you are interested in evaluating
	\[ E[\theta] \pause = \int_{-\infty}^\infty \theta \frac{1}{\sqrt{2\pi}} e^{-\theta^2/2} d\theta \]

	\pause Then set
	\begin{itemize}
	\item $h(\theta) = \theta$ \pause and
	\item $g(\theta) = \phi(\theta)$\pause, i.e. $\theta\sim N(0,1)$. \pause
	\end{itemize}
  and approximate by a Monte Carlo estimate via
  \begin{enumerate}[<+->]
  \item For $s=1,\ldots,S$,
    \begin{enumerate}
    \item sample $\theta^{(s)} \stackrel{ind}{\sim} N(0,1)$ and
    \item calculate $h\left(\theta^{(s)}\right)$.
    \end{enumerate}
  \item Calculate
  \[ E[\theta] \approx \frac{1}{S} \sum_{s=1}^S h(\theta^{(s)}). \]
  \end{enumerate}
}

\begin{frame}[fragile]
\frametitle{Non-uniform sampling}
<<echo=FALSE>>=
d <- data.frame(
  theta = rnorm(1000)
) |>
  mutate(
    ptheta = theta,
    
    # Cumulative mean
    cumsum  = cumsum(ptheta),
    id      = 1:n(),
    cummean = cumsum / id,
    
    # Cumulative variance
    cumth2 = cumsum(ptheta^2),
    cumvar = cumth2/id - cummean^2,
    
    # Interval
    ucl    = cummean + qnorm(.975) * sqrt(cumvar/id),
    lcl    = cummean - qnorm(.975) * sqrt(cumvar/id)
  )

subsample <- bind_rows(
  d |> slice_sample(n = 10) |> mutate(`Number of samples` = 10),
  d |> slice_sample(n = 100) |> mutate(`Number of samples` = 100)
)

ggplot(subsample,
       aes(x    = theta,
           y    = 0)
) +
  geom_segment(aes(xend = theta, yend = theta)) +
  geom_abline(intercept = 0, slope = 1) +
  facet_wrap(~`Number of samples`, ncol = 1,
             labeller = label_both) +
  labs(y = expression(h(theta)),
       x = expression(theta)) 
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Monte Carlo estimate}
<<infinite>>=
# Normal expectation
ggplot(d |> filter(id > 2), 
       aes(x = id,
           y = cummean)) +
  geom_line(color = "gray") +
  geom_hline(yintercept = 0,
             color = "red") +
  labs(y = "Estimate",
       x = "Number of samples",
       title = "Monte Carlo Central Limit Theorem Uncertainty") +
  geom_line(aes(y = lcl)) +
  geom_line(aes(y = ucl)) 
@
\end{frame}



\subsection{Gridding}
\frame{\frametitle{Monte Carlo approximation via gridding}
Rather than determining $c(y)$ and then $E[\theta|y]$ via deterministic gridding (all $w_i$ are equal), 
we can use the grid as a discrete approximation to the posterior, 
i.e. 
  \[ p(\theta|y) \approx \sum_{i=1}^N p_i \delta_{\theta_i}(\theta) \qquad p_i = \frac{q(\theta_i|y)}{\sum_{s=1}^N q(\theta_j|y)}  \]
\pause 
where $\delta_{\theta_i}(\theta)$ is the Dirac delta function, i.e. 
$\delta_{\theta_i}(\theta) = 0 \,\forall\, \theta\ne \theta_i \qquad \int \delta_{\theta_i}(\theta) d\theta=1.$
\pause  
This discrete approximation to $p(\theta|y)$ can be used to approximate the expectation $E[h(\theta)|y]$ deterministically or via simulation, \pause i.e. 
  \[ E[h(\theta)|y]\approx \sum_{i=1}^N p_i h(\theta_i) \pause \qquad E[h(\theta)|y]\approx \frac{1}{S} \sum_{s=1}^S h\left(\theta^{(s)}\right) \] 
\pause  where $\theta^{(s)} \stackrel{ind}{\sim} \sum_{i=1}^N p_i \delta_{\theta_i}(\theta)$ (with replacement).
}

\begin{frame}[fragile]
\frametitle{Example: Normal-Cauchy model }
<<normal_cauchy_posterior_mean_grid, dependson='data', echo=TRUE>>=
<<data>> 

# Small number of grid locations
theta = seq(-5,5,length=1e2+1)+y; p = q(theta,y)/sum(q(theta,y)); sum(p*theta)
mean(sample(theta,prob=p,replace=TRUE))

# Large number of grid locations
theta = seq(-5,5,length=1e6+1)+y; p = q(theta,y)/sum(q(theta,y)); sum(p*theta) 
mean(sample(theta,1e2,prob=p,replace=TRUE)) # But small MC sample

# Truth
post_expectation(1)
@
\end{frame}



\subsection{Inverse CDF method}
\frame{\frametitle{Inverse cumulative distribution function}
\begin{definition}
The \alert{cumulative distribution function} (cdf) of a random variable $X$ is defined by 
\[ F_X(x) = P_X(X\le x) \qquad\mbox{for all }x. \]
\end{definition}
	
\vspace{0.2in} \pause
	
\begin{lemma}
Let $X$ be a random variable whose cdf is $F(x)$ \pause
and you have access to the inverse cdf of $X$, i.e. if
\[
u = F(x) \quad \implies \quad x = F^{-1}(u).
\]
\pause
If $U\sim Unif(0,1)$, then $X=F^{-1}(U)$ is a simulation from the distribution
for $X$. 	
\end{lemma}
}


\begin{frame}[fragile]
\frametitle{Inverse CDF}
<<standard-normal-inverse-cdf>>=
arrows <- data.frame(
  u <- c(0.1, 0.9)
) |>
  mutate(
    q = qnorm(u)
  )
  
ggplot(data.frame(x = c(-3,3)),
       aes(x = x)) +
  stat_function(fun = pnorm) +
  geom_segment(data = arrows,
               mapping = aes(
                 x = -3, xend = q,
                 y = u,  yend = u
               ),
               color = "blue",
               arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(data = arrows,
               mapping = aes(
                 x = q, xend = q,
                 y = u,  yend = 0
               ),
               color = "blue",
               arrow = arrow(length = unit(0.5, "cm"))) +
  labs(
    title = "Standard normal cdf"
  )
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Exponential example}
	For example, to sample $X\sim Exp(1)$, 
	
	\pause
	
	\begin{enumerate}
	\item Sample $U\sim Unif(0,1)$. \pause
	\item Set $X = -\log(1-U)$, \pause or $X=-\log(U)$.  
	\end{enumerate}

\pause

<<exponential, message=FALSE, fig.width=8>>=
n.reps <- 1e4
d <- data.frame(x = -log(runif(n.reps)))
ggplot(d, aes(x=x)) + 
  geom_histogram(
    aes(
      y = after_stat(density)), 
    bins     = 100,
    boundary = 0) + 
  stat_function(fun=dexp, 
                color="red")
@
\end{frame}



\frame{\frametitle{Sampling from a univariate truncated distribution}
Suppose you wish to sample from $X\sim N(\mu,\sigma^2)\mathrm{I}(a<X<b)$,
i.e. a normal random variable with untruncated mean $\mu$ and variance 
$\sigma^2$, but truncated to the interval $(a,b)$. 
\pause 
Suppose the untruncated cdf is $F$ and inverse cdf is $F^{-1}$.  
\pause 
	
\begin{enumerate}[\,1.]
\item Calculate endpoints $p_a = F(a)$ and $p_b = F(b)$. \pause
\item Sample $U\sim Unif(p_a,p_b)$. \pause
\item Set $X=F^{-1}(U)$. \pause
\end{enumerate}	
  
\vspace{0.2in} \pause
  
This just avoids having to recalculate the normalizing constant for the pdf, 
i.e. $1/(F^{-1}(b)-F^{-1}(a))$.
}


\begin{frame}[fragile]
\frametitle{Truncated normal}
<<truncated-normal-inverse-cdf>>=
mu    <- 5
sigma <- 9

# truncation points
truncation <- data.frame(
  x = c(1, 6)
) |>
  mutate(
    q = pnorm(x, mu, sigma)
  )

arrows <- data.frame(
  u = c(0.48)
) |>
  mutate(
    q = qnorm(u, mu, sigma)
  )
  
ggplot(data.frame(x = c(-20,20)),
       aes(x = x)) +
  stat_function(fun = pnorm,
                args = list(mean = mu, sd = sigma)) +
  geom_segment(data = arrows,
               mapping = aes(
                 x = -20, xend = q,
                 y = u,  yend = u
               ),
               color = "blue",
               arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(data = arrows,
               mapping = aes(
                 x = q, xend = q,
                 y = u,  yend = 0
               ),
               color = "blue",
               arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(data = truncation,
               mapping = aes(
                 x = -20, xend = x,
                 y = q,  yend = q
               ),
               color = "red") +
  geom_segment(data = truncation,
               mapping = aes(
                 x = x, xend = x,
                 y = q,  yend = 0
               ),
               color = "red") +
  labs(
    title = "N(5,9^2)I(1< X < 6)"
  ) +
  ylim(0,1)
@
\end{frame}




\begin{frame}[fragile]
\frametitle{Truncated normal}
$X\sim N(5,9^2)\mathrm{I}(1\le X \le 6)$
<<truncated-normal-stochastic, fig.width=6>>=
d <- data.frame(
  x = 5 + 3 * qnorm(runif(n.reps, 
                          min = pnorm((1-5)/3), 
                          max   = pnorm((6-5)/3))))

dtnorm <- function(x, mu = 5, sigma = 3, a=1, b=6) {
  dnorm(x, mu, sigma)/diff(pnorm(c(a, b), mu, sigma))
} 

ggplot(d, aes(x=x)) + 
  geom_histogram(
    aes(
      y = after_stat(density)), 
    boundary = 1, 
    binwidth = 0.1) + 
  stat_function(fun = dtnorm, 
                color = "red")
@
\end{frame}




% \section{Introduction to Bayesian computation}
% \frame{\frametitle{Notation}
%   \begin{itemize}
%   \item Target distribution: $p(\theta|y)$
%   \item Unnormalized target distribution: $q(\theta|y)$, i.e.
%   \[ p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)} \propto q(\theta|y) \]
%   \item Proposal distribution: $g(\theta)$ which may depend on $y$
%   \end{itemize}
% }

\subsection{Rejection sampling}
\frame{\frametitle{Rejection sampling}
Suppose you wish to obtain samples $\theta\sim p(\theta|y)$,
\pause 
rejection sampling performs the following 
\pause 
\begin{enumerate}[\,1.]
\item Sample a proposal $\theta^*\sim g(\theta)$ \pause and $U\sim Unif(0,1)$. 
\pause 
\item Accept $\theta=\theta^*$ as a draw from $p(\theta|y)$ if 
$U\le p(\theta^*|y)/Mg(\theta^*)$\pause, otherwise return to step 1. 
\end{enumerate}
\pause
where $M$ satisfies $M\, g(\theta)\ge p(\theta|y)$ for all $\theta$. 
	
\vspace{0.2in} \pause
	
\begin{itemize}[<+->]
\item For a given proposal distribution $g(\theta)$, 
the optimal $M$ is $M=\sup_\theta p(\theta|y)/g(\theta)$. 
\item The probability of acceptance is $1/M$.
\end{itemize}
\pause The accept-reject idea is to create an envelope, 
$M\, g(\theta)$, above $p(\theta|y)$.
}


\frame{\frametitle{Rejection sampling with unnormalized density}
Suppose you wish to obtain samples $\theta\sim p(\theta|y)\propto q(\theta|y)$, 
\pause 
rejection sampling performs the following  
  
\begin{enumerate}[\,1.]
\item Sample a proposal $\theta^*\sim g(\theta)$ \pause and $U\sim Unif(0,1)$.  
\item Accept $\theta=\theta^*$ as a draw from $p(\theta|y)$ if 
$U\le q(\theta^*|y)/M^\dagger g(\theta^*)$, otherwise return to step 1. 
\end{enumerate}
\pause
where $M^\dagger$ satisfies $M^\dagger\, g(\theta)\ge q(\theta|y)$ for all $\theta$. 
	
\vspace{0.2in} \pause
	
\begin{itemize}[<+->]
\item For a given proposal distribution $g(\theta)$, 
the optimal $M^\dagger$ is $M^\dagger=\sup_\theta q(\theta|y)/g(\theta)$. 
\item The acceptance probability is $1/M = c(y)/M^\dagger$.
\end{itemize}
\pause 
The accept-reject idea is to create an envelope, $M^\dagger\, g(\theta)$, above $q(\theta|y)$.
}

\begin{frame}
\frametitle{Example: Normal-Cauchy model}
  If $Y \sim N(\theta,1)$ and $\theta\sim Ca(0,1)$, then 
  \[ p(\theta|y) \propto e^{-(y-\theta)^2/2} \frac{1}{(1+\theta^2)} \]
  for $\theta\in\mathbb{R}$. 
  
  \vspace{0.2in} \pause 
  
  Choose a $N(y,1)$ as a proposal distribution, i.e. 
  \[ g(\theta)=\frac{1}{\sqrt{2\pi}} e^{-(\theta-y)^2/2} \]
  with 
  \[ M^\dagger = \sup_\theta \frac{q(\theta|y)}{g(\theta)} \pause 
  = \sup_\theta \frac{e^{-(y-\theta)^2/2} \frac{1}{(1+\theta^2)}}{\frac{1}{\sqrt{2\pi}}  e^{-(\theta-y)^2/2} } \pause 
  = \frac{\sqrt{2\pi}}{(1+\theta^2)} \pause 
  \le \sqrt{2\pi} \]
  \pause
  The acceptance rate is 
  $1/M = c(y)/M^\dagger = \Sexpr{cy} / \sqrt{2\pi} = \Sexpr{cy/sqrt(2*pi)}$.
\end{frame}



\begin{frame}[fragile]
\frametitle{Example: Normal-Cauchy model}
<<rejection_sampling_plot, dependson=c("data","normal_cauchy_posterior"), echo=FALSE>>=
M = sqrt(2*pi)
g = function(theta,y,log=FALSE) dnorm(theta,y,log=log)

n = 1e2
d = data.frame(x = rnorm(n,y), 
               u = runif(n)) %>%
  mutate(u_scaled = u*M*g(x,y),
         accept   = u_scaled < q(x,y))

# xx = seq(-3,5,by=0.01)
# dd = data.frame(x=xx,
#                 y=c(M*g(xx,y),q(xx,y)),
#                 distribution = c(rep("envelope",length(xx)),
#                                  rep("target",length(xx))))

gg <- ggplot(d, 
            aes(
              x = x,
              y = u_scaled,
              col = accept)) + 
  geom_point()

clrs <- unique(ggplot_build(gg)$data[[1]]$colour)

gg + stat_function(fun=function(x) M*g(x,y), col = clrs[2]) +
  stat_function(fun=function(x) q(x,y), col = clrs[1]) + 
  labs(x="sample",y=expression(paste("u M g(",theta,")")))

cat("Observed acceptance rate was",mean(d$accept))
@
\end{frame}


\frame{\frametitle{Heavy-tailed proposals}
  Suppose our target is a standard Cauchy and our (proposed) proposal is a standard normal, then
  \[ \frac{p(\theta|y)}{g(\theta)} = 
  \frac{\frac{1}{\pi(1+\theta^2)}}{\frac{1}{\sqrt{2\pi}}e^{-\theta^2/2}}  \]
  \pause and 
  \[
  \frac{\frac{1}{\pi(1+\theta^2)}}{\frac{1}{\sqrt{2\pi}}e^{-\theta^2/2}} \pause 
  \stackrel{\theta\to\infty}{\longrightarrow} \infty  \]
  since $e^{-a}$ converges to zero faster than $1/(1+a)$. 
  \pause Thus, there is no value $M$ such that $M \, g(\theta) \ge p(\theta|y)$ for all $\theta$.
  
  \vspace{0.2in} \pause
  
  TL;DR the condition $M\, g(\theta)\ge p(\theta|y)$ requires the proposal to have tails at least as thick (heavy) as the target.
}


\end{document}
