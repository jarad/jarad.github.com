\documentclass[handout,aspectratio=169]{beamer}

\input{../frontmatter}
\input{../commands}

\title[Metropolis-Hastings]{Metropolis-Hastings algorithm}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment    = NA, 
               fig.width  = 5, 
               fig.height = 3, 
               size       = 'tiny', 
               out.width  = '0.8\\textwidth', 
               fig.align  = 'center', 
               message    = FALSE,
               echo       = FALSE,
               cache      = TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library(MASS)
library("tidyverse"); theme_set(theme_bw())
library(GGally)
library(gridExtra)
@

<<set_seed>>=
set.seed(20240326)
@

\frame{\maketitle}

\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Metropolis-Hastings algorithm
\item Independence proposal
\item Random-walk proposal
  \begin{itemize}
  \item Optimal tuning parameter
  \item Binomial example
  \item Normal example
  \item Binomial hierarchical example
  \end{itemize}
\end{itemize}

\end{frame}





\section{Metropolis-Hastings algorithm}
\begin{frame}
\frametitle{Metropolis-Hastings algorithm}

Let 
\begin{itemize}
\item $p(\theta|y)$ be the target distribution and
\item $\theta^{(t)}$ be the current draw from $p(\theta|y)$.
\end{itemize}

\vspace{0.2in} \pause

The Metropolis-Hastings algorithm performs the following \pause 
\begin{enumerate}
\item propose $\theta^*\sim g(\theta|\theta^{(t)})$ \pause 
\item accept $\theta^{(t+1)}=\theta^*$ with probability $\min\{1,r\}$ \pause where 
\[ r = r(\theta^{(t)},\theta^*) \pause 
= \frac{p(\theta^*|y)/g(\theta^*|\theta^{(t)})}{p(\theta^{(t)}|y)/g(\theta^{(t)}|\theta^*)} \pause 
= \frac{p(\theta^*|y)}{p(\theta^{(t)}|y)}\frac{g(\theta^{(t)}|\theta^*)}{g(\theta^*|\theta^{(t)})} \]
\pause otherwise, set $\theta^{(t+1)}=\theta^{(t)}$. 
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Metropolis-Hastings algorithm}

Suppose we only know the target up to a normalizing constant, i.e. 
\[ 
p(\theta|y) = q(\theta|y)/q(y)
\]
where we only know $q(\theta|y)$. 

\vspace{0.2in} \pause

The Metropolis-Hastings algorithm performs the following \pause 
\begin{enumerate}
\item propose $\theta^*\sim g(\theta|\theta^{(t)})$ \pause 
\item accept $\theta^{(t+1)}=\theta^*$ with probability $\min\{1,r\}$ \pause where 
{\scriptsize
\[ r = r(\theta^{(t)},\theta^*) \pause 
= \frac{p(\theta^*|y)}{p(\theta^{(t)}|y)}\frac{g(\theta^{(t)}|\theta^*)}{g(\theta^*|\theta^{(t)})} \pause
= \frac{q(\theta^*|y)/q(y)}{q(\theta^{(t)}|y)/q(y)}\frac{g(\theta^{(t)}|\theta^*)}{g(\theta^*|\theta^{(t)})}\pause
= \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)}\frac{g(\theta^{(t)}|\theta^*)}{g(\theta^*|\theta^{(t)})} 
\]
}
\pause otherwise, set $\theta^{(t+1)}=\theta^{(t)}$. 
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Two standard Metropolis-Hastings algorithms}

\begin{itemize}
\item Independent Metropolis-Hastings
  \begin{itemize}
  \item Independent proposal, i.e. $g(\theta|\theta^{(t)}) = g(\theta)$
  \end{itemize}
\item Random-walk Metropolis
  \begin{itemize}
  \item Symmetric proposal, i.e. $g(\theta|\theta^{(t)}) = g(\theta^{(t)}|\theta)$ for all $\theta,\theta^{(t)}$. 
  \end{itemize}
\end{itemize}
\end{frame}


% \begin{frame}
% \frametitle{Example}
% \footnotesize
% Let the state space be $\{0,1\}$ with target distribution $(2/3\,\,1/3)$ \pause and 
% \[ g = \bordermatrix{ & 0 & 1 \cr 0 & 0.5 & 0.5 \cr 1 & 0.3 & 0.7 } \]
% \pause and acceptance probability is $\min\{1,r\}$ \pause where 
% \[ r = \bordermatrix{ & 0 & 1 \cr 0 & 1 & \frac{1/3}{2/3} \frac{0.3}{0.5} \cr 1 & \frac{2/3}{1/3} \frac{0.5}{0.3} & 1 } \pause = \bordermatrix{ & 0 & 1 \cr 0 & 1 & 0.3 \cr 1 & 3.33 & 1 }  \]
% \pause Thus, the transition distribution is 
% \[\begin{array}{rlll}
% p &= p(\theta^{(t+1)}=1|\theta^{(t)}=0) \pause = p(\theta^*=1|\theta^{(t)}=0)p(\theta^*\mbox{ accepted}) &\pause= 0.5\times 0.3 &= 0.15 \pause \\
% q &= p(\theta^{(t+1)}=0|\theta^{(t)}=1) \pause = p(\theta^*=0|\theta^{(t)}=1)p(\theta^*\mbox{ accepted}) &\pause = 0.3\times 1 &\pause = 0.3
% \end{array} \]
% and the stationary distribution is $(0.3,0.15)/(0.3+0.15)=(2/3\,\,1/3)$. 
% \end{frame}
% 
% 
%
% \begin{frame}[fragile]
% \frametitle{Discrete example}
% 
% <<simple_chain, fig.width=20,tidy=FALSE>>=
% # states are 1 and 2 rather than 0 and 1
% n = 100
% q = c(2,1)/3
% J = rbind(c(.5,.5), c(.3,.7))
% theta = rep(1,n)
% for (i in 2:n) {
%   proposed = sample(2, 1, prob=J[theta[i-1],])
%   r = q[proposed]/q[theta[i-1]] * J[proposed,theta[i-1]]/J[theta[i-1],proposed]
%   theta[i] = ifelse(runif(1)<r, proposed, theta[i-1])
% }
% table(theta-1)/n
% qplot(1:n, theta-1)+labs(x="t",y=expression(theta))
% @
% \end{frame}
% 
% 
% 
% 
% \begin{frame}
% \frametitle{Ergodicity of the MH algorithm}
% \begin{itemize}[<+->]
% \item Markov chain by construction
% \item Irreducible: 
%   \begin{itemize}
%   \item is the support of $p$ connected?
%   \item does the support of $J$ match the support of $p$?
%   \end{itemize}
% \item Aperiodic: 
%   \begin{itemize}
%   \item non-zero probability of staying at $\theta$ for some $\theta$
%   \end{itemize}
% \end{itemize}
% 
% \vspace{0.2in} \pause
% 
% \begin{itemize}[<+->]
% \item Is there a stationary distribution?
% \item Does the chain converge to the stationary distribution?
% \end{itemize} 
% 
% \vspace{0.2in} \pause
% 
% \begin{theorem}
% For an \alert{irreducible} and \alert{aperiodic} Markov chain, if there exists a positive vector $\pi$ such that $\pi=\pi P$ and $\sum_i \pi_i=1$, then it must be the stationary distribution and $\lim_{t\to\infty} \pi^{(t)} = \pi$.
% \end{theorem}
% \end{frame}
% 
% 
% \begin{frame}
% \frametitle{Proof }
% \begin{theorem}
% \small
% For Metropolis-Hastings, the target distribution is the stationary distribution of the Markov chain.
% \end{theorem}
% \small
% \pause
% \begin{proof}
% For a Metropolis-Hastings chain with $\theta_a$ and $\theta_b$ drawn from $p(\theta|y)$ \pause such that 
% \[p(\theta_b|y)g(\theta_a|\theta_b)\ge p(\theta_a|y)g(\theta_b|\theta_a), \]
% \pause then 
% \[ \begin{array}{rl}
% p(\theta^{t}=\theta_a, \theta^{(t+1)}=\theta_b) &= p(\theta_a|y)g(\theta_b|\theta_a) \pause \\
% p(\theta^{t}=\theta_b, \theta^{(t+1)}=\theta_a) &= p(\theta_b|y)g(\theta_a|\theta_b) \frac{p(\theta_a|y) g(\theta_b|\theta_a)}{p(\theta_b|y) g(\theta_a|\theta_b)} \pause \\
% &= p(\theta_a|y) g(\theta_b|\theta_a)
% \end{array} \]
% \pause Since the joint distribution is symmetric, the marginal distributions for $\theta^{t}$ and $\theta^{(t+1)}$ are the same and equal to $p(\theta|y)$. 
% \end{proof}
% 
% \end{frame}


\section{Independence Metropolis-Hastings}
\begin{frame}
\frametitle{Independence Metropolis-Hastings}

Let 
\begin{itemize}
\item $p(\theta|y)\propto q(\theta|y)$ be the target distribution, 
\item $\theta^{(t)}$ be the current draw from $p(\theta|y)$, \pause and
\item $g(\theta|\theta^{(t)}) = g(\theta)$, i.e. the proposal is \alert{independent} of the current value.
\end{itemize}

\vspace{0.2in} \pause

The \alert{independence Metropolis-Hastings algorithm} performs the following \pause
\begin{enumerate}
\item propose $\theta^*\sim g(\theta)$ \pause
\item accept $\theta^{(t+1)}=\theta^*$ with probability $\min\{1,r\}$ \pause where 
\[ r = 
\frac{q(\theta^*|y)/g(\theta^*)}{q(\theta^{(t)}|y)/g(\theta^{(t)})} = \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)}\frac{g(\theta^{(t)})}{g(\theta^*)} \]
\pause otherwise, set $\theta^{(t+1)}=\theta^{(t)}$. 
\end{enumerate}
\end{frame}


\begin{frame}[fragile]
\frametitle{Intuition through examples}

<<independent_metropolis_hastings_intuition>>=
curves <- data.frame(theta = seq(-2, 3, length=101)) |>
  mutate(target   = dnorm(theta),
         proposal = dnorm(theta,1)) |>
  pivot_longer(
    target:proposal,
    names_to = "Distribution",
    values_to = "density"
  )

d <- expand.grid(
  current  = -1:1, 
  proposed = -1:1) |>
  
  mutate(
    ratio = 
      dnorm(proposed ) / dnorm(current   ) * 
      dnorm(current,1) / dnorm(proposed,1), 
    cv = paste("current=", current),
    pv = paste("proposed=", proposed)) |>
  
  pivot_longer(
    current:proposed,
    names_to = "value",
    values_to = "theta"
  ) 

ggplot(d, 
       aes(
         x = theta, 
         y = 0)) +
  geom_segment(
    aes(
      x    = as.numeric(gsub('current= ', '',cv)),
      xend = as.numeric(gsub('proposed= ','',pv)),
      y = 0,
      yend = 0
    ),
    arrow = arrow(length = unit(0.1, "cm"))
  ) + 
  geom_point(
    aes(
      color = ratio > 0.5, 
      shape = value), 
    alpha = 0.7,
    size = 2) + 
  facet_grid(
    cv ~ pv,
    switch = 'y') +
  geom_line(data = curves, 
            aes(
              theta,
              density,
              linetype = Distribution)) +
  labs(
    x = expression(theta),
    y = "density",
    title = "Independence proposal sampling"
  ) 
@

\end{frame}



\begin{frame}
\frametitle{Example: Normal-Cauchy model}
  Let $Y\sim N(\theta,1)$ with $\theta\sim Ca(0,1)$ \pause such that the posterior is
  \[ p(\theta|y) \propto p(y|\theta)p(\theta) \propto \frac{\exp(-(y-\theta)^2/2)}{1+\theta^2} \]
\pause Use $N(y,1)$ as the proposal, \pause then the Metropolis-Hastings acceptance probability is the $\min\{1,r\}$ \pause with 
\[ 
\begin{array}{rl}
r &= \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)}\frac{g(\theta^{(t)})}{g(\theta^*)} \pause \\
&= \frac{\exp(-(y-\theta^*)^2/2)/1+(\theta^*)^2}{\exp(-(y-\theta^{(t)})^2/2)/1+(\theta^{(t)})^2} \frac{\exp(-(\theta^{(t)}-y)^2/2)}{\exp(-(\theta^*-y)^2/2)} \pause \\
&= \frac{1+(\theta^{(t)})^2}{1+(\theta^*)^2} 
\end{array} 
\] 
\end{frame}


\begin{frame}[fragile]
\frametitle{Example: Normal-Cauchy model}
<<sampling, fig.height = 3>>=
q <- function(theta, y, log = FALSE) {
  out <- -(y-theta)^2/2 - log(1+theta^2)
  if (log) return(out)
  return(exp(out))
}
g <- function(theta, y, log = FALSE) dnorm(theta, y, log=log)

d <- data.frame(theta = seq(-3, 3, by = 0.1)) |>
  mutate(target = q(theta,1)/integrate(function(x) q(x,1), -Inf, Inf)$value,
         proposal = g(theta,1))

ggplot(d |> 
         pivot_longer(
           target:proposal,
           names_to = "distribution",
           values_to = "density"), 
       aes(x     = theta, 
           y     = density, 
           color = distribution)) + 
  geom_line()
@
\end{frame}




\begin{frame}[fragile]
\frametitle{Example: Normal-Cauchy model}
<<normal-cauchy-independence>>=
set.seed(1)
y <- 0 
n <- 100
theta <- rep(NA, n)
theta[1] <- rnorm(1,y)
for (i in 2:n) {
  proposed <- rnorm(1,y)
  logr <- log(1+theta[i-1]^2) - log(1+proposed^2)
  theta[i] <- ifelse(log(runif(1)) < logr, proposed, theta[i-1])
}

ggplot(data.frame(Iteration = 1:n, 
                  theta     = theta),
       aes(x = Iteration, 
           y = theta)) +
  geom_point() +
  labs(x     = "Iteration (t)", 
       y     = expression(theta), 
       title = "Independence Metropolis-Hastings") 
@
\end{frame}

\begin{frame}
\frametitle{Example: Normal-Cauchy model - poor starting value}
<<normal-cauchy-independence-poor-starting-value>>=
theta[1] = 10 # This line changed
for (i in 2:n) {
  proposed = rnorm(1,y)
  logr = log(1+theta[i-1]^2)-log(1+proposed^2)
  theta[i] = ifelse(log(runif(1))<logr, proposed, theta[i-1])
}

ggplot(data.frame(Iteration = 1:n,
                  theta     = theta),
       aes(
         x = Iteration,
         y = theta
       )) +
  geom_point() +
  labs(
    x     = "Iteration (t)", 
    y     = expression(theta), 
    title = "Independence Metropolis-Hastings (poor starting value)") 
@
\end{frame}



\begin{frame}
\frametitle{Need heavy tails}

Recall that 
\begin{itemize}[<+->]
\item rejection sampling requires the proposal to have heavy tails and
\item importance sampling is efficient only when the proposal has heavy tails.
\end{itemize}

\vspace{0.2in} \pause

Independence Metropolis-Hastings also requires heavy tailed proposals for
efficiency
\pause 
since if $\theta^{(t)}$ is 
\begin{itemize}
\item in a region where $p(\theta^{(t)}|y)>>g(\theta^{(t)})$\pause, 
i.e. target has heavier tails than the proposal, \pause then
\item any proposal $\theta^*$ such that $p(\theta^*|y)\approx g(\theta^*)$\pause,
i.e. in the center of the target and proposal, \pause 
\end{itemize}
will result in 
\[ r = \frac{g(\theta^{(t)})}{p(\theta^{(t)}|y)}\frac{p(\theta^*|y)}{g(\theta^*)} \pause \approx 0 \]
\pause and few samples will be accepted.
\end{frame}


\begin{frame}[fragile]
\frametitle{Need heavy tails - example}

Suppose $\theta|y \sim Ca(0,1)$ and we use a standard normal as a proposal. 
\pause 
Then 

<<need-heavy-tails-example, fig.height=2.5>>=
d <- data.frame(x = seq(-4, 4, by = .1)) |>
  mutate(
    target    = dcauchy(x),
    proposal  = dnorm(x),
    log_ratio = log(target) - log(proposal))

g1 <- ggplot(d |>
               dplyr::select(x, target, proposal) |>
               pivot_longer(target:proposal,
                            names_to  = "density",
                            values_to = "value"),
       aes(x     = x, 
           y     = value, 
           color = density)) +
  geom_line() + 
  theme(legend.position = "none")

g2 <- ggplot(d, 
             aes(
               x = x,
               y = exp(log_ratio))) + 
  geom_line() +
  labs(x = "theta", y = "target / proposal") +
  scale_y_log10()

grid.arrange(g1, g2, ncol = 1)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Need heavy tails}
<<heavy_tails, cache=TRUE>>=
n=1000
theta = rep(0,n)
for (i in 2:n) {
  proposed <- rnorm(1)
  logr <- dcauchy(proposed,   log=TRUE) - dcauchy(theta[i-1], log = TRUE)+
          dnorm(  theta[i-1], log=TRUE) - dnorm(    proposed, log = TRUE)
  theta[i] <- ifelse(log(runif(1))<logr, proposed, theta[i-1])
}

ggplot(data.frame(Iteration = 1:n,
                  theta     = theta),
       aes(
         x = Iteration,
         y = theta
       )) +
  geom_point() +
  labs(
    x     = "Iteration (t)", 
    y     = expression(theta), 
    title = "Independence Metropolis-Hastings (light-tailed proposal)") 
@

\end{frame}



\section{Random-walk Metropolis}
\begin{frame}
\frametitle{Random-walk Metropolis}

Let 
\begin{itemize}
\item $p(\theta|y)\propto q(\theta|y)$ be the target distribution, 
\item $\theta^{(t)}$ be the current draw from $p(\theta|y)$, \pause and
\item $g(\theta^*|\theta^{(t)}) = g(\theta^{(t)}|\theta^*)$, i.e. the proposal is \alert{symmetric}.
\end{itemize}

\vspace{0.2in} \pause

The \alert{Metropolis algorithm} performs the following \pause
\begin{enumerate}
\item propose $\theta^*\sim g(\theta|\theta^{(t)})$ \pause 
\item accept $\theta^{(t+1)}=\theta^*$ with probability $\min\{1,r\}$ \pause where 
\[ r = \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)}\frac{g(\theta^{(t)}|\theta^*)}{g(\theta^*|\theta^{(t)})} \pause =
\frac{q(\theta^*|y)}{q(\theta^{(t)}|y)} 
\]
\pause otherwise, set $\theta^{(t+1)}=\theta^{(t)}$. 
\end{enumerate}
\pause This is also referred to as \alert{random-walk Metropolis}.
\end{frame}


\begin{frame}[fragile]
\frametitle{Stochastic hill climbing}

Notice that $r = q(\theta^*|y)/q(\theta^{(t)}|y)$ and thus will accept whenever the target density is larger when evaluated at the proposed value than it is when evaluated at the current value. 

\vspace{.2in} \pause

Suppose $\theta|y \sim N(0,1)$, $\theta^{(t)} = 1$, and $\theta^* \sim N(\theta^{(t)},1)$. \pause 

<<hill_climbing, fig.width=8>>=
curve(dnorm, -3, 3, lwd=2)
curve(dnorm, -1, 1, lwd=5, add=TRUE)
segments(1,0,1,dnorm(1))
curve(dnorm(x,1)/dnorm(0)*dnorm(1), add=TRUE, col="red", lwd=2)
curve(dnorm(x,1)/dnorm(0)*dnorm(1), -1, 1, add=TRUE, col="red", lwd=5)
segments(1,0,1,dnorm(1))
legend("topright",c("Target","Proposal"), col=1:2, lwd=2)
@
\end{frame}



\begin{frame}
\frametitle{Example: Normal-Cauchy model}
  Let $Y\sim N(\theta,1)$ with $\theta\sim Ca(0,1)$ \pause such that the posterior is
  \[ p(\theta|y) \propto p(y|\theta)p(\theta) \propto \frac{\exp(-(y-\theta)^2/2)}{1+\theta^2} \]

\vspace{0.2in} \pause 

Use $N(\theta^{(t)},v^2)$ as the proposal\pause, then the acceptance probability is the $\min\{1,r\}$ \pause with 
\[ r = \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)} \pause = \frac{p(y|\theta^*)p(\theta^*)}{p(y|\theta^{(t)})p(\theta^{(t)})}. \] 
For this example, let $v^2 = 1$. 
\end{frame}



\begin{frame}[fragile]
\frametitle{Example: Normal-Cauchy model }
<<normal_cauchy_random_walk, fig.height=2>>=
n <- 100
log_q <- function(theta,y) {
  dnorm(y,  theta, log = TRUE) +
    dcauchy(theta, log = TRUE)
}
current <- rnorm(1,y)
samps <- rep(NA, n)
for (i in 1:n) {
  proposed <- rnorm(1, current)
  logr     <- log_q(proposed, y) - log_q(current, y)
  current  <- ifelse(log(runif(1)) < logr, proposed, current)
  samps[i] <- current
}

ggplot(data.frame(Iteration = 1:n,
                  theta     = samps),
       aes(
         x = Iteration,
         y = theta
       )) +
  geom_point() +
  labs(
    x     = "Iteration (t)", 
    y     = expression(theta), 
    title = "Random-walk Metropolis") 
@
\end{frame}

\begin{frame}
\frametitle{Example: Normal-Cauchy model - poor starting value}
<<normal-cauchy-model-rw-poor-starting-value>>=
current <- 10 # This line changed
for (i in 1:n) {
  proposed <- rnorm(1, current)
  logr     <- log_q(proposed, y) - log_q(current, y)
  current  <- ifelse(log(runif(1)) < logr, proposed, current)
  samps[i] <- current
}

ggplot(data.frame(Iteration = 1:n,
                  theta     = samps),
       aes(
         x = Iteration,
         y = theta
       )) +
  geom_point() + 
  labs(
    x     = "Iteration (t)", 
    y     = expression(theta), 
    title = "Random-walk Metropolis (poor starting value)") 

@
\end{frame}


\subsection{Optimal tuning parameter}
\begin{frame}
\frametitle{Random-walk tuning parameter}
\small 
  Let $p(\theta|y)$ be the target distribution\pause, the proposal is symmetric with scale $v^2$, \pause and $\theta^{(t)}$ is (approximately) distributed according to $p(\theta|y)$. \pause 

\begin{itemize}
\item If $v^2\approx 0$\pause, then $\theta^*\approx \theta^{(t)}$ \pause and
\[ r = \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)} \approx 1 \]
\pause and all proposals are accepted\pause, 
but $\theta^* \approx \theta^{(t)}$. \pause 
\item As $v^2\to\infty$\pause, then $q(\theta^*|y)\approx 0$ \pause since $\theta^*$ will be far from the mass of the target distribution \pause and 
\[ r = \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)} \pause \approx 0 \]
\pause so all proposed values are rejected.
\end{itemize}

So there is an optimal $v^2$ somewhere. 
\pause 
For normal targets, the optimal random-walk proposal variance is 
$2.4^2Var(\theta|y)/d$ where $d$ is the dimension of $\theta$ which results 
in an acceptance rate of 40\% for $d=1$ down to 20\% as $d\to\infty$. 
\end{frame}


\begin{frame}[fragile]
\frametitle{Random-walk with tuning parameter that is too big and too small}

Let $y|\theta \sim N(\theta,1)$, $\theta\sim Ca(0,1)$, and $y=1$. 

<<tuning_parameter, fig.width=5.5>>=
random_walk <- function(n, v, theta0, log_target) {
  theta <- rep(theta0,n)
  for (i in 2:n) {
    proposed <- rnorm(1, theta[i-1], v)
    logr <- log_target(proposed, y) - log_target(theta[i-1], y)
    theta[i] <- ifelse(log(runif(1)) < logr, proposed, theta[i-1])
  }
  return(data.frame(iteration = 1:n, theta = theta))
}

r <- data.frame(v = c(1/10, 10)) |>
  group_by(v) |>
  do(random_walk(100, .$v, 0, log_q))

ggplot(r, 
       aes(
         x     = iteration, 
         y     = theta, 
         color = as.factor(v))) +
  geom_point() +
  labs(
    x = "Iteration (t)",
    y = expression(theta),
    title = "Random walk - tuning parameter"
  )
@

\end{frame}


\subsection{Binomial model}
\begin{frame}
\frametitle{Binomial model}

Let $Y \sim Bin(n,\theta)$ and $\theta \sim Be(1/2,1/2)$\pause, thus the posterior is \pause 
\[ p(\theta|y)\propto \theta^{y-0.5}(1-\theta)^{n-y-0.5}\mathrm{I}(0<\theta<1). \] 

\vspace{0.2in} \pause

To construct a random-walk Metropolis algorithm, we choose the proposal 
\[ \theta^* \sim N(\theta^{(t)},0.4^2) \]
\pause and accept, i.e. $\theta^{(t+1)} = \theta^*$ with probability $\min\{1,r\}$ \pause where 
\[ r = \frac{p(\theta^*|y)}{p(\theta^{(t)}|y)} \pause = \frac{(\theta^*)^{y-0.5}(1-\theta^*)^{n-y-0.5}\mathrm{I}(0<\theta^*<1)}{(\theta^{(t)})^{y-0.5}(1-\theta^{(t)})^{n-y-0.5}\mathrm{I}(0<\theta^{(t)}<1)} \]
\pause
otherwise, set $\theta^{(t+1)}=\theta^{(t)}$.
\end{frame}


\begin{frame}[fragile]
\frametitle{Binomial model}

<<binomial, echo=TRUE>>=
n <- 10000
log_q <- function(theta, y = 3, n = 10) {
  if (theta < 0 | theta > 1) return(-Inf)
  (y-0.5) * log(theta) + (n-y-0.5) * log(1-theta)
}
current <- 0.5     # Initial value
samps <- rep(NA,n) 
for (i in 1:n) {
  proposed <- rnorm(1, current, 0.4) # tuning parameter is 0.4
  
  logr <- log_q(proposed)-log_q(current)
  if (log(runif(1)) < logr) current <- proposed
  
  samps[i] <- current
}
length(unique(samps))/n # acceptance rate
@

\end{frame}




\begin{frame}[fragile]
\frametitle{Binomial}
<<binomial_traceplot>>=
par(mfrow=c(1,2))
plot(samps, type="l")
hist(samps, prob=TRUE)
curve(dbeta(x, 3.5,7.5), add=TRUE, col="red", lwd=2)
@
\end{frame}





\subsection{Normal model}
\begin{frame}
\frametitle{Normal model}

Assume
\[ Y_i \stackrel{ind}{\sim} N(\mu,\sigma^2) \qquad \mbox{and} \qquad p(\mu,\sigma) \propto Ca^+(\sigma; 0, 1) \]
\pause and thus 
\[ \begin{array}{rl}
p(\mu,\sigma|y) &\propto 
\left[ \prod_{i=1}^n \sigma^{-1} \exp\left( -\frac{1}{2\sigma^2} (y_i-\mu)^2 \right)\right] \frac{1}{1+\sigma^2} \mathrm{I}(\sigma>0)
\pause \\
&= \sigma^{-n} \exp\left( -\frac{1}{2\sigma^2} \left[ \sum_{i=1}^n y_i^2 -2\mu n\overline{y} + \mu^2 \right] \right) \frac{1}{1+\sigma^2} \mathrm{I}(\sigma>0)
\end{array} \]

\vspace{0.2in} \pause

Perform a random-walk Metropolis using a normal proposal, i.e. if $\mu^{(t)}$ and $\sigma^{(t)}$ are the current values for $\mu$ and $\sigma$, then 
\[ \left( \begin{array}{c} \mu^* \\ \sigma^*
\end{array} \right) \sim N\left( \left[ \begin{array}{c} \mu^{(t)} \\ \sigma^{(t)} \end{array} \right], S \right)
\]
where $S$ is the tuning parameter.
\end{frame}



\begin{frame}
\frametitle{Adapting the tuning parameter}

Recall that the optimal random-walk tuning parameter (if the target is normal) is $2.4^2 Var(\theta|y)/d$ \pause where $Var(\theta|y)$ is the unknown posterior covariance matrix. \pause We can estimate $Var(\theta|y)$ using the sample covariance matrix of draws from the posterior. 

\vspace{0.2in} \pause 

Proposed automatic adapting of the Metropolis-Hastings tuning parameter: \pause
\begin{enumerate}[<+->]
\item Start with $S_0$. Set $b=0$.
\item Run $M$ iterations of the MCMC using $2.4^2S_b/d$. 
\item Set $S_{b+1}$ to the sample covariance matrix of all previous draws. 
\item If $b<B$, set $b=b+1$ and return to step 2. Otherwise, throw away all previous draws and go to step 5.
\item Run $K$ iterations of the MCMC using $2.4^2S_B/d$.
\end{enumerate}

\end{frame}



\begin{frame}[fragile]
\frametitle{R code for Metropolis-Hastings}

<<mh, echo=TRUE>>=
n      <- 20
y      <- rnorm(n)
sum_y2 <- sum(y^2)
nybar  <- mean(y)

log_q  <- function(x) {
  if (x[2]<0) return(-Inf)
  -n*log(x[2])-(sum_y2-2*nybar*x[1]+n*x[1]^2)/(2*x[2]^2)-log(1+x[2]^2)
}

B <- 10
M <- 100

samps <- matrix(NA, nrow=B*M, ncol=2)
a_rate <- rep(NA, B)

# Initialize
S       <- diag(2) # initial covariance matrix
current <- c(0,1)  # initial draw
@

\end{frame}


\begin{frame}[fragile]
\frametitle{R code for Metropolis-Hastings - Adapting}

<<mh-adapting, echo=TRUE>>=
# Adapt
for (b in 1:B) {
  for (m in 1:M) {
    i <- (b-1)*M+m
    
    proposed <- mvrnorm(1, current, 2.4^2*S/2)
    
    logr <- log_q(proposed) - log_q(current)
    if (log(runif(1)) < logr) current <- proposed
    samps[i,] <- current    
  }
  a_rate[b] <- length(unique(samps[1:i,1]))/length(samps[1:i,1])
  S <- var(samps[1:i,])
}
a_rate
var(samps) # S_B
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Metropolis-Hastings - Adapting Traceplots}

<<mh-adapting-plot>>=
samps = as.data.frame(samps); names(samps) = c("mu","sigma"); samps$iteration = 1:nrow(samps)

ggplot(samps |>
         pivot_longer(-iteration,
                      names_to = 'parameter',
                      values_to = 'value'), 
       aes(x = iteration, 
           y = value)) + 
  geom_line() +
  facet_wrap(~parameter, ncol = 1, scales='free')
@

\end{frame}




\begin{frame}[fragile]
\frametitle{R code for Metropolis-Hastings - Inference}

<<mh-inference, echo=TRUE>>=
# Final run
K     <- 10000
samps <- matrix(NA, nrow = K, ncol = 2)

for (k in 1:K) {
  proposed <- mvrnorm(1, current, 2.4^2*S/2)
    
  logr <- log_q(proposed) - log_q(current)
  if (log(runif(1)) < logr) current <- proposed
  samps[k,] <- current   
}

length(unique(na.omit(samps[,1])))/length(na.omit(samps[,1])) # acceptance rate
@

\end{frame}



\begin{frame}[fragile]
\frametitle{R code for Metropolis-Hastings - Inference}

<<mh-inference2,fig.width=13>>=
samps <- as.data.frame(samps); names(samps) = c("mu","sigma"); samps$iteration = 1:nrow(samps)
m     <- samps |>
  pivot_longer(-iteration,
               names_to  = 'parameter',
               values_to = 'value')

ggplot(m, 
       aes(
         x = iteration, 
         y = value)) + 
  geom_line() +
  facet_wrap(~parameter, scales='free')

ggplot(m, 
       aes(
         x = value, 
         y = after_stat(density))) + 
  geom_histogram(binwidth=0.1) +
  facet_wrap(~parameter, scales='free')
@

\end{frame}




\subsection{Hierarchical binomial model}
\begin{frame}
\frametitle{Hierarchical binomial model}

Recall the hierarchical binomial model
\[ Y_i \stackrel{ind}{\sim} Bin(n_i,\theta_i), \quad 
\theta_i \stackrel{ind}{\sim} Be(\alpha,\beta), \quad 
p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2} \]
\pause 
and after marginalizing out the $\theta_i$
\[ Y_i \stackrel{ind}{\sim} \mbox{Beta-binomial}(n_i,\alpha,\beta), \quad
p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2}\mathrm{I}(a>0)\mathrm{I}(b>0) \]
\pause
Thus the posterior is 
\[ p(\alpha,\beta|y) \propto \left[ \prod_{i=1}^n \frac{B(\alpha+y_i, \beta+n_i-y_i)}{B(\alpha,\beta)} \right] (\alpha+\beta)^{-5/2}\mathrm{I}(a>0)\mathrm{I}(b>0) \]
where $B(\cdot)$ is the beta function.

\vspace{0.2in} \pause

We can perform exactly the same adapting procedure, but now using this posterior as the target distribution. 

\end{frame}



\begin{frame}[fragile]
\frametitle{Beta-binomial hyperparameter posterior}

<<beta_binomial>>=
d <- read.csv('../Ch05/Ch05a-dawkins.csv')
y <- d$made
n <- d$attempt
log_q <- function(x) {
  if (any(x<0)) return(-Inf)
  sum(lbeta(x[1]+y, x[2]+n-y)-lbeta(x[1],x[2]))-5/2*log(x[1]+x[2])
}

B <- 10
M <- 1000

samps <- matrix(NA, nrow=B*M, ncol=2)
a_rate <- rep(NA, B)

# Initialize
S       <- diag(2)
current <- c(1,1)

# Adapt
for (b in 1:B) {
  for (m in 1:M) {
    i <- (b-1)*M+m
    
    proposed <- mvrnorm(1, current, 2.4^2*S/2)
    
    logr <- log_q(proposed) - log_q(current)
    if (log(runif(1)) < logr) current <- proposed
    samps[i,] <- current    
  }
  a_rate[b] <- length(unique(samps[1:i,1]))/length(samps[1:i,1])
  S <- var(samps[1:i,])
}

# Final run
K <- 10000
samps <- matrix(NA, nrow=K, ncol=2)
for (k in 1:K) {
  proposed <- mvrnorm(1, current, 2.4^2*S/2)
    
  logr <- log_q(proposed) - log_q(current)
  if (log(runif(1)) < logr) current <- proposed
  samps[k,] <- current   
}

samps <- as.data.frame(samps); names(samps) <- c("alpha","beta")
@

<<dependson='beta_binomial',message=FALSE, warning=FALSE, fig.height=2.5>>=
ggpairs(samps, 
        lower = list(continuous='points'), 
        diag  = list(continuous='barDiag'))
@

\end{frame}


\section{Summary}
\begin{frame}
\frametitle{Metropolis-Hastings summary}

\begin{itemize}[<+->]
\item The Metropolis-Hastings algorithm, samples $\theta^*\sim g(\cdot|\theta^{(t)})$ and sets $\theta^{(t+1)}=\theta^*$ with probability equal to $\min\{1,r\}$ where 
\[ r = \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)} \frac{g(\theta^{(t)}|\theta^*)}{g(\theta^*|\theta^{(t)})} \]
and otherwise sets $\theta^{(t+1)}=\theta^{(t)}$.
\item There are two common Metropolis-Hastings proposals
\begin{itemize}
\item independent proposal: $g(\theta^*|\theta^{(t)}) = g(\theta^*)$
\item random-walk proposal: $g(\theta^*|\theta^{(t)}) = g(\theta^{(t)}|\theta^*)$
\end{itemize}
\item Independent proposals suffer from the same heavy-tail problems as rejection sampling proposals.
\item Random-walk proposals require tuning of the random walk parameter. 
\end{itemize}

\end{frame}



\end{document}
