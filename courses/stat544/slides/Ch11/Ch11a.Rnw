\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title[Metropolis-Hastings]{Metropolis-Hastings algorithm}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library(MASS)
library(reshape2)
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(xtable)
library(GGally)
@

<<set_seed>>=
set.seed(1)
@

\frame{\maketitle}

\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Metropolis-Hastings algorithm
\item Independence proposal
\item Random-walk proposal
  \begin{itemize}
  \item Optimal tuning parameter
  \item Binomial example
  \item Normal example
  \item Binomial hierarchical example
  \end{itemize}
\end{itemize}

\end{frame}





\section{Metropolis-Hastings algorithm}
\begin{frame}
\frametitle{Metropolis-Hastings algorithm}

Let 
\begin{itemize}
\item $p(\theta|y)$ be the target distribution and
\item $\theta^{(t)}$ be the current draw from $p(\theta|y)$.
\end{itemize}

\vspace{0.2in} \pause

The Metropolis-Hastings algorithm performs the following \pause 
\begin{enumerate}
\item propose $\theta^*\sim g(\theta|\theta^{(t)})$ \pause 
\item accept $\theta^{(t+1)}=\theta^*$ with probability $\min\{1,r\}$ \pause where 
\[ r = r(\theta^{(t)},\theta^*) \pause 
= \frac{p(\theta^*|y)/g(\theta^*|\theta^{(t)})}{p(\theta^{(t)}|y)/g(\theta^{(t)}|\theta^*)} \pause 
= \frac{p(\theta^*|y)}{p(\theta^{(t)}|y)}\frac{g(\theta^{(t)}|\theta^*)}{g(\theta^*|\theta^{(t)})} \]
\pause otherwise, set $\theta^{(t+1)}=\theta^{(t)}$. 
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Metropolis-Hastings algorithm}

Suppose we only know the target up to a normalizing constant, i.e. 
\[ 
p(\theta|y) = q(\theta|y)/q(y)
\]
where we only know $q(\theta|y)$. 

\vspace{0.2in} \pause

The Metropolis-Hastings algorithm performs the following \pause 
\begin{enumerate}
\item propose $\theta^*\sim g(\theta|\theta^{(t)})$ \pause 
\item accept $\theta^{(t+1)}=\theta^*$ with probability $\min\{1,r\}$ \pause where 
{\scriptsize
\[ r = r(\theta^{(t)},\theta^*) \pause 
= \frac{p(\theta^*|y)}{p(\theta^{(t)}|y)}\frac{g(\theta^{(t)}|\theta^*)}{g(\theta^*|\theta^{(t)})} \pause
= \frac{q(\theta^*|y)/q(y)}{q(\theta^{(t)}|y)/q(y)}\frac{g(\theta^{(t)}|\theta^*)}{g(\theta^*|\theta^{(t)})}\pause
= \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)}\frac{g(\theta^{(t)}|\theta^*)}{g(\theta^*|\theta^{(t)})} 
\]
}
\pause otherwise, set $\theta^{(t+1)}=\theta^{(t)}$. 
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Two standard Metropolis-Hastings algorithms}

\begin{itemize}
\item Independent Metropolis-Hastings
  \begin{itemize}
  \item Independent proposal, i.e. $g(\theta|\theta^{(t)}) = g(\theta)$
  \end{itemize}
\item Random-walk Metropolis
  \begin{itemize}
  \item Symmetric proposal, i.e. $g(\theta|\theta^{(t)}) = g(\theta^{(t)}|\theta)$ for all $\theta,\theta^{(t)}$. 
  \end{itemize}
\end{itemize}
\end{frame}


% \begin{frame}
% \frametitle{Example}
% \footnotesize
% Let the state space be $\{0,1\}$ with target distribution $(2/3\,\,1/3)$ \pause and 
% \[ g = \bordermatrix{ & 0 & 1 \cr 0 & 0.5 & 0.5 \cr 1 & 0.3 & 0.7 } \]
% \pause and acceptance probability is $\min\{1,r\}$ \pause where 
% \[ r = \bordermatrix{ & 0 & 1 \cr 0 & 1 & \frac{1/3}{2/3} \frac{0.3}{0.5} \cr 1 & \frac{2/3}{1/3} \frac{0.5}{0.3} & 1 } \pause = \bordermatrix{ & 0 & 1 \cr 0 & 1 & 0.3 \cr 1 & 3.33 & 1 }  \]
% \pause Thus, the transition distribution is 
% \[\begin{array}{rlll}
% p &= p(\theta^{(t+1)}=1|\theta^{(t)}=0) \pause = p(\theta^*=1|\theta^{(t)}=0)p(\theta^*\mbox{ accepted}) &\pause= 0.5\times 0.3 &= 0.15 \pause \\
% q &= p(\theta^{(t+1)}=0|\theta^{(t)}=1) \pause = p(\theta^*=0|\theta^{(t)}=1)p(\theta^*\mbox{ accepted}) &\pause = 0.3\times 1 &\pause = 0.3
% \end{array} \]
% and the stationary distribution is $(0.3,0.15)/(0.3+0.15)=(2/3\,\,1/3)$. 
% \end{frame}
% 
% 
%
% \begin{frame}[fragile]
% \frametitle{Discrete example}
% 
% <<simple_chain, fig.width=20,tidy=FALSE>>=
% # states are 1 and 2 rather than 0 and 1
% n = 100
% q = c(2,1)/3
% J = rbind(c(.5,.5), c(.3,.7))
% theta = rep(1,n)
% for (i in 2:n) {
%   proposed = sample(2, 1, prob=J[theta[i-1],])
%   r = q[proposed]/q[theta[i-1]] * J[proposed,theta[i-1]]/J[theta[i-1],proposed]
%   theta[i] = ifelse(runif(1)<r, proposed, theta[i-1])
% }
% table(theta-1)/n
% qplot(1:n, theta-1)+labs(x="t",y=expression(theta))
% @
% \end{frame}
% 
% 
% 
% 
% \begin{frame}
% \frametitle{Ergodicity of the MH algorithm}
% \begin{itemize}[<+->]
% \item Markov chain by construction
% \item Irreducible: 
%   \begin{itemize}
%   \item is the support of $p$ connected?
%   \item does the support of $J$ match the support of $p$?
%   \end{itemize}
% \item Aperiodic: 
%   \begin{itemize}
%   \item non-zero probability of staying at $\theta$ for some $\theta$
%   \end{itemize}
% \end{itemize}
% 
% \vspace{0.2in} \pause
% 
% \begin{itemize}[<+->]
% \item Is there a stationary distribution?
% \item Does the chain converge to the stationary distribution?
% \end{itemize} 
% 
% \vspace{0.2in} \pause
% 
% \begin{theorem}
% For an \alert{irreducible} and \alert{aperiodic} Markov chain, if there exists a positive vector $\pi$ such that $\pi=\pi P$ and $\sum_i \pi_i=1$, then it must be the stationary distribution and $\lim_{t\to\infty} \pi^{(t)} = \pi$.
% \end{theorem}
% \end{frame}
% 
% 
% \begin{frame}
% \frametitle{Proof }
% \begin{theorem}
% \small
% For Metropolis-Hastings, the target distribution is the stationary distribution of the Markov chain.
% \end{theorem}
% \small
% \pause
% \begin{proof}
% For a Metropolis-Hastings chain with $\theta_a$ and $\theta_b$ drawn from $p(\theta|y)$ \pause such that 
% \[p(\theta_b|y)g(\theta_a|\theta_b)\ge p(\theta_a|y)g(\theta_b|\theta_a), \]
% \pause then 
% \[ \begin{array}{rl}
% p(\theta^{t}=\theta_a, \theta^{(t+1)}=\theta_b) &= p(\theta_a|y)g(\theta_b|\theta_a) \pause \\
% p(\theta^{t}=\theta_b, \theta^{(t+1)}=\theta_a) &= p(\theta_b|y)g(\theta_a|\theta_b) \frac{p(\theta_a|y) g(\theta_b|\theta_a)}{p(\theta_b|y) g(\theta_a|\theta_b)} \pause \\
% &= p(\theta_a|y) g(\theta_b|\theta_a)
% \end{array} \]
% \pause Since the joint distribution is symmetric, the marginal distributions for $\theta^{t}$ and $\theta^{(t+1)}$ are the same and equal to $p(\theta|y)$. 
% \end{proof}
% 
% \end{frame}


\section{Independence Metropolis-Hastings}
\begin{frame}
\frametitle{Independence Metropolis-Hastings}

Let 
\begin{itemize}
\item $p(\theta|y)\propto q(\theta|y)$ be the target distribution, 
\item $\theta^{(t)}$ be the current draw from $p(\theta|y)$, \pause and
\item $g(\theta|\theta^{(t)}) = g(\theta)$, i.e. the proposal is \alert{independent} of the current value.
\end{itemize}

\vspace{0.2in} \pause

The \alert{independence Metropolis-Hastings algorithm} performs the following \pause
\begin{enumerate}
\item propose $\theta^*\sim g(\theta)$ \pause
\item accept $\theta^{(t+1)}=\theta^*$ with probability $\min\{1,r\}$ \pause where 
\[ r = 
\frac{q(\theta^*|y)/g(\theta^*)}{q(\theta^{(t)}|y)/g(\theta^{(t)})} = \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)}\frac{g(\theta^{(t)})}{g(\theta^*)} \]
\pause otherwise, set $\theta^{(t+1)}=\theta^{(t)}$. 
\end{enumerate}
\end{frame}


\begin{frame}[fragile]
\frametitle{Intuition through examples}

<<independent_metropolis_hastings_intuition>>=
curves = data.frame(theta=seq(-2,3, length=101)) %>%
  mutate(target   = dnorm(theta),
         proposal = dnorm(theta,1)) %>%
  gather(distribution,density,-theta)

d = expand.grid(current=-1:1, proposed=-1:1) %>%
  mutate(cv = paste("current= ", current),
         pv = paste("proposed=", proposed),
    acceptance_probability = 
           dnorm(proposed)/dnorm(current)*dnorm(current,1)/dnorm(proposed,1),
         accept = acceptance_probability>.5) %>%
  dplyr::select(-acceptance_probability) %>%
  gather(value, theta, -cv, -pv, -accept) 

ggplot(d, aes(x=theta, y=0)) + 
  geom_point(aes(color=accept, shape=value), size=2) + 
  facet_grid(cv~pv, switch='y') +
  geom_line(data=curves, aes(theta,density,linetype=distribution)) +
  theme_bw()
@

\end{frame}



\begin{frame}
\frametitle{Example: Normal-Cauchy model}
  Let $Y\sim N(\theta,1)$ with $\theta\sim Ca(0,1)$ \pause such that the posterior is
  \[ p(\theta|y) \propto p(y|\theta)p(\theta) \propto \frac{\exp(-(y-\theta)^2/2)}{1+\theta^2} \]
\pause Use $N(y,1)$ as the proposal, \pause then the Metropolis-Hastings acceptance probability is the $\min\{1,r\}$ \pause with 
\[ 
\begin{array}{rl}
r &= \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)}\frac{g(\theta^{(t)})}{g(\theta^*)} \pause \\
&= \frac{\exp(-(y-\theta^*)^2/2)/1+(\theta^*)^2}{\exp(-(y-\theta^{(t)})^2/2)/1+(\theta^{(t)})^2} \frac{\exp(-(\theta^{(t)}-y)^2/2)}{\exp(-(\theta^*-y)^2/2)} \\
&= \frac{1+(\theta^{(t)})^2}{1+(\theta^*)^2} 
\end{array} 
\] 
\end{frame}


\begin{frame}[fragile]
\frametitle{Example: Normal-Cauchy model}
<<sampling, fig.width=8>>=
q = function(theta,y,log=FALSE) {
  out = -(y-theta)^2/2-log(1+theta^2)
  if (log) return(out)
  return(exp(out))
}
g = function(theta,y,log=FALSE) dnorm(theta,y,log=log)

d = mutate(data.frame(x = seq(-3,3,by=0.1)),
           target = q(x,1)/integrate(function(x) q(x,1), -Inf, Inf)$value,
           proposal = g(x,1))

ggplot(melt(d, id.vars='x', variable.name='density'), 
       aes(x=x, y=value, color=density)) + geom_line()+
  theme_bw()
@
\end{frame}




\begin{frame}[fragile]
\frametitle{Example: Normal-Cauchy model}
<<normal_cauchy_independence, fig.width=15>>=
set.seed(1)
y = 0 
n = 100
theta = rep(NA, n)
theta[1] = rnorm(1,y)
for (i in 2:n) {
  proposed = rnorm(1,y)
  logr = log(1+theta[i-1]^2)-log(1+proposed^2)
  theta[i] = ifelse(log(runif(1))<logr, proposed, theta[i-1])
}
qplot(1:n, theta) +
  labs(x="Iteration", y=expression(theta), title="Independence Metropolis-Hastings") +
  theme_bw()

theta[1] = 10 # This line changed
for (i in 2:n) {
  proposed = rnorm(1,y)
  logr = log(1+theta[i-1]^2)-log(1+proposed^2)
  theta[i] = ifelse(log(runif(1))<logr, proposed, theta[i-1])
}
qplot(1:n, theta) +
  labs(x="t", y=expression(theta), title="Independence Metropolis-Hastings (poor starting value)") +
  theme_bw()
@
\end{frame}



\begin{frame}
\frametitle{Need heavy tails}

Recall that 
\begin{itemize}[<+->]
\item rejection sampling requires the proposal to have heavy tails and
\item importance sampling is efficient only when the proposal has heavy tails.
\end{itemize}

\vspace{0.2in} \pause

Independence Metropolis-Hastings also requires heavy tailed proposals for
efficiency
\pause 
since if $\theta^{(t)}$ is 
\begin{itemize}
\item in a region where $p(\theta^{(t)}|y)>>g(\theta^{(t)})$ \pause then
\item any proposal $\theta^*$ such that $p(\theta^*|y)\approx g(\theta^*)$ \pause 
\end{itemize}
will result in 
\[ r = \frac{g(\theta^{(t)})}{p(\theta^{(t)}|y)}\frac{p(\theta^*|y)}{g(\theta^*)} \pause \approx 0 \]
\pause and few samples will be accepted.
\end{frame}


\begin{frame}[fragile]
\frametitle{Need heavy tails - example}

Suppose $\theta|y \sim Ca(0,1)$ and we use a standard normal as a proposal. \pause Then 

<<fig.width=15>>=
d = mutate(data.frame(x = seq(-4,4,by=.1)), 
           target = dcauchy(x),
           proposal = dnorm(x),
           log_ratio = log(target)-log(proposal))
ggplot(melt(d[,c('x','target','proposal')], 
            id.var='x', variable.name='density'),
       aes(x=x, y=value, color=density)) +
  geom_line() + 
  theme(legend.position="none")+
  theme_bw()
ggplot(d, aes(x=x,y=log_ratio)) + geom_line()+
  theme_bw()
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Need heavy tails}
<<heavy_tails, fig.width=10, cache=TRUE>>=
n=1000
theta = rep(0,n)
for (i in 2:n) {
  proposed = rnorm(1)
  logr = dcauchy(proposed,log=TRUE)-dcauchy(theta[i-1],log=TRUE)+
         dnorm(theta[i-1],log=TRUE)-dnorm(proposed,log=TRUE)
  theta[i] = ifelse(log(runif(1))<logr, proposed, theta[i-1])
}
qplot(1:n, theta)+labs(x="t",y=expression(theta)) +theme_bw()
@

\end{frame}



\section{Random-walk Metropolis}
\begin{frame}
\frametitle{Random-walk Metropolis}

Let 
\begin{itemize}
\item $p(\theta|y)\propto q(\theta|y)$ be the target distribution, 
\item $\theta^{(t)}$ be the current draw from $p(\theta|y)$, \pause and
\item $g(\theta^*|\theta^{(t)}) = g(\theta^{(t)}|\theta^*)$, i.e. the proposal is \alert{symmetric}.
\end{itemize}

\vspace{0.2in} \pause

The \alert{Metropolis algorithm} performs the following \pause
\begin{enumerate}
\item propose $\theta^*\sim g(\theta|\theta^{(t)})$ \pause 
\item accept $\theta^{(t+1)}=\theta^*$ with probability $\min\{1,r\}$ \pause where 
\[ r = \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)}\frac{g(\theta^{(t)}|\theta^*)}{g(\theta^*|\theta^{(t)})} \pause =
\frac{q(\theta^*|y)}{q(\theta^{(t)}|y)} 
\]
\pause otherwise, set $\theta^{(t+1)}=\theta^{(t)}$. 
\end{enumerate}
\pause This is also referred to as \alert{random-walk Metropolis}.
\end{frame}


\begin{frame}[fragile]
\frametitle{Stochastic hill climbing}

Notice that $r = q(\theta^*|y)/q(\theta^{(t)}|y)$ and thus will accept whenever the target density is larger when evaluated at the proposed value than it is when evaluated at the current value. 

\vspace{.2in} \pause

Suppose $\theta|y \sim N(0,1)$, $\theta^{(t)} = 1$, and $\theta^* \sim N(\theta^{(t)},1)$. \pause 

<<hill_climbing, fig.width=10>>=
curve(dnorm, -3, 3, lwd=2)
curve(dnorm, -1, 1, lwd=5, add=TRUE)
segments(1,0,1,dnorm(1))
curve(dnorm(x,1)/dnorm(0)*dnorm(1), add=TRUE, col="red", lwd=2)
curve(dnorm(x,1)/dnorm(0)*dnorm(1), -1, 1, add=TRUE, col="red", lwd=5)
segments(1,0,1,dnorm(1))
legend("topright",c("Target","Proposal"), col=1:2, lwd=2)
@
\end{frame}



\begin{frame}
\frametitle{Example: Normal-Cauchy model}
  Let $Y\sim N(\theta,1)$ with $\theta\sim Ca(0,1)$ \pause such that the posterior is
  \[ p(\theta|y) \propto p(y|\theta)p(\theta) \propto \frac{\exp(-(y-\theta)^2/2)}{1+\theta^2} \]

\vspace{0.2in} \pause 

Use $N(\theta^{(t)},\tau^2)$ as the proposal\pause, then the acceptance probability is the $\min\{1,r\}$ \pause with 
\[ r = \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)} \pause = \frac{p(y|\theta^*)p(\theta^*)}{p(y|\theta^{(t)})p(\theta^{(t)})}. \] 
For this example, let $\tau^2 = 1$. 
\end{frame}



\begin{frame}[fragile]
\frametitle{Example: Normal-Cauchy model }
<<normal_cauchy_random_walk, fig.width=15>>=
n = 100
log_q = function(theta,y) dnorm(y,theta,log=TRUE)+dcauchy(theta,log=TRUE)
current = rnorm(1,y)
samps = rep(NA, n)
for (i in 1:n) {
  proposed = rnorm(1,current)
  logr = log_q(proposed,y)-log_q(current,y)
  current = ifelse(log(runif(1))<logr, proposed, current)
  samps[i] = current
}
qplot(1:n, samps) + 
  labs(x="t", y=expression(theta), title="Random-walk Metropolis") +
  theme_bw()

current = 10 # This line changed
for (i in 1:n) {
  proposed = rnorm(1,current)
  logr = log_q(proposed,y)-log_q(current,y)
  current = ifelse(log(runif(1))<logr, proposed, current)
  samps[i] = current
}
qplot(1:n, samps) + 
  labs(x = "t", 
       y = expression(theta), 
       title = "Random-walk Metropolis (poor starting value)") +
  theme_bw()
@
\end{frame}


\subsection{Optimal tuning parameter}
\begin{frame}
\frametitle{Random-walk tuning parameter}
\small 
  Let $p(\theta|y)$ be the target distribution\pause, the proposal is symmetric with scale $\tau^2$, \pause and $\theta^{(t)}$ is (approximately) distributed according to $p(\theta|y)$. \pause 

\begin{itemize}
\item If $\tau^2\approx 0$\pause, then $\theta^*\approx \theta^{(t)}$ \pause and
\[ r = \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)} \approx 1 \]
\pause and all proposals are accepted. \pause 
\item As $\tau^2\to\infty$\pause, then $q(\theta^*|y)\approx 0$ \pause since $\theta^*$ will be far from the mass of the target distribution \pause and 
\[ r = \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)} \pause \approx 0 \]
\pause so all proposed values are rejected.
\end{itemize}

So there is an optimal $\tau^2$ somewhere. \pause For normal targets, the optimal random-walk proposal variance is $2.4^2Var(\theta|y)/d$ where $d$ is the dimension of $\theta$ which results in an acceptance rate of 40\% for $d=1$ down to 20\% as $d\to\infty$. 
\end{frame}


\begin{frame}[fragile]
\frametitle{Random-walk with tuning parameter that is too big and too small}

Let $y|\theta \sim N(\theta,1)$, $\theta\sim Ca(0,1)$, and $y=1$. 

<<tuning_parameter, fig.width=10>>=
random_walk = function(n, tau, theta0, log_target) {
  theta = rep(theta0,n)
  for (i in 2:n) {
    proposed = rnorm(1, theta[i-1], tau)
    logr = log_target(proposed,y)-log_target(theta[i-1],y)
    theta[i] = ifelse(log(runif(1))<logr, proposed, theta[i-1])
  }
  return(data.frame(iteration=1:n, theta=theta))
}

d = expand.grid(tau=c(1/10,10))
r = ddply(d, .(tau), function(x) {
  random_walk(100, x$tau, 0, log_q)
})
ggplot(r, aes(x=iteration, y=theta, color=as.factor(tau)))+geom_point()+
  theme_bw()
@

\end{frame}


\subsection{Binomial model}
\begin{frame}
\frametitle{Binomial model}

Let $Y \sim Bin(n,\theta)$ and $\theta \sim Be(1/2,1/2)$\pause, thus the posterior is \pause 
\[ p(\theta|y)\propto \theta^{y-0.5}(1-\theta)^{n-y-0.5}\mathrm{I}(0<\theta<1). \] 

\vspace{0.2in} \pause

To construct a random-walk Metropolis algorithm, we choose the proposal 
\[ \theta^* \sim N(\theta^{(t)},0.4^2) \]
\pause and accept with probability $\min\{1,r\}$ \pause where 
\[ r = \frac{p(\theta^*|y)}{p(\theta^{(t)}|y)} \pause = \frac{(\theta^*)^{y-0.5}(1-\theta^*)^{n-y-0.5}\mathrm{I}(0<\theta^*<1)}{(\theta^{(t)})^{y-0.5}(1-\theta^{(t)})^{n-y-0.5}\mathrm{I}(0<\theta^{(t)}<1)} \]
\end{frame}


\begin{frame}[fragile]
\frametitle{Binomial model}

<<binomial, echo=TRUE>>=
n = 10000
log_q = function(theta, y=3, n=10) {
  if (theta<0 | theta>1) return(-Inf)
  (y-0.5)*log(theta)+(n-y-0.5)*log(1-theta)
}
current = 0.5     # Initial value
samps = rep(NA,n) 
for (i in 1:n) {
  proposed = rnorm(1, current, 0.4) # tuning parameter is 0.4
  
  logr = log_q(proposed)-log_q(current)
  if (log(runif(1)) < logr) current = proposed
  
  samps[i] = current
}
length(unique(samps))/n # acceptance rate
@

\end{frame}




\begin{frame}[fragile]
\frametitle{Binomial}
<<binomial_traceplot, fig.width=10>>=
par(mfrow=c(1,2))
plot(samps, type="l")
hist(samps, prob=TRUE)
curve(dbeta(x, 3.5,7.5), add=TRUE, col="red", lwd=2)
@
\end{frame}





\subsection{Normal model}
\begin{frame}
\frametitle{Normal model}

Assume
\[ Y_i \stackrel{ind}{\sim} N(\mu,\sigma^2) \qquad \mbox{and} \qquad p(\mu,\sigma) \propto Ca^+(\sigma; 0, 1) \]
\pause and thus 
\[ \begin{array}{rl}
p(\mu,\sigma|y) &\propto 
\left[ \prod_{i=1}^n \sigma^{-1} \exp\left( -\frac{1}{2\sigma^2} (y_i-\mu)^2 \right)\right] \frac{1}{1+\sigma^2} \mathrm{I}(\sigma>0)
\pause \\
&= \sigma^{-n} \exp\left( -\frac{1}{2\sigma^2} \left[ \sum_{i=1}^n y_i^2 -2\mu n\overline{y} + \mu^2 \right] \right) \frac{1}{1+\sigma^2} \mathrm{I}(\sigma>0)
\end{array} \]

\vspace{0.2in} \pause

Perform a random-walk Metropolis using a normal proposal, i.e. if $\mu^{(t)}$ and $\sigma^{(t)}$ are the current values for $\mu$ and $\sigma$, then 
\[ \left( \begin{array}{c} \mu^* \\ \sigma^*
\end{array} \right) \sim N\left( \left[ \begin{array}{c} \mu^{(t)} \\ \sigma^{(t)} \end{array} \right], \mathrm\Sigma \right)
\]
where $\mathrm\Sigma$ is the tuning parameter.
\end{frame}



\begin{frame}
\frametitle{Adapting the tuning parameter}

Recall that the optimal random-walk tuning parameter (if the target is normal) is $2.4^2 Var(\theta|y)/d$ \pause where $Var(\theta|y)$ is the unknown posterior covariance matrix. \pause We can estimate $Var(\theta|y)$ using the sample covariance matrix of draws from the posterior. 

\vspace{0.2in} \pause 

Proposed automatic adapting of the Metropolis-Hastings tuning parameter: \pause
\begin{enumerate}[<+->]
\item Start with $\mathrm\Sigma_0$. Set $b=0$.
\item Run $M$ iterations of the MCMC using $2.4^2\mathrm\Sigma_b/d$. 
\item Set $\mathrm\Sigma_{b+1}$ to the sample covariance matrix of all previous draws. 
\item If $b<B$, set $b=b+1$ and return to step 2. Otherwise, throw away all previous draws and go to step 5.
\item Run $K$ iterations of the MCMC using $2.4^2\mathrm\Sigma_B/d$.
\end{enumerate}

\end{frame}



\begin{frame}[fragile]
\frametitle{R code for Metropolis-Hastings}

<<echo=TRUE>>=
n = 20
y = rnorm(n)
sum_y2 = sum(y^2)
nybar  = mean(y)
log_q = function(x) {
  if (x[2]<0) return(-Inf)
  -n*log(x[2])-(sum_y2-2*nybar*x[1]+n*x[1]^2)/(2*x[2]^2)-log(1+x[2]^2)
}

B = 10
M = 100

samps = matrix(NA, nrow=B*M, ncol=2)
a_rate = rep(NA, B)

# Initialize
Sigma = diag(2)  # Sigma_0
current = c(0,1)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{R code for Metropolis-Hastings - Adapting}

<<echo=TRUE>>=
# Adapt
for (b in 1:B) {
  for (m in 1:M) {
    i = (b-1)*M+m
    
    proposed = mvrnorm(1, current, 2.4^2*Sigma/2)
    
    logr = log_q(proposed) - log_q(current)
    if (log(runif(1)) < logr) current = proposed
    samps[i,] = current    
  }
  a_rate[b] = length(unique(samps[1:i,1]))/length(samps[1:i,1])
  Sigma = var(samps[1:i,])
}
a_rate
var(samps) # Sigma_B
@

\end{frame}



\begin{frame}[fragile]
\frametitle{R code for Metropolis-Hastings - Adapting}

<<echo=TRUE, fig.width=10>>=
samps = as.data.frame(samps); names(samps) = c("mu","sigma"); samps$iteration = 1:nrow(samps)
ggplot(melt(samps, id.var='iteration', variable.name='parameter'), aes(x=iteration, y=value)) + 
  geom_line() +
  facet_wrap(~parameter, scales='free')+
  theme_bw()
@

\end{frame}




\begin{frame}[fragile]
\frametitle{R code for Metropolis-Hastings - Inference}

<<echo=TRUE>>=
# Final run
K = 10000
samps = matrix(NA, nrow=K, ncol=2)
for (k in 1:K) {
  proposed = mvrnorm(1, current, 2.4^2*Sigma/2)
    
  logr = log_q(proposed) - log_q(current)
  if (log(runif(1)) < logr) current = proposed
  samps[k,] = current   
}
length(unique(na.omit(samps[,1])))/length(na.omit(samps[,1])) # acceptance rate
@

\end{frame}



\begin{frame}[fragile]
\frametitle{R code for Metropolis-Hastings - Inference}

<<fig.width=13>>=
samps = as.data.frame(samps); names(samps) = c("mu","sigma"); samps$iteration = 1:nrow(samps)
m = melt(samps, id.var='iteration', variable.name='parameter')
ggplot(m, aes(x=iteration, y=value)) + 
  geom_line() +
  facet_wrap(~parameter, scales='free')+
  theme_bw()

ggplot(m, aes(x=value, y=..density..)) + 
  geom_histogram(binwidth=0.1) +
  facet_wrap(~parameter, scales='free')+
  theme_bw()
@

\end{frame}




\subsection{Hierarchical binomial model}
\begin{frame}
\frametitle{Hierarchical binomial model}

Recall the hierarchical binomial model
\[ Y_i \stackrel{ind}{\sim} Bin(n_i,\theta_i), \quad 
\theta_i \stackrel{ind}{\sim} Be(\alpha,\beta), \quad 
p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2} \]
\pause 
and after marginalizing out the $\theta_i$
\[ Y_i \stackrel{ind}{\sim} \mbox{Beta-binomial}(n_i,\alpha,\beta), \quad
p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2}\mathrm{I}(a>0)\mathrm{I}(b>0) \]
\pause
Thus the posterior is 
\[ p(\alpha,\beta|y) \propto \left[ \prod_{i=1}^n \frac{B(\alpha+y_i, \beta+n_i-y_i)}{B(\alpha,\beta)} \right] (\alpha+\beta)^{-5/2}\mathrm{I}(a>0)\mathrm{I}(b>0) \]
where $B(\cdot)$ is the beta function.

\vspace{0.2in} \pause

We can perform exactly the same adapting procedure, but now using this posterior as the target distribution. 

\end{frame}



\begin{frame}[fragile]
\frametitle{Beta-binomial hyperparameter posterior}

<<beta_binomial, cache=TRUE>>=
d = read.csv('../Ch05/Ch05a-dawkins.csv')
y = d$made
n = d$attempt
log_q = function(x) {
  if (any(x<0)) return(-Inf)
  sum(lbeta(x[1]+y, x[2]+n-y)-lbeta(x[1],x[2]))-5/2*log(x[1]+x[2])
}

B = 10
M = 1000

samps = matrix(NA, nrow=B*M, ncol=2)
a_rate = rep(NA, B)

# Initialize
Sigma = diag(2)
current = c(1,1)

# Adapt
for (b in 1:B) {
  for (m in 1:M) {
    i = (b-1)*M+m
    
    proposed = mvrnorm(1, current, 2.4^2*Sigma/2)
    
    logr = log_q(proposed) - log_q(current)
    if (log(runif(1)) < logr) current = proposed
    samps[i,] = current    
  }
  a_rate[b] = length(unique(samps[1:i,1]))/length(samps[1:i,1])
  Sigma = var(samps[1:i,])
}

# Final run
K = 10000
samps = matrix(NA, nrow=K, ncol=2)
for (k in 1:K) {
  proposed = mvrnorm(1, current, 2.4^2*Sigma/2)
    
  logr = log_q(proposed) - log_q(current)
  if (log(runif(1)) < logr) current = proposed
  samps[k,] = current   
}

samps = as.data.frame(samps); names(samps) = c("alpha","beta")
@

<<dependson='beta_binomial',message=FALSE, warning=FALSE, fig.width=10>>=
ggpairs(samps, 
        lower = list(continuous='points'), 
        diag  = list(continuous='bar'))
@

\end{frame}


\section{Summary}
\begin{frame}
\frametitle{Metropolis-Hastings summary}

\begin{itemize}[<+->]
\item The Metropolis-Hastings algorithm, samples $\theta^*\sim g(\cdot|\theta^{(t)})$ and sets $\theta^{(t+1)}=\theta^*$ with probability equal to $\min\{1,r\}$ where 
\[ r = \frac{q(\theta^*|y)}{q(\theta^{(t)}|y)} \frac{g(\theta^{(t)}|\theta^*)}{g(\theta^*|\theta^{(t)})} \]
and otherwise sets $\theta^{(t+1)}=\theta^{(t)}$.
\item There are two common Metropolis-Hastings proposals
\begin{itemize}
\item independent proposal: $g(\theta^*|\theta^{(t)}) = g(\theta^*)$
\item random-walk proposal: $g(\theta^*|\theta^{(t)}) = g(\theta^{(t)}|\theta^*)$
\end{itemize}
\item Independent proposals suffer from the same heavy-tail problems as rejection sampling proposals.
\item Random-walk proposals require tuning of the random walk parameter. 
\end{itemize}

\end{frame}



\end{document}
