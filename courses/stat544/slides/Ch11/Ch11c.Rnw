\documentclass[handout]{beamer}

\input{../frontmatter}
\input{../commands}

\title{Markov chains}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=6, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE>>=
library(reshape2)
library(plyr)
library(ggplot2)
library(xtable)
@

<<set_seed>>=
set.seed(1)
@

\frame{\maketitle}

\begin{frame}
\frametitle{Discrete-time, discrete-space Markov chain theory}

\begin{itemize}
\item Markov chains
  \begin{itemize}
  \item Discrete-time
  \item Discrete-space
  \item Time-homogeneous
  \item Examples
  \end{itemize}
\item Convergence to a stationary distribution
  \begin{itemize}
  \item Aperiodic
  \item Irreducible
  \item (Positive) Recurrent
  \end{itemize}
\end{itemize}

\end{frame}


\section{Markov chains}
\begin{frame}
\frametitle{Markov chains}

\begin{definition}
A \alert{discrete-time, time-homogeneous Markov chain} is a sequence of random variables $\theta^{(t)}$ \pause such that
\[ p\left(\theta^{(t)}\left|\theta^{(t-1)},\ldots,\theta^{(0)}\right.\right) = 
p\left(\theta^{(t)}\left|\theta^{(t-1)}\right.\right) \]
\pause which is known as the \alert{transition distribution}.
\end{definition}

\pause 

\begin{definition}
The \alert{state space} is the support of the Markov chain.
\end{definition}

\pause 

\begin{definition}
The transition distribution of a Markov chain whose state space is finite \pause can be represented with a  \alert{transition matrix} $P$ \pause with elements $P_{ij}$ representing the probability of moving from state $i$ to state $j$ in one time-step. 
\end{definition}
\end{frame}





\subsection{Correlated coin flip}
\begin{frame}
\frametitle{Correlated coin flip}
Let 
\[ P = \bordermatrix{ & 0 & 1 \cr 0 & 1-p & p \cr 1 & q & 1-q } \]
\pause where 
\begin{itemize}[<+->]
\item the state space is $\{0,1\}$, 
\item $p$ is the probability of switching from 0 to 1, and 
\item $q$ is the probability of switching from 1 to 0.
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Correlated coin flip}
p=0.2, q=0.4

<<correlted_coin_flip, fig.width=10>>=
rflip = function(n, p, q, theta0) {
  theta = rep(theta0, n+1)
  for (i in 1:n) theta[i+1] = ifelse(theta[i], rbinom(1,1,1-q), rbinom(1,1,p))
  theta
}
set.seed(1)
n = 100
qplot(0:n, rflip(n,p <- .2,q <- .4, rbinom(1,1,.5))) +
  labs(x="Time",y="State",title="Correlated coin flip") +
  theme_bw()
@
\end{frame}


\subsection{DNA sequence}
\begin{frame}
\frametitle{DNA sequence}
\[ P = \bordermatrix{ & A & C & G & T \cr 
A & 0.60 & 0.10 & 0.10 & 0.20 \cr 
C & 0.10 & 0.50 & 0.30 & 0.10 \cr
G & 0.05 & 0.20 & 0.70 & 0.05 \cr
T & 0.40 & 0.05 & 0.05 & 0.50 } \]
\pause with 
\begin{itemize}
\item state space \{A,C,G,T\} \pause and
\item each cell provides the probability of moving from the row nucleotide to the column nucleotide. \pause
\end{itemize}

{\tiny \url{http://tata-box-blog.blogspot.com/2012/04/introduction-to-markov-chains-and.html}}
\end{frame}


\begin{frame}[fragile]
\frametitle{DNA sequence}
<<dna_seq, fig.width=12>>=
nucleotides = factor(c("A","C","G","T"))
dna_seq_m = matrix(rbind(c(.6,.1,.1,.2), c(.1,.5,.3,.1), c(.05,.2,.7,.05), c(.4,.05,.05,.5)),4)
r_dna_seq = function(n,theta0) {
  theta = rep(theta0,n+1)
  for (i in 1:n) theta[i+1] = sample(nucleotides,1,prob=dna_seq_m[as.numeric(theta[i]),])
  theta
}
(my_seq = r_dna_seq(n, sample(nucleotides,1)))
qplot(0:n,my_seq) + 
  labs(x="Time",y="Nucleotide",main="Markov chain for a DNA sequence") +
  theme_bw()
@
\end{frame}



\subsection{Random walk on the integers}
\begin{frame}
\frametitle{Random walk on the integers}
Let 
\[ 
P_{ij} = \left\{ \begin{array}{ll}
1/3 &\mbox j\in \{i-1,i,i+1\} \\
0 & \mbox{otherwise}
\end{array} \right.
\]
where 
\begin{itemize}
\item the state space is the integers, i.e. $\{\ldots,-1,0,1,\ldots\}$ \pause and 
\item the transition matrix $P$ is infinite-dimensional.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Random walk on the integers}
<<random_walk_on_the_integers, fig.width=12>>=
rwalk = function(n, theta0) {
  theta = rep(theta0, n+1)
  for (i in 1:n) theta[i+1] = sample(c(-1,0,1), 1)+theta[i]
  theta
}
qplot(0:n, rwalk(n, theta0=0)) + 
  labs(x="Time",y="State",title="Random walk on the integers") +
  theme_bw()
@
\end{frame}







\section{Markov chain theory}
\begin{frame}
\frametitle{Stationary distribution}
\footnotesize
  Let $\pi^{(t)}$ denote a row vector with 
  \[ \pi_i^{(t)} = Pr\left(\theta^{(t)}=i\right). \]
  \pause Then 
  \[ \pi^{(t)} = \pi^{(t-1)} P. \]
  \pause Thus, $\pi^{(0)}$ and $P$ completely characterize $\pi^{(t)} \pause =\pi^{(0)} P^t$ where $P^t = P^{t-1}P$ for $t>1$ and $P^1=P$. \pause 
  \begin{definition}
  A \alert{stationary distribution} is a distribution $\pi$ such that
  \[ \pi = \pi P. \]
  
  \pause 
  
  This is also called the \alert{invariant} or \alert{equilibrium distribution}.
  \end{definition}

  \pause 
  
  Given a transition matrix $P$, \pause 
  \begin{itemize}
  \item Does a $\pi$ exist? \pause  Is $\pi$ unique? \pause
  \item If $\pi$ is unique, does $\lim_{t\to\infty} \pi^{(t)} = \pi$ for all $\pi^{(0)}$? \pause In this case, $\pi$ is often called the \alert{limiting distribution}.
  \end{itemize}

\end{frame}


\begin{frame}
\frametitle{Stationary distribution exists, but is not unique}

Let 
\[ P = \bordermatrix{ & 0 & 1 \cr 0 & 1 & 0 \cr 1 & 0 & 1 } \]
\pause then 
\[ \pi = \pi P \]
for any $\pi$. 

\vspace{0.2in} \pause 

This Markov chain stays where it is.

\end{frame}








\subsection{Irreducibility}
\begin{frame}
\frametitle{Irreducibility}

\begin{definition}
A Markov chain is \alert{irreducible} if for all $i$ and $j$
\[ Pr\left(\theta^{t_{ij}}=j\left|\theta^{(0)}=i\right.\right) > 0 \]
for some $t_{ij}\ge 0$. \pause Otherwise the chain is \alert{reducible}.
\end{definition} \pause
\begin{theorem}
A \alert{finite} state space, \alert{irreducible} Markov chain has a unique stationary distribution $\pi$.
\end{theorem}
\pause 
Reducible example:
{\scriptsize
\[ P = \bordermatrix{ & 0 & 1 & 2 & 3  \cr 
0 & 0.5 & 0.5 & 0 & 0 \cr
1 & 0.5 & 0.5 & 0 & 0 \cr
2 & 0 & 0 & 0.5 & 0.5 \cr
3 & 0 & 0 & 0.5 & 0.5 } \]
}
\end{frame}




\begin{frame}
\frametitle{Stationary distribution is unique, but is not the limiting distribution.}

Let 
\[ P = \bordermatrix{ & 0 & 1 \cr 0 & 0 & 1  \cr 1 & 1 & 0 } \]
\pause then $\pi=\left(\frac{1}{2}\,\,\frac{1}{2}\right)$ since $\pi=\pi P$\pause, but
\[ \lim_{t\to\infty} \pi^{(t)} \ne \pi \,\forall\, \pi^{(0)}\]
\pause since 

\[ 
\pi^{(t)} = 
\left\{ \begin{array}{rll}
\pi^{(0)} & t\mbox{ even} \pause \\
1-\pi^{(0)} & t\mbox{ odd}
\end{array} \right. \]

\vspace{0.2in} \pause 

This Markov chain jumps back and forth.

\end{frame}





\subsection{Aperiodic}
\begin{frame}\frametitle{Aperiodic}

{\small

\begin{definition}
The \alert{period} $k_i$ of a state $i$ is 
\[ k_i = \mbox{gcd}\{t: Pr\left(\theta^{(t)}=i|\theta^{(0)}=i\right)>0\}\]
where gcd is the greatest common divisor. \pause If $k_i=1$, then state $i$ is said to be \alert{aperiodic}\pause, i.e.
\[ Pr\left(\theta^{(t)}=i|\theta^{(0)}=i\right)>0 \]
for $t>t_0$ for some $t_0$. \pause A Markov chain is \alert{aperiodic} if every state is aperiodic.
\end{definition}


\pause 
Periodic example:

\[ P = \bordermatrix{ & 0 & 1 & 2 & 3  \cr 
0 & 0 & 1 & 0 & 0 \cr
1 & 0 & 0 & 1 & 0 \cr
2 & 0 & 0 & 0 & 1 \cr
3 & 1 & 0 & 0 & 0 } \]
}

\end{frame}


\begin{frame}\frametitle{Example}
Let
\[ P = 
\bordermatrix{ & 0 & 1 \cr 0 & 0 & 1 \cr 1 & \frac{1}{2} & \frac{1}{2} }
 \]
\pause Note that 
\[ \begin{array}{rl}
Pr\left(\theta^{(1)}=0|\theta^{(0)}=0\right) &= \pause0 \pause  \\
Pr\left(\theta^{(2)}=0|\theta^{(0)}=0\right) &= \pause\frac{1}{2} \pause \\
Pr\left(\theta^{(3)}=0|\theta^{(0)}=0\right) &= \pause\frac{1}{2}\frac{1}{2} = \frac{1}{4} \pause \\
Pr\left(\theta^{(4)}=0|\theta^{(0)}=0\right) &= \pause\frac{1}{2}\frac{1}{2}+\frac{1}{2}\frac{1}{2}\frac{1}{2} = \frac{3}{8} \pause \\ 
\vdots
\end{array} \]
generally $Pr\left(\theta^{(t)}=0|\theta^{(0)}=0\right)>0$ for all $t>1$. \pause The \alert{period} $k$ of state 0 is 
\[ \mbox{gcd}\{t: Pr\left(\theta^{(t)}=i|\theta^{(0)}=i\right)>0\}\pause =\mbox{gcd}\{2,3,4,5,\ldots \}\pause =1 \]
\pause Thus state $0$ is aperiodic. \pause State $1$ is trivially aperiodic since $P(\theta^{(1)}=1|\theta^{(0)}=1)=1/2>0$. \pause Thus the Markov chain is aperiodic.

\end{frame}



\subsection{Finite support convergence}
\begin{frame}\frametitle{Finite support convergence}

\begin{lemma}
Every state in an irreducible Markov chain has the same period. Thus, in an irreducible Markov chain, if one state is aperiodic, then the Markov chain is aperiodic. 
\end{lemma}
\pause
\begin{theorem}
A \alert{finite} state space, \alert{irreducible} Markov chain has a unique stationary distribution $\pi$. \pause If the chain is \alert{aperiodic}, \pause then $\lim_{t\to\infty} \pi^{(t)}=\pi$ for all $\pi^{(0)}$.
\end{theorem}
\end{frame}




\begin{frame}
\frametitle{Correlated coin flips}
\small

For \[ P = \bordermatrix{ & 0 & 1 \cr 0 & 1-p & p \cr 1 & q & 1-q } \]
is irreducible and aperiodic if $0<p,q<1$\pause, thus the Markov chain with transition matrix $P$ has a unique stationary distribution and the chain converges to this distribution. 

\pause Since $\pi = \pi P$ and $\pi_0+\pi_1=1$, we have 

\[ \begin{array}{rl}
\pi_0 &= \pi_0(1-p)+\pi_1 q \pause \implies \\
\frac{p}{q} &= \frac{\pi_1}{\pi_0} \pause = \frac{\pi_1}{1-\pi_1} \pause \implies \\
\pi_1 &= \frac{p}{p+q} \pause \implies  \\
\pi_0 &= \frac{q}{p+q}
\end{array} \]
\pause 

So, the stationary distribution for $P$ is $\pi=(q,p)/(p+q)$.
\end{frame}



\begin{frame}[fragile]
\frametitle{Calculate numerically}

For finite state space and $P^t = P^{t-1} P$\pause, we have 
\[ \lim_{t\to\infty} \pi^{(t)} 
= \lim_{t\to\infty} \pi^{(0)} P^t \pause
= \pi^{(0)} \lim_{t\to\infty} P^t \pause 
= \pi^{(0)} \left[ \begin{array}{c} \pi \\ \vdots \\ \pi \end{array} \right] \pause 
= \pi \pause  \]

<<P, echo=TRUE>>=
p = 0.2; q = 0.4
create_P = function(p,q) matrix(c(1-p,p,q,1-q), 2, byrow=TRUE)
P = Pt = create_P(p,q)
for (i in 1:100) Pt = Pt%*%P
Pt
c(q,p)/(p+q)
@

\end{frame}




\begin{frame}
\frametitle{Random walk on the integers}
Let 
\[ P_{ij} = \left\{ \begin{array}{ll} 
1/3 & j\in \{i-1,i,i+1\} \\
0 & \mbox{otherwise} \end{array} \right.. \]
\pause 

Then, this Markov chain is 
\begin{itemize}
\item irreducible 
\[ Pr\left(\theta^{(|j-i|)}=j|\theta^{(0)}=i\right) = 3^{-|j-i|}>0, \]
\pause
\item and aperiodic 
\[ Pr\left(\theta^{(t)}=i|\theta^{(t-1)}=i\right)=1/3>0, \]
\end{itemize}
\pause
but the Markov chain does not have a stationary distribution.

\vspace{0.2in} \pause

The Markov chain can wander off forever.

\end{frame}




\begin{frame}
\frametitle{}
\tiny 

A stationary distribution must satisfy $\pi=\pi P$ with 
\[ P = \left( \begin{array}{ccccccccc} 
&&&& \vdots &&&& \\
& 0 & 1/3 & 1/3 & 1/3 & 0 & 0 & 0 & \\
\cdots & 0 & 0 & 1/3 & 1/3 & 1/3 & 0 & 0 & \cdots \\
& 0 & 0 & 0 & 1/3 & 1/3 & 1/3 & 0 & \\
&&&& \vdots &&&& 
\end{array}  \right) \]
or, more succinctly, 
\[ \pi_i = \frac{1}{3}\pi_{i-1} +\frac{1}{3}\pi_{i} +\frac{1}{3}\pi_{i+1}. \]
Thus we must solve for $\{\pi_i \}$ that satisfy
\[ \begin{array}{rlrl}
2\pi_i &= \pi_{i-1}+\pi_{i+1} \,\forall\, i\\
\sum_{i=-\infty}^\infty \pi_i&=1 \\
\pi_i & \ge 0 \,\forall\, i
\end{array} \]
Note that 
\[ \begin{array}{rlrl}
\pi_2 &= 2\pi_1-\pi_0 \\
\pi_3 &= 2\pi_2-\pi_1 = 3\pi_1-2\pi_0 \\
\vdots \\
\pi_i &= i\pi_1-(i-1)\pi_0
\end{array} \]
Thus
\[ \begin{array}{rlrl}
\mbox{if } \pi_1=\pi_0>0, & \mbox{then }\pi_i=\pi_1, \,\forall\, i\ge 2\mbox{ and } \sum_{i=0}^\infty \pi_i > 1 \\
\mbox{if } \pi_1>\pi_0,   & \mbox{then } \pi_i \to  \infty \\
\mbox{if } \pi_1<\pi_0,   & \mbox{then } \pi_i \to -\infty \\
\mbox{if } \pi_1=\pi_0=0, & \mbox{then } \pi_i=0 \,\forall\, i\ge 0
\end{array} \]
But we also have $\pi_i = 2\pi_{i+1}-\pi_{i+2}$ so that 
\[ \begin{array}{rlrl}
\mbox{if } \pi_1=\pi_0=0, & \mbox{then } \pi_i=0 \,\forall\, i\le 0
\end{array} \]
Thus a stationary distribution does not exist.

\end{frame}




\subsection{Recurrence}
\begin{frame}
\frametitle{Recurrence}

\small

\begin{definition}
Let $T_i$ be the first return time to state $i$\pause, i.e.
\[ T_i =\mbox{inf}\{t\ge 1: \theta^{(t)}=i|\theta^{(0)}=i\} \]
\pause 
A state is \alert{recurrent} if $Pr\left(T_i<\infty\right)=1$ and is \alert{transient} otherwise. \pause A recurrent state is \alert{positive recurrent} if $E[T_i]<\infty$ and is \alert{null recurrent} otherwise. \pause A Markov chain is called \alert{positive recurrent} if all of its states are positive recurrent.
\end{definition}

\pause

\begin{lemma}
If a Markov chain is irreducible and one of its states is positive (null) recurrent, then all of its states are positive (null) recurrent.
\end{lemma}

\pause

\begin{lemma}
If state $i$ of a Markov chain is aperiodic, then $\lim_{t\to\infty} \pi^{(t)}_i = 1/E[T_i]$. 
\end{lemma}

\end{frame}



% \begin{frame}
% \frametitle{Recurrence example}
% 
% Let
% \[ P = \bordermatrix{
%   & A   & C   & G & T \cr 
% A & 1/4 & 1/4 & 1/4 & 1/4 \cr 
% C & 1/4 & 1/4 & 1/4 & 1/4 \cr
% G & 1/4 & 1/4 & 1/4 & 1/4 \cr
% T & 0 & 0 & 0 & 1
% } \]
% 
% \vspace{0.2in} \pause
% 
% \begin{itemize}
% \item States A, C, and G are transient. \pause
% \item State T is (positive) recurrent \pause and absorbing.
% \end{itemize}
% 
% \vspace{0.2in} \pause
% 
% \alert{If state 2 is positive recurrent, why aren't all the states positive recurrent?}
% \end{frame}



\begin{frame}

\frametitle{Ergodic theorem}
\begin{theorem}
For an \alert{irreducible} and \alert{aperiodic} Markov chain, 
\begin{itemize}
\item if the Markov chain is \alert{positive recurrent}, then there exists a unique $\pi$ so that $\pi=\pi P$ and $\lim_{t\to\infty} \pi^{(t)} = \pi$ with $\pi_i=1/E[T_i]$, \pause
\item if there exists a positive vector $\pi$ such that $\pi=\pi P$ and $\sum_i \pi_i=1$, then it must be the stationary distribution and $\lim_{t\to\infty} \pi^{(t)} = \pi$, \pause and
\item if there exists a positive vector $\pi$ such that $\pi=\pi P$ and $\sum_i \pi_i$ is infinite, then a stationary distribution does not exist and $\lim_{t\to\infty} \pi^{(t)}_i=0$ for all $i$. 
\end{itemize}
\pause
If the chain is irreducible, aperiodic, and positive recurrent, we call it \alert{ergodic}.
\end{theorem}

\vspace{0.2in} \pause

When the state-space of the Markov chain has continuous support, then we talk about probabilities of being in sets, e.g. $\pi_i = P(\theta\in A_i).$ 

\end{frame}




\section{AR1 example}
\begin{frame}
\frametitle{Autoregressive process of order 1}
Let the transition distribution be
\[ \theta^{(t)}|\theta^{(t-1)} \sim N(\mu+ \rho [\theta^{(t-1)}-\mu], \sigma^2). \]
with $|\rho|<1$. \pause This defines an autoregressive process of order 1.

\vspace{0.2in} \pause

It is 
\begin{itemize}[<+->]
\item irreducible
\item aperiodic, and
\item positive recurrent.
\end{itemize}

\vspace{0.2in} \pause 

Thus this Markov chain has a stationary distribution and converges to that stationary distribution.
\end{frame}

\begin{frame}[fragile]
\frametitle{Autoregressive process of order 1}
<<autoregressive_process_1, fig.width=10>>=
rar1 = function(n, mu, rho, sigma, theta0) {
  theta = rep(theta0, n+1)
  for (i in 1:n) theta[i+1] = rnorm(1, mu+rho*(theta[i]-mu), sigma)
  theta
}
qplot(0:n, rar1(n, 0,.95,1,0), geom="point") +
  theme_bw()
@
\end{frame}



\begin{frame}
\frametitle{Stationary distribution for AR1 process}

Let $\theta^{(t)}|\theta^{(t-1)} \sim N(\mu+\rho[\theta^{(t-1)}-\mu],\sigma^2)$, or, equivalently
\[ \theta^{(t)} = \mu+\rho[\theta^{(t-1)}-\mu] + \epsilon_t \]
where $\epsilon_t \sim N(0,\sigma^2)$. \pause If $\theta^{(t-1)} \sim N(\mu,\sigma^2/[1-\rho^2])$, then 
\[ \begin{array}{rl}
E[\theta^{(t)}] &= \mu \\
V[\theta^{(t)}] &= \rho^2 \frac{\sigma^2}{1-\rho^2} + \sigma^2 = \frac{\sigma^2}{1-\rho^2}
\end{array} \]
Thus $\theta^{(t)} \sim N(\mu,\sigma^2/[1-\rho^2])$ is the stationary distribution for an AR1 process.
\end{frame}

\begin{frame}[fragile]
\frametitle{Approximate via simulation}
<<echo=TRUE>>=
mu = 10; sigma = 4; rho = 0.9
@

<<ar1_stationary_distribution, fig.width=10>>=
d = rdply(1000, function(x) {
  x = rcauchy(1) # initial draw (clearly not from stationary distribution)
  for (i in 1:100) x = rnorm(1,mu+rho*(x-mu),sigma)
  data.frame(x=x)
})

ggplot(d, aes(x=x)) + 
  geom_histogram(aes(y=after_stat(density)), binwidth=1) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu,
                           sd = sigma/sqrt(1-rho^2)),
                col=2, lwd=2) +
  theme_bw()
@
\end{frame}

\begin{frame}
\frametitle{Summary}

Markov chains converge to their stationary distribution if the chain is ergodic, i.e. it is
\begin{itemize}
\item aperiodic,
\item irreducible, and
\item positive recurrent
\end{itemize}

\vspace{0.2in} \pause

MCMC algorithms, e.g. Gibbs sampling, Metropolis-Hastings, and Metropolis-within-Gibbs, by construction
\begin{itemize}
\item have a unique stationary distribution $p(\theta|y)$ and 
\item converge to that stationary distribution.
\end{itemize}

\end{frame}


\end{document}
